{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "# DIPY API\n",
    "url = \"https://docs.dipy.org/stable/reference/index.html\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=10000, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# # DIPY Tutorials\n",
    "# url = \"https://docs.dipy.org/stable/examples_built/\"\n",
    "# loader = RecursiveUrlLoader(\n",
    "#     url=url, max_depth=10000, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    "# )\n",
    "# docs_tutorials = loader.load()\n",
    "\n",
    "# # DIPY CLI Workflows\n",
    "# url = \"https://docs.dipy.org/stable/interfaces/\"\n",
    "# loader = RecursiveUrlLoader(\n",
    "#     url=url, max_depth=10000, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    "# )\n",
    "# docs_workflows = loader.load()\n",
    "\n",
    "# # DIPY CLI API\n",
    "# url = \"https://docs.dipy.org/stable/reference_cmd/\"\n",
    "# loader = RecursiveUrlLoader(\n",
    "#     url=url, max_depth=10000, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    "# )\n",
    "# docs_cli_api = loader.load()\n",
    "\n",
    "# # DIPY Discussions\n",
    "# url = \"https://github.com/dipy/dipy/discussions\"\n",
    "# loader = RecursiveUrlLoader(\n",
    "#     url=url, max_depth=10000, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    "# )\n",
    "# docs_discuss = loader.load()\n",
    "\n",
    "# # Doc texts\n",
    "# docs.extend([*docs_tutorials, *docs_workflows, *docs_cli_api, *docs_discuss])\n",
    "docs_texts = [d.page_content for d in docs]\n",
    "\n",
    "# Calculate the number of tokens for each document\n",
    "counts = [num_tokens_from_string(d, \"cl100k_base\") for d in docs_texts]\n",
    "\n",
    "# Plotting the histogram of token counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(counts, bins=30, color=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
    "plt.title(\"Histogram of Token Counts\")\n",
    "plt.xlabel(\"Token Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis=\"y\", alpha=0.75)\n",
    "\n",
    "# Display the histogram\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens in all context: 20032\n"
     ]
    }
   ],
   "source": [
    "# Doc texts concat\n",
    "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "d_reversed = list(reversed(d_sorted))\n",
    "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "    [doc.page_content for doc in d_reversed]\n",
    ")\n",
    "print(\n",
    "    \"Num tokens in all context: %s\"\n",
    "    % num_tokens_from_string(concatenated_content, \"cl100k_base\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc texts split\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size_tok = 2000\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=chunk_size_tok, chunk_overlap=0\n",
    ")\n",
    "texts_split = text_splitter.split_text(concatenated_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 112 tensors from /home/aajais/Desktop/DiPyCodeAssistant/llama.cpp/models/nomic-embed-text-v1.5.Q5_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert\n",
      "llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5\n",
      "llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12\n",
      "llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768\n",
      "llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072\n",
      "llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12\n",
      "llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000\n",
      "llama_model_loader: - kv   8:                          general.file_type u32              = 16\n",
      "llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false\n",
      "llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1\n",
      "llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000\n",
      "llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2\n",
      "llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101\n",
      "llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = [\"[PAD]\", \"[unused0]\", \"[unused1]\", \"...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100\n",
      "llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   51 tensors\n",
      "llama_model_loader: - type q5_K:   61 tensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_load_vocab: mismatch in special tokens definition ( 7104/30522 vs 5/30522 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = nomic-bert\n",
      "llm_load_print_meta: vocab type       = WPM\n",
      "llm_load_print_meta: n_vocab          = 30522\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 768\n",
      "llm_load_print_meta: n_head           = 12\n",
      "llm_load_print_meta: n_head_kv        = 12\n",
      "llm_load_print_meta: n_layer          = 12\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 768\n",
      "llm_load_print_meta: n_embd_v_gqa     = 768\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-12\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 3072\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 1\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 137M\n",
      "llm_load_print_meta: model ftype      = Q5_K - Small\n",
      "llm_load_print_meta: model params     = 136.73 M\n",
      "llm_load_print_meta: model size       = 89.77 MiB (5.51 BPW) \n",
      "llm_load_print_meta: general.name     = nomic-embed-text-v1.5\n",
      "llm_load_print_meta: BOS token        = 101 '[CLS]'\n",
      "llm_load_print_meta: EOS token        = 102 '[SEP]'\n",
      "llm_load_print_meta: UNK token        = 100 '[UNK]'\n",
      "llm_load_print_meta: SEP token        = 102 '[SEP]'\n",
      "llm_load_print_meta: PAD token        = 0 '[PAD]'\n",
      "llm_load_tensors: ggml ctx size =    0.04 MiB\n",
      "llm_load_tensors:        CPU buffer size =    89.77 MiB\n",
      ".......................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 1000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     3.51 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    21.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.seperator_token_id': '102', 'tokenizer.ggml.unknown_token_id': '100', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'bert', 'tokenizer.ggml.eos_token_id': '102', 'tokenizer.ggml.bos_token_id': '101', 'general.architecture': 'nomic-bert', 'general.name': 'nomic-embed-text-v1.5', 'nomic-bert.block_count': '12', 'nomic-bert.pooling_type': '1', 'nomic-bert.attention.head_count': '12', 'tokenizer.ggml.token_type_count': '2', 'nomic-bert.context_length': '2048', 'nomic-bert.rope.freq_base': '1000.000000', 'nomic-bert.embedding_length': '768', 'nomic-bert.feed_forward_length': '3072', 'nomic-bert.attention.layer_norm_epsilon': '0.000000', 'general.file_type': '16', 'nomic-bert.attention.causal': 'false'}\n",
      "Using fallback chat format: None\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 363 tensors from /home/aajais/Desktop/DiPyCodeAssistant/model/codellama-13b-instruct.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = codellama_codellama-13b-instruct-hf\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 16384\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32016]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32016]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32016]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_K:  241 tensors\n",
      "llama_model_loader: - type q6_K:   41 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 264/32016 vs 259/32016 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32016\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 16384\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 16384\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 7.33 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = codellama_codellama-13b-instruct-hf\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =  7500.96 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 5000\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  3906.25 MiB\n",
      "llama_new_context_with_model: KV self size  = 3906.25 MiB, K (f16): 1953.12 MiB, V (f16): 1953.12 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    20.81 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   430.63 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '16384', 'general.name': 'codellama_codellama-13b-instruct-hf', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '40', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import LlamaCppEmbeddings\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# Nomic v1 or v1.5\n",
    "embd_model_path = \"/home/aajais/Desktop/DiPyCodeAssistant/llama.cpp/models/nomic-embed-text-v1.5.Q5_K_S.gguf\"\n",
    "embd = LlamaCppEmbeddings(model_path=embd_model_path, n_batch=512)\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "model = LlamaCpp(\n",
    "    model_path=\"/home/aajais/Desktop/DiPyCodeAssistant/model/codellama-13b-instruct.Q4_K_M.gguf\",\n",
    "    n_ctx=5000,\n",
    "    n_gpu_layers=4,\n",
    "    n_batch=512,\n",
    "    f16_kv=True,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aajais/Desktop/DiPyCodeAssistant/llm_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "RANDOM_SEED = 224  # Fixed seed for reproducibility\n",
    "\n",
    "### --- Code from citations referenced above (added comments and docstrings) --- ###\n",
    "\n",
    "\n",
    "def global_cluster_embeddings(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    n_neighbors: Optional[int] = None,\n",
    "    metric: str = \"cosine\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform global dimensionality reduction on the embeddings using UMAP.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for the reduced space.\n",
    "    - n_neighbors: Optional; the number of neighbors to consider for each point.\n",
    "                   If not provided, it defaults to the square root of the number of embeddings.\n",
    "    - metric: The distance metric to use for UMAP.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
    "    \"\"\"\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=n_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def local_cluster_embeddings(\n",
    "    embeddings: np.ndarray, dim: int, num_neighbors: int = 10, metric: str = \"cosine\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform local dimensionality reduction on the embeddings using UMAP, typically after global clustering.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for the reduced space.\n",
    "    - num_neighbors: The number of neighbors to consider for each point.\n",
    "    - metric: The distance metric to use for UMAP.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
    "    \"\"\"\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=num_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def get_optimal_clusters(\n",
    "    embeddings: np.ndarray, max_clusters: int = 50, random_state: int = RANDOM_SEED\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Determine the optimal number of clusters using the Bayesian Information Criterion (BIC) with a Gaussian Mixture Model.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - max_clusters: The maximum number of clusters to consider.\n",
    "    - random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - An integer representing the optimal number of clusters found.\n",
    "    \"\"\"\n",
    "    max_clusters = min(max_clusters, len(embeddings))\n",
    "    n_clusters = np.arange(1, max_clusters)\n",
    "    bics = []\n",
    "    for n in n_clusters:\n",
    "        gm = GaussianMixture(n_components=n, random_state=random_state)\n",
    "        gm.fit(embeddings)\n",
    "        bics.append(gm.bic(embeddings))\n",
    "    return n_clusters[np.argmin(bics)]\n",
    "\n",
    "\n",
    "def GMM_cluster(embeddings: np.ndarray, threshold: float, random_state: int = 0):\n",
    "    \"\"\"\n",
    "    Cluster embeddings using a Gaussian Mixture Model (GMM) based on a probability threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - threshold: The probability threshold for assigning an embedding to a cluster.\n",
    "    - random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing the cluster labels and the number of clusters determined.\n",
    "    \"\"\"\n",
    "    n_clusters = get_optimal_clusters(embeddings)\n",
    "    gm = GaussianMixture(n_components=n_clusters, random_state=random_state)\n",
    "    gm.fit(embeddings)\n",
    "    probs = gm.predict_proba(embeddings)\n",
    "    labels = [np.where(prob > threshold)[0] for prob in probs]\n",
    "    return labels, n_clusters\n",
    "\n",
    "\n",
    "def perform_clustering(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    threshold: float,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Perform clustering on the embeddings by first reducing their dimensionality globally, then clustering\n",
    "    using a Gaussian Mixture Model, and finally performing local clustering within each global cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for UMAP reduction.\n",
    "    - threshold: The probability threshold for assigning an embedding to a cluster in GMM.\n",
    "\n",
    "    Returns:\n",
    "    - A list of numpy arrays, where each array contains the cluster IDs for each embedding.\n",
    "    \"\"\"\n",
    "    if len(embeddings) <= dim + 1:\n",
    "        # Avoid clustering when there's insufficient data\n",
    "        return [np.array([0]) for _ in range(len(embeddings))]\n",
    "\n",
    "    # Global dimensionality reduction\n",
    "    reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)\n",
    "    # Global clustering\n",
    "    global_clusters, n_global_clusters = GMM_cluster(\n",
    "        reduced_embeddings_global, threshold\n",
    "    )\n",
    "\n",
    "    all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n",
    "    total_clusters = 0\n",
    "\n",
    "    # Iterate through each global cluster to perform local clustering\n",
    "    for i in range(n_global_clusters):\n",
    "        # Extract embeddings belonging to the current global cluster\n",
    "        global_cluster_embeddings_ = embeddings[\n",
    "            np.array([i in gc for gc in global_clusters])\n",
    "        ]\n",
    "\n",
    "        if len(global_cluster_embeddings_) == 0:\n",
    "            continue\n",
    "        if len(global_cluster_embeddings_) <= dim + 1:\n",
    "            # Handle small clusters with direct assignment\n",
    "            local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n",
    "            n_local_clusters = 1\n",
    "        else:\n",
    "            # Local dimensionality reduction and clustering\n",
    "            reduced_embeddings_local = local_cluster_embeddings(\n",
    "                global_cluster_embeddings_, dim\n",
    "            )\n",
    "            local_clusters, n_local_clusters = GMM_cluster(\n",
    "                reduced_embeddings_local, threshold\n",
    "            )\n",
    "\n",
    "        # Assign local cluster IDs, adjusting for total clusters already processed\n",
    "        for j in range(n_local_clusters):\n",
    "            local_cluster_embeddings_ = global_cluster_embeddings_[\n",
    "                np.array([j in lc for lc in local_clusters])\n",
    "            ]\n",
    "            indices = np.where(\n",
    "                (embeddings == local_cluster_embeddings_[:, None]).all(-1)\n",
    "            )[1]\n",
    "            for idx in indices:\n",
    "                all_local_clusters[idx] = np.append(\n",
    "                    all_local_clusters[idx], j + total_clusters\n",
    "                )\n",
    "\n",
    "        total_clusters += n_local_clusters\n",
    "\n",
    "    return all_local_clusters\n",
    "\n",
    "\n",
    "### --- Our code below --- ###\n",
    "\n",
    "\n",
    "def embed(texts):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of text documents.\n",
    "\n",
    "    This function assumes the existence of an `embd` object with a method `embed_documents`\n",
    "    that takes a list of texts and returns their embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], a list of text documents to be embedded.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: An array of embeddings for the given text documents.\n",
    "    \"\"\"\n",
    "    text_embeddings = embd.embed_documents(texts)\n",
    "    text_embeddings_np = np.array(text_embeddings)\n",
    "    return text_embeddings_np\n",
    "\n",
    "\n",
    "def embed_cluster_texts(texts):\n",
    "    \"\"\"\n",
    "    Embeds a list of texts and clusters them, returning a DataFrame with texts, their embeddings, and cluster labels.\n",
    "\n",
    "    This function combines embedding generation and clustering into a single step. It assumes the existence\n",
    "    of a previously defined `perform_clustering` function that performs clustering on the embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], a list of text documents to be processed.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A DataFrame containing the original texts, their embeddings, and the assigned cluster labels.\n",
    "    \"\"\"\n",
    "    text_embeddings_np = embed(texts)  # Generate embeddings\n",
    "    cluster_labels = perform_clustering(\n",
    "        text_embeddings_np, 10, 0.1\n",
    "    )  # Perform clustering on the embeddings\n",
    "    df = pd.DataFrame()  # Initialize a DataFrame to store the results\n",
    "    df[\"text\"] = texts  # Store original texts\n",
    "    df[\"embd\"] = list(text_embeddings_np)  # Store embeddings as a list in the DataFrame\n",
    "    df[\"cluster\"] = cluster_labels  # Store cluster labels\n",
    "    return df\n",
    "\n",
    "\n",
    "def fmt_txt(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Formats the text documents in a DataFrame into a single string.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the 'text' column with text documents to format.\n",
    "\n",
    "    Returns:\n",
    "    - A single string where all text documents are joined by a specific delimiter.\n",
    "    \"\"\"\n",
    "    unique_txt = df[\"text\"].tolist()\n",
    "    return \"--- --- \\n --- --- \".join(unique_txt)\n",
    "\n",
    "\n",
    "def embed_cluster_summarize_texts(\n",
    "    texts: List[str], level: int\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Embeds, clusters, and summarizes a list of texts. This function first generates embeddings for the texts,\n",
    "    clusters them based on similarity, expands the cluster assignments for easier processing, and then summarizes\n",
    "    the content within each cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: A list of text documents to be processed.\n",
    "    - level: An integer parameter that could define the depth or detail of processing.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple containing two DataFrames:\n",
    "      1. The first DataFrame (`df_clusters`) includes the original texts, their embeddings, and cluster assignments.\n",
    "      2. The second DataFrame (`df_summary`) contains summaries for each cluster, the specified level of detail,\n",
    "         and the cluster identifiers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed and cluster the texts, resulting in a DataFrame with 'text', 'embd', and 'cluster' columns\n",
    "    df_clusters = embed_cluster_texts(texts)\n",
    "\n",
    "    # Prepare to expand the DataFrame for easier manipulation of clusters\n",
    "    expanded_list = []\n",
    "\n",
    "    # Expand DataFrame entries to document-cluster pairings for straightforward processing\n",
    "    for index, row in df_clusters.iterrows():\n",
    "        for cluster in row[\"cluster\"]:\n",
    "            expanded_list.append(\n",
    "                {\"text\": row[\"text\"], \"embd\": row[\"embd\"], \"cluster\": cluster}\n",
    "            )\n",
    "\n",
    "    # Create a new DataFrame from the expanded list\n",
    "    expanded_df = pd.DataFrame(expanded_list)\n",
    "\n",
    "    # Retrieve unique cluster identifiers for processing\n",
    "    all_clusters = expanded_df[\"cluster\"].unique()\n",
    "\n",
    "    print(f\"--Generated {len(all_clusters)} clusters--\")\n",
    "\n",
    "    # Summarization\n",
    "    template = \"\"\"Here is a sub-set of LangChain Expression Langauge doc. \n",
    "    \n",
    "    LangChain Expression Langauge provides a way to compose chain in LangChain.\n",
    "    \n",
    "    Give a detailed summary of the documentation provided.\n",
    "    \n",
    "    Documentation:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "\n",
    "    # Format text within each cluster for summarization\n",
    "    summaries = []\n",
    "    for i in all_clusters:\n",
    "        df_cluster = expanded_df[expanded_df[\"cluster\"] == i]\n",
    "        formatted_txt = fmt_txt(df_cluster)\n",
    "        summaries.append(chain.invoke({\"context\": formatted_txt}))\n",
    "\n",
    "    # Create a DataFrame to store summaries with their corresponding cluster and level\n",
    "    df_summary = pd.DataFrame(\n",
    "        {\n",
    "            \"summaries\": summaries,\n",
    "            \"level\": [level] * len(summaries),\n",
    "            \"cluster\": list(all_clusters),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df_clusters, df_summary\n",
    "\n",
    "\n",
    "def recursive_embed_cluster_summarize(\n",
    "    texts: List[str], level: int = 1, n_levels: int = 3\n",
    ") -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Recursively embeds, clusters, and summarizes texts up to a specified level or until\n",
    "    the number of unique clusters becomes 1, storing the results at each level.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], texts to be processed.\n",
    "    - level: int, current recursion level (starts at 1).\n",
    "    - n_levels: int, maximum depth of recursion.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[int, Tuple[pd.DataFrame, pd.DataFrame]], a dictionary where keys are the recursion\n",
    "      levels and values are tuples containing the clusters DataFrame and summaries DataFrame at that level.\n",
    "    \"\"\"\n",
    "    results = {}  # Dictionary to store results at each level\n",
    "\n",
    "    # Perform embedding, clustering, and summarization for the current level\n",
    "    df_clusters, df_summary = embed_cluster_summarize_texts(texts, level)\n",
    "\n",
    "    # Store the results of the current level\n",
    "    results[level] = (df_clusters, df_summary)\n",
    "\n",
    "    # Determine if further recursion is possible and meaningful\n",
    "    unique_clusters = df_summary[\"cluster\"].nunique()\n",
    "    if level < n_levels and unique_clusters > 1:\n",
    "        # Use summaries as the input texts for the next level of recursion\n",
    "        new_texts = df_summary[\"summaries\"].tolist()\n",
    "        next_level_results = recursive_embed_cluster_summarize(\n",
    "            new_texts, level + 1, n_levels\n",
    "        )\n",
    "\n",
    "        # Merge the results from the next level into the current results dictionary\n",
    "        results.update(next_level_results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2838.05 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    2667.34 ms /   512 tokens (    5.21 ms per token,   191.95 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    2854.40 ms /   513 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Generated 1 clusters--\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Requested tokens (32085) exceed context window of 5000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m leaf_texts \u001b[38;5;241m=\u001b[39m docs_texts\n\u001b[0;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrecursive_embed_cluster_summarize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleaf_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_levels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 326\u001b[0m, in \u001b[0;36mrecursive_embed_cluster_summarize\u001b[0;34m(texts, level, n_levels)\u001b[0m\n\u001b[1;32m    323\u001b[0m results \u001b[38;5;241m=\u001b[39m {}  \u001b[38;5;66;03m# Dictionary to store results at each level\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# Perform embedding, clustering, and summarization for the current level\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m df_clusters, df_summary \u001b[38;5;241m=\u001b[39m \u001b[43membed_cluster_summarize_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# Store the results of the current level\u001b[39;00m\n\u001b[1;32m    329\u001b[0m results[level] \u001b[38;5;241m=\u001b[39m (df_clusters, df_summary)\n",
      "Cell \u001b[0;32mIn[7], line 293\u001b[0m, in \u001b[0;36membed_cluster_summarize_texts\u001b[0;34m(texts, level)\u001b[0m\n\u001b[1;32m    291\u001b[0m     df_cluster \u001b[38;5;241m=\u001b[39m expanded_df[expanded_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m i]\n\u001b[1;32m    292\u001b[0m     formatted_txt \u001b[38;5;241m=\u001b[39m fmt_txt(df_cluster)\n\u001b[0;32m--> 293\u001b[0m     summaries\u001b[38;5;241m.\u001b[39mappend(\u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatted_txt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# Create a DataFrame to store summaries with their corresponding cluster and level\u001b[39;00m\n\u001b[1;32m    296\u001b[0m df_summary \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m    297\u001b[0m     {\n\u001b[1;32m    298\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummaries\u001b[39m\u001b[38;5;124m\"\u001b[39m: summaries,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    301\u001b[0m     }\n\u001b[1;32m    302\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/DiPyCodeAssistant/llm_env/lib/python3.10/site-packages/langchain_core/runnables/base.py:2075\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2074\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2075\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2076\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2077\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2081\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/DiPyCodeAssistant/llm_env/lib/python3.10/site-packages/langchain_core/language_models/llms.py:273\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    270\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    271\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 273\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    284\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/DiPyCodeAssistant/llm_env/lib/python3.10/site-packages/langchain_core/language_models/llms.py:568\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    562\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    566\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    567\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/DiPyCodeAssistant/llm_env/lib/python3.10/site-packages/langchain_core/language_models/llms.py:741\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    726\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    727\u001b[0m         )\n\u001b[1;32m    728\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    729\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    730\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    739\u001b[0m         )\n\u001b[1;32m    740\u001b[0m     ]\n\u001b[0;32m--> 741\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/DiPyCodeAssistant/llm_env/lib/python3.10/site-packages/langchain_core/language_models/llms.py:605\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    604\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    606\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/Desktop/DiPyCodeAssistant/llm_env/lib/python3.10/site-packages/langchain_core/language_models/llms.py:592\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    584\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    591\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 592\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    596\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    600\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    601\u001b[0m         )\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Desktop/DiPyCodeAssistant/llm_env/lib/python3.10/site-packages/langchain_core/language_models/llms.py:1177\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m   1176\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1177\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1180\u001b[0m     )\n\u001b[1;32m   1181\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/Desktop/DiPyCodeAssistant/llm_env/lib/python3.10/site-packages/langchain_community/llms/llamacpp.py:288\u001b[0m, in \u001b[0;36mLlamaCpp._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreaming:\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;66;03m# If streaming is enabled, we use the stream\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# method that yields as they are generated\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# and return the combined strings from the first choices's text:\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     combined_text_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\n\u001b[1;32m    289\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m    290\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    291\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    293\u001b[0m     ):\n\u001b[1;32m    294\u001b[0m         combined_text_output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined_text_output\n",
      "File \u001b[0;32m~/Desktop/DiPyCodeAssistant/llm_env/lib/python3.10/site-packages/langchain_community/llms/llamacpp.py:341\u001b[0m, in \u001b[0;36mLlamaCpp._stream\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_parameters(stop), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m    340\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient(prompt\u001b[38;5;241m=\u001b[39mprompt, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 341\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[1;32m    342\u001b[0m     logprobs \u001b[38;5;241m=\u001b[39m part[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    343\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m GenerationChunk(\n\u001b[1;32m    344\u001b[0m         text\u001b[38;5;241m=\u001b[39mpart[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    345\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs},\n\u001b[1;32m    346\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/DiPyCodeAssistant/llm_env/lib/python3.10/site-packages/llama_cpp/llama.py:953\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ctx\u001b[38;5;241m.\u001b[39mreset_timings()\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prompt_tokens) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_ctx:\n\u001b[0;32m--> 953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    954\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested tokens (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(prompt_tokens)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exceed context window of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllama_cpp\u001b[38;5;241m.\u001b[39mllama_n_ctx(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    955\u001b[0m     )\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m max_tokens \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;66;03m# Unlimited, depending on n_ctx.\u001b[39;00m\n\u001b[1;32m    959\u001b[0m     max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_ctx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(prompt_tokens)\n",
      "\u001b[0;31mValueError\u001b[0m: Requested tokens (32085) exceed context window of 5000"
     ]
    }
   ],
   "source": [
    "leaf_texts = docs_texts\n",
    "results = recursive_embed_cluster_summarize(leaf_texts, level=1, n_levels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Corrective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 112 tensors from /home/aajais/Desktop/DiPyCodeAssistant/llama.cpp/models/nomic-embed-text-v1.5.Q5_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert\n",
      "llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5\n",
      "llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12\n",
      "llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768\n",
      "llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072\n",
      "llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12\n",
      "llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000\n",
      "llama_model_loader: - kv   8:                          general.file_type u32              = 16\n",
      "llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false\n",
      "llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1\n",
      "llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000\n",
      "llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2\n",
      "llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101\n",
      "llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = [\"[PAD]\", \"[unused0]\", \"[unused1]\", \"...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100\n",
      "llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   51 tensors\n",
      "llama_model_loader: - type q5_K:   61 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 7104/30522 vs 5/30522 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = nomic-bert\n",
      "llm_load_print_meta: vocab type       = WPM\n",
      "llm_load_print_meta: n_vocab          = 30522\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 768\n",
      "llm_load_print_meta: n_head           = 12\n",
      "llm_load_print_meta: n_head_kv        = 12\n",
      "llm_load_print_meta: n_layer          = 12\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 768\n",
      "llm_load_print_meta: n_embd_v_gqa     = 768\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-12\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 3072\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 1\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 137M\n",
      "llm_load_print_meta: model ftype      = Q5_K - Small\n",
      "llm_load_print_meta: model params     = 136.73 M\n",
      "llm_load_print_meta: model size       = 89.77 MiB (5.51 BPW) \n",
      "llm_load_print_meta: general.name     = nomic-embed-text-v1.5\n",
      "llm_load_print_meta: BOS token        = 101 '[CLS]'\n",
      "llm_load_print_meta: EOS token        = 102 '[SEP]'\n",
      "llm_load_print_meta: UNK token        = 100 '[UNK]'\n",
      "llm_load_print_meta: SEP token        = 102 '[SEP]'\n",
      "llm_load_print_meta: PAD token        = 0 '[PAD]'\n",
      "llm_load_tensors: ggml ctx size =    0.04 MiB\n",
      "llm_load_tensors:        CPU buffer size =    89.77 MiB\n",
      ".......................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 1000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     3.51 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    21.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.seperator_token_id': '102', 'tokenizer.ggml.unknown_token_id': '100', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'bert', 'tokenizer.ggml.eos_token_id': '102', 'tokenizer.ggml.bos_token_id': '101', 'general.architecture': 'nomic-bert', 'general.name': 'nomic-embed-text-v1.5', 'nomic-bert.block_count': '12', 'nomic-bert.pooling_type': '1', 'nomic-bert.attention.head_count': '12', 'tokenizer.ggml.token_type_count': '2', 'nomic-bert.context_length': '2048', 'nomic-bert.rope.freq_base': '1000.000000', 'nomic-bert.embedding_length': '768', 'nomic-bert.feed_forward_length': '3072', 'nomic-bert.attention.layer_norm_epsilon': '0.000000', 'general.file_type': '16', 'nomic-bert.attention.causal': 'false'}\n",
      "Using fallback chat format: None\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     224.04 ms /    71 tokens (    3.16 ms per token,   316.91 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     350.02 ms /    72 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     139.35 ms /    55 tokens (    2.53 ms per token,   394.68 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     139.22 ms /    56 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     190.83 ms /    66 tokens (    2.89 ms per token,   345.85 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     190.59 ms /    67 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     269.77 ms /    91 tokens (    2.96 ms per token,   337.32 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     270.19 ms /    92 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     876.09 ms /   352 tokens (    2.49 ms per token,   401.78 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     876.39 ms /   353 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1256.47 ms /   433 tokens (    2.90 ms per token,   344.62 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1257.97 ms /   434 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1322.83 ms /   462 tokens (    2.86 ms per token,   349.25 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1323.91 ms /   463 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1326.17 ms /   462 tokens (    2.87 ms per token,   348.37 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1326.35 ms /   463 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     932.04 ms /   327 tokens (    2.85 ms per token,   350.84 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     931.79 ms /   328 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1451.67 ms /   460 tokens (    3.16 ms per token,   316.88 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1452.30 ms /   461 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1332.81 ms /   479 tokens (    2.78 ms per token,   359.39 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1333.47 ms /   480 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1213.68 ms /   377 tokens (    3.22 ms per token,   310.63 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1214.66 ms /   378 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1167.87 ms /   408 tokens (    2.86 ms per token,   349.35 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1169.24 ms /   409 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1200.27 ms /   423 tokens (    2.84 ms per token,   352.42 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1201.27 ms /   424 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1095.10 ms /   396 tokens (    2.77 ms per token,   361.61 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1095.41 ms /   397 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     986.21 ms /   376 tokens (    2.62 ms per token,   381.26 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     986.48 ms /   377 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1100.25 ms /   396 tokens (    2.78 ms per token,   359.92 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1101.01 ms /   397 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1488.14 ms /   457 tokens (    3.26 ms per token,   307.10 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1488.98 ms /   458 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1773.45 ms /   442 tokens (    4.01 ms per token,   249.23 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1774.70 ms /   443 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1555.28 ms /   444 tokens (    3.50 ms per token,   285.48 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1555.91 ms /   445 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1631.51 ms /   377 tokens (    4.33 ms per token,   231.07 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1632.45 ms /   378 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1637.12 ms /   401 tokens (    4.08 ms per token,   244.94 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1637.85 ms /   402 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1655.12 ms /   384 tokens (    4.31 ms per token,   232.01 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1655.39 ms /   385 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1113.94 ms /   428 tokens (    2.60 ms per token,   384.22 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1115.54 ms /   429 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1042.54 ms /   402 tokens (    2.59 ms per token,   385.60 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1043.17 ms /   403 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1423.20 ms /   389 tokens (    3.66 ms per token,   273.33 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1423.62 ms /   390 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1163.55 ms /   389 tokens (    2.99 ms per token,   334.32 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1164.28 ms /   390 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1116.85 ms /   409 tokens (    2.73 ms per token,   366.21 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1117.60 ms /   410 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1254.30 ms /   460 tokens (    2.73 ms per token,   366.74 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1255.75 ms /   461 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1103.41 ms /   431 tokens (    2.56 ms per token,   390.61 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1104.69 ms /   432 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1352.66 ms /   474 tokens (    2.85 ms per token,   350.42 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1353.63 ms /   475 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1074.52 ms /   430 tokens (    2.50 ms per token,   400.18 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1074.89 ms /   431 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1126.19 ms /   445 tokens (    2.53 ms per token,   395.14 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1126.81 ms /   446 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1286.01 ms /   469 tokens (    2.74 ms per token,   364.70 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1286.92 ms /   470 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1207.34 ms /   451 tokens (    2.68 ms per token,   373.55 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1208.26 ms /   452 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1175.06 ms /   445 tokens (    2.64 ms per token,   378.70 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1176.44 ms /   446 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1093.24 ms /   434 tokens (    2.52 ms per token,   396.98 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1094.32 ms /   435 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     727.52 ms /   303 tokens (    2.40 ms per token,   416.49 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     727.84 ms /   304 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1299.42 ms /   496 tokens (    2.62 ms per token,   381.71 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1299.79 ms /   497 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1498.34 ms /   471 tokens (    3.18 ms per token,   314.35 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1499.26 ms /   472 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     841.70 ms /   340 tokens (    2.48 ms per token,   403.94 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     842.48 ms /   341 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1103.70 ms /   445 tokens (    2.48 ms per token,   403.19 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1104.41 ms /   446 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1055.47 ms /   424 tokens (    2.49 ms per token,   401.72 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1055.97 ms /   425 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     961.87 ms /   396 tokens (    2.43 ms per token,   411.70 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     961.78 ms /   397 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1239.78 ms /   464 tokens (    2.67 ms per token,   374.26 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1240.80 ms /   465 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1191.94 ms /   452 tokens (    2.64 ms per token,   379.21 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1192.41 ms /   453 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1171.33 ms /   451 tokens (    2.60 ms per token,   385.03 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1172.03 ms /   452 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1288.53 ms /   450 tokens (    2.86 ms per token,   349.23 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1289.37 ms /   451 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1366.36 ms /   488 tokens (    2.80 ms per token,   357.15 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1366.34 ms /   489 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1082.05 ms /   416 tokens (    2.60 ms per token,   384.46 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1082.46 ms /   417 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1066.02 ms /   404 tokens (    2.64 ms per token,   378.98 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1066.37 ms /   405 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1301.38 ms /   482 tokens (    2.70 ms per token,   370.38 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1301.91 ms /   483 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1358.22 ms /   447 tokens (    3.04 ms per token,   329.11 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1358.26 ms /   448 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1255.55 ms /   486 tokens (    2.58 ms per token,   387.08 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1256.31 ms /   487 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1267.73 ms /   439 tokens (    2.89 ms per token,   346.29 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1268.41 ms /   440 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1643.75 ms /   406 tokens (    4.05 ms per token,   247.00 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1644.78 ms /   407 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     842.48 ms /   326 tokens (    2.58 ms per token,   386.95 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     843.42 ms /   327 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1394.67 ms /   452 tokens (    3.09 ms per token,   324.09 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1395.37 ms /   453 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1052.03 ms /   376 tokens (    2.80 ms per token,   357.40 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1051.70 ms /   377 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1448.72 ms /   512 tokens (    2.83 ms per token,   353.42 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1450.61 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1182.00 ms /   432 tokens (    2.74 ms per token,   365.48 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1183.51 ms /   433 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1260.68 ms /   413 tokens (    3.05 ms per token,   327.60 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1262.04 ms /   414 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     870.35 ms /   345 tokens (    2.52 ms per token,   396.39 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     871.09 ms /   346 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1019.59 ms /   400 tokens (    2.55 ms per token,   392.31 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1020.68 ms /   401 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1390.90 ms /   401 tokens (    3.47 ms per token,   288.30 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1391.57 ms /   402 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1037.41 ms /   398 tokens (    2.61 ms per token,   383.65 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1038.56 ms /   399 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1034.44 ms /   397 tokens (    2.61 ms per token,   383.78 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1035.21 ms /   398 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1256.44 ms /   435 tokens (    2.89 ms per token,   346.22 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1256.44 ms /   436 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1006.66 ms /   392 tokens (    2.57 ms per token,   389.41 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1006.98 ms /   393 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1122.33 ms /   401 tokens (    2.80 ms per token,   357.29 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1123.12 ms /   402 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1057.47 ms /   397 tokens (    2.66 ms per token,   375.43 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1057.64 ms /   398 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1381.77 ms /   445 tokens (    3.11 ms per token,   322.05 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1382.47 ms /   446 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1877.01 ms /   512 tokens (    3.67 ms per token,   272.77 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1878.22 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    2295.55 ms /   474 tokens (    4.84 ms per token,   206.49 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    2296.20 ms /   475 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    2016.45 ms /   449 tokens (    4.49 ms per token,   222.67 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    2017.71 ms /   450 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1690.89 ms /   417 tokens (    4.05 ms per token,   246.62 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1691.08 ms /   418 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1638.85 ms /   450 tokens (    3.64 ms per token,   274.58 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1639.66 ms /   451 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     798.93 ms /   206 tokens (    3.88 ms per token,   257.84 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     798.72 ms /   207 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     383.81 ms /   104 tokens (    3.69 ms per token,   270.97 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     384.12 ms /   105 tokens\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "# from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.embeddings import LlamaCppEmbeddings\n",
    "\n",
    "# Load\n",
    "url = \"https://docs.dipy.org/stable/reference/index\"\n",
    "loader = WebBaseLoader(url)\n",
    "docs = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=100\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed and index\n",
    "    # GPT4All\n",
    "    # embedding = GPT4AllEmbeddings()\n",
    "    # Nomic v1 or v1.5\n",
    "embd_model_path = \"/home/aajais/Desktop/DiPyCodeAssistant/llama.cpp/models/nomic-embed-text-v1.5.Q5_K_S.gguf\"\n",
    "embedding = LlamaCppEmbeddings(model_path=embd_model_path, n_batch=512)\n",
    "\n",
    "# Index\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=embedding,\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Dict, TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        keys: A dictionary where each key is a string.\n",
    "    \"\"\"\n",
    "\n",
    "    keys: Dict[str, any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "### Nodes ###\n",
    "\n",
    "\n",
    "def retrieve(state, retriever):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    local = state_dict[\"local\"]\n",
    "    documents = retriever.get_relevant_documents(question)\n",
    "    return {\"keys\": {\"documents\": documents, \"local\": local, \"question\": question}}\n",
    "\n",
    "\n",
    "def generate(state, llm):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    local = state_dict[\"local\"]\n",
    "\n",
    "    # Prompt\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # Post-processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\n",
    "        \"keys\": {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "    }\n",
    "\n",
    "\n",
    "def grade_documents(state, llm):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    local = state_dict[\"local\"]\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keywords related to the user question, grade it as relevant. \\n\n",
    "        It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "        Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\"\"\",\n",
    "        input_variables=[\"question\", \"context\"],\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm | JsonOutputParser()\n",
    "\n",
    "    # Score\n",
    "    filtered_docs = []\n",
    "    search = \"No\"  # Default do not opt for web search to supplement retrieval\n",
    "    for d in documents:\n",
    "        score = chain.invoke(\n",
    "            {\n",
    "                \"question\": question,\n",
    "                \"context\": d.page_content,\n",
    "            }\n",
    "        )\n",
    "        grade = score[\"score\"]\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            search = \"Yes\"  # Perform web search\n",
    "            continue\n",
    "\n",
    "    return {\n",
    "        \"keys\": {\n",
    "            \"documents\": filtered_docs,\n",
    "            \"question\": question,\n",
    "            \"local\": local,\n",
    "            \"run_web_search\": search,\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def transform_query(state, llm):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    local = state_dict[\"local\"]\n",
    "\n",
    "    # Create a prompt template with format instructions and the query\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are generating questions that is well optimized for retrieval. \\n \n",
    "        Look at the input and try to reason about the underlying sematic intent / meaning. \\n \n",
    "        Here is the initial question:\n",
    "        \\n ------- \\n\n",
    "        {question} \n",
    "        \\n ------- \\n\n",
    "        Provide an improved question without any premable, only respond with the updated question: \"\"\",\n",
    "        input_variables=[\"question\"],\n",
    "    )\n",
    "\n",
    "    # Prompt\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    better_question = chain.invoke({\"question\": question})\n",
    "\n",
    "    return {\n",
    "        \"keys\": {\"documents\": documents, \"question\": better_question, \"local\": local}\n",
    "    }\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question using Tavily API.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Web results appended to documents.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    local = state_dict[\"local\"]\n",
    "\n",
    "    tool = TavilySearchResults()\n",
    "    docs = tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "\n",
    "    return {\"keys\": {\"documents\": documents, \"local\": local, \"question\": question}}\n",
    "\n",
    "\n",
    "### Edges\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer or re-generate a question for web search.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current state of the agent, including all keys.\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---DECIDE TO GENERATE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    filtered_documents = state_dict[\"documents\"]\n",
    "    search = state_dict[\"run_web_search\"]\n",
    "\n",
    "    if search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: TRANSFORM QUERY and RUN WEB SEARCH---\")\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/home/aajais/Desktop/DiPyCodeAssistant/model/codellama-13b-instruct.Q4_K_M.gguf\",\n",
    "    n_ctx=5000,\n",
    "    n_gpu_layers=4,\n",
    "    n_batch=512,\n",
    "    f16_kv=True,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", lambda state: retrieve(state, retriever=retriever))  # retrieve\n",
    "workflow.add_node(\"grade_documents\", lambda state: grade_documents(state, llm=llm))  # grade documents\n",
    "workflow.add_node(\"generate\", lambda state: generate(state, llm=llm))  # generatae\n",
    "workflow.add_node(\"transform_query\", lambda state: transform_query(state, llm=llm))  # transform_query\n",
    "workflow.add_node(\"web_search\", web_search)  # web search\n",
    "\n",
    "# Build graph\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"web_search\")\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1473.34 ms /    12 tokens (  122.78 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1546.80 ms /    13 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK RELEVANCE---\n",
      " The JSON should simply be {\"score\": yes/no"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\n",
    "    \"keys\": {\n",
    "        \"question\": \"Explain how the different types of agent memory work?\",\n",
    "        \"local\": \"yes\",\n",
    "    }\n",
    "}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint.pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint.pprint(value[\"keys\"][\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
