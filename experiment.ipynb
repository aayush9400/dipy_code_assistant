{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "# DIPY API\n",
    "url = \"https://docs.dipy.org/stable/reference/index.html\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=10000, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# # DIPY Tutorials\n",
    "# url = \"https://docs.dipy.org/stable/examples_built/\"\n",
    "# loader = RecursiveUrlLoader(\n",
    "#     url=url, max_depth=10000, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    "# )\n",
    "# docs_tutorials = loader.load()\n",
    "\n",
    "# # DIPY CLI Workflows\n",
    "# url = \"https://docs.dipy.org/stable/interfaces/\"\n",
    "# loader = RecursiveUrlLoader(\n",
    "#     url=url, max_depth=10000, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    "# )\n",
    "# docs_workflows = loader.load()\n",
    "\n",
    "# # DIPY CLI API\n",
    "# url = \"https://docs.dipy.org/stable/reference_cmd/\"\n",
    "# loader = RecursiveUrlLoader(\n",
    "#     url=url, max_depth=10000, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    "# )\n",
    "# docs_cli_api = loader.load()\n",
    "\n",
    "# # DIPY Discussions\n",
    "# url = \"https://github.com/dipy/dipy/discussions\"\n",
    "# loader = RecursiveUrlLoader(\n",
    "#     url=url, max_depth=10000, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    "# )\n",
    "# docs_discuss = loader.load()\n",
    "\n",
    "# # Doc texts\n",
    "# docs.extend([*docs_tutorials, *docs_workflows, *docs_cli_api, *docs_discuss])\n",
    "docs_texts = [d.page_content for d in docs]\n",
    "\n",
    "# Calculate the number of tokens for each document\n",
    "counts = [num_tokens_from_string(d, \"cl100k_base\") for d in docs_texts]\n",
    "\n",
    "# Plotting the histogram of token counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(counts, bins=30, color=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
    "plt.title(\"Histogram of Token Counts\")\n",
    "plt.xlabel(\"Token Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis=\"y\", alpha=0.75)\n",
    "\n",
    "# Display the histogram\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens in all context: 20032\n"
     ]
    }
   ],
   "source": [
    "# Doc texts concat\n",
    "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "d_reversed = list(reversed(d_sorted))\n",
    "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "    [doc.page_content for doc in d_reversed]\n",
    ")\n",
    "print(\n",
    "    \"Num tokens in all context: %s\"\n",
    "    % num_tokens_from_string(concatenated_content, \"cl100k_base\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc texts split\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size_tok = 2000\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=chunk_size_tok, chunk_overlap=0\n",
    ")\n",
    "texts_split = text_splitter.split_text(concatenated_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 112 tensors from /home/aajais/Desktop/DiPyCodeAssistant/llama.cpp/models/nomic-embed-text-v1.5.Q5_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert\n",
      "llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5\n",
      "llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12\n",
      "llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768\n",
      "llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072\n",
      "llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12\n",
      "llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000\n",
      "llama_model_loader: - kv   8:                          general.file_type u32              = 16\n",
      "llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false\n",
      "llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1\n",
      "llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000\n",
      "llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2\n",
      "llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101\n",
      "llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = [\"[PAD]\", \"[unused0]\", \"[unused1]\", \"...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100\n",
      "llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   51 tensors\n",
      "llama_model_loader: - type q5_K:   61 tensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_load_vocab: mismatch in special tokens definition ( 7104/30522 vs 5/30522 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = nomic-bert\n",
      "llm_load_print_meta: vocab type       = WPM\n",
      "llm_load_print_meta: n_vocab          = 30522\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 768\n",
      "llm_load_print_meta: n_head           = 12\n",
      "llm_load_print_meta: n_head_kv        = 12\n",
      "llm_load_print_meta: n_layer          = 12\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 768\n",
      "llm_load_print_meta: n_embd_v_gqa     = 768\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-12\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 3072\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 1\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 137M\n",
      "llm_load_print_meta: model ftype      = Q5_K - Small\n",
      "llm_load_print_meta: model params     = 136.73 M\n",
      "llm_load_print_meta: model size       = 89.77 MiB (5.51 BPW) \n",
      "llm_load_print_meta: general.name     = nomic-embed-text-v1.5\n",
      "llm_load_print_meta: BOS token        = 101 '[CLS]'\n",
      "llm_load_print_meta: EOS token        = 102 '[SEP]'\n",
      "llm_load_print_meta: UNK token        = 100 '[UNK]'\n",
      "llm_load_print_meta: SEP token        = 102 '[SEP]'\n",
      "llm_load_print_meta: PAD token        = 0 '[PAD]'\n",
      "llm_load_tensors: ggml ctx size =    0.04 MiB\n",
      "llm_load_tensors:        CPU buffer size =    89.77 MiB\n",
      ".......................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 1000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     3.51 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    21.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.seperator_token_id': '102', 'tokenizer.ggml.unknown_token_id': '100', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'bert', 'tokenizer.ggml.eos_token_id': '102', 'tokenizer.ggml.bos_token_id': '101', 'general.architecture': 'nomic-bert', 'general.name': 'nomic-embed-text-v1.5', 'nomic-bert.block_count': '12', 'nomic-bert.pooling_type': '1', 'nomic-bert.attention.head_count': '12', 'tokenizer.ggml.token_type_count': '2', 'nomic-bert.context_length': '2048', 'nomic-bert.rope.freq_base': '1000.000000', 'nomic-bert.embedding_length': '768', 'nomic-bert.feed_forward_length': '3072', 'nomic-bert.attention.layer_norm_epsilon': '0.000000', 'general.file_type': '16', 'nomic-bert.attention.causal': 'false'}\n",
      "Using fallback chat format: None\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 363 tensors from /home/aajais/Desktop/DiPyCodeAssistant/model/codellama-13b-instruct.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = codellama_codellama-13b-instruct-hf\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 16384\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32016]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32016]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32016]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_K:  241 tensors\n",
      "llama_model_loader: - type q6_K:   41 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 264/32016 vs 259/32016 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32016\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 16384\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 16384\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 7.33 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = codellama_codellama-13b-instruct-hf\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =  7500.96 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 5000\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  3906.25 MiB\n",
      "llama_new_context_with_model: KV self size  = 3906.25 MiB, K (f16): 1953.12 MiB, V (f16): 1953.12 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    20.81 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   430.63 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '16384', 'general.name': 'codellama_codellama-13b-instruct-hf', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '40', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import LlamaCppEmbeddings\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# Nomic v1 or v1.5\n",
    "embd_model_path = \"/home/aajais/Desktop/DiPyCodeAssistant/llama.cpp/models/nomic-embed-text-v1.5.Q5_K_S.gguf\"\n",
    "embd = LlamaCppEmbeddings(model_path=embd_model_path, n_batch=512)\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "model = LlamaCpp(\n",
    "    model_path=\"/home/aajais/Desktop/DiPyCodeAssistant/model/codellama-13b-instruct.Q4_K_M.gguf\",\n",
    "    n_ctx=5000,\n",
    "    n_gpu_layers=4,\n",
    "    n_batch=512,\n",
    "    f16_kv=True,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aajais/Desktop/DiPyCodeAssistant/llm_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "RANDOM_SEED = 224  # Fixed seed for reproducibility\n",
    "\n",
    "### --- Code from citations referenced above (added comments and docstrings) --- ###\n",
    "\n",
    "\n",
    "def global_cluster_embeddings(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    n_neighbors: Optional[int] = None,\n",
    "    metric: str = \"cosine\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform global dimensionality reduction on the embeddings using UMAP.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for the reduced space.\n",
    "    - n_neighbors: Optional; the number of neighbors to consider for each point.\n",
    "                   If not provided, it defaults to the square root of the number of embeddings.\n",
    "    - metric: The distance metric to use for UMAP.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
    "    \"\"\"\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=n_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def local_cluster_embeddings(\n",
    "    embeddings: np.ndarray, dim: int, num_neighbors: int = 10, metric: str = \"cosine\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform local dimensionality reduction on the embeddings using UMAP, typically after global clustering.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for the reduced space.\n",
    "    - num_neighbors: The number of neighbors to consider for each point.\n",
    "    - metric: The distance metric to use for UMAP.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
    "    \"\"\"\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=num_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def get_optimal_clusters(\n",
    "    embeddings: np.ndarray, max_clusters: int = 50, random_state: int = RANDOM_SEED\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Determine the optimal number of clusters using the Bayesian Information Criterion (BIC) with a Gaussian Mixture Model.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - max_clusters: The maximum number of clusters to consider.\n",
    "    - random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - An integer representing the optimal number of clusters found.\n",
    "    \"\"\"\n",
    "    max_clusters = min(max_clusters, len(embeddings))\n",
    "    n_clusters = np.arange(1, max_clusters)\n",
    "    bics = []\n",
    "    for n in n_clusters:\n",
    "        gm = GaussianMixture(n_components=n, random_state=random_state)\n",
    "        gm.fit(embeddings)\n",
    "        bics.append(gm.bic(embeddings))\n",
    "    return n_clusters[np.argmin(bics)]\n",
    "\n",
    "\n",
    "def GMM_cluster(embeddings: np.ndarray, threshold: float, random_state: int = 0):\n",
    "    \"\"\"\n",
    "    Cluster embeddings using a Gaussian Mixture Model (GMM) based on a probability threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - threshold: The probability threshold for assigning an embedding to a cluster.\n",
    "    - random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing the cluster labels and the number of clusters determined.\n",
    "    \"\"\"\n",
    "    n_clusters = get_optimal_clusters(embeddings)\n",
    "    gm = GaussianMixture(n_components=n_clusters, random_state=random_state)\n",
    "    gm.fit(embeddings)\n",
    "    probs = gm.predict_proba(embeddings)\n",
    "    labels = [np.where(prob > threshold)[0] for prob in probs]\n",
    "    return labels, n_clusters\n",
    "\n",
    "\n",
    "def perform_clustering(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    threshold: float,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Perform clustering on the embeddings by first reducing their dimensionality globally, then clustering\n",
    "    using a Gaussian Mixture Model, and finally performing local clustering within each global cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for UMAP reduction.\n",
    "    - threshold: The probability threshold for assigning an embedding to a cluster in GMM.\n",
    "\n",
    "    Returns:\n",
    "    - A list of numpy arrays, where each array contains the cluster IDs for each embedding.\n",
    "    \"\"\"\n",
    "    if len(embeddings) <= dim + 1:\n",
    "        # Avoid clustering when there's insufficient data\n",
    "        return [np.array([0]) for _ in range(len(embeddings))]\n",
    "\n",
    "    # Global dimensionality reduction\n",
    "    reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)\n",
    "    # Global clustering\n",
    "    global_clusters, n_global_clusters = GMM_cluster(\n",
    "        reduced_embeddings_global, threshold\n",
    "    )\n",
    "\n",
    "    all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n",
    "    total_clusters = 0\n",
    "\n",
    "    # Iterate through each global cluster to perform local clustering\n",
    "    for i in range(n_global_clusters):\n",
    "        # Extract embeddings belonging to the current global cluster\n",
    "        global_cluster_embeddings_ = embeddings[\n",
    "            np.array([i in gc for gc in global_clusters])\n",
    "        ]\n",
    "\n",
    "        if len(global_cluster_embeddings_) == 0:\n",
    "            continue\n",
    "        if len(global_cluster_embeddings_) <= dim + 1:\n",
    "            # Handle small clusters with direct assignment\n",
    "            local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n",
    "            n_local_clusters = 1\n",
    "        else:\n",
    "            # Local dimensionality reduction and clustering\n",
    "            reduced_embeddings_local = local_cluster_embeddings(\n",
    "                global_cluster_embeddings_, dim\n",
    "            )\n",
    "            local_clusters, n_local_clusters = GMM_cluster(\n",
    "                reduced_embeddings_local, threshold\n",
    "            )\n",
    "\n",
    "        # Assign local cluster IDs, adjusting for total clusters already processed\n",
    "        for j in range(n_local_clusters):\n",
    "            local_cluster_embeddings_ = global_cluster_embeddings_[\n",
    "                np.array([j in lc for lc in local_clusters])\n",
    "            ]\n",
    "            indices = np.where(\n",
    "                (embeddings == local_cluster_embeddings_[:, None]).all(-1)\n",
    "            )[1]\n",
    "            for idx in indices:\n",
    "                all_local_clusters[idx] = np.append(\n",
    "                    all_local_clusters[idx], j + total_clusters\n",
    "                )\n",
    "\n",
    "        total_clusters += n_local_clusters\n",
    "\n",
    "    return all_local_clusters\n",
    "\n",
    "\n",
    "### --- Our code below --- ###\n",
    "\n",
    "\n",
    "def embed(texts):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of text documents.\n",
    "\n",
    "    This function assumes the existence of an `embd` object with a method `embed_documents`\n",
    "    that takes a list of texts and returns their embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], a list of text documents to be embedded.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: An array of embeddings for the given text documents.\n",
    "    \"\"\"\n",
    "    text_embeddings = embd.embed_documents(texts)\n",
    "    text_embeddings_np = np.array(text_embeddings)\n",
    "    return text_embeddings_np\n",
    "\n",
    "\n",
    "def embed_cluster_texts(texts):\n",
    "    \"\"\"\n",
    "    Embeds a list of texts and clusters them, returning a DataFrame with texts, their embeddings, and cluster labels.\n",
    "\n",
    "    This function combines embedding generation and clustering into a single step. It assumes the existence\n",
    "    of a previously defined `perform_clustering` function that performs clustering on the embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], a list of text documents to be processed.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A DataFrame containing the original texts, their embeddings, and the assigned cluster labels.\n",
    "    \"\"\"\n",
    "    text_embeddings_np = embed(texts)  # Generate embeddings\n",
    "    cluster_labels = perform_clustering(\n",
    "        text_embeddings_np, 10, 0.1\n",
    "    )  # Perform clustering on the embeddings\n",
    "    df = pd.DataFrame()  # Initialize a DataFrame to store the results\n",
    "    df[\"text\"] = texts  # Store original texts\n",
    "    df[\"embd\"] = list(text_embeddings_np)  # Store embeddings as a list in the DataFrame\n",
    "    df[\"cluster\"] = cluster_labels  # Store cluster labels\n",
    "    return df\n",
    "\n",
    "\n",
    "def fmt_txt(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Formats the text documents in a DataFrame into a single string.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the 'text' column with text documents to format.\n",
    "\n",
    "    Returns:\n",
    "    - A single string where all text documents are joined by a specific delimiter.\n",
    "    \"\"\"\n",
    "    unique_txt = df[\"text\"].tolist()\n",
    "    return \"--- --- \\n --- --- \".join(unique_txt)\n",
    "\n",
    "\n",
    "def embed_cluster_summarize_texts(\n",
    "    texts: List[str], level: int\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Embeds, clusters, and summarizes a list of texts. This function first generates embeddings for the texts,\n",
    "    clusters them based on similarity, expands the cluster assignments for easier processing, and then summarizes\n",
    "    the content within each cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: A list of text documents to be processed.\n",
    "    - level: An integer parameter that could define the depth or detail of processing.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple containing two DataFrames:\n",
    "      1. The first DataFrame (`df_clusters`) includes the original texts, their embeddings, and cluster assignments.\n",
    "      2. The second DataFrame (`df_summary`) contains summaries for each cluster, the specified level of detail,\n",
    "         and the cluster identifiers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed and cluster the texts, resulting in a DataFrame with 'text', 'embd', and 'cluster' columns\n",
    "    df_clusters = embed_cluster_texts(texts)\n",
    "\n",
    "    # Prepare to expand the DataFrame for easier manipulation of clusters\n",
    "    expanded_list = []\n",
    "\n",
    "    # Expand DataFrame entries to document-cluster pairings for straightforward processing\n",
    "    for index, row in df_clusters.iterrows():\n",
    "        for cluster in row[\"cluster\"]:\n",
    "            expanded_list.append(\n",
    "                {\"text\": row[\"text\"], \"embd\": row[\"embd\"], \"cluster\": cluster}\n",
    "            )\n",
    "\n",
    "    # Create a new DataFrame from the expanded list\n",
    "    expanded_df = pd.DataFrame(expanded_list)\n",
    "\n",
    "    # Retrieve unique cluster identifiers for processing\n",
    "    all_clusters = expanded_df[\"cluster\"].unique()\n",
    "\n",
    "    print(f\"--Generated {len(all_clusters)} clusters--\")\n",
    "\n",
    "    # Summarization\n",
    "    template = \"\"\"Here is a sub-set of LangChain Expression Langauge doc. \n",
    "    \n",
    "    LangChain Expression Langauge provides a way to compose chain in LangChain.\n",
    "    \n",
    "    Give a detailed summary of the documentation provided.\n",
    "    \n",
    "    Documentation:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "\n",
    "    # Format text within each cluster for summarization\n",
    "    summaries = []\n",
    "    for i in all_clusters:\n",
    "        df_cluster = expanded_df[expanded_df[\"cluster\"] == i]\n",
    "        formatted_txt = fmt_txt(df_cluster)\n",
    "        summaries.append(chain.invoke({\"context\": formatted_txt}))\n",
    "\n",
    "    # Create a DataFrame to store summaries with their corresponding cluster and level\n",
    "    df_summary = pd.DataFrame(\n",
    "        {\n",
    "            \"summaries\": summaries,\n",
    "            \"level\": [level] * len(summaries),\n",
    "            \"cluster\": list(all_clusters),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df_clusters, df_summary\n",
    "\n",
    "\n",
    "def recursive_embed_cluster_summarize(\n",
    "    texts: List[str], level: int = 1, n_levels: int = 3\n",
    ") -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Recursively embeds, clusters, and summarizes texts up to a specified level or until\n",
    "    the number of unique clusters becomes 1, storing the results at each level.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], texts to be processed.\n",
    "    - level: int, current recursion level (starts at 1).\n",
    "    - n_levels: int, maximum depth of recursion.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[int, Tuple[pd.DataFrame, pd.DataFrame]], a dictionary where keys are the recursion\n",
    "      levels and values are tuples containing the clusters DataFrame and summaries DataFrame at that level.\n",
    "    \"\"\"\n",
    "    results = {}  # Dictionary to store results at each level\n",
    "\n",
    "    # Perform embedding, clustering, and summarization for the current level\n",
    "    df_clusters, df_summary = embed_cluster_summarize_texts(texts, level)\n",
    "\n",
    "    # Store the results of the current level\n",
    "    results[level] = (df_clusters, df_summary)\n",
    "\n",
    "    # Determine if further recursion is possible and meaningful\n",
    "    unique_clusters = df_summary[\"cluster\"].nunique()\n",
    "    if level < n_levels and unique_clusters > 1:\n",
    "        # Use summaries as the input texts for the next level of recursion\n",
    "        new_texts = df_summary[\"summaries\"].tolist()\n",
    "        next_level_results = recursive_embed_cluster_summarize(\n",
    "            new_texts, level + 1, n_levels\n",
    "        )\n",
    "\n",
    "        # Merge the results from the next level into the current results dictionary\n",
    "        results.update(next_level_results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2838.05 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    2667.34 ms /   512 tokens (    5.21 ms per token,   191.95 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    2854.40 ms /   513 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Generated 1 clusters--\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Requested tokens (32085) exceed context window of 5000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m leaf_texts \u001b[38;5;241m=\u001b[39m docs_texts\n\u001b[0;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrecursive_embed_cluster_summarize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleaf_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_levels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 326\u001b[0m, in \u001b[0;36mrecursive_embed_cluster_summarize\u001b[0;34m(texts, level, n_levels)\u001b[0m\n\u001b[1;32m    323\u001b[0m results \u001b[38;5;241m=\u001b[39m {}  \u001b[38;5;66;03m# Dictionary to store results at each level\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# Perform embedding, clustering, and summarization for the current level\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m df_clusters, df_summary \u001b[38;5;241m=\u001b[39m \u001b[43membed_cluster_summarize_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# Store the results of the current level\u001b[39;00m\n\u001b[1;32m    329\u001b[0m results[level] \u001b[38;5;241m=\u001b[39m (df_clusters, df_summary)\n",
      "Cell \u001b[0;32mIn[7], line 293\u001b[0m, in \u001b[0;36membed_cluster_summarize_texts\u001b[0;34m(texts, level)\u001b[0m\n\u001b[1;32m    291\u001b[0m     df_cluster \u001b[38;5;241m=\u001b[39m expanded_df[expanded_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m i]\n\u001b[1;32m    292\u001b[0m     formatted_txt \u001b[38;5;241m=\u001b[39m fmt_txt(df_cluster)\n\u001b[0;32m--> 293\u001b[0m     summaries\u001b[38;5;241m.\u001b[39mappend(\u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatted_txt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# Create a DataFrame to store summaries with their corresponding cluster and level\u001b[39;00m\n\u001b[1;32m    296\u001b[0m df_summary \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m    297\u001b[0m     {\n\u001b[1;32m    298\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummaries\u001b[39m\u001b[38;5;124m\"\u001b[39m: summaries,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    301\u001b[0m     }\n\u001b[1;32m    302\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/DiPyCodeAssistant/llm_env/lib/python3.10/site-packages/langchain_core/runnables/base.py:2075\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2074\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2075\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2076\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2077\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2081\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/DiPyCodeAssistant/llm_env/lib/python3.10/site-packages/langchain_core/language_models/llms.py:273\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    270\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    271\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 273\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    284\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/DiPyCodeAssistant/llm_env/lib/python3.10/site-packages/langchain_core/language_models/llms.py:568\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    562\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    566\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    567\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/DiPyCodeAssistant/llm_env/lib/python3.10/site-packages/langchain_core/language_models/llms.py:741\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    726\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    727\u001b[0m         )\n\u001b[1;32m    728\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    729\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    730\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    739\u001b[0m         )\n\u001b[1;32m    740\u001b[0m     ]\n\u001b[0;32m--> 741\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/DiPyCodeAssistant/llm_env/lib/python3.10/site-packages/langchain_core/language_models/llms.py:605\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    604\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    606\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/Desktop/DiPyCodeAssistant/llm_env/lib/python3.10/site-packages/langchain_core/language_models/llms.py:592\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    584\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    591\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 592\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    596\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    600\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    601\u001b[0m         )\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Desktop/DiPyCodeAssistant/llm_env/lib/python3.10/site-packages/langchain_core/language_models/llms.py:1177\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m   1176\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1177\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1180\u001b[0m     )\n\u001b[1;32m   1181\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/Desktop/DiPyCodeAssistant/llm_env/lib/python3.10/site-packages/langchain_community/llms/llamacpp.py:288\u001b[0m, in \u001b[0;36mLlamaCpp._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreaming:\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;66;03m# If streaming is enabled, we use the stream\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# method that yields as they are generated\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# and return the combined strings from the first choices's text:\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     combined_text_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\n\u001b[1;32m    289\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m    290\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    291\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    293\u001b[0m     ):\n\u001b[1;32m    294\u001b[0m         combined_text_output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined_text_output\n",
      "File \u001b[0;32m~/Desktop/DiPyCodeAssistant/llm_env/lib/python3.10/site-packages/langchain_community/llms/llamacpp.py:341\u001b[0m, in \u001b[0;36mLlamaCpp._stream\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_parameters(stop), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m    340\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient(prompt\u001b[38;5;241m=\u001b[39mprompt, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 341\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[1;32m    342\u001b[0m     logprobs \u001b[38;5;241m=\u001b[39m part[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    343\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m GenerationChunk(\n\u001b[1;32m    344\u001b[0m         text\u001b[38;5;241m=\u001b[39mpart[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    345\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs},\n\u001b[1;32m    346\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/DiPyCodeAssistant/llm_env/lib/python3.10/site-packages/llama_cpp/llama.py:953\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ctx\u001b[38;5;241m.\u001b[39mreset_timings()\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prompt_tokens) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_ctx:\n\u001b[0;32m--> 953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    954\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested tokens (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(prompt_tokens)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exceed context window of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllama_cpp\u001b[38;5;241m.\u001b[39mllama_n_ctx(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    955\u001b[0m     )\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m max_tokens \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;66;03m# Unlimited, depending on n_ctx.\u001b[39;00m\n\u001b[1;32m    959\u001b[0m     max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_ctx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(prompt_tokens)\n",
      "\u001b[0;31mValueError\u001b[0m: Requested tokens (32085) exceed context window of 5000"
     ]
    }
   ],
   "source": [
    "leaf_texts = docs_texts\n",
    "results = recursive_embed_cluster_summarize(leaf_texts, level=1, n_levels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Corrective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 112 tensors from /home/aajais/Desktop/DiPyCodeAssistant/llama.cpp/models/nomic-embed-text-v1.5.Q5_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert\n",
      "llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5\n",
      "llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12\n",
      "llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768\n",
      "llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072\n",
      "llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12\n",
      "llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000\n",
      "llama_model_loader: - kv   8:                          general.file_type u32              = 16\n",
      "llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false\n",
      "llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1\n",
      "llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000\n",
      "llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2\n",
      "llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101\n",
      "llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = [\"[PAD]\", \"[unused0]\", \"[unused1]\", \"...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100\n",
      "llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   51 tensors\n",
      "llama_model_loader: - type q5_K:   61 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 7104/30522 vs 5/30522 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = nomic-bert\n",
      "llm_load_print_meta: vocab type       = WPM\n",
      "llm_load_print_meta: n_vocab          = 30522\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 768\n",
      "llm_load_print_meta: n_head           = 12\n",
      "llm_load_print_meta: n_head_kv        = 12\n",
      "llm_load_print_meta: n_layer          = 12\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 768\n",
      "llm_load_print_meta: n_embd_v_gqa     = 768\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-12\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 3072\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 1\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 137M\n",
      "llm_load_print_meta: model ftype      = Q5_K - Small\n",
      "llm_load_print_meta: model params     = 136.73 M\n",
      "llm_load_print_meta: model size       = 89.77 MiB (5.51 BPW) \n",
      "llm_load_print_meta: general.name     = nomic-embed-text-v1.5\n",
      "llm_load_print_meta: BOS token        = 101 '[CLS]'\n",
      "llm_load_print_meta: EOS token        = 102 '[SEP]'\n",
      "llm_load_print_meta: UNK token        = 100 '[UNK]'\n",
      "llm_load_print_meta: SEP token        = 102 '[SEP]'\n",
      "llm_load_print_meta: PAD token        = 0 '[PAD]'\n",
      "llm_load_tensors: ggml ctx size =    0.04 MiB\n",
      "llm_load_tensors:        CPU buffer size =    89.77 MiB\n",
      ".......................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 1000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     3.51 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    21.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.seperator_token_id': '102', 'tokenizer.ggml.unknown_token_id': '100', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'bert', 'tokenizer.ggml.eos_token_id': '102', 'tokenizer.ggml.bos_token_id': '101', 'general.architecture': 'nomic-bert', 'general.name': 'nomic-embed-text-v1.5', 'nomic-bert.block_count': '12', 'nomic-bert.pooling_type': '1', 'nomic-bert.attention.head_count': '12', 'tokenizer.ggml.token_type_count': '2', 'nomic-bert.context_length': '2048', 'nomic-bert.rope.freq_base': '1000.000000', 'nomic-bert.embedding_length': '768', 'nomic-bert.feed_forward_length': '3072', 'nomic-bert.attention.layer_norm_epsilon': '0.000000', 'general.file_type': '16', 'nomic-bert.attention.causal': 'false'}\n",
      "Using fallback chat format: None\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     224.04 ms /    71 tokens (    3.16 ms per token,   316.91 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     350.02 ms /    72 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     139.35 ms /    55 tokens (    2.53 ms per token,   394.68 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     139.22 ms /    56 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     190.83 ms /    66 tokens (    2.89 ms per token,   345.85 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     190.59 ms /    67 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     269.77 ms /    91 tokens (    2.96 ms per token,   337.32 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     270.19 ms /    92 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     876.09 ms /   352 tokens (    2.49 ms per token,   401.78 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     876.39 ms /   353 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1256.47 ms /   433 tokens (    2.90 ms per token,   344.62 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1257.97 ms /   434 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1322.83 ms /   462 tokens (    2.86 ms per token,   349.25 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1323.91 ms /   463 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1326.17 ms /   462 tokens (    2.87 ms per token,   348.37 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1326.35 ms /   463 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     932.04 ms /   327 tokens (    2.85 ms per token,   350.84 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     931.79 ms /   328 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1451.67 ms /   460 tokens (    3.16 ms per token,   316.88 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1452.30 ms /   461 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1332.81 ms /   479 tokens (    2.78 ms per token,   359.39 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1333.47 ms /   480 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1213.68 ms /   377 tokens (    3.22 ms per token,   310.63 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1214.66 ms /   378 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1167.87 ms /   408 tokens (    2.86 ms per token,   349.35 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1169.24 ms /   409 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1200.27 ms /   423 tokens (    2.84 ms per token,   352.42 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1201.27 ms /   424 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1095.10 ms /   396 tokens (    2.77 ms per token,   361.61 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1095.41 ms /   397 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     986.21 ms /   376 tokens (    2.62 ms per token,   381.26 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     986.48 ms /   377 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1100.25 ms /   396 tokens (    2.78 ms per token,   359.92 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1101.01 ms /   397 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1488.14 ms /   457 tokens (    3.26 ms per token,   307.10 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1488.98 ms /   458 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1773.45 ms /   442 tokens (    4.01 ms per token,   249.23 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1774.70 ms /   443 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1555.28 ms /   444 tokens (    3.50 ms per token,   285.48 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1555.91 ms /   445 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1631.51 ms /   377 tokens (    4.33 ms per token,   231.07 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1632.45 ms /   378 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1637.12 ms /   401 tokens (    4.08 ms per token,   244.94 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1637.85 ms /   402 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1655.12 ms /   384 tokens (    4.31 ms per token,   232.01 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1655.39 ms /   385 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1113.94 ms /   428 tokens (    2.60 ms per token,   384.22 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1115.54 ms /   429 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1042.54 ms /   402 tokens (    2.59 ms per token,   385.60 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1043.17 ms /   403 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1423.20 ms /   389 tokens (    3.66 ms per token,   273.33 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1423.62 ms /   390 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1163.55 ms /   389 tokens (    2.99 ms per token,   334.32 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1164.28 ms /   390 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1116.85 ms /   409 tokens (    2.73 ms per token,   366.21 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1117.60 ms /   410 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1254.30 ms /   460 tokens (    2.73 ms per token,   366.74 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1255.75 ms /   461 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1103.41 ms /   431 tokens (    2.56 ms per token,   390.61 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1104.69 ms /   432 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1352.66 ms /   474 tokens (    2.85 ms per token,   350.42 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1353.63 ms /   475 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1074.52 ms /   430 tokens (    2.50 ms per token,   400.18 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1074.89 ms /   431 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1126.19 ms /   445 tokens (    2.53 ms per token,   395.14 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1126.81 ms /   446 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1286.01 ms /   469 tokens (    2.74 ms per token,   364.70 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1286.92 ms /   470 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1207.34 ms /   451 tokens (    2.68 ms per token,   373.55 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1208.26 ms /   452 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1175.06 ms /   445 tokens (    2.64 ms per token,   378.70 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1176.44 ms /   446 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1093.24 ms /   434 tokens (    2.52 ms per token,   396.98 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1094.32 ms /   435 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     727.52 ms /   303 tokens (    2.40 ms per token,   416.49 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     727.84 ms /   304 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1299.42 ms /   496 tokens (    2.62 ms per token,   381.71 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1299.79 ms /   497 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1498.34 ms /   471 tokens (    3.18 ms per token,   314.35 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1499.26 ms /   472 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     841.70 ms /   340 tokens (    2.48 ms per token,   403.94 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     842.48 ms /   341 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1103.70 ms /   445 tokens (    2.48 ms per token,   403.19 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1104.41 ms /   446 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1055.47 ms /   424 tokens (    2.49 ms per token,   401.72 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1055.97 ms /   425 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     961.87 ms /   396 tokens (    2.43 ms per token,   411.70 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     961.78 ms /   397 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1239.78 ms /   464 tokens (    2.67 ms per token,   374.26 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1240.80 ms /   465 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1191.94 ms /   452 tokens (    2.64 ms per token,   379.21 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1192.41 ms /   453 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1171.33 ms /   451 tokens (    2.60 ms per token,   385.03 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1172.03 ms /   452 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1288.53 ms /   450 tokens (    2.86 ms per token,   349.23 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1289.37 ms /   451 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1366.36 ms /   488 tokens (    2.80 ms per token,   357.15 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1366.34 ms /   489 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1082.05 ms /   416 tokens (    2.60 ms per token,   384.46 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1082.46 ms /   417 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1066.02 ms /   404 tokens (    2.64 ms per token,   378.98 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1066.37 ms /   405 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1301.38 ms /   482 tokens (    2.70 ms per token,   370.38 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1301.91 ms /   483 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1358.22 ms /   447 tokens (    3.04 ms per token,   329.11 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1358.26 ms /   448 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1255.55 ms /   486 tokens (    2.58 ms per token,   387.08 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1256.31 ms /   487 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1267.73 ms /   439 tokens (    2.89 ms per token,   346.29 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1268.41 ms /   440 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1643.75 ms /   406 tokens (    4.05 ms per token,   247.00 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1644.78 ms /   407 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     842.48 ms /   326 tokens (    2.58 ms per token,   386.95 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     843.42 ms /   327 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1394.67 ms /   452 tokens (    3.09 ms per token,   324.09 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1395.37 ms /   453 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1052.03 ms /   376 tokens (    2.80 ms per token,   357.40 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1051.70 ms /   377 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1448.72 ms /   512 tokens (    2.83 ms per token,   353.42 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1450.61 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1182.00 ms /   432 tokens (    2.74 ms per token,   365.48 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1183.51 ms /   433 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1260.68 ms /   413 tokens (    3.05 ms per token,   327.60 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1262.04 ms /   414 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     870.35 ms /   345 tokens (    2.52 ms per token,   396.39 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     871.09 ms /   346 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1019.59 ms /   400 tokens (    2.55 ms per token,   392.31 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1020.68 ms /   401 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1390.90 ms /   401 tokens (    3.47 ms per token,   288.30 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1391.57 ms /   402 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1037.41 ms /   398 tokens (    2.61 ms per token,   383.65 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1038.56 ms /   399 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1034.44 ms /   397 tokens (    2.61 ms per token,   383.78 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1035.21 ms /   398 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1256.44 ms /   435 tokens (    2.89 ms per token,   346.22 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1256.44 ms /   436 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1006.66 ms /   392 tokens (    2.57 ms per token,   389.41 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1006.98 ms /   393 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1122.33 ms /   401 tokens (    2.80 ms per token,   357.29 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1123.12 ms /   402 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1057.47 ms /   397 tokens (    2.66 ms per token,   375.43 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1057.64 ms /   398 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1381.77 ms /   445 tokens (    3.11 ms per token,   322.05 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1382.47 ms /   446 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1877.01 ms /   512 tokens (    3.67 ms per token,   272.77 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1878.22 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    2295.55 ms /   474 tokens (    4.84 ms per token,   206.49 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    2296.20 ms /   475 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    2016.45 ms /   449 tokens (    4.49 ms per token,   222.67 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    2017.71 ms /   450 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1690.89 ms /   417 tokens (    4.05 ms per token,   246.62 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1691.08 ms /   418 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1638.85 ms /   450 tokens (    3.64 ms per token,   274.58 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1639.66 ms /   451 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     798.93 ms /   206 tokens (    3.88 ms per token,   257.84 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     798.72 ms /   207 tokens\n",
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     383.81 ms /   104 tokens (    3.69 ms per token,   270.97 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     384.12 ms /   105 tokens\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "# from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.embeddings import LlamaCppEmbeddings\n",
    "\n",
    "# Load\n",
    "url = \"https://docs.dipy.org/stable/reference/index\"\n",
    "loader = WebBaseLoader(url)\n",
    "docs = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=100\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed and index\n",
    "    # GPT4All\n",
    "    # embedding = GPT4AllEmbeddings()\n",
    "    # Nomic v1 or v1.5\n",
    "embd_model_path = \"/home/aajais/Desktop/DiPyCodeAssistant/llama.cpp/models/nomic-embed-text-v1.5.Q5_K_S.gguf\"\n",
    "embedding = LlamaCppEmbeddings(model_path=embd_model_path, n_batch=512)\n",
    "\n",
    "# Index\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=embedding,\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Dict, TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        keys: A dictionary where each key is a string.\n",
    "    \"\"\"\n",
    "\n",
    "    keys: Dict[str, any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "### Nodes ###\n",
    "\n",
    "\n",
    "def retrieve(state, retriever):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    local = state_dict[\"local\"]\n",
    "    documents = retriever.get_relevant_documents(question)\n",
    "    return {\"keys\": {\"documents\": documents, \"local\": local, \"question\": question}}\n",
    "\n",
    "\n",
    "def generate(state, llm):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    local = state_dict[\"local\"]\n",
    "\n",
    "    # Prompt\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # Post-processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\n",
    "        \"keys\": {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "    }\n",
    "\n",
    "\n",
    "def grade_documents(state, llm):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    local = state_dict[\"local\"]\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keywords related to the user question, grade it as relevant. \\n\n",
    "        It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "        Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\"\"\",\n",
    "        input_variables=[\"question\", \"context\"],\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm | JsonOutputParser()\n",
    "\n",
    "    # Score\n",
    "    filtered_docs = []\n",
    "    search = \"No\"  # Default do not opt for web search to supplement retrieval\n",
    "    for d in documents:\n",
    "        score = chain.invoke(\n",
    "            {\n",
    "                \"question\": question,\n",
    "                \"context\": d.page_content,\n",
    "            }\n",
    "        )\n",
    "        grade = score[\"score\"]\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            search = \"Yes\"  # Perform web search\n",
    "            continue\n",
    "\n",
    "    return {\n",
    "        \"keys\": {\n",
    "            \"documents\": filtered_docs,\n",
    "            \"question\": question,\n",
    "            \"local\": local,\n",
    "            \"run_web_search\": search,\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def transform_query(state, llm):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    local = state_dict[\"local\"]\n",
    "\n",
    "    # Create a prompt template with format instructions and the query\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are generating questions that is well optimized for retrieval. \\n \n",
    "        Look at the input and try to reason about the underlying sematic intent / meaning. \\n \n",
    "        Here is the initial question:\n",
    "        \\n ------- \\n\n",
    "        {question} \n",
    "        \\n ------- \\n\n",
    "        Provide an improved question without any premable, only respond with the updated question: \"\"\",\n",
    "        input_variables=[\"question\"],\n",
    "    )\n",
    "\n",
    "    # Prompt\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    better_question = chain.invoke({\"question\": question})\n",
    "\n",
    "    return {\n",
    "        \"keys\": {\"documents\": documents, \"question\": better_question, \"local\": local}\n",
    "    }\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question using Tavily API.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Web results appended to documents.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    local = state_dict[\"local\"]\n",
    "\n",
    "    tool = TavilySearchResults()\n",
    "    docs = tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "\n",
    "    return {\"keys\": {\"documents\": documents, \"local\": local, \"question\": question}}\n",
    "\n",
    "\n",
    "### Edges\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer or re-generate a question for web search.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current state of the agent, including all keys.\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---DECIDE TO GENERATE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    filtered_documents = state_dict[\"documents\"]\n",
    "    search = state_dict[\"run_web_search\"]\n",
    "\n",
    "    if search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: TRANSFORM QUERY and RUN WEB SEARCH---\")\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/home/aajais/Desktop/DiPyCodeAssistant/model/codellama-13b-instruct.Q4_K_M.gguf\",\n",
    "    n_ctx=5000,\n",
    "    n_gpu_layers=4,\n",
    "    n_batch=512,\n",
    "    f16_kv=True,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", lambda state: retrieve(state, retriever=retriever))  # retrieve\n",
    "workflow.add_node(\"grade_documents\", lambda state: grade_documents(state, llm=llm))  # grade documents\n",
    "workflow.add_node(\"generate\", lambda state: generate(state, llm=llm))  # generatae\n",
    "workflow.add_node(\"transform_query\", lambda state: transform_query(state, llm=llm))  # transform_query\n",
    "workflow.add_node(\"web_search\", web_search)  # web search\n",
    "\n",
    "# Build graph\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"web_search\")\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     350.32 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1473.34 ms /    12 tokens (  122.78 ms per token,     8.14 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1546.80 ms /    13 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK RELEVANCE---\n",
      " The JSON should simply be {\"score\": yes/no"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\n",
    "    \"keys\": {\n",
    "        \"question\": \"Explain how the different types of agent memory work?\",\n",
    "        \"local\": \"yes\",\n",
    "    }\n",
    "}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint.pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint.pprint(value[\"keys\"][\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_newlines(text):\n",
    "    \"\"\"\n",
    "    Remove all newline characters and replace them with a single newline character.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text to clean.\n",
    "\n",
    "    Returns:\n",
    "    str: The cleaned text with single newline characters.\n",
    "    \"\"\"\n",
    "    return \"\\n\".join(line.strip() for line in text.splitlines() if line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from bs4 import BeautifulSoup as Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index\n",
    "url = \"https://docs.dipy.org/stable/reference/index.html\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "docs = dict(docs[0]).get('page_content')\n",
    "docs = clean_newlines(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://docs.dipy.org/stable/examples_built/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=2, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs_tutorials = loader.load()\n",
    "cleaned_docs_tutorials = []\n",
    "for doc_id in range(len(docs_tutorials)):\n",
    "    # docs_tutorials = dict(docs_tutorials[0]).get('page_content')\n",
    "    cleaned_docs_tutorials.append(clean_newlines(docs_tutorials[doc_id].page_content)[5167:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cleaned_docs_tutorials[8][:-700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test - Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_community.document_loaders import DirectoryLoader,TextLoader\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 646/647 [00:00<00:00, 4290.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files loaded: 646\n",
      "Number of documents : 5653\n"
     ]
    }
   ],
   "source": [
    "src_dir = r\"C:\\Users\\Aayush\\OneDrive\\Desktop\\projects\\dipy_code_assistant\\converted_codebase\"\n",
    "loader = DirectoryLoader(src_dir, show_progress=True, loader_cls=TextLoader)\n",
    "repo_files = loader.load()\n",
    "print(f\"Number of files loaded: {len(repo_files)}\")\n",
    "#\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1712, chunk_overlap=500)\n",
    "documents = text_splitter.split_documents(documents=repo_files)\n",
    "print(f\"Number of documents : {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='project(\\n  \\'dipy\\',\\n  \\'c\\', \\'cpp\\', \\'cython\\',\\n  version: \\'1.10.0dev\\',\\n  license: \\'BSD-3\\',\\n  meson_version: \\'>= 1.1.0\\',\\n  default_options: [\\n    \\'buildtype=debugoptimized\\',\\n    \\'c_std=c99\\',\\n    \\'cpp_std=c++14\\',\\n    \\'optimization=2\\',\\n  ],\\n)\\n\\n# https://mesonbuild.com/Python-module.html\\npy_mod = import(\\'python\\')\\npy3 = py_mod.find_installation(pure: false)\\npy3_dep = py3.dependency()\\n\\n# filesystem Module to manage files and directories\\nfs = import(\\'fs\\')\\n\\ncc = meson.get_compiler(\\'c\\')\\ncpp = meson.get_compiler(\\'cpp\\')\\ncy = meson.get_compiler(\\'cython\\')\\nhost_system = host_machine.system()\\nhost_cpu_family = host_machine.cpu_family()\\n\\ncython = find_program(\\'cython\\')\\n\\n# Check compiler is recent enough (see \"Toolchain Roadmap\" for details)\\nif cc.get_id() == \\'gcc\\'\\n  if not cc.version().version_compare(\\'>=8.0\\')\\n    error(\\'DIPY requires GCC >= 8.0\\')\\n  endif\\nelif cc.get_id() == \\'msvc\\'\\n  if not cc.version().version_compare(\\'>=19.20\\')\\n    error(\\'DIPY requires at least vc142 (default with Visual Studio 2019) \\' + \\\\\\n          \\'when building with MSVC\\')\\n  endif\\nendif\\nif not cy.version().version_compare(\\'>=0.29.35\\')\\n  error(\\'DIPY requires Cython >= 0.29.35\\')\\nendif\\n\\n\\n# TODO: the below -Wno flags are all needed to silence warnings in\\n# f2py-generated code. This should be fixed in f2py itself.\\n#_global_c_args = cc.get_supported_arguments(\\n#  \\'-Wno-unused-but-set-variable\\',\\n#  \\'-Wno-unused-function\\',\\n#  \\'-Wno-conversion\\',\\n#  \\'-Wno-misleading-indentation\\',\\n#  \\'-Wno-incompatible-pointer-types\\',\\n#)\\n#add_project_arguments(_global_c_args, language : \\'c\\')', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\meson.build.txt'}),\n",
       " Document(page_content=\"# TODO: the below -Wno flags are all needed to silence warnings in\\n# f2py-generated code. This should be fixed in f2py itself.\\n#_global_c_args = cc.get_supported_arguments(\\n#  '-Wno-unused-but-set-variable',\\n#  '-Wno-unused-function',\\n#  '-Wno-conversion',\\n#  '-Wno-misleading-indentation',\\n#  '-Wno-incompatible-pointer-types',\\n#)\\n#add_project_arguments(_global_c_args, language : 'c')\\n\\n# We need -lm for all C code (assuming it uses math functions, which is safe to\\n# assume for dipy). For C++ it isn't needed, because libstdc++/libc++ is\\n# guaranteed to depend on it.\\nm_dep = cc.find_library('m', required : false)\\nif m_dep.found()\\n  add_project_link_arguments('-lm', language : 'c')\\nendif\\n\\nsubdir('dipy')\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\meson.build.txt'}),\n",
       " Document(page_content=\".. image:: doc/_static/images/logos/dipy-logo.png\\n  :height: 180px\\n  :target: http://dipy.org\\n  :alt: DIPY - Diffusion Imaging in Python\\n\\n|\\n\\n.. image:: https://github.com/dipy/dipy/actions/workflows/test.yml/badge.svg?branch=master\\n  :target: https://github.com/dipy/dipy/actions/workflows/test.yml\\n\\n.. image:: https://codecov.io/gh/dipy/dipy/branch/master/graph/badge.svg\\n  :target: https://codecov.io/gh/dipy/dipy\\n\\n.. image:: https://img.shields.io/pypi/v/dipy.svg\\n  :target: https://pypi.python.org/pypi/dipy\\n\\n.. image:: https://anaconda.org/conda-forge/dipy/badges/platforms.svg\\n  :target: https://anaconda.org/conda-forge/dipy\\n\\n.. image:: https://anaconda.org/conda-forge/dipy/badges/downloads.svg\\n  :target: https://anaconda.org/conda-forge/dipy\\n\\n.. image:: https://img.shields.io/badge/License-BSD%203--Clause-blue.svg\\n  :target: https://github.com/dipy/dipy/blob/master/LICENSE\\n\\n\\nDIPY [DIPYREF]_ is a python library for the analysis of MR diffusion imaging.\\n\\nDIPY is for research only; please contact admins@dipy.org if you plan to deploy\\nin clinical settings.\\n\\nWebsite\\n=======\\n\\nCurrent information can always be found from the DIPY website - http://dipy.org\\n\\nMailing Lists\\n=============\\n\\nPlease see the DIPY community list at\\nhttps://mail.python.org/mailman3/lists/dipy.python.org/\\n\\nPlease see the users' forum at\\nhttps://github.com/dipy/dipy/discussions\\n\\nPlease join the gitter chatroom `here <https://gitter.im/dipy/dipy>`_.\\n\\nCode\\n====\\n\\nYou can find our sources and single-click downloads:\\n\\n* `Main repository`_ on Github.\\n* Documentation_ for all releases and current development tree.\\n* Download as a tar/zip file the `current trunk`_.\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\README.rst.txt'}),\n",
       " Document(page_content=\"Mailing Lists\\n=============\\n\\nPlease see the DIPY community list at\\nhttps://mail.python.org/mailman3/lists/dipy.python.org/\\n\\nPlease see the users' forum at\\nhttps://github.com/dipy/dipy/discussions\\n\\nPlease join the gitter chatroom `here <https://gitter.im/dipy/dipy>`_.\\n\\nCode\\n====\\n\\nYou can find our sources and single-click downloads:\\n\\n* `Main repository`_ on Github.\\n* Documentation_ for all releases and current development tree.\\n* Download as a tar/zip file the `current trunk`_.\\n\\n.. _main repository: http://github.com/dipy/dipy\\n.. _Documentation: http://dipy.org\\n.. _current trunk: http://github.com/dipy/dipy/archives/master\\n\\n\\nInstalling DIPY\\n===============\\n\\nDIPY can be installed using `pip`::\\n\\n    pip install dipy\\n\\nor using `conda`::\\n\\n    conda install -c conda-forge dipy\\n\\nFor detailed installation instructions, including instructions for installing\\nfrom source, please read our `installation documentation <https://docs.dipy.org/stable/user_guide/installation.html>`_.\\n\\nPython versions and dependencies\\n--------------------------------\\n\\nDIPY follows the `Scientific Python`_ `SPEC 0  Minimum Supported Versions`_\\nrecommendation as closely as possible, including the supported Python and\\ndependencies versions.\\n\\nFurther information can be found in `Toolchain Roadmap <https://docs.dipy.org/stable/devel/toolchain.html>`_.\\n\\nLicense\\n=======\\n\\nDIPY is licensed under the terms of the BSD license.\\nPlease see the `LICENSE file <https://github.com/dipy/dipy/blob/master/LICENSE>`_.\\n\\nContributing\\n============\\n\\nWe welcome contributions from the community. Please read our `Contributing guidelines <https://github.com/dipy/dipy/blob/master/.github/CONTRIBUTING.md>`_.\\n\\nReference\\n=========\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\README.rst.txt'}),\n",
       " Document(page_content='Further information can be found in `Toolchain Roadmap <https://docs.dipy.org/stable/devel/toolchain.html>`_.\\n\\nLicense\\n=======\\n\\nDIPY is licensed under the terms of the BSD license.\\nPlease see the `LICENSE file <https://github.com/dipy/dipy/blob/master/LICENSE>`_.\\n\\nContributing\\n============\\n\\nWe welcome contributions from the community. Please read our `Contributing guidelines <https://github.com/dipy/dipy/blob/master/.github/CONTRIBUTING.md>`_.\\n\\nReference\\n=========\\n\\n.. [DIPYREF] E. Garyfallidis, M. Brett, B. Amirbekian, A. Rokem,\\n    S. Van Der Walt, M. Descoteaux, I. Nimmo-Smith and DIPY contributors,\\n    \"DIPY, a library for the analysis of diffusion MRI data\",\\n    Frontiers in Neuroinformatics, vol. 8, p. 8, Frontiers, 2014.\\n\\n\\n.. _`Scientific Python`: https://scientific-python.org/\\n.. _`SPEC 0  Minimum Supported Versions`: https://scientific-python.org/specs/spec-0000/', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\README.rst.txt'}),\n",
       " Document(page_content='# Check against .travis.yml file and dipy/info.py\\ncython>=0.29.35, !=0.29.29\\nnumpy>=1.22.4\\nscipy>=1.8.1\\nnibabel>=4.0.0\\nh5py>=3.7.0\\npackaging>=19.0\\ntqdm>=4.30.0\\ntrx-python>=0.2.9', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\requirements.txt.txt'}),\n",
       " Document(page_content='\"\"\"\\nThis file is used by asv_compare.conf.json.tpl.\\n\\nNote\\n----\\n\\nThis file is copied (possibly with major modifications) from the\\nsources of the numpy project - https://github.com/numpy/numpy.\\nIt remains licensed as the rest of NUMPY (BSD 3-Clause as of November 2023).\\n\\n# ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##\\n#\\n#   See COPYING file distributed along with the Numpy package for the\\n#   copyright and license terms.\\n#\\n# ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##\\n\"\"\"\\nimport subprocess\\nimport sys\\n\\n# pip ignores \\'--global-option\\' when pep517 is enabled therefore we disable it.\\ncmd = [sys.executable, \\'-mpip\\', \\'wheel\\', \\'--no-use-pep517\\']\\ntry:\\n    output = subprocess.check_output(cmd, stderr=subprocess.STDOUT, text=True)\\nexcept Exception as e:\\n    output = str(e.output)\\nif \"no such option\" in output:\\n    print(\"old version of pip, escape \\'--no-use-pep517\\'\")\\n    cmd.pop()\\n\\nsubprocess.run(cmd + sys.argv[1:])', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\benchmarks\\\\asv_pip_nopep517.py.txt'}),\n",
       " Document(page_content='=====================\\n DIPY Benchmarks \\n=====================\\n\\nBenchmarking Dipy with Airspeed Velocity (ASV). Measure the speed and performance of DIPY functions easily!\\n\\nPrerequisites \\n---------------------\\n\\nBefore you start, make sure you have ASV and installed:\\n\\n.. code-block:: bash\\n\\n    pip install asv\\n    pip install virtualenv\\n\\nGetting Started \\u200d\\n------------------\\n\\nDIPY Benchmarking is as easy as a piece of  with ASV. You don\\'t need to install a development version of DIPY into your current Python environment. ASV manages virtual environments and builds DIPY automatically.\\n\\nRunning Benchmarks \\n---------------------\\n\\nTo run all available benchmarks, navigate to the root DIPY directory at the command line and execute:\\n\\n.. code-block:: bash\\n\\n    spin bench\\n\\nThis command builds DIPY and runs all available benchmarks defined in the ``benchmarks/`` directory. Be patient; this could take a while as each benchmark is run multiple times to measure execution time distribution.\\n\\nFor local testing without replications, unleash the power of :\\n\\n.. code-block:: bash\\n\\n    cd benchmarks/\\n    export REGEXP=\"bench.*Ufunc\"\\n    asv run --dry-run --show-stderr --python=same --quick -b $REGEXP\\n\\nHere, ``$REGEXP`` is a regular expression used to match benchmarks, and ``--quick`` is used to avoid repetitions.\\n\\nTo run benchmarks from a particular benchmark module, such as ``bench_segment.py``, simply append the filename without the extension:\\n\\n.. code-block:: bash\\n\\n    spin bench -t bench_segment\\n\\nTo run a benchmark defined in a class, such as ``BenchQuickbundles`` from ``bench_segment.py``, show your benchmarking ninja skills:\\n\\n.. code-block:: bash', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\benchmarks\\\\README.rst.txt'}),\n",
       " Document(page_content=\"Here, ``$REGEXP`` is a regular expression used to match benchmarks, and ``--quick`` is used to avoid repetitions.\\n\\nTo run benchmarks from a particular benchmark module, such as ``bench_segment.py``, simply append the filename without the extension:\\n\\n.. code-block:: bash\\n\\n    spin bench -t bench_segment\\n\\nTo run a benchmark defined in a class, such as ``BenchQuickbundles`` from ``bench_segment.py``, show your benchmarking ninja skills:\\n\\n.. code-block:: bash\\n\\n    spin bench -t bench_segment.BenchQuickbundles\\n\\nComparing Results \\n--------------------\\n\\nTo compare benchmark results with another version/commit/branch, use the ``--compare`` option (or ``-c``):\\n\\n.. code-block:: bash\\n\\n    spin bench --compare v1.7.0 -t bench_segment\\n    spin bench --compare 20d03bcfd -t bench_segment\\n    spin bench -c master -t bench_segment\\n\\nThese commands display results in the console but don't save them for future comparisons. For greater control and to save results for future comparisons, use ASV commands:\\n\\n.. code-block:: bash\\n\\n    cd benchmarks\\n    asv run -n -e --python=same\\n    asv publish\\n    asv preview\\n\\nBenchmarking Versions \\n------------------------\\n\\nTo benchmark or visualize releases on different machines locally, generate tags with their commits:\\n\\n.. code-block:: bash\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\benchmarks\\\\README.rst.txt'}),\n",
       " Document(page_content='These commands display results in the console but don\\'t save them for future comparisons. For greater control and to save results for future comparisons, use ASV commands:\\n\\n.. code-block:: bash\\n\\n    cd benchmarks\\n    asv run -n -e --python=same\\n    asv publish\\n    asv preview\\n\\nBenchmarking Versions \\n------------------------\\n\\nTo benchmark or visualize releases on different machines locally, generate tags with their commits:\\n\\n.. code-block:: bash\\n\\n    cd benchmarks\\n    # Get commits for tags\\n    # delete tag_commits.txt before re-runs\\n    for gtag in $(git tag --list --sort taggerdate | grep \"^v\"); do\\n    git log $gtag --oneline -n1 --decorate=no | awk \\'{print $1;}\\' >> tag_commits.txt\\n    done\\n    # Use the last 20 versions for maximum power \\n    tail --lines=20 tag_commits.txt > 20_vers.txt\\n    asv run HASHFILE:20_vers.txt\\n    # Publish and view\\n    asv publish\\n    asv preview\\n\\nContributing \\n---------------\\n\\nTBD\\n\\nWriting Benchmarks \\n---------------------\\n\\nSee `ASV documentation<https://asv.readthedocs.io/>`__ for basics on how to write benchmarks.\\n\\nThings to consider:\\n\\n- The benchmark suite should be importable with multiple DIPY version.\\n- Benchmark parameters should not depend on which DIPY version is installed.\\n- Keep the runtime of the benchmark reasonable.\\n- Prefer ASV\\'s ``time_`` methods for benchmarking times.\\n- Prepare arrays in the setup method rather than in the ``time_`` methods.\\n- Be mindful of large arrays created.\\n\\nEmbrace the Speed! \\n---------------------\\n\\nNow you\\'re all set to benchmark DIPY with ASV and watch your code reach for the stars! Happy benchmarking! ', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\benchmarks\\\\README.rst.txt'}),\n",
       " Document(page_content='\"\"\" Benchmarks for``dipy.reconst`` module.\"\"\"\\n\\nimport numpy as np\\n\\nfrom dipy.core.sphere import unique_edges\\nfrom dipy.core.gradients import GradientTable\\nfrom dipy.data import default_sphere, read_stanford_labels\\nfrom dipy.io.image import load_nifti_data\\nfrom dipy.reconst.csdeconv import ConstrainedSphericalDeconvModel\\nfrom dipy.reconst.recspeed import local_maxima\\nfrom dipy.reconst.vec_val_sum import vec_val_vect\\n\\n\\nclass BenchRecSpeed:\\n\\n    def setup(self):\\n        vertices, faces = default_sphere.vertices, default_sphere.faces\\n        self.edges = unique_edges(faces)\\n        self.odf = np.zeros(len(vertices))\\n        self.odf[1] = 1.\\n        self.odf[143] = 143.\\n        self.odf[305] = 305.\\n\\n    def time_local_maxima(self):\\n        local_maxima(self.odf, self.edges)\\n\\n\\nclass BenchVecValSum:\\n\\n    def setup(self):\\n\\n        def make_vecs_vals(shape):\\n            return (np.random.randn(*shape),\\n                    np.random.randn(*(shape[:-2] + shape[-1:])))\\n\\n        shape = (10, 12, 3, 3)\\n        self.evecs, self.evals = make_vecs_vals(shape)\\n\\n    def time_vec_val_vect(self):\\n        vec_val_vect(self.evecs, self.evals)\\n\\n\\n# class BenchCSD:\\n\\n#     def setup(self):\\n#         img, self.gtab, labels_img = read_stanford_labels()\\n#         data = img.get_fdata()\\n\\n#         labels = labels_img.get_fdata()\\n#         shape = labels.shape\\n#         mask = np.in1d(labels, [1, 2])\\n#         mask.shape = shape\\n\\n#         center = (50, 40, 40)\\n#         width = 12\\n#         a, b, c = center\\n#         hw = width // 2\\n#         idx = (slice(a - hw, a + hw), slice(b - hw, b + hw),\\n#                slice(c - hw, c + hw))', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\benchmarks\\\\benchmarks\\\\bench_reconst.py.txt'}),\n",
       " Document(page_content='# class BenchCSD:\\n\\n#     def setup(self):\\n#         img, self.gtab, labels_img = read_stanford_labels()\\n#         data = img.get_fdata()\\n\\n#         labels = labels_img.get_fdata()\\n#         shape = labels.shape\\n#         mask = np.in1d(labels, [1, 2])\\n#         mask.shape = shape\\n\\n#         center = (50, 40, 40)\\n#         width = 12\\n#         a, b, c = center\\n#         hw = width // 2\\n#         idx = (slice(a - hw, a + hw), slice(b - hw, b + hw),\\n#                slice(c - hw, c + hw))\\n\\n#         self.data_small = data[idx].copy()\\n#         self.mask_small = mask[idx].copy()\\n#         voxels = self.mask_small.sum()\\n#         self.small_gtab = GradientTable(self.gtab.gradients[:75])\\n\\n\\n    # def time_csdeconv_basic(self):\\n    #     # TODO: define response and remove None\\n    #     sh_order_max = 8\\n    #     model = ConstrainedSphericalDeconvModel(self.gtab, None,\\n    #                                             sh_order_max=sh_order_max)\\n    #     model.fit(self.data_small, self.mask_small)\\n\\n    # def time_csdeconv_small_dataset(self):\\n    #      # TODO: define response and remove None\\n    #     # Smaller data set\\n    #     # data_small = data_small[..., :75].copy()\\n    #     sh_order_max = 8\\n    #     model = ConstrainedSphericalDeconvModel(self.small_gtab, None,\\n    #                                             sh_order_max=sh_order_max)\\n    #     model.fit(self.data_small, self.mask_small)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\benchmarks\\\\benchmarks\\\\bench_reconst.py.txt'}),\n",
       " Document(page_content='# def time_csdeconv_small_dataset(self):\\n    #      # TODO: define response and remove None\\n    #     # Smaller data set\\n    #     # data_small = data_small[..., :75].copy()\\n    #     sh_order_max = 8\\n    #     model = ConstrainedSphericalDeconvModel(self.small_gtab, None,\\n    #                                             sh_order_max=sh_order_max)\\n    #     model.fit(self.data_small, self.mask_small)\\n\\n    # def time_csdeconv_super_resolution(self):\\n    #      # TODO: define response and remove None\\n    #     # Super resolution\\n    #     sh_order_max = 12\\n    #     model = ConstrainedSphericalDeconvModel(self.gtab, None,\\n    #                                             sh_order_max=sh_order_max)\\n    #     model.fit(self.data_small, self.mask_small)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\benchmarks\\\\benchmarks\\\\bench_reconst.py.txt'}),\n",
       " Document(page_content='\"\"\" Benchmarks for ``dipy.segment`` module.\"\"\"\\n\\nimport numpy as np\\n\\nfrom dipy.data import get_fnames\\nfrom dipy.io.streamline import load_tractogram\\nfrom dipy.tracking.streamline import Streamlines, set_number_of_points\\nfrom dipy.segment.metricspeed import Metric\\nfrom dipy.segment.clustering import QuickBundles as QB_New\\nfrom dipy.segment.mask import bounding_box\\n\\n\\nclass BenchMask:\\n\\n    def setup(self):\\n        self.dense_vol = np.zeros((100, 100, 100))\\n        self.dense_vol[:] = 10\\n        self.sparse_vol = np.zeros((100, 100, 100))\\n        self.sparse_vol[0, 0, 0] = 1\\n\\n    def time_bounding_box_sparse(self):\\n        bounding_box(self.sparse_vol)\\n\\n    def time_bounding_box_dense(self):\\n        bounding_box(self.dense_vol)\\n\\n\\nclass BenchQuickbundles:\\n\\n    def setup(self):\\n        dtype = \"float32\"\\n        nb_points = 12\\n        # The expected number of clusters of the fornix using threshold=10\\n        # is 4.\\n        self.basic_parameters = {\"threshold\": 10,\\n                                 \"expected_nb_clusters\": 4 * 8,\\n                                 }\\n\\n        fname = get_fnames(\\'fornix\\')\\n\\n        fornix = load_tractogram(fname, \\'same\\',\\n                                 bbox_valid_check=False).streamlines\\n\\n        fornix_streamlines = Streamlines(fornix)\\n        fornix_streamlines = set_number_of_points(fornix_streamlines,\\n                                                  nb_points)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\benchmarks\\\\benchmarks\\\\bench_segment.py.txt'}),\n",
       " Document(page_content=\"fname = get_fnames('fornix')\\n\\n        fornix = load_tractogram(fname, 'same',\\n                                 bbox_valid_check=False).streamlines\\n\\n        fornix_streamlines = Streamlines(fornix)\\n        fornix_streamlines = set_number_of_points(fornix_streamlines,\\n                                                  nb_points)\\n\\n        # Create eight copies of the fornix to be clustered (one in\\n        # each octant).\\n        self.streamlines = []\\n        self.streamlines += [s + np.array([100, 100, 100], dtype)\\n                             for s in fornix_streamlines]\\n        self.streamlines += [s + np.array([100, -100, 100], dtype)\\n                             for s in fornix_streamlines]\\n        self.streamlines += [s + np.array([100, 100, -100], dtype)\\n                             for s in fornix_streamlines]\\n        self.streamlines += [s + np.array([100, -100, -100], dtype)\\n                             for s in fornix_streamlines]\\n        self.streamlines += [s + np.array([-100, 100, 100], dtype)\\n                             for s in fornix_streamlines]\\n        self.streamlines += [s + np.array([-100, -100, 100], dtype)\\n                             for s in fornix_streamlines]\\n        self.streamlines += [s + np.array([-100, 100, -100], dtype)\\n                             for s in fornix_streamlines]\\n        self.streamlines += [s + np.array([-100, -100, -100], dtype)\\n                             for s in fornix_streamlines]\\n\\n        class MDFpy(Metric):\\n            def are_compatible(self, shape1, shape2):\\n                return shape1 == shape2\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\benchmarks\\\\benchmarks\\\\bench_segment.py.txt'}),\n",
       " Document(page_content=\"class MDFpy(Metric):\\n            def are_compatible(self, shape1, shape2):\\n                return shape1 == shape2\\n\\n            def dist(self, features1, features2):\\n                dist = np.sqrt(np.sum((features1 - features2)**2, axis=1))\\n                dist = np.sum(dist / len(features1))\\n                return dist\\n\\n        self.custom_metric = MDFpy()\\n\\n    def time_quickbundles(self):\\n        qb2 = QB_New(self.basic_parameters.get('threshold', 10))\\n        _ = qb2.cluster(self.streamlines)\\n\\n    def time_quickbundles_metric(self):\\n        qb = QB_New(self.basic_parameters.get('threshold', 10),\\n                    metric=self.custom_metric)\\n        _ = qb.cluster(self.streamlines)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\benchmarks\\\\benchmarks\\\\bench_segment.py.txt'}),\n",
       " Document(page_content='\"\"\" Benchmarks for functions related to streamline in ``dipy.tracking``module.\\n\"\"\"\\n\\nimport numpy as np\\n\\nfrom dipy.data import get_fnames\\nfrom dipy.io.streamline import load_tractogram\\n\\nfrom dipy.tracking.streamline import set_number_of_points, length\\nfrom dipy.tracking.streamlinespeed import compress_streamlines\\n\\nfrom dipy.tracking import Streamlines\\n\\n\\nclass BenchStreamlines:\\n\\n    def setup(self):\\n        rng = np.random.RandomState(42)\\n        nb_streamlines = 20000\\n        min_nb_points = 2\\n        max_nb_points = 100\\n\\n        def generate_streamlines(nb_streamlines, min_nb_points,\\n                                 max_nb_points, rng):\\n            streamlines = \\\\\\n                [rng.rand(*(rng.randint(min_nb_points, max_nb_points), 3))\\n                 for _ in range(nb_streamlines)]\\n            return streamlines\\n\\n        self.data = {}\\n        self.data[\\'rng\\'] = rng\\n        self.data[\\'nb_streamlines\\'] = nb_streamlines\\n        self.data[\\'streamlines\\'] = generate_streamlines(\\n            nb_streamlines, min_nb_points, max_nb_points, rng=rng)\\n        self.data[\\'streamlines_arrseq\\'] = Streamlines(self.data[\\'streamlines\\'])\\n\\n        fname = get_fnames(\\'fornix\\')\\n        fornix = load_tractogram(fname, \\'same\\',\\n                                 bbox_valid_check=False).streamlines\\n\\n        self.fornix_streamlines = Streamlines(fornix)\\n\\n    def time_set_number_of_points(self):\\n        streamlines = self.data[\\'streamlines\\']\\n        set_number_of_points(streamlines, 50)\\n\\n    def time_set_number_of_points_arrseq(self):\\n        streamlines = self.data[\\'streamlines_arrseq\\']\\n        set_number_of_points(streamlines, 50)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\benchmarks\\\\benchmarks\\\\bench_tracking.py.txt'}),\n",
       " Document(page_content=\"fname = get_fnames('fornix')\\n        fornix = load_tractogram(fname, 'same',\\n                                 bbox_valid_check=False).streamlines\\n\\n        self.fornix_streamlines = Streamlines(fornix)\\n\\n    def time_set_number_of_points(self):\\n        streamlines = self.data['streamlines']\\n        set_number_of_points(streamlines, 50)\\n\\n    def time_set_number_of_points_arrseq(self):\\n        streamlines = self.data['streamlines_arrseq']\\n        set_number_of_points(streamlines, 50)\\n\\n    def time_length(self):\\n        streamlines = self.data['streamlines']\\n        length(streamlines)\\n\\n    def time_length_arrseq(self):\\n        streamlines = self.data['streamlines_arrseq']\\n        length(streamlines)\\n\\n    def time_compress_streamlines(self):\\n        compress_streamlines(self.fornix_streamlines)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\benchmarks\\\\benchmarks\\\\bench_tracking.py.txt'}),\n",
       " Document(page_content=\"# THIS FILE IS GENERATED DURING THE DIPY BUILD\\n# See tools/version_utils.py for details\\n\\nshort_version = '1.10.0'\\nversion = '1.10.0'\\nfull_version = '1.10.0.dev0+215.1ccbcae'\\ngit_revision = '1ccbcae'\\ncommit_count = '215'\\nrelease = False\\n\\nif not release:\\n    version = full_version\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\dipy\\\\version.py.txt'}),\n",
       " Document(page_content='\"\"\"\\nDiffusion Imaging in Python\\n============================\\n\\nFor more information, please visit https://dipy.org\\n\\nSubpackages\\n-----------\\n::\\n\\n align         -- Registration, streamline alignment, volume resampling\\n core          -- Spheres, gradient tables\\n core.geometry -- Spherical geometry, coordinate and vector manipulation\\n core.meshes   -- Point distributions on the sphere\\n data          -- Small testing datasets\\n denoise       -- Denoising algorithms\\n direction     -- Manage peaks and tracking\\n io            -- Loading/saving of dpy datasets\\n nn            -- Neural networks algorithms\\n reconst       -- Signal reconstruction modules (tensor, spherical harmonics,\\n                  diffusion spectrum, etc.)\\n segment       -- Tractography segmentation\\n sims          -- MRI phantom signal simulation\\n stats         -- Tractometry\\n tracking      -- Tractography, metrics for streamlines\\n viz           -- Visualization and GUIs\\n workflows      -- Predefined Command line for common tasks\\n\\nUtilities\\n---------\\n::\\n\\n test          -- Run unittests\\n __version__   -- Dipy version\\n\\n\"\"\"\\nimport sys\\n\\nfrom dipy.version import version as __version__\\n\\n# Plumb in version etc info stuff\\nfrom .pkg_info import get_pkg_info as _get_pkg_info\\n\\n\\ndef get_info():\\n    from os.path import dirname\\n    return _get_pkg_info(dirname(__file__))\\n\\n\\ndel sys\\n\\nsubmodules = [\\n    \\'align\\',\\n    \\'core\\',\\n    \\'data\\',\\n    \\'denoise\\',\\n    \\'direction\\',\\n    \\'io\\',\\n    \\'nn\\',\\n    \\'reconst\\',\\n    \\'segment\\',\\n    \\'sims\\',\\n    \\'stats\\',\\n    \\'tracking\\',\\n    \\'utils\\',\\n    \\'viz\\',\\n    \\'workflows\\',\\n    \\'tests\\',\\n    \\'testing\\'\\n]\\n\\n__all__ = submodules + [\\'__version__\\', \\'setup_test\\', \\'get_info\\']', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\dipy\\\\__init__.py.txt'}),\n",
       " Document(page_content='Build started at 2024-03-10T19:52:31.922444\\nMain binary: C:\\\\Users\\\\Aayush\\\\.conda\\\\envs\\\\wigner_test\\\\python.exe\\nBuild Options: -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md \\'--native-file=C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-python-native-file.ini\\'\\nPython system: Windows\\nThe Meson build system\\nVersion: 1.3.2\\nSource dir: C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\nBuild dir: C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\nBuild type: native build\\nProject name: dipy\\nProject version: 1.10.0dev\\nActivating VS 17.9.2\\nC compiler for the host machine: cl (msvc 19.39.33521 \"Microsoft (R) C/C++ Optimizing Compiler Version 19.39.33521 for x64\")\\nC linker for the host machine: link link 14.39.33521.0\\n-----------\\nDetecting archiver via: `lib /?` -> 1100\\nstdout:\\nMicrosoft (R) Library Manager Version 14.39.33521.0\\nCopyright (C) Microsoft Corporation.  All rights reserved.\\n\\nusage: LIB [options] [files]\\n\\n   options:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='usage: LIB [options] [files]\\n\\n   options:\\n\\n      /DEF[:filename]\\n      /ERRORREPORT:{NONE|PROMPT|QUEUE|SEND}\\n      /EXPORT:symbol\\n      /EXTRACT:membername\\n      /INCLUDE:symbol\\n      /LIBPATH:dir\\n      /LINKREPRO:dir\\n      /LINKREPROTARGET:filename\\n      /LIST[:filename]\\n      /LTCG\\n      /MACHINE:{ARM|ARM64|ARM64X|EBC|X64|X86}\\n      /NAME:filename\\n      /NODEFAULTLIB[:library]\\n      /NOLOGO\\n      /OUT:filename\\n      /REMOVE:membername\\n      /SUBSYSTEM:{BOOT_APPLICATION|CONSOLE|EFI_APPLICATION|\\n                  EFI_BOOT_SERVICE_DRIVER|EFI_ROM|EFI_RUNTIME_DRIVER|\\n                  NATIVE|POSIX|WINDOWS|WINDOWSCE}[,#[.##]]\\n      /VERBOSE\\n      /WX[:NO]\\n      /WX[:nnnn[,nnnn...]]\\n-----------\\nC++ compiler for the host machine: cl (msvc 19.39.33521 \"Microsoft (R) C/C++ Optimizing Compiler Version 19.39.33521 for x64\")\\nC++ linker for the host machine: link link 14.39.33521.0\\nCython compiler for the host machine: cython (cython 3.0.9)\\nC compiler for the build machine: cl (msvc 19.39.33521 \"Microsoft (R) C/C++ Optimizing Compiler Version 19.39.33521 for x64\")\\nC linker for the build machine: link link 14.39.33521.0\\n-----------\\nDetecting archiver via: `lib /?` -> 1100\\nstdout:\\nMicrosoft (R) Library Manager Version 14.39.33521.0\\nCopyright (C) Microsoft Corporation.  All rights reserved.\\n\\nusage: LIB [options] [files]\\n\\n   options:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='/DEF[:filename]\\n      /ERRORREPORT:{NONE|PROMPT|QUEUE|SEND}\\n      /EXPORT:symbol\\n      /EXTRACT:membername\\n      /INCLUDE:symbol\\n      /LIBPATH:dir\\n      /LINKREPRO:dir\\n      /LINKREPROTARGET:filename\\n      /LIST[:filename]\\n      /LTCG\\n      /MACHINE:{ARM|ARM64|ARM64X|EBC|X64|X86}\\n      /NAME:filename\\n      /NODEFAULTLIB[:library]\\n      /NOLOGO\\n      /OUT:filename\\n      /REMOVE:membername\\n      /SUBSYSTEM:{BOOT_APPLICATION|CONSOLE|EFI_APPLICATION|\\n                  EFI_BOOT_SERVICE_DRIVER|EFI_ROM|EFI_RUNTIME_DRIVER|\\n                  NATIVE|POSIX|WINDOWS|WINDOWSCE}[,#[.##]]\\n      /VERBOSE\\n      /WX[:NO]\\n      /WX[:nnnn[,nnnn...]]\\n-----------\\nC++ compiler for the build machine: cl (msvc 19.39.33521 \"Microsoft (R) C/C++ Optimizing Compiler Version 19.39.33521 for x64\")\\nC++ linker for the build machine: link link 14.39.33521.0\\nCython compiler for the build machine: cython (cython 3.0.9)\\nBuild machine cpu family: x86_64\\nBuild machine cpu: x86_64\\nHost machine cpu family: x86_64\\nHost machine cpu: x86_64\\nTarget machine cpu family: x86_64\\nTarget machine cpu: x86_64\\n\\'utf-8\\' codec can\\'t decode byte 0x90 in position 2: invalid start byte\\nUnusable script \\'C:\\\\\\\\Users\\\\\\\\Aayush\\\\\\\\.conda\\\\\\\\envs\\\\\\\\wigner_test\\\\\\\\python.exe\\'\\nProgram python found: YES (C:\\\\Users\\\\Aayush\\\\.conda\\\\envs\\\\wigner_test\\\\python.exe)\\nProgram cython found: YES (C:\\\\Users\\\\Aayush\\\\AppData\\\\Local\\\\Temp\\\\pip-build-env-x29_b_nj\\\\overlay\\\\Scripts\\\\cython.EXE)\\nLibrary m found: NO\\nUsing cached compile:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"Host machine cpu family: x86_64\\nHost machine cpu: x86_64\\nTarget machine cpu family: x86_64\\nTarget machine cpu: x86_64\\n'utf-8' codec can't decode byte 0x90 in position 2: invalid start byte\\nUnusable script 'C:\\\\\\\\Users\\\\\\\\Aayush\\\\\\\\.conda\\\\\\\\envs\\\\\\\\wigner_test\\\\\\\\python.exe'\\nProgram python found: YES (C:\\\\Users\\\\Aayush\\\\.conda\\\\envs\\\\wigner_test\\\\python.exe)\\nProgram cython found: YES (C:\\\\Users\\\\Aayush\\\\AppData\\\\Local\\\\Temp\\\\pip-build-env-x29_b_nj\\\\overlay\\\\Scripts\\\\cython.EXE)\\nLibrary m found: NO\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmp7iy3dd0z\\\\testfile.c /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmp7iy3dd0z\\\\output.obj /nologo /showIncludes /utf-8 /c /nologo /showIncludes /utf-8 /c /Od /Oi- -Wmaybe-uninitialized -Wno-maybe-uninitialized\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"Code:\\n extern int i;\\nint i;\\n\\nCached compiler stdout:\\n \\nCached compiler stderr:\\n cl : Command line error D8021 : invalid numeric argument '/Wmaybe-uninitialized'\\n\\nCompiler for C supports arguments -Wno-maybe-uninitialized: NO (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmp22w5jpp0\\\\testfile.c /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmp22w5jpp0\\\\output.obj /nologo /showIncludes /utf-8 /c /nologo /showIncludes /utf-8 /c /Od /Oi- -Wdiscarded-qualifiers -Wno-discarded-qualifiers \\n\\nCode:\\n extern int i;\\nint i;\\n\\nCached compiler stdout:\\n \\nCached compiler stderr:\\n cl : Command line error D8021 : invalid numeric argument '/Wdiscarded-qualifiers'\\n\\nCompiler for C supports arguments -Wno-discarded-qualifiers: NO (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmprsue7_rl\\\\testfile.c /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmprsue7_rl\\\\output.obj /nologo /showIncludes /utf-8 /c /nologo /showIncludes /utf-8 /c /Od /Oi- -Wempty-body -Wno-empty-body \\n\\nCode:\\n extern int i;\\nint i;\\n\\nCached compiler stdout:\\n \\nCached compiler stderr:\\n cl : Command line error D8021 : invalid numeric argument '/Wempty-body'\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"Code:\\n extern int i;\\nint i;\\n\\nCached compiler stdout:\\n \\nCached compiler stderr:\\n cl : Command line error D8021 : invalid numeric argument '/Wempty-body'\\n\\nCompiler for C supports arguments -Wno-empty-body: NO (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmp2q_tkly2\\\\testfile.c /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmp2q_tkly2\\\\output.obj /nologo /showIncludes /utf-8 /c /nologo /showIncludes /utf-8 /c /Od /Oi- -Wimplicit-function-declaration -Wno-implicit-function-declaration \\n\\nCode:\\n extern int i;\\nint i;\\n\\nCached compiler stdout:\\n \\nCached compiler stderr:\\n cl : Command line error D8021 : invalid numeric argument '/Wimplicit-function-declaration'\\n\\nCompiler for C supports arguments -Wno-implicit-function-declaration: NO (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmpqxhj9fax\\\\testfile.c /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmpqxhj9fax\\\\output.obj /nologo /showIncludes /utf-8 /c /nologo /showIncludes /utf-8 /c /Od /Oi- -Wparentheses -Wno-parentheses \\n\\nCode:\\n extern int i;\\nint i;\\n\\nCached compiler stdout:\\n \\nCached compiler stderr:\\n cl : Command line error D8021 : invalid numeric argument '/Wparentheses'\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"Code:\\n extern int i;\\nint i;\\n\\nCached compiler stdout:\\n \\nCached compiler stderr:\\n cl : Command line error D8021 : invalid numeric argument '/Wparentheses'\\n\\nCompiler for C supports arguments -Wno-parentheses: NO (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmp58siqlpj\\\\testfile.c /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmp58siqlpj\\\\output.obj /nologo /showIncludes /utf-8 /c /nologo /showIncludes /utf-8 /c /Od /Oi- -Wswitch -Wno-switch \\n\\nCode:\\n extern int i;\\nint i;\\n\\nCached compiler stdout:\\n \\nCached compiler stderr:\\n cl : Command line error D8021 : invalid numeric argument '/Wswitch'\\n\\nCompiler for C supports arguments -Wno-switch: NO (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmpjia_694g\\\\testfile.c /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmpjia_694g\\\\output.obj /nologo /showIncludes /utf-8 /c /nologo /showIncludes /utf-8 /c /Od /Oi- -Wunused-label -Wno-unused-label \\n\\nCode:\\n extern int i;\\nint i;\\n\\nCached compiler stdout:\\n \\nCached compiler stderr:\\n cl : Command line error D8021 : invalid numeric argument '/Wunused-label'\\n\\nCompiler for C supports arguments -Wno-unused-label: NO (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmp1_fr2hnv\\\\testfile.c /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmp1_fr2hnv\\\\output.obj /nologo /showIncludes /utf-8 /c /nologo /showIncludes /utf-8 /c /Od /Oi- -Wunused-variable -Wno-unused-variable \\n\\nCode:\\n extern int i;\\nint i;\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"Compiler for C supports arguments -Wno-unused-label: NO (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmp1_fr2hnv\\\\testfile.c /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmp1_fr2hnv\\\\output.obj /nologo /showIncludes /utf-8 /c /nologo /showIncludes /utf-8 /c /Od /Oi- -Wunused-variable -Wno-unused-variable \\n\\nCode:\\n extern int i;\\nint i;\\n\\nCached compiler stdout:\\n \\nCached compiler stderr:\\n cl : Command line error D8021 : invalid numeric argument '/Wunused-variable'\\n\\nCompiler for C supports arguments -Wno-unused-variable: NO (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmpnvxiidbm\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmpnvxiidbm\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wcpp -Wno-cpp \\n\\nCode:\\n extern int i;\\nint i;\\n\\nCached compiler stdout:\\n \\nCached compiler stderr:\\n cl : Command line error D8021 : invalid numeric argument '/Wcpp'\\n\\nCompiler for C++ supports arguments -Wno-cpp: NO (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmp_2jhjmop\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmp_2jhjmop\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wdeprecated-declarations -Wno-deprecated-declarations \\n\\nCode:\\n extern int i;\\nint i;\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"Compiler for C++ supports arguments -Wno-cpp: NO (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmp_2jhjmop\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmp_2jhjmop\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wdeprecated-declarations -Wno-deprecated-declarations \\n\\nCode:\\n extern int i;\\nint i;\\n\\nCached compiler stdout:\\n \\nCached compiler stderr:\\n cl : Command line error D8021 : invalid numeric argument '/Wdeprecated-declarations'\\n\\nCompiler for C++ supports arguments -Wno-deprecated-declarations: NO (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmpg7yvqmql\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmpg7yvqmql\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wclass-memaccess -Wno-class-memaccess \\n\\nCode:\\n extern int i;\\nint i;\\n\\nCached compiler stdout:\\n \\nCached compiler stderr:\\n cl : Command line error D8021 : invalid numeric argument '/Wclass-memaccess'\\n\\nCompiler for C++ supports arguments -Wno-class-memaccess: NO (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmpuusrwh1d\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmpuusrwh1d\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wformat-truncation -Wno-format-truncation\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"Compiler for C++ supports arguments -Wno-class-memaccess: NO (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmpuusrwh1d\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmpuusrwh1d\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wformat-truncation -Wno-format-truncation \\n\\nCode:\\n extern int i;\\nint i;\\n\\nCached compiler stdout:\\n \\nCached compiler stderr:\\n cl : Command line error D8021 : invalid numeric argument '/Wformat-truncation'\\n\\nCompiler for C++ supports arguments -Wno-format-truncation: NO (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmp9464prs0\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmp9464prs0\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wformat-extra-args -Wno-format-extra-args \\n\\nCode:\\n extern int i;\\nint i;\\n\\nCached compiler stdout:\\n \\nCached compiler stderr:\\n cl : Command line error D8021 : invalid numeric argument '/Wformat-extra-args'\\n\\nCompiler for C++ supports arguments -Wno-format-extra-args: NO (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmph3qin6_1\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmph3qin6_1\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wformat -Wno-format \\n\\nCode:\\n extern int i;\\nint i;\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"Compiler for C++ supports arguments -Wno-format-extra-args: NO (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmph3qin6_1\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmph3qin6_1\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wformat -Wno-format \\n\\nCode:\\n extern int i;\\nint i;\\n\\nCached compiler stdout:\\n \\nCached compiler stderr:\\n cl : Command line error D8021 : invalid numeric argument '/Wformat'\\n\\nCompiler for C++ supports arguments -Wno-format: NO (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmp6u0aqp5z\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmp6u0aqp5z\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wnon-virtual-dtor -Wno-non-virtual-dtor \\n\\nCode:\\n extern int i;\\nint i;\\n\\nCached compiler stdout:\\n \\nCached compiler stderr:\\n cl : Command line error D8021 : invalid numeric argument '/Wnon-virtual-dtor'\\n\\nCompiler for C++ supports arguments -Wno-non-virtual-dtor: NO (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmp5uet3421\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmp5uet3421\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wsign-compare -Wno-sign-compare \\n\\nCode:\\n extern int i;\\nint i;\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"Compiler for C++ supports arguments -Wno-non-virtual-dtor: NO (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmp5uet3421\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmp5uet3421\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wsign-compare -Wno-sign-compare \\n\\nCode:\\n extern int i;\\nint i;\\n\\nCached compiler stdout:\\n \\nCached compiler stderr:\\n cl : Command line error D8021 : invalid numeric argument '/Wsign-compare'\\n\\nCompiler for C++ supports arguments -Wno-sign-compare: NO (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmp2peq2cfh\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmp2peq2cfh\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wswitch -Wno-switch \\n\\nCode:\\n extern int i;\\nint i;\\n\\nCached compiler stdout:\\n \\nCached compiler stderr:\\n cl : Command line error D8021 : invalid numeric argument '/Wswitch'\\n\\nCompiler for C++ supports arguments -Wno-switch: NO (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmpfl6i48ps\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmpfl6i48ps\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wterminate -Wno-terminate \\n\\nCode:\\n extern int i;\\nint i;\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"Compiler for C++ supports arguments -Wno-switch: NO (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmpfl6i48ps\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmpfl6i48ps\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wterminate -Wno-terminate \\n\\nCode:\\n extern int i;\\nint i;\\n\\nCached compiler stdout:\\n \\nCached compiler stderr:\\n cl : Command line error D8021 : invalid numeric argument '/Wterminate'\\n\\nCompiler for C++ supports arguments -Wno-terminate: NO (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmptyvt2am9\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmptyvt2am9\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wunused-but-set-variable -Wno-unused-but-set-variable \\n\\nCode:\\n extern int i;\\nint i;\\n\\nCached compiler stdout:\\n \\nCached compiler stderr:\\n cl : Command line error D8021 : invalid numeric argument '/Wunused-but-set-variable'\\n\\nCompiler for C++ supports arguments -Wno-unused-but-set-variable: NO (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmphqjc13jm\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmphqjc13jm\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wunused-function -Wno-unused-function \\n\\nCode:\\n extern int i;\\nint i;\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"Compiler for C++ supports arguments -Wno-unused-but-set-variable: NO (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmphqjc13jm\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmphqjc13jm\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wunused-function -Wno-unused-function \\n\\nCode:\\n extern int i;\\nint i;\\n\\nCached compiler stdout:\\n \\nCached compiler stderr:\\n cl : Command line error D8021 : invalid numeric argument '/Wunused-function'\\n\\nCompiler for C++ supports arguments -Wno-unused-function: NO (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmppckci7s_\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmppckci7s_\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wunused-local-typedefs -Wno-unused-local-typedefs \\n\\nCode:\\n extern int i;\\nint i;\\n\\nCached compiler stdout:\\n \\nCached compiler stderr:\\n cl : Command line error D8021 : invalid numeric argument '/Wunused-local-typedefs'\\n\\nCompiler for C++ supports arguments -Wno-unused-local-typedefs: NO (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmpkgqw09qw\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmpkgqw09qw\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wunused-variable -Wno-unused-variable\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='Compiler for C++ supports arguments -Wno-unused-local-typedefs: NO (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmpkgqw09qw\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmpkgqw09qw\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wunused-variable -Wno-unused-variable \\n\\nCode:\\n extern int i;\\nint i;\\n\\nCached compiler stdout:\\n \\nCached compiler stderr:\\n cl : Command line error D8021 : invalid numeric argument \\'/Wunused-variable\\'\\n\\nCompiler for C++ supports arguments -Wno-unused-variable: NO (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmp8ci9_b7p\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmp8ci9_b7p\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wint-in-bool-context -Wno-int-in-bool-context \\n\\nCode:\\n extern int i;\\nint i;\\n\\nCached compiler stdout:\\n \\nCached compiler stderr:\\n cl : Command line error D8021 : invalid numeric argument \\'/Wint-in-bool-context\\'\\n\\nCompiler for C++ supports arguments -Wno-int-in-bool-context: NO (cached)\\nRunning command: C:\\\\Users\\\\Aayush\\\\.conda\\\\envs\\\\wigner_test\\\\python.exe -c \"\\nimport numpy as np\\ntry:\\n  incdir = os.path.relpath(np.get_include())\\nexcept Exception:\\n  incdir = np.get_include()\\nprint(incdir)\\n  \"\\n--- stdout ---\\nC:\\\\Users\\\\Aayush\\\\AppData\\\\Local\\\\Temp\\\\pip-build-env-x29_b_nj\\\\overlay\\\\Lib\\\\site-packages\\\\numpy\\\\core\\\\include\\n\\n--- stderr ---', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='Compiler for C++ supports arguments -Wno-int-in-bool-context: NO (cached)\\nRunning command: C:\\\\Users\\\\Aayush\\\\.conda\\\\envs\\\\wigner_test\\\\python.exe -c \"\\nimport numpy as np\\ntry:\\n  incdir = os.path.relpath(np.get_include())\\nexcept Exception:\\n  incdir = np.get_include()\\nprint(incdir)\\n  \"\\n--- stdout ---\\nC:\\\\Users\\\\Aayush\\\\AppData\\\\Local\\\\Temp\\\\pip-build-env-x29_b_nj\\\\overlay\\\\Lib\\\\site-packages\\\\numpy\\\\core\\\\include\\n\\n--- stderr ---\\n\\n\\nRunning command: C:\\\\Users\\\\Aayush\\\\.conda\\\\envs\\\\wigner_test\\\\python.exe -c \"import os; os.chdir(\\\\\"..\\\\\"); import numpy; print(numpy.get_include())\"\\n--- stdout ---\\nC:\\\\Users\\\\Aayush\\\\AppData\\\\Local\\\\Temp\\\\pip-build-env-x29_b_nj\\\\overlay\\\\Lib\\\\site-packages\\\\numpy\\\\core\\\\include\\n\\n--- stderr ---\\n\\n\\nDependency openmp found: YES 2.0 (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmp_thxf909\\\\testfile.c /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmp_thxf909\\\\output.obj /nologo /showIncludes /utf-8 /c /nologo /showIncludes /utf-8 /c /Od /Oi- /arch:SSE2', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='Code:\\n \\n#if defined(__GNUC__)\\n# if !defined(__amd64__) && !defined(__x86_64__)\\n#   error \"SSE2 intrinsics are only available on x86_64\"\\n# endif\\n#elif defined (_MSC_VER) && !defined (_M_X64) && !defined (_M_AMD64)\\n# error \"SSE2 intrinsics not supported on x86 MSVC builds\"\\n#endif\\n#if defined(__SSE__) || (_M_X64 > 0)\\n# include <mmintrin.h>\\n# include <xmmintrin.h>\\n# include <emmintrin.h>\\n#else\\n# error \"No SSE intrinsics available\"\\n#endif\\nint main () {\\n    __m128i a = _mm_set1_epi32 (0), b = _mm_set1_epi32 (0), c;\\n    c = _mm_xor_si128 (a, b);\\n    return 0;\\n}\\nCached compiler stdout:\\n testfile.c\\nNote: including file: C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\mmintrin.h\\nNote: including file: C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\xmmintrin.h\\nNote: including file:  C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\malloc.h\\nNote: including file:   C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\corecrt.h\\nNote: including file:    C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\vcruntime.h\\nNote: including file:     C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\sal.h\\nNote: including file:      C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\concurrencysal.h\\nNote: including file:     C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\vadefs.h\\nNote: including file:   C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\corecrt_malloc.h', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='Note: including file:     C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\sal.h\\nNote: including file:      C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\concurrencysal.h\\nNote: including file:     C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\vadefs.h\\nNote: including file:   C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\corecrt_malloc.h\\nNote: including file: C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\emmintrin.h', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='Cached compiler stderr:\\n cl : Command line warning D9002 : ignoring unknown option \\'/arch:SSE2\\'\\n\\nChecking if \"SSE intrinsics\" compiles: YES (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmppie7li39\\\\testfile.c /FeC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmppie7li39\\\\output.exe /nologo /showIncludes /utf-8 /MD /nologo /showIncludes /utf-8 /Od /Oi-', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='Code:\\n #include <immintrin.h>\\n                      int main (int argc, char ** argv) {\\n                        static __m256 mtest;\\n                        mtest = _mm256_setzero_ps();\\n                        return *((unsigned char *) &mtest) != 0;\\n                      }\\nCached compiler stdout:\\n testfile.c\\nNote: including file: C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\immintrin.h\\nNote: including file:  C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\wmmintrin.h\\nNote: including file:   C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\nmmintrin.h\\nNote: including file:    C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\smmintrin.h\\nNote: including file:     C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\tmmintrin.h\\nNote: including file:      C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\pmmintrin.h\\nNote: including file:       C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\emmintrin.h\\nNote: including file:        C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\xmmintrin.h\\nNote: including file:         C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\mmintrin.h\\nNote: including file:         C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\malloc.h\\nNote: including file:          C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\corecrt.h', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='Note: including file:        C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\xmmintrin.h\\nNote: including file:         C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\mmintrin.h\\nNote: including file:         C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\malloc.h\\nNote: including file:          C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\corecrt.h\\nNote: including file:           C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\vcruntime.h\\nNote: including file:            C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\sal.h\\nNote: including file:             C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\concurrencysal.h\\nNote: including file:            C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\vadefs.h\\nNote: including file:          C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\corecrt_malloc.h\\nNote: including file:  C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\zmmintrin.h', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='Cached compiler stderr:\\n \\nChecking if \"compiler supports AVX intrinsics\" : links: YES (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmpwljhfdaq\\\\testfile.c /FeC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmpwljhfdaq\\\\output.exe /nologo /showIncludes /utf-8 /MD /nologo /showIncludes /utf-8 /Od /Oi-', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='Code:\\n #include <immintrin.h>\\n                      int main (int argc, char ** argv) {\\n                        static __m256i mtest;\\n                        mtest = _mm256_setzero_si256();\\n                        return *((unsigned char *) &mtest) != 0;\\n                      }\\nCached compiler stdout:\\n testfile.c\\nNote: including file: C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\immintrin.h\\nNote: including file:  C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\wmmintrin.h\\nNote: including file:   C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\nmmintrin.h\\nNote: including file:    C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\smmintrin.h\\nNote: including file:     C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\tmmintrin.h\\nNote: including file:      C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\pmmintrin.h\\nNote: including file:       C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\emmintrin.h\\nNote: including file:        C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\xmmintrin.h\\nNote: including file:         C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\mmintrin.h\\nNote: including file:         C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\malloc.h\\nNote: including file:          C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\corecrt.h', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='Note: including file:        C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\xmmintrin.h\\nNote: including file:         C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\mmintrin.h\\nNote: including file:         C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\malloc.h\\nNote: including file:          C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\corecrt.h\\nNote: including file:           C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\vcruntime.h\\nNote: including file:            C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\sal.h\\nNote: including file:             C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\concurrencysal.h\\nNote: including file:            C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\vadefs.h\\nNote: including file:          C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\corecrt_malloc.h\\nNote: including file:  C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\zmmintrin.h', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='Cached compiler stderr:\\n \\nChecking if \"compiler supports AVX2 intrinsics\" : links: YES (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmpwpft48t8\\\\testfile.c /FeC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmpwpft48t8\\\\output.exe /nologo /showIncludes /utf-8 /MD /nologo /showIncludes /utf-8 /Od /Oi-', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='Code:\\n #include <immintrin.h>\\n                      int main (int argc, char ** argv) {\\n                        static __m512 mtest;\\n                        mtest = _mm512_setzero_si512();\\n                        return *((unsigned char *) &mtest) != 0;\\n                      }\\nCached compiler stdout:\\n testfile.c\\nNote: including file: C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\immintrin.h\\nNote: including file:  C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\wmmintrin.h\\nNote: including file:   C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\nmmintrin.h\\nNote: including file:    C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\smmintrin.h\\nNote: including file:     C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\tmmintrin.h\\nNote: including file:      C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\pmmintrin.h\\nNote: including file:       C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\emmintrin.h\\nNote: including file:        C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\xmmintrin.h\\nNote: including file:         C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\mmintrin.h\\nNote: including file:         C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\malloc.h\\nNote: including file:          C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\corecrt.h', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"Note: including file:        C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\xmmintrin.h\\nNote: including file:         C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\mmintrin.h\\nNote: including file:         C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\malloc.h\\nNote: including file:          C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\corecrt.h\\nNote: including file:           C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\vcruntime.h\\nNote: including file:            C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\sal.h\\nNote: including file:             C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\concurrencysal.h\\nNote: including file:            C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\vadefs.h\\nNote: including file:          C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\corecrt_malloc.h\\nNote: including file:  C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\zmmintrin.h\\nC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmpwpft48t8\\\\testfile.c(4): error C2440: '=': cannot convert from '__m512i' to '__m512'\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='Cached compiler stderr:\\n \\nChecking if \"compiler supports AVX512 intrinsics\" : links: NO (cached)\\nUsing cached compile:\\nCached command line:  cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmpiaat9yvg\\\\testfile.c /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-private\\\\tmpiaat9yvg\\\\output.obj /nologo /showIncludes /utf-8 /c /nologo /showIncludes /utf-8 /c /Od /Oi-', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='Code:\\n \\n#if !defined (_MSC_VER) || defined (__clang__)\\n# if !defined (_M_ARM64) && !defined (__aarch64__)\\n#  ifndef __ARM_EABI__\\n#   error \"EABI is required (to be sure that calling conventions are compatible)\"\\n#  endif\\n#   ifndef __ARM_NEON__\\n#    error \"No ARM NEON instructions available\"\\n#   endif\\n# endif\\n#endif\\n#if defined (_MSC_VER) && (_MSC_VER < 1920) && defined (_M_ARM64)\\n# include <arm64_neon.h>\\n#else\\n# include <arm_neon.h>\\n#endif\\nint main () {\\n    const float32_t __v[4] = { 1, 2, 3, 4 }; \\\\\\n    const unsigned int __umask[4] = { \\\\\\n      0x80000000, \\\\\\n      0x80000000, \\\\\\n      0x80000000, \\\\\\n      0x80000000 \\\\\\n    }; \\\\\\n    const uint32x4_t __mask = vld1q_u32 (__umask); \\\\\\n    float32x4_t s = vld1q_f32 (__v); \\\\\\n    float32x4_t c = vreinterpretq_f32_u32 (veorq_u32 (vreinterpretq_u32_f32 (s), __mask)); \\\\\\n    return 0;\\n}\\nCached compiler stdout:\\n testfile.c\\nNote: including file: C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\arm_neon.h\\nNote: including file:  C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\stdint.h\\nNote: including file:   C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\vcruntime.h\\nNote: including file:    C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\sal.h\\nNote: including file:     C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\concurrencysal.h\\nNote: including file:    C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\vadefs.h', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='Note: including file:    C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\sal.h\\nNote: including file:     C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\concurrencysal.h\\nNote: including file:    C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\vadefs.h\\nC:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\arm_neon.h(21): fatal error C1189: #error:  This header is specific to ARM targets', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='Cached compiler stderr:\\n \\nChecking if \"ARM NEON intrinsics\" compiles: NO (cached)\\nBuild targets in project: 45\\n\\ndipy 1.10.0dev\\n\\n  User defined options\\n    Native files: C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp310\\\\meson-python-native-file.ini\\n    buildtype   : release\\n    b_ndebug    : if-release\\n    b_vscrt     : md\\n\\nFailed to guess install tag for c:\\\\Lib\\\\site-packages\\\\dipy\\\\version.py\\nFailed to guess install tag for c:\\\\share\\\\doc\\\\dipy\\\\dummy\\nFound ninja.EXE-1.11.1.git.kitware.jobserver-1 at C:\\\\Users\\\\Aayush\\\\AppData\\\\Local\\\\Temp\\\\pip-build-env-x29_b_nj\\\\normal\\\\Scripts\\\\ninja.EXE\\nFailed to guess install tag for c:\\\\Lib\\\\site-packages\\\\dipy\\\\version.py\\nFailed to guess install tag for c:\\\\share\\\\doc\\\\dipy\\\\dummy\\nFailed to guess install tag for c:\\\\Lib\\\\site-packages\\\\dipy\\\\version.py\\nFailed to guess install tag for c:\\\\share\\\\doc\\\\dipy\\\\dummy', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"[options]\\nbuildtype = release\\nb_ndebug = if-release\\nb_vscrt = md\\n\\n[properties]\\nnative_file = ['C:\\\\\\\\Users\\\\\\\\Aayush\\\\\\\\OneDrive\\\\\\\\Desktop\\\\\\\\DiPY\\\\\\\\dipy\\\\\\\\build\\\\\\\\cp310\\\\\\\\meson-python-native-file.ini']\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-private\\\\cmd_line.txt.txt'}),\n",
       " Document(page_content='# Copyright 2016 The Meson development team\\n\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\n# ignore all lints for this file, since it is run by python2 as well\\n\\n# type: ignore\\n# pylint: disable=deprecated-module\\n\\nimport json, os, subprocess, sys\\nfrom compileall import compile_file\\n\\nquiet = int(os.environ.get(\\'MESON_INSTALL_QUIET\\', 0))\\n\\ndef compileall(files):\\n    for f in files:\\n        # f is prefixed by {py_xxxxlib}, both variants are 12 chars\\n        # the key is the middle 10 chars of the prefix\\n        key = f[1:11].upper()\\n        f = f[12:]\\n\\n        ddir = None\\n        fullpath = absf = os.environ[\\'MESON_INSTALL_DESTDIR_\\'+key] + f\\n        f = os.environ[\\'MESON_INSTALL_\\'+key] + f\\n\\n        if absf != f:\\n            ddir = os.path.dirname(f)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-private\\\\pycompile.py.txt'}),\n",
       " Document(page_content=\"quiet = int(os.environ.get('MESON_INSTALL_QUIET', 0))\\n\\ndef compileall(files):\\n    for f in files:\\n        # f is prefixed by {py_xxxxlib}, both variants are 12 chars\\n        # the key is the middle 10 chars of the prefix\\n        key = f[1:11].upper()\\n        f = f[12:]\\n\\n        ddir = None\\n        fullpath = absf = os.environ['MESON_INSTALL_DESTDIR_'+key] + f\\n        f = os.environ['MESON_INSTALL_'+key] + f\\n\\n        if absf != f:\\n            ddir = os.path.dirname(f)\\n\\n        if os.path.isdir(absf):\\n            for root, _, files in os.walk(absf):\\n                if ddir is not None:\\n                    ddir = root.replace(absf, f, 1)\\n                for dirf in files:\\n                    if dirf.endswith('.py'):\\n                        fullpath = os.path.join(root, dirf)\\n                        compile_file(fullpath, ddir, force=True, quiet=quiet)\\n        else:\\n            compile_file(fullpath, ddir, force=True, quiet=quiet)\\n\\ndef run(manifest):\\n    data_file = os.path.join(os.path.dirname(__file__), manifest)\\n    with open(data_file, 'rb') as f:\\n        dat = json.load(f)\\n    compileall(dat)\\n\\nif __name__ == '__main__':\\n    manifest = sys.argv[1]\\n    run(manifest)\\n    if len(sys.argv) > 2:\\n        optlevel = int(sys.argv[2])\\n        # python2 only needs one or the other\\n        if optlevel == 1 or (sys.version_info >= (3,) and optlevel > 0):\\n            subprocess.check_call([sys.executable, '-O'] + sys.argv[:2])\\n        if optlevel == 2:\\n            subprocess.check_call([sys.executable, '-OO'] + sys.argv[:2])\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp310\\\\meson-private\\\\pycompile.py.txt'}),\n",
       " Document(page_content=\"# THIS FILE IS GENERATED DURING THE DIPY BUILD\\n# See tools/version_utils.py for details\\n\\nshort_version = '1.10.0'\\nversion = '1.10.0'\\nfull_version = '1.10.0.dev0+215.1ccbcae'\\ngit_revision = '1ccbcae'\\ncommit_count = '215'\\nrelease = False\\n\\nif not release:\\n    version = full_version\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\dipy\\\\version.py.txt'}),\n",
       " Document(page_content='\"\"\"\\nDiffusion Imaging in Python\\n============================\\n\\nFor more information, please visit https://dipy.org\\n\\nSubpackages\\n-----------\\n::\\n\\n align         -- Registration, streamline alignment, volume resampling\\n core          -- Spheres, gradient tables\\n core.geometry -- Spherical geometry, coordinate and vector manipulation\\n core.meshes   -- Point distributions on the sphere\\n data          -- Small testing datasets\\n denoise       -- Denoising algorithms\\n direction     -- Manage peaks and tracking\\n io            -- Loading/saving of dpy datasets\\n nn            -- Neural networks algorithms\\n reconst       -- Signal reconstruction modules (tensor, spherical harmonics,\\n                  diffusion spectrum, etc.)\\n segment       -- Tractography segmentation\\n sims          -- MRI phantom signal simulation\\n stats         -- Tractometry\\n tracking      -- Tractography, metrics for streamlines\\n viz           -- Visualization and GUIs\\n workflows      -- Predefined Command line for common tasks\\n\\nUtilities\\n---------\\n::\\n\\n test          -- Run unittests\\n __version__   -- Dipy version\\n\\n\"\"\"\\nimport sys\\n\\nfrom dipy.version import version as __version__\\n\\n# Plumb in version etc info stuff\\nfrom .pkg_info import get_pkg_info as _get_pkg_info\\n\\n\\ndef get_info():\\n    from os.path import dirname\\n    return _get_pkg_info(dirname(__file__))\\n\\n\\ndel sys\\n\\nsubmodules = [\\n    \\'align\\',\\n    \\'core\\',\\n    \\'data\\',\\n    \\'denoise\\',\\n    \\'direction\\',\\n    \\'io\\',\\n    \\'nn\\',\\n    \\'reconst\\',\\n    \\'segment\\',\\n    \\'sims\\',\\n    \\'stats\\',\\n    \\'tracking\\',\\n    \\'utils\\',\\n    \\'viz\\',\\n    \\'workflows\\',\\n    \\'tests\\',\\n    \\'testing\\'\\n]\\n\\n__all__ = submodules + [\\'__version__\\', \\'setup_test\\', \\'get_info\\']', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\dipy\\\\__init__.py.txt'}),\n",
       " Document(page_content='Build started at 2024-03-10T19:56:05.735652\\nMain binary: C:\\\\ProgramData\\\\miniconda3\\\\python.exe\\nBuild Options: -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md \\'--native-file=C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-python-native-file.ini\\'\\nPython system: Windows\\nThe Meson build system\\nVersion: 1.3.2\\nSource dir: C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\nBuild dir: C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\nBuild type: native build\\nProject name: dipy\\nProject version: 1.10.0dev\\nActivating VS 17.9.2\\n-----------\\nDetecting compiler via: `icl \"\"` -> [WinError 2] The system cannot find the file specified\\n-----------\\nDetecting compiler via: `cl /?` -> 0\\nstdout:\\nC/C++ COMPILER OPTIONS\\n\\n\\n                              -OPTIMIZATION-\\n\\n/O1 maximum optimizations (favor space) /O2 maximum optimizations (favor speed)\\n/Ob<n> inline expansion (default n=0)   /Od disable optimizations (default)\\n/Og enable global optimization          /Oi[-] enable intrinsic functions\\n/Os favor code space                    /Ot favor code speed\\n/Ox optimizations (favor speed)         \\n/favor:<blend|AMD64|INTEL64|ATOM> select processor to optimize for, one of:\\n    blend - a combination of optimizations for several different x64 processors\\n    AMD64 - 64-bit AMD processors\\n    INTEL64 - Intel(R)64 architecture processors\\n    ATOM - Intel(R) Atom(TM) processors\\n\\n                             -CODE GENERATION-', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='/Gu[-] ensure distinct functions have distinct addresses\\n/Gw[-] separate global variables for linker\\n/GF enable read-only string pooling     /Gm[-] enable minimal rebuild\\n/Gy[-] separate functions for linker    /GS[-] enable security checks\\n/GR[-] enable C++ RTTI                  /GX[-] enable C++ EH (same as /EHsc)\\n/guard:cf[-] enable CFG (control flow guard)\\n/guard:ehcont[-] enable EH continuation metadata (CET)\\n/EHs enable C++ EH (no SEH exceptions)  /EHa enable C++ EH (w/ SEH exceptions)\\n/EHc extern \"C\" defaults to nothrow     \\n/EHr always generate noexcept runtime termination checks\\n/fp:<contract|except[-]|fast|precise|strict> choose floating-point model:\\n    contract - consider floating-point contractions when generating code\\n    except[-] - consider floating-point exceptions when generating code\\n    fast - \"fast\" floating-point model; results are less predictable\\n    precise - \"precise\" floating-point model; results are predictable\\n    strict - \"strict\" floating-point model (implies /fp:except)\\n/Qfast_transcendentals generate inline FP intrinsics even with /fp:except\\n/Qspectre[-] enable mitigations for CVE 2017-5753\\n/Qpar[-] enable parallel code generation\\n/Qpar-report:1 auto-parallelizer diagnostic; indicate parallelized loops\\n/Qpar-report:2 auto-parallelizer diagnostic; indicate loops not parallelized\\n/Qvec-report:1 auto-vectorizer diagnostic; indicate vectorized loops\\n/Qvec-report:2 auto-vectorizer diagnostic; indicate loops not vectorized\\n/GL[-] enable link-time code generation \\n/volatile:<iso|ms> choose volatile model:\\n    iso - Acquire/release semantics not guaranteed on volatile accesses\\n    ms  - Acquire/release semantics guaranteed on volatile accesses', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='/Qpar-report:2 auto-parallelizer diagnostic; indicate loops not parallelized\\n/Qvec-report:1 auto-vectorizer diagnostic; indicate vectorized loops\\n/Qvec-report:2 auto-vectorizer diagnostic; indicate loops not vectorized\\n/GL[-] enable link-time code generation \\n/volatile:<iso|ms> choose volatile model:\\n    iso - Acquire/release semantics not guaranteed on volatile accesses\\n    ms  - Acquire/release semantics guaranteed on volatile accesses\\n/GA optimize for Windows Application    /Ge force stack checking for all funcs\\n/Gs[num] control stack checking calls   /Gh enable _penter function call\\n/GH enable _pexit function call         /GT generate fiber-safe TLS accesses\\n/RTC1 Enable fast checks (/RTCsu)       /RTCc Convert to smaller type checks\\n/RTCs Stack Frame runtime checking      /RTCu Uninitialized local usage checks\\n/clr[:option] compile for common language runtime, where option is:\\n    pure : produce IL-only output file (no native executable code)\\n    safe : produce IL-only verifiable output file\\n    netcore : produce assemblies targeting .NET Core runtime\\n    noAssembly : do not produce an assembly\\n    nostdlib : ignore the system .NET framework directory when searching for assemblies\\n    nostdimport : do not import any required assemblies implicitly\\n    initialAppDomain : enable initial AppDomain behavior of Visual C++ 2002\\n    implicitKeepAlive- : turn off implicit emission of System::GC::KeepAlive(this)\\n/fsanitize=address Enable address sanitizer codegen\\n/homeparams Force parameters passed in registers to be written to the stack\\n/GZ Enable stack checks (/RTCs)         /Gv __vectorcall calling convention\\n/arch:<AVX|AVX2|AVX512> minimum CPU architecture requirements, one of:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='initialAppDomain : enable initial AppDomain behavior of Visual C++ 2002\\n    implicitKeepAlive- : turn off implicit emission of System::GC::KeepAlive(this)\\n/fsanitize=address Enable address sanitizer codegen\\n/homeparams Force parameters passed in registers to be written to the stack\\n/GZ Enable stack checks (/RTCs)         /Gv __vectorcall calling convention\\n/arch:<AVX|AVX2|AVX512> minimum CPU architecture requirements, one of:\\n   AVX - enable use of instructions available with AVX-enabled CPUs\\n   AVX2 - enable use of instructions available with AVX2-enabled CPUs\\n   AVX512 - enable use of instructions available with AVX-512-enabled CPUs\\n/QIntel-jcc-erratum enable mitigations for Intel JCC erratum\\n/Qspectre-load Enable spectre mitigations for all instructions which load memory\\n/Qspectre-load-cf Enable spectre mitigations for all control-flow instructions which load memory\\n/Qspectre-jmp[-] Enable spectre mitigations for unconditional jump instructions\\n/fpcvt:<IA|BC> FP to unsigned integer conversion compatibility\\n   IA - results compatible with VCVTTSD2USI instruction\\n   BC - results compatible with VS2017 and earlier compiler\\n/jumptablerdata Place jump tables for switch case statements in .rdata section', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='-OUTPUT FILES-\\n\\n/Fa[file] name assembly listing file    /FA[scu] configure assembly listing\\n/Fd[file] name .PDB file                /Fe<file> name executable file\\n/Fm[file] name map file                 /Fo<file> name object file\\n/Fp<file> name precompiled header file  /Fr[file] name source browser file\\n/FR[file] name extended .SBR file       /Fi[file] name preprocessed file\\n/Fd: <file> name .PDB file              /Fe: <file> name executable file\\n/Fm: <file> name map file               /Fo: <file> name object file\\n/Fp: <file> name .PCH file              /FR: <file> name extended .SBR file\\n/Fi: <file> name preprocessed file      \\n/Ft<dir> location of the header files generated for #import\\n/doc[file] process XML documentation comments and optionally name the .xdc file\\n\\n                              -PREPROCESSOR-\\n\\n/AI<dir> add to assembly search path    /FU<file> import .NET assembly/module\\n/FU:asFriend<file> import .NET assembly/module as friend\\n/C don\\'t strip comments                 /D<name>{=|#}<text> define macro\\n/E preprocess to stdout                 /EP preprocess to stdout, no #line\\n/P preprocess to file                   /Fx merge injected code to file\\n/FI<file> name forced include file      /U<name> remove predefined macro\\n/u remove all predefined macros         /I<dir> add to include search path\\n/X ignore \"standard places\"             \\n/PH generate #pragma file_hash when preprocessing\\n/PD print all macro definitions         \\n\\n                                -LANGUAGE-', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='/std:<c++14|c++17|c++20|c++latest> C++ standard version\\n    c++14 - ISO/IEC 14882:2014 (default)\\n    c++17 - ISO/IEC 14882:2017\\n    c++20 - ISO/IEC 14882:2020\\n    c++latest - latest draft standard (feature set subject to change)\\n/std:<c11|c17|clatest> C standard version\\n    c11 - ISO/IEC 9899:2011\\n    c17 - ISO/IEC 9899:2018\\n    clatest - latest draft standard (feature set subject to change)\\n/permissive[-] enable some nonconforming code to compile\\n               (feature set subject to change) (off by default in C++20 and later)\\n/Ze (deprecated) enable extensions (default)\\n/Za disable extensions (not recommended for C++)\\n/ZW enable WinRT language extensions    /Zs syntax check only\\n/await enable resumable functions extension\\n/await:strict enable standard C++20 coroutine support with earlier language versions\\n/constexpr:depth<N>     recursion depth limit for constexpr evaluation (default: 512)\\n/constexpr:backtrace<N> show N constexpr evaluations in diagnostics (default: 10)\\n/constexpr:steps<N>     terminate constexpr evaluation after N steps (default: 1048576)\\n/Zi enable debugging information        /Z7 enable old-style debug info\\n/Zo[-] generate richer debugging information for optimized code (on by default)\\n/ZH:[MD5|SHA1|SHA_256] hash algorithm for calculation of file checksum in debug info (default: SHA_256)\\n/Zp[n] pack structs on n-byte boundary  /Zl omit default library name in .OBJ\\n/vd{0|1|2} disable/enable vtordisp      /vm<x> type of pointers to members\\n/Zc:arg1[,arg2] language conformance, where arguments can be:\\n  forScope[-]           enforce Standard C++ for scoping rules\\n  wchar_t[-]            wchar_t is the native type, not a typedef', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='/ZH:[MD5|SHA1|SHA_256] hash algorithm for calculation of file checksum in debug info (default: SHA_256)\\n/Zp[n] pack structs on n-byte boundary  /Zl omit default library name in .OBJ\\n/vd{0|1|2} disable/enable vtordisp      /vm<x> type of pointers to members\\n/Zc:arg1[,arg2] language conformance, where arguments can be:\\n  forScope[-]           enforce Standard C++ for scoping rules\\n  wchar_t[-]            wchar_t is the native type, not a typedef\\n  auto[-]               enforce the new Standard C++ meaning for auto\\n  trigraphs[-]          enable trigraphs (off by default)\\n  rvalueCast[-]         enforce Standard C++ explicit type conversion rules\\n                        (on by default in C++20 or later, implied by /permissive-)\\n  strictStrings[-]      disable string-literal to [char|wchar_t]*\\n                        conversion (on by default in C++20 or later, implied by /permissive-)\\n  implicitNoexcept[-]   enable implicit noexcept on required functions\\n  threadSafeInit[-]     enable thread-safe local static initialization\\n  inline[-]             remove unreferenced function or data if it is\\n                        COMDAT or has internal linkage only (off by default)\\n  sizedDealloc[-]       enable C++14 global sized deallocation\\n                        functions (on by default)\\n  throwingNew[-]        assume operator new throws on failure (off by default)\\n  referenceBinding[-]   a temporary will not bind to a non-const\\n                        lvalue reference (on by default in C++20 or later, implied by /permissive-)\\n  twoPhase-             disable two-phase name lookup\\n  ternary[-]            enforce C++11 rules for conditional operator', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='sizedDealloc[-]       enable C++14 global sized deallocation\\n                        functions (on by default)\\n  throwingNew[-]        assume operator new throws on failure (off by default)\\n  referenceBinding[-]   a temporary will not bind to a non-const\\n                        lvalue reference (on by default in C++20 or later, implied by /permissive-)\\n  twoPhase-             disable two-phase name lookup\\n  ternary[-]            enforce C++11 rules for conditional operator\\n                        (on by default in C++20 or later, implied by /permissive-)\\n  noexceptTypes[-]      enforce C++17 noexcept rules (on by default in C++17 or later)\\n  alignedNew[-]         enable C++17 alignment of dynamically allocated objects (on by default)\\n  hiddenFriend[-]       enforce Standard C++ hidden friend rules\\n                        (on by default in C++20 or later, implied by /permissive-)\\n  externC[-]            enforce Standard C++ rules for \\'extern \"C\"\\' functions\\n                        (on by default in C++20 or later, implied by /permissive-)\\n  lambda[-]             better lambda support by using the newer lambda processor\\n                        (on by default in C++20 or later, implied by /permissive-)\\n  tlsGuards[-]          generate runtime checks for TLS variable initialization (on by default)\\n  zeroSizeArrayNew[-]   call member new/delete for 0-size arrays of objects (on by default)\\n  static_assert[-]      strict handling of \\'static_assert\\' (on by default in C++20 or later,\\n                        implied by /permissive-)\\n  gotoScope[-]          cannot jump past the initialization of a variable (implied by /permissive-)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"tlsGuards[-]          generate runtime checks for TLS variable initialization (on by default)\\n  zeroSizeArrayNew[-]   call member new/delete for 0-size arrays of objects (on by default)\\n  static_assert[-]      strict handling of 'static_assert' (on by default in C++20 or later,\\n                        implied by /permissive-)\\n  gotoScope[-]          cannot jump past the initialization of a variable (implied by /permissive-)\\n  templateScope[-]      enforce Standard C++ template parameter shadowing rules\\n  enumTypes[-]          enable Standard C++ underlying enum types (off by default)\\n  checkGwOdr[-]         enforce Standard C++ one definition rule violations\\n                        when /Gw has been enabled (off by default)\\n  nrvo[-]               enable optional copy and move elision (on by default in C++20 or later,\\n                        implied by /permissive- or /O2)\\n  __STDC__              define __STDC__ to 1 in C\\n  __cplusplus[-]        __cplusplus macro reports the supported C++ standard (off by default)\\n  char8_t[-]            enable C++20 native `u8` literal support as `const char8_t`\\n                        (on by default in C++20 or later)\\n  externConstexpr[-]    enable external linkage for constexpr variables in C++\\n                        (on by default in C++20 or later, implied by /permissive-)\\n  preprocessor[-]       enable standard conforming preprocessor in C/C++\\n                        (on by default in C11 or later)\\n/ZI enable Edit and Continue debug info \\n/openmp enable OpenMP 2.0 language extensions\\n/openmp:experimental enable OpenMP 2.0 language extensions plus select OpenMP 3.0+ language extensions\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='externConstexpr[-]    enable external linkage for constexpr variables in C++\\n                        (on by default in C++20 or later, implied by /permissive-)\\n  preprocessor[-]       enable standard conforming preprocessor in C/C++\\n                        (on by default in C11 or later)\\n/ZI enable Edit and Continue debug info \\n/openmp enable OpenMP 2.0 language extensions\\n/openmp:experimental enable OpenMP 2.0 language extensions plus select OpenMP 3.0+ language extensions\\n/openmp:llvm OpenMP language extensions using LLVM runtime', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"-MISCELLANEOUS-\\n\\n@<file> options response file           /?, /help print this help message\\n/bigobj generate extended object format /c compile only, no link\\n/errorReport:option deprecated. Report internal compiler errors to Microsoft\\n    none - do not send report\\n    prompt - prompt to immediately send report\\n    queue - at next admin logon, prompt to send report (default)\\n    send - send report automatically\\n/FC use full pathnames in diagnostics   /H<num> max external name length\\n/J default char type is unsigned        \\n/MP[n] use up to 'n' processes for compilation\\n/nologo suppress copyright message      /showIncludes show include file names\\n/Tc<source file> compile file as .c     /Tp<source file> compile file as .cpp\\n/TC compile all files as .c             /TP compile all files as .cpp\\n/V<string> set version string           /Yc[file] create .PCH file\\n/Yd put debug info in every .OBJ        /Yl[sym] inject .PCH ref for debug lib\\n/Yu[file] use .PCH file                 /Y- disable all PCH options\\n/Zm<n> max memory alloc (% of default)  /FS force to use MSPDBSRV.EXE\\n/source-charset:<iana-name>|.nnnn set source character set\\n/execution-charset:<iana-name>|.nnnn set execution character set\\n/utf-8 set source and execution character set to UTF-8\\n/validate-charset[-] validate UTF-8 files for only legal characters\\n/fastfail[-] enable fast-fail mode      /JMC[-] enable native just my code\\n/presetPadding[-] zero initialize padding for stack based class types\\n/volatileMetadata[-] generate metadata on volatile memory accesses\\n/sourcelink [file] file containing source link information\\n\\n                                -LINKING-\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='-LINKING-\\n\\n/LD Create .DLL                         /LDd Create .DLL debug library\\n/LN Create a .netmodule                 /F<num> set stack size\\n/link [linker options and libraries]    /MD link with MSVCRT.LIB\\n/MT link with LIBCMT.LIB                /MDd link with MSVCRTD.LIB debug lib\\n/MTd link with LIBCMTD.LIB debug lib    \\n\\n                              -CODE ANALYSIS-\\n\\n/analyze[-] Enable native analysis      /analyze:quiet[-] No warning to console\\n/analyze:log<name> Warnings to file     /analyze:autolog Log to *.pftlog\\n/analyze:autolog:ext<ext> Log to *.<ext>/analyze:autolog- No log file\\n/analyze:WX- Warnings not fatal         /analyze:stacksize<num> Max stack frame\\n/analyze:max_paths<num> Max paths       /analyze:only Analyze, no code gen\\n\\n                              -DIAGNOSTICS-', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='-DIAGNOSTICS-\\n\\n/diagnostics:<args,...> controls the format of diagnostic messages:\\n             classic   - retains prior format\\n             column[-] - prints column information\\n             caret[-]  - prints column and the indicated line of source\\n/Wall enable all warnings               /w   disable all warnings\\n/W<n> set warning level (default n=1)   \\n/Wv:xx[.yy[.zzzzz]] disable warnings introduced after version xx.yy.zzzzz\\n/WX treat warnings as errors            /WL enable one line diagnostics\\n/wd<n> disable warning n                /we<n> treat warning n as an error\\n/wo<n> issue warning n once             /w<l><n> set warning level 1-4 for n\\n/external:I <path>      - location of external headers\\n/external:env:<var>     - environment variable with locations of external headers\\n/external:anglebrackets - treat all headers included via <> as external\\n/external:W<n>          - warning level for external headers\\n/external:templates[-]  - evaluate warning level across template instantiation chain\\n/sdl enable additional security features and warnings\\n/options:strict unrecognized compiler options are an error\\n-----------\\nstderr:\\nMicrosoft (R) C/C++ Optimizing Compiler Version 19.39.33521 for x64\\nCopyright (C) Microsoft Corporation.  All rights reserved.\\n-----------\\nSanity testing C compiler: cl\\nIs cross compiler: False.\\nSanity check compiler command line: cl sanitycheckc.c /Fesanitycheckc.exe /MD /nologo /showIncludes /utf-8 /link\\nSanity check compile stdout:\\nsanitycheckc.c\\n\\n-----\\nSanity check compile stderr:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='-----\\nSanity check compile stderr:\\n\\n-----\\nRunning test binary command:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\sanitycheckc.exe\\nC compiler for the host machine: cl (msvc 19.39.33521 \"Microsoft (R) C/C++ Optimizing Compiler Version 19.39.33521 for x64\")\\nC linker for the host machine: link link 14.39.33521.0\\n-----------\\nDetecting archiver via: `lib /?` -> 1100\\nstdout:\\nMicrosoft (R) Library Manager Version 14.39.33521.0\\nCopyright (C) Microsoft Corporation.  All rights reserved.\\n\\nusage: LIB [options] [files]\\n\\n   options:\\n\\n      /DEF[:filename]\\n      /ERRORREPORT:{NONE|PROMPT|QUEUE|SEND}\\n      /EXPORT:symbol\\n      /EXTRACT:membername\\n      /INCLUDE:symbol\\n      /LIBPATH:dir\\n      /LINKREPRO:dir\\n      /LINKREPROTARGET:filename\\n      /LIST[:filename]\\n      /LTCG\\n      /MACHINE:{ARM|ARM64|ARM64X|EBC|X64|X86}\\n      /NAME:filename\\n      /NODEFAULTLIB[:library]\\n      /NOLOGO\\n      /OUT:filename\\n      /REMOVE:membername\\n      /SUBSYSTEM:{BOOT_APPLICATION|CONSOLE|EFI_APPLICATION|\\n                  EFI_BOOT_SERVICE_DRIVER|EFI_ROM|EFI_RUNTIME_DRIVER|\\n                  NATIVE|POSIX|WINDOWS|WINDOWSCE}[,#[.##]]\\n      /VERBOSE\\n      /WX[:NO]\\n      /WX[:nnnn[,nnnn...]]\\n-----------\\n-----------\\nDetecting compiler via: `icl \"\"` -> [WinError 2] The system cannot find the file specified\\n-----------\\nDetecting compiler via: `cl /?` -> 0\\nstdout:\\nC/C++ COMPILER OPTIONS\\n\\n\\n                              -OPTIMIZATION-', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='-OPTIMIZATION-\\n\\n/O1 maximum optimizations (favor space) /O2 maximum optimizations (favor speed)\\n/Ob<n> inline expansion (default n=0)   /Od disable optimizations (default)\\n/Og enable global optimization          /Oi[-] enable intrinsic functions\\n/Os favor code space                    /Ot favor code speed\\n/Ox optimizations (favor speed)         \\n/favor:<blend|AMD64|INTEL64|ATOM> select processor to optimize for, one of:\\n    blend - a combination of optimizations for several different x64 processors\\n    AMD64 - 64-bit AMD processors\\n    INTEL64 - Intel(R)64 architecture processors\\n    ATOM - Intel(R) Atom(TM) processors\\n\\n                             -CODE GENERATION-', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='/Gu[-] ensure distinct functions have distinct addresses\\n/Gw[-] separate global variables for linker\\n/GF enable read-only string pooling     /Gm[-] enable minimal rebuild\\n/Gy[-] separate functions for linker    /GS[-] enable security checks\\n/GR[-] enable C++ RTTI                  /GX[-] enable C++ EH (same as /EHsc)\\n/guard:cf[-] enable CFG (control flow guard)\\n/guard:ehcont[-] enable EH continuation metadata (CET)\\n/EHs enable C++ EH (no SEH exceptions)  /EHa enable C++ EH (w/ SEH exceptions)\\n/EHc extern \"C\" defaults to nothrow     \\n/EHr always generate noexcept runtime termination checks\\n/fp:<contract|except[-]|fast|precise|strict> choose floating-point model:\\n    contract - consider floating-point contractions when generating code\\n    except[-] - consider floating-point exceptions when generating code\\n    fast - \"fast\" floating-point model; results are less predictable\\n    precise - \"precise\" floating-point model; results are predictable\\n    strict - \"strict\" floating-point model (implies /fp:except)\\n/Qfast_transcendentals generate inline FP intrinsics even with /fp:except\\n/Qspectre[-] enable mitigations for CVE 2017-5753\\n/Qpar[-] enable parallel code generation\\n/Qpar-report:1 auto-parallelizer diagnostic; indicate parallelized loops\\n/Qpar-report:2 auto-parallelizer diagnostic; indicate loops not parallelized\\n/Qvec-report:1 auto-vectorizer diagnostic; indicate vectorized loops\\n/Qvec-report:2 auto-vectorizer diagnostic; indicate loops not vectorized\\n/GL[-] enable link-time code generation \\n/volatile:<iso|ms> choose volatile model:\\n    iso - Acquire/release semantics not guaranteed on volatile accesses\\n    ms  - Acquire/release semantics guaranteed on volatile accesses', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='/Qpar-report:2 auto-parallelizer diagnostic; indicate loops not parallelized\\n/Qvec-report:1 auto-vectorizer diagnostic; indicate vectorized loops\\n/Qvec-report:2 auto-vectorizer diagnostic; indicate loops not vectorized\\n/GL[-] enable link-time code generation \\n/volatile:<iso|ms> choose volatile model:\\n    iso - Acquire/release semantics not guaranteed on volatile accesses\\n    ms  - Acquire/release semantics guaranteed on volatile accesses\\n/GA optimize for Windows Application    /Ge force stack checking for all funcs\\n/Gs[num] control stack checking calls   /Gh enable _penter function call\\n/GH enable _pexit function call         /GT generate fiber-safe TLS accesses\\n/RTC1 Enable fast checks (/RTCsu)       /RTCc Convert to smaller type checks\\n/RTCs Stack Frame runtime checking      /RTCu Uninitialized local usage checks\\n/clr[:option] compile for common language runtime, where option is:\\n    pure : produce IL-only output file (no native executable code)\\n    safe : produce IL-only verifiable output file\\n    netcore : produce assemblies targeting .NET Core runtime\\n    noAssembly : do not produce an assembly\\n    nostdlib : ignore the system .NET framework directory when searching for assemblies\\n    nostdimport : do not import any required assemblies implicitly\\n    initialAppDomain : enable initial AppDomain behavior of Visual C++ 2002\\n    implicitKeepAlive- : turn off implicit emission of System::GC::KeepAlive(this)\\n/fsanitize=address Enable address sanitizer codegen\\n/homeparams Force parameters passed in registers to be written to the stack\\n/GZ Enable stack checks (/RTCs)         /Gv __vectorcall calling convention\\n/arch:<AVX|AVX2|AVX512> minimum CPU architecture requirements, one of:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='initialAppDomain : enable initial AppDomain behavior of Visual C++ 2002\\n    implicitKeepAlive- : turn off implicit emission of System::GC::KeepAlive(this)\\n/fsanitize=address Enable address sanitizer codegen\\n/homeparams Force parameters passed in registers to be written to the stack\\n/GZ Enable stack checks (/RTCs)         /Gv __vectorcall calling convention\\n/arch:<AVX|AVX2|AVX512> minimum CPU architecture requirements, one of:\\n   AVX - enable use of instructions available with AVX-enabled CPUs\\n   AVX2 - enable use of instructions available with AVX2-enabled CPUs\\n   AVX512 - enable use of instructions available with AVX-512-enabled CPUs\\n/QIntel-jcc-erratum enable mitigations for Intel JCC erratum\\n/Qspectre-load Enable spectre mitigations for all instructions which load memory\\n/Qspectre-load-cf Enable spectre mitigations for all control-flow instructions which load memory\\n/Qspectre-jmp[-] Enable spectre mitigations for unconditional jump instructions\\n/fpcvt:<IA|BC> FP to unsigned integer conversion compatibility\\n   IA - results compatible with VCVTTSD2USI instruction\\n   BC - results compatible with VS2017 and earlier compiler\\n/jumptablerdata Place jump tables for switch case statements in .rdata section', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='-OUTPUT FILES-\\n\\n/Fa[file] name assembly listing file    /FA[scu] configure assembly listing\\n/Fd[file] name .PDB file                /Fe<file> name executable file\\n/Fm[file] name map file                 /Fo<file> name object file\\n/Fp<file> name precompiled header file  /Fr[file] name source browser file\\n/FR[file] name extended .SBR file       /Fi[file] name preprocessed file\\n/Fd: <file> name .PDB file              /Fe: <file> name executable file\\n/Fm: <file> name map file               /Fo: <file> name object file\\n/Fp: <file> name .PCH file              /FR: <file> name extended .SBR file\\n/Fi: <file> name preprocessed file      \\n/Ft<dir> location of the header files generated for #import\\n/doc[file] process XML documentation comments and optionally name the .xdc file\\n\\n                              -PREPROCESSOR-\\n\\n/AI<dir> add to assembly search path    /FU<file> import .NET assembly/module\\n/FU:asFriend<file> import .NET assembly/module as friend\\n/C don\\'t strip comments                 /D<name>{=|#}<text> define macro\\n/E preprocess to stdout                 /EP preprocess to stdout, no #line\\n/P preprocess to file                   /Fx merge injected code to file\\n/FI<file> name forced include file      /U<name> remove predefined macro\\n/u remove all predefined macros         /I<dir> add to include search path\\n/X ignore \"standard places\"             \\n/PH generate #pragma file_hash when preprocessing\\n/PD print all macro definitions         \\n\\n                                -LANGUAGE-', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='/std:<c++14|c++17|c++20|c++latest> C++ standard version\\n    c++14 - ISO/IEC 14882:2014 (default)\\n    c++17 - ISO/IEC 14882:2017\\n    c++20 - ISO/IEC 14882:2020\\n    c++latest - latest draft standard (feature set subject to change)\\n/std:<c11|c17|clatest> C standard version\\n    c11 - ISO/IEC 9899:2011\\n    c17 - ISO/IEC 9899:2018\\n    clatest - latest draft standard (feature set subject to change)\\n/permissive[-] enable some nonconforming code to compile\\n               (feature set subject to change) (off by default in C++20 and later)\\n/Ze (deprecated) enable extensions (default)\\n/Za disable extensions (not recommended for C++)\\n/ZW enable WinRT language extensions    /Zs syntax check only\\n/await enable resumable functions extension\\n/await:strict enable standard C++20 coroutine support with earlier language versions\\n/constexpr:depth<N>     recursion depth limit for constexpr evaluation (default: 512)\\n/constexpr:backtrace<N> show N constexpr evaluations in diagnostics (default: 10)\\n/constexpr:steps<N>     terminate constexpr evaluation after N steps (default: 1048576)\\n/Zi enable debugging information        /Z7 enable old-style debug info\\n/Zo[-] generate richer debugging information for optimized code (on by default)\\n/ZH:[MD5|SHA1|SHA_256] hash algorithm for calculation of file checksum in debug info (default: SHA_256)\\n/Zp[n] pack structs on n-byte boundary  /Zl omit default library name in .OBJ\\n/vd{0|1|2} disable/enable vtordisp      /vm<x> type of pointers to members\\n/Zc:arg1[,arg2] language conformance, where arguments can be:\\n  forScope[-]           enforce Standard C++ for scoping rules\\n  wchar_t[-]            wchar_t is the native type, not a typedef', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='/ZH:[MD5|SHA1|SHA_256] hash algorithm for calculation of file checksum in debug info (default: SHA_256)\\n/Zp[n] pack structs on n-byte boundary  /Zl omit default library name in .OBJ\\n/vd{0|1|2} disable/enable vtordisp      /vm<x> type of pointers to members\\n/Zc:arg1[,arg2] language conformance, where arguments can be:\\n  forScope[-]           enforce Standard C++ for scoping rules\\n  wchar_t[-]            wchar_t is the native type, not a typedef\\n  auto[-]               enforce the new Standard C++ meaning for auto\\n  trigraphs[-]          enable trigraphs (off by default)\\n  rvalueCast[-]         enforce Standard C++ explicit type conversion rules\\n                        (on by default in C++20 or later, implied by /permissive-)\\n  strictStrings[-]      disable string-literal to [char|wchar_t]*\\n                        conversion (on by default in C++20 or later, implied by /permissive-)\\n  implicitNoexcept[-]   enable implicit noexcept on required functions\\n  threadSafeInit[-]     enable thread-safe local static initialization\\n  inline[-]             remove unreferenced function or data if it is\\n                        COMDAT or has internal linkage only (off by default)\\n  sizedDealloc[-]       enable C++14 global sized deallocation\\n                        functions (on by default)\\n  throwingNew[-]        assume operator new throws on failure (off by default)\\n  referenceBinding[-]   a temporary will not bind to a non-const\\n                        lvalue reference (on by default in C++20 or later, implied by /permissive-)\\n  twoPhase-             disable two-phase name lookup\\n  ternary[-]            enforce C++11 rules for conditional operator', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='sizedDealloc[-]       enable C++14 global sized deallocation\\n                        functions (on by default)\\n  throwingNew[-]        assume operator new throws on failure (off by default)\\n  referenceBinding[-]   a temporary will not bind to a non-const\\n                        lvalue reference (on by default in C++20 or later, implied by /permissive-)\\n  twoPhase-             disable two-phase name lookup\\n  ternary[-]            enforce C++11 rules for conditional operator\\n                        (on by default in C++20 or later, implied by /permissive-)\\n  noexceptTypes[-]      enforce C++17 noexcept rules (on by default in C++17 or later)\\n  alignedNew[-]         enable C++17 alignment of dynamically allocated objects (on by default)\\n  hiddenFriend[-]       enforce Standard C++ hidden friend rules\\n                        (on by default in C++20 or later, implied by /permissive-)\\n  externC[-]            enforce Standard C++ rules for \\'extern \"C\"\\' functions\\n                        (on by default in C++20 or later, implied by /permissive-)\\n  lambda[-]             better lambda support by using the newer lambda processor\\n                        (on by default in C++20 or later, implied by /permissive-)\\n  tlsGuards[-]          generate runtime checks for TLS variable initialization (on by default)\\n  zeroSizeArrayNew[-]   call member new/delete for 0-size arrays of objects (on by default)\\n  static_assert[-]      strict handling of \\'static_assert\\' (on by default in C++20 or later,\\n                        implied by /permissive-)\\n  gotoScope[-]          cannot jump past the initialization of a variable (implied by /permissive-)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"tlsGuards[-]          generate runtime checks for TLS variable initialization (on by default)\\n  zeroSizeArrayNew[-]   call member new/delete for 0-size arrays of objects (on by default)\\n  static_assert[-]      strict handling of 'static_assert' (on by default in C++20 or later,\\n                        implied by /permissive-)\\n  gotoScope[-]          cannot jump past the initialization of a variable (implied by /permissive-)\\n  templateScope[-]      enforce Standard C++ template parameter shadowing rules\\n  enumTypes[-]          enable Standard C++ underlying enum types (off by default)\\n  checkGwOdr[-]         enforce Standard C++ one definition rule violations\\n                        when /Gw has been enabled (off by default)\\n  nrvo[-]               enable optional copy and move elision (on by default in C++20 or later,\\n                        implied by /permissive- or /O2)\\n  __STDC__              define __STDC__ to 1 in C\\n  __cplusplus[-]        __cplusplus macro reports the supported C++ standard (off by default)\\n  char8_t[-]            enable C++20 native `u8` literal support as `const char8_t`\\n                        (on by default in C++20 or later)\\n  externConstexpr[-]    enable external linkage for constexpr variables in C++\\n                        (on by default in C++20 or later, implied by /permissive-)\\n  preprocessor[-]       enable standard conforming preprocessor in C/C++\\n                        (on by default in C11 or later)\\n/ZI enable Edit and Continue debug info \\n/openmp enable OpenMP 2.0 language extensions\\n/openmp:experimental enable OpenMP 2.0 language extensions plus select OpenMP 3.0+ language extensions\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='externConstexpr[-]    enable external linkage for constexpr variables in C++\\n                        (on by default in C++20 or later, implied by /permissive-)\\n  preprocessor[-]       enable standard conforming preprocessor in C/C++\\n                        (on by default in C11 or later)\\n/ZI enable Edit and Continue debug info \\n/openmp enable OpenMP 2.0 language extensions\\n/openmp:experimental enable OpenMP 2.0 language extensions plus select OpenMP 3.0+ language extensions\\n/openmp:llvm OpenMP language extensions using LLVM runtime', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"-MISCELLANEOUS-\\n\\n@<file> options response file           /?, /help print this help message\\n/bigobj generate extended object format /c compile only, no link\\n/errorReport:option deprecated. Report internal compiler errors to Microsoft\\n    none - do not send report\\n    prompt - prompt to immediately send report\\n    queue - at next admin logon, prompt to send report (default)\\n    send - send report automatically\\n/FC use full pathnames in diagnostics   /H<num> max external name length\\n/J default char type is unsigned        \\n/MP[n] use up to 'n' processes for compilation\\n/nologo suppress copyright message      /showIncludes show include file names\\n/Tc<source file> compile file as .c     /Tp<source file> compile file as .cpp\\n/TC compile all files as .c             /TP compile all files as .cpp\\n/V<string> set version string           /Yc[file] create .PCH file\\n/Yd put debug info in every .OBJ        /Yl[sym] inject .PCH ref for debug lib\\n/Yu[file] use .PCH file                 /Y- disable all PCH options\\n/Zm<n> max memory alloc (% of default)  /FS force to use MSPDBSRV.EXE\\n/source-charset:<iana-name>|.nnnn set source character set\\n/execution-charset:<iana-name>|.nnnn set execution character set\\n/utf-8 set source and execution character set to UTF-8\\n/validate-charset[-] validate UTF-8 files for only legal characters\\n/fastfail[-] enable fast-fail mode      /JMC[-] enable native just my code\\n/presetPadding[-] zero initialize padding for stack based class types\\n/volatileMetadata[-] generate metadata on volatile memory accesses\\n/sourcelink [file] file containing source link information\\n\\n                                -LINKING-\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='-LINKING-\\n\\n/LD Create .DLL                         /LDd Create .DLL debug library\\n/LN Create a .netmodule                 /F<num> set stack size\\n/link [linker options and libraries]    /MD link with MSVCRT.LIB\\n/MT link with LIBCMT.LIB                /MDd link with MSVCRTD.LIB debug lib\\n/MTd link with LIBCMTD.LIB debug lib    \\n\\n                              -CODE ANALYSIS-\\n\\n/analyze[-] Enable native analysis      /analyze:quiet[-] No warning to console\\n/analyze:log<name> Warnings to file     /analyze:autolog Log to *.pftlog\\n/analyze:autolog:ext<ext> Log to *.<ext>/analyze:autolog- No log file\\n/analyze:WX- Warnings not fatal         /analyze:stacksize<num> Max stack frame\\n/analyze:max_paths<num> Max paths       /analyze:only Analyze, no code gen\\n\\n                              -DIAGNOSTICS-', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='-DIAGNOSTICS-\\n\\n/diagnostics:<args,...> controls the format of diagnostic messages:\\n             classic   - retains prior format\\n             column[-] - prints column information\\n             caret[-]  - prints column and the indicated line of source\\n/Wall enable all warnings               /w   disable all warnings\\n/W<n> set warning level (default n=1)   \\n/Wv:xx[.yy[.zzzzz]] disable warnings introduced after version xx.yy.zzzzz\\n/WX treat warnings as errors            /WL enable one line diagnostics\\n/wd<n> disable warning n                /we<n> treat warning n as an error\\n/wo<n> issue warning n once             /w<l><n> set warning level 1-4 for n\\n/external:I <path>      - location of external headers\\n/external:env:<var>     - environment variable with locations of external headers\\n/external:anglebrackets - treat all headers included via <> as external\\n/external:W<n>          - warning level for external headers\\n/external:templates[-]  - evaluate warning level across template instantiation chain\\n/sdl enable additional security features and warnings\\n/options:strict unrecognized compiler options are an error\\n-----------\\nstderr:\\nMicrosoft (R) C/C++ Optimizing Compiler Version 19.39.33521 for x64\\nCopyright (C) Microsoft Corporation.  All rights reserved.\\n-----------\\nSanity testing C++ compiler: cl\\nIs cross compiler: False.\\nSanity check compiler command line: cl sanitycheckcpp.cc /Fesanitycheckcpp.exe /MD /nologo /showIncludes /utf-8 /Zc:__cplusplus /link\\nSanity check compile stdout:\\nsanitycheckcpp.cc\\n\\n-----\\nSanity check compile stderr:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='-----\\nSanity check compile stderr:\\n\\n-----\\nRunning test binary command:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\sanitycheckcpp.exe\\nC++ compiler for the host machine: cl (msvc 19.39.33521 \"Microsoft (R) C/C++ Optimizing Compiler Version 19.39.33521 for x64\")\\nC++ linker for the host machine: link link 14.39.33521.0\\n-----------\\nDetecting compiler via: `cython -V` -> 0\\nstdout:\\nCython version 3.0.9\\n-----------\\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\AppData\\\\Local\\\\Temp\\\\tmpmvepxyuc\\nCode:\\n print(\"hello world\")\\n-----------\\nCommand line: `cython C:\\\\Users\\\\Aayush\\\\AppData\\\\Local\\\\Temp\\\\tmpmvepxyuc\\\\testfile.pyx -o C:\\\\Users\\\\Aayush\\\\AppData\\\\Local\\\\Temp\\\\tmpmvepxyuc\\\\output.exe --fast-fail` -> 0\\nstderr:\\nC:\\\\Users\\\\Aayush\\\\AppData\\\\Local\\\\Temp\\\\pip-build-env-u9xp56_3\\\\overlay\\\\Lib\\\\site-packages\\\\Cython\\\\Compiler\\\\Main.py:381: FutureWarning: Cython directive \\'language_level\\' not set, using \\'3str\\' for now (Py3). This has changed from earlier releases! File: C:\\\\Users\\\\Aayush\\\\AppData\\\\Local\\\\Temp\\\\tmpmvepxyuc\\\\testfile.pyx\\n  tree = Parsing.p_module(s, pxd, full_module_name)\\n-----------\\nCython compiler for the host machine: cython (cython 3.0.9)\\n-----------\\nDetecting compiler via: `icl \"\"` -> [WinError 2] The system cannot find the file specified\\n-----------\\nDetecting compiler via: `cl /?` -> 0\\nstdout:\\nC/C++ COMPILER OPTIONS\\n\\n\\n                              -OPTIMIZATION-', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='-OPTIMIZATION-\\n\\n/O1 maximum optimizations (favor space) /O2 maximum optimizations (favor speed)\\n/Ob<n> inline expansion (default n=0)   /Od disable optimizations (default)\\n/Og enable global optimization          /Oi[-] enable intrinsic functions\\n/Os favor code space                    /Ot favor code speed\\n/Ox optimizations (favor speed)         \\n/favor:<blend|AMD64|INTEL64|ATOM> select processor to optimize for, one of:\\n    blend - a combination of optimizations for several different x64 processors\\n    AMD64 - 64-bit AMD processors\\n    INTEL64 - Intel(R)64 architecture processors\\n    ATOM - Intel(R) Atom(TM) processors\\n\\n                             -CODE GENERATION-', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='/Gu[-] ensure distinct functions have distinct addresses\\n/Gw[-] separate global variables for linker\\n/GF enable read-only string pooling     /Gm[-] enable minimal rebuild\\n/Gy[-] separate functions for linker    /GS[-] enable security checks\\n/GR[-] enable C++ RTTI                  /GX[-] enable C++ EH (same as /EHsc)\\n/guard:cf[-] enable CFG (control flow guard)\\n/guard:ehcont[-] enable EH continuation metadata (CET)\\n/EHs enable C++ EH (no SEH exceptions)  /EHa enable C++ EH (w/ SEH exceptions)\\n/EHc extern \"C\" defaults to nothrow     \\n/EHr always generate noexcept runtime termination checks\\n/fp:<contract|except[-]|fast|precise|strict> choose floating-point model:\\n    contract - consider floating-point contractions when generating code\\n    except[-] - consider floating-point exceptions when generating code\\n    fast - \"fast\" floating-point model; results are less predictable\\n    precise - \"precise\" floating-point model; results are predictable\\n    strict - \"strict\" floating-point model (implies /fp:except)\\n/Qfast_transcendentals generate inline FP intrinsics even with /fp:except\\n/Qspectre[-] enable mitigations for CVE 2017-5753\\n/Qpar[-] enable parallel code generation\\n/Qpar-report:1 auto-parallelizer diagnostic; indicate parallelized loops\\n/Qpar-report:2 auto-parallelizer diagnostic; indicate loops not parallelized\\n/Qvec-report:1 auto-vectorizer diagnostic; indicate vectorized loops\\n/Qvec-report:2 auto-vectorizer diagnostic; indicate loops not vectorized\\n/GL[-] enable link-time code generation \\n/volatile:<iso|ms> choose volatile model:\\n    iso - Acquire/release semantics not guaranteed on volatile accesses\\n    ms  - Acquire/release semantics guaranteed on volatile accesses', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='/Qpar-report:2 auto-parallelizer diagnostic; indicate loops not parallelized\\n/Qvec-report:1 auto-vectorizer diagnostic; indicate vectorized loops\\n/Qvec-report:2 auto-vectorizer diagnostic; indicate loops not vectorized\\n/GL[-] enable link-time code generation \\n/volatile:<iso|ms> choose volatile model:\\n    iso - Acquire/release semantics not guaranteed on volatile accesses\\n    ms  - Acquire/release semantics guaranteed on volatile accesses\\n/GA optimize for Windows Application    /Ge force stack checking for all funcs\\n/Gs[num] control stack checking calls   /Gh enable _penter function call\\n/GH enable _pexit function call         /GT generate fiber-safe TLS accesses\\n/RTC1 Enable fast checks (/RTCsu)       /RTCc Convert to smaller type checks\\n/RTCs Stack Frame runtime checking      /RTCu Uninitialized local usage checks\\n/clr[:option] compile for common language runtime, where option is:\\n    pure : produce IL-only output file (no native executable code)\\n    safe : produce IL-only verifiable output file\\n    netcore : produce assemblies targeting .NET Core runtime\\n    noAssembly : do not produce an assembly\\n    nostdlib : ignore the system .NET framework directory when searching for assemblies\\n    nostdimport : do not import any required assemblies implicitly\\n    initialAppDomain : enable initial AppDomain behavior of Visual C++ 2002\\n    implicitKeepAlive- : turn off implicit emission of System::GC::KeepAlive(this)\\n/fsanitize=address Enable address sanitizer codegen\\n/homeparams Force parameters passed in registers to be written to the stack\\n/GZ Enable stack checks (/RTCs)         /Gv __vectorcall calling convention\\n/arch:<AVX|AVX2|AVX512> minimum CPU architecture requirements, one of:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='initialAppDomain : enable initial AppDomain behavior of Visual C++ 2002\\n    implicitKeepAlive- : turn off implicit emission of System::GC::KeepAlive(this)\\n/fsanitize=address Enable address sanitizer codegen\\n/homeparams Force parameters passed in registers to be written to the stack\\n/GZ Enable stack checks (/RTCs)         /Gv __vectorcall calling convention\\n/arch:<AVX|AVX2|AVX512> minimum CPU architecture requirements, one of:\\n   AVX - enable use of instructions available with AVX-enabled CPUs\\n   AVX2 - enable use of instructions available with AVX2-enabled CPUs\\n   AVX512 - enable use of instructions available with AVX-512-enabled CPUs\\n/QIntel-jcc-erratum enable mitigations for Intel JCC erratum\\n/Qspectre-load Enable spectre mitigations for all instructions which load memory\\n/Qspectre-load-cf Enable spectre mitigations for all control-flow instructions which load memory\\n/Qspectre-jmp[-] Enable spectre mitigations for unconditional jump instructions\\n/fpcvt:<IA|BC> FP to unsigned integer conversion compatibility\\n   IA - results compatible with VCVTTSD2USI instruction\\n   BC - results compatible with VS2017 and earlier compiler\\n/jumptablerdata Place jump tables for switch case statements in .rdata section', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='-OUTPUT FILES-\\n\\n/Fa[file] name assembly listing file    /FA[scu] configure assembly listing\\n/Fd[file] name .PDB file                /Fe<file> name executable file\\n/Fm[file] name map file                 /Fo<file> name object file\\n/Fp<file> name precompiled header file  /Fr[file] name source browser file\\n/FR[file] name extended .SBR file       /Fi[file] name preprocessed file\\n/Fd: <file> name .PDB file              /Fe: <file> name executable file\\n/Fm: <file> name map file               /Fo: <file> name object file\\n/Fp: <file> name .PCH file              /FR: <file> name extended .SBR file\\n/Fi: <file> name preprocessed file      \\n/Ft<dir> location of the header files generated for #import\\n/doc[file] process XML documentation comments and optionally name the .xdc file\\n\\n                              -PREPROCESSOR-\\n\\n/AI<dir> add to assembly search path    /FU<file> import .NET assembly/module\\n/FU:asFriend<file> import .NET assembly/module as friend\\n/C don\\'t strip comments                 /D<name>{=|#}<text> define macro\\n/E preprocess to stdout                 /EP preprocess to stdout, no #line\\n/P preprocess to file                   /Fx merge injected code to file\\n/FI<file> name forced include file      /U<name> remove predefined macro\\n/u remove all predefined macros         /I<dir> add to include search path\\n/X ignore \"standard places\"             \\n/PH generate #pragma file_hash when preprocessing\\n/PD print all macro definitions         \\n\\n                                -LANGUAGE-', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='/std:<c++14|c++17|c++20|c++latest> C++ standard version\\n    c++14 - ISO/IEC 14882:2014 (default)\\n    c++17 - ISO/IEC 14882:2017\\n    c++20 - ISO/IEC 14882:2020\\n    c++latest - latest draft standard (feature set subject to change)\\n/std:<c11|c17|clatest> C standard version\\n    c11 - ISO/IEC 9899:2011\\n    c17 - ISO/IEC 9899:2018\\n    clatest - latest draft standard (feature set subject to change)\\n/permissive[-] enable some nonconforming code to compile\\n               (feature set subject to change) (off by default in C++20 and later)\\n/Ze (deprecated) enable extensions (default)\\n/Za disable extensions (not recommended for C++)\\n/ZW enable WinRT language extensions    /Zs syntax check only\\n/await enable resumable functions extension\\n/await:strict enable standard C++20 coroutine support with earlier language versions\\n/constexpr:depth<N>     recursion depth limit for constexpr evaluation (default: 512)\\n/constexpr:backtrace<N> show N constexpr evaluations in diagnostics (default: 10)\\n/constexpr:steps<N>     terminate constexpr evaluation after N steps (default: 1048576)\\n/Zi enable debugging information        /Z7 enable old-style debug info\\n/Zo[-] generate richer debugging information for optimized code (on by default)\\n/ZH:[MD5|SHA1|SHA_256] hash algorithm for calculation of file checksum in debug info (default: SHA_256)\\n/Zp[n] pack structs on n-byte boundary  /Zl omit default library name in .OBJ\\n/vd{0|1|2} disable/enable vtordisp      /vm<x> type of pointers to members\\n/Zc:arg1[,arg2] language conformance, where arguments can be:\\n  forScope[-]           enforce Standard C++ for scoping rules\\n  wchar_t[-]            wchar_t is the native type, not a typedef', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='/ZH:[MD5|SHA1|SHA_256] hash algorithm for calculation of file checksum in debug info (default: SHA_256)\\n/Zp[n] pack structs on n-byte boundary  /Zl omit default library name in .OBJ\\n/vd{0|1|2} disable/enable vtordisp      /vm<x> type of pointers to members\\n/Zc:arg1[,arg2] language conformance, where arguments can be:\\n  forScope[-]           enforce Standard C++ for scoping rules\\n  wchar_t[-]            wchar_t is the native type, not a typedef\\n  auto[-]               enforce the new Standard C++ meaning for auto\\n  trigraphs[-]          enable trigraphs (off by default)\\n  rvalueCast[-]         enforce Standard C++ explicit type conversion rules\\n                        (on by default in C++20 or later, implied by /permissive-)\\n  strictStrings[-]      disable string-literal to [char|wchar_t]*\\n                        conversion (on by default in C++20 or later, implied by /permissive-)\\n  implicitNoexcept[-]   enable implicit noexcept on required functions\\n  threadSafeInit[-]     enable thread-safe local static initialization\\n  inline[-]             remove unreferenced function or data if it is\\n                        COMDAT or has internal linkage only (off by default)\\n  sizedDealloc[-]       enable C++14 global sized deallocation\\n                        functions (on by default)\\n  throwingNew[-]        assume operator new throws on failure (off by default)\\n  referenceBinding[-]   a temporary will not bind to a non-const\\n                        lvalue reference (on by default in C++20 or later, implied by /permissive-)\\n  twoPhase-             disable two-phase name lookup\\n  ternary[-]            enforce C++11 rules for conditional operator', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='sizedDealloc[-]       enable C++14 global sized deallocation\\n                        functions (on by default)\\n  throwingNew[-]        assume operator new throws on failure (off by default)\\n  referenceBinding[-]   a temporary will not bind to a non-const\\n                        lvalue reference (on by default in C++20 or later, implied by /permissive-)\\n  twoPhase-             disable two-phase name lookup\\n  ternary[-]            enforce C++11 rules for conditional operator\\n                        (on by default in C++20 or later, implied by /permissive-)\\n  noexceptTypes[-]      enforce C++17 noexcept rules (on by default in C++17 or later)\\n  alignedNew[-]         enable C++17 alignment of dynamically allocated objects (on by default)\\n  hiddenFriend[-]       enforce Standard C++ hidden friend rules\\n                        (on by default in C++20 or later, implied by /permissive-)\\n  externC[-]            enforce Standard C++ rules for \\'extern \"C\"\\' functions\\n                        (on by default in C++20 or later, implied by /permissive-)\\n  lambda[-]             better lambda support by using the newer lambda processor\\n                        (on by default in C++20 or later, implied by /permissive-)\\n  tlsGuards[-]          generate runtime checks for TLS variable initialization (on by default)\\n  zeroSizeArrayNew[-]   call member new/delete for 0-size arrays of objects (on by default)\\n  static_assert[-]      strict handling of \\'static_assert\\' (on by default in C++20 or later,\\n                        implied by /permissive-)\\n  gotoScope[-]          cannot jump past the initialization of a variable (implied by /permissive-)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"tlsGuards[-]          generate runtime checks for TLS variable initialization (on by default)\\n  zeroSizeArrayNew[-]   call member new/delete for 0-size arrays of objects (on by default)\\n  static_assert[-]      strict handling of 'static_assert' (on by default in C++20 or later,\\n                        implied by /permissive-)\\n  gotoScope[-]          cannot jump past the initialization of a variable (implied by /permissive-)\\n  templateScope[-]      enforce Standard C++ template parameter shadowing rules\\n  enumTypes[-]          enable Standard C++ underlying enum types (off by default)\\n  checkGwOdr[-]         enforce Standard C++ one definition rule violations\\n                        when /Gw has been enabled (off by default)\\n  nrvo[-]               enable optional copy and move elision (on by default in C++20 or later,\\n                        implied by /permissive- or /O2)\\n  __STDC__              define __STDC__ to 1 in C\\n  __cplusplus[-]        __cplusplus macro reports the supported C++ standard (off by default)\\n  char8_t[-]            enable C++20 native `u8` literal support as `const char8_t`\\n                        (on by default in C++20 or later)\\n  externConstexpr[-]    enable external linkage for constexpr variables in C++\\n                        (on by default in C++20 or later, implied by /permissive-)\\n  preprocessor[-]       enable standard conforming preprocessor in C/C++\\n                        (on by default in C11 or later)\\n/ZI enable Edit and Continue debug info \\n/openmp enable OpenMP 2.0 language extensions\\n/openmp:experimental enable OpenMP 2.0 language extensions plus select OpenMP 3.0+ language extensions\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='externConstexpr[-]    enable external linkage for constexpr variables in C++\\n                        (on by default in C++20 or later, implied by /permissive-)\\n  preprocessor[-]       enable standard conforming preprocessor in C/C++\\n                        (on by default in C11 or later)\\n/ZI enable Edit and Continue debug info \\n/openmp enable OpenMP 2.0 language extensions\\n/openmp:experimental enable OpenMP 2.0 language extensions plus select OpenMP 3.0+ language extensions\\n/openmp:llvm OpenMP language extensions using LLVM runtime', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"-MISCELLANEOUS-\\n\\n@<file> options response file           /?, /help print this help message\\n/bigobj generate extended object format /c compile only, no link\\n/errorReport:option deprecated. Report internal compiler errors to Microsoft\\n    none - do not send report\\n    prompt - prompt to immediately send report\\n    queue - at next admin logon, prompt to send report (default)\\n    send - send report automatically\\n/FC use full pathnames in diagnostics   /H<num> max external name length\\n/J default char type is unsigned        \\n/MP[n] use up to 'n' processes for compilation\\n/nologo suppress copyright message      /showIncludes show include file names\\n/Tc<source file> compile file as .c     /Tp<source file> compile file as .cpp\\n/TC compile all files as .c             /TP compile all files as .cpp\\n/V<string> set version string           /Yc[file] create .PCH file\\n/Yd put debug info in every .OBJ        /Yl[sym] inject .PCH ref for debug lib\\n/Yu[file] use .PCH file                 /Y- disable all PCH options\\n/Zm<n> max memory alloc (% of default)  /FS force to use MSPDBSRV.EXE\\n/source-charset:<iana-name>|.nnnn set source character set\\n/execution-charset:<iana-name>|.nnnn set execution character set\\n/utf-8 set source and execution character set to UTF-8\\n/validate-charset[-] validate UTF-8 files for only legal characters\\n/fastfail[-] enable fast-fail mode      /JMC[-] enable native just my code\\n/presetPadding[-] zero initialize padding for stack based class types\\n/volatileMetadata[-] generate metadata on volatile memory accesses\\n/sourcelink [file] file containing source link information\\n\\n                                -LINKING-\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='-LINKING-\\n\\n/LD Create .DLL                         /LDd Create .DLL debug library\\n/LN Create a .netmodule                 /F<num> set stack size\\n/link [linker options and libraries]    /MD link with MSVCRT.LIB\\n/MT link with LIBCMT.LIB                /MDd link with MSVCRTD.LIB debug lib\\n/MTd link with LIBCMTD.LIB debug lib    \\n\\n                              -CODE ANALYSIS-\\n\\n/analyze[-] Enable native analysis      /analyze:quiet[-] No warning to console\\n/analyze:log<name> Warnings to file     /analyze:autolog Log to *.pftlog\\n/analyze:autolog:ext<ext> Log to *.<ext>/analyze:autolog- No log file\\n/analyze:WX- Warnings not fatal         /analyze:stacksize<num> Max stack frame\\n/analyze:max_paths<num> Max paths       /analyze:only Analyze, no code gen\\n\\n                              -DIAGNOSTICS-', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='-DIAGNOSTICS-\\n\\n/diagnostics:<args,...> controls the format of diagnostic messages:\\n             classic   - retains prior format\\n             column[-] - prints column information\\n             caret[-]  - prints column and the indicated line of source\\n/Wall enable all warnings               /w   disable all warnings\\n/W<n> set warning level (default n=1)   \\n/Wv:xx[.yy[.zzzzz]] disable warnings introduced after version xx.yy.zzzzz\\n/WX treat warnings as errors            /WL enable one line diagnostics\\n/wd<n> disable warning n                /we<n> treat warning n as an error\\n/wo<n> issue warning n once             /w<l><n> set warning level 1-4 for n\\n/external:I <path>      - location of external headers\\n/external:env:<var>     - environment variable with locations of external headers\\n/external:anglebrackets - treat all headers included via <> as external\\n/external:W<n>          - warning level for external headers\\n/external:templates[-]  - evaluate warning level across template instantiation chain\\n/sdl enable additional security features and warnings\\n/options:strict unrecognized compiler options are an error\\n-----------\\nstderr:\\nMicrosoft (R) C/C++ Optimizing Compiler Version 19.39.33521 for x64\\nCopyright (C) Microsoft Corporation.  All rights reserved.\\n-----------\\nSanity testing C compiler: cl\\nIs cross compiler: False.\\nSanity check compiler command line: cl sanitycheckc.c /Fesanitycheckc.exe /MD /nologo /showIncludes /utf-8 /link\\nSanity check compile stdout:\\nsanitycheckc.c\\n\\n-----\\nSanity check compile stderr:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='-----\\nSanity check compile stderr:\\n\\n-----\\nRunning test binary command:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\sanitycheckc.exe\\nC compiler for the build machine: cl (msvc 19.39.33521 \"Microsoft (R) C/C++ Optimizing Compiler Version 19.39.33521 for x64\")\\nC linker for the build machine: link link 14.39.33521.0\\n-----------\\nDetecting archiver via: `lib /?` -> 1100\\nstdout:\\nMicrosoft (R) Library Manager Version 14.39.33521.0\\nCopyright (C) Microsoft Corporation.  All rights reserved.\\n\\nusage: LIB [options] [files]\\n\\n   options:\\n\\n      /DEF[:filename]\\n      /ERRORREPORT:{NONE|PROMPT|QUEUE|SEND}\\n      /EXPORT:symbol\\n      /EXTRACT:membername\\n      /INCLUDE:symbol\\n      /LIBPATH:dir\\n      /LINKREPRO:dir\\n      /LINKREPROTARGET:filename\\n      /LIST[:filename]\\n      /LTCG\\n      /MACHINE:{ARM|ARM64|ARM64X|EBC|X64|X86}\\n      /NAME:filename\\n      /NODEFAULTLIB[:library]\\n      /NOLOGO\\n      /OUT:filename\\n      /REMOVE:membername\\n      /SUBSYSTEM:{BOOT_APPLICATION|CONSOLE|EFI_APPLICATION|\\n                  EFI_BOOT_SERVICE_DRIVER|EFI_ROM|EFI_RUNTIME_DRIVER|\\n                  NATIVE|POSIX|WINDOWS|WINDOWSCE}[,#[.##]]\\n      /VERBOSE\\n      /WX[:NO]\\n      /WX[:nnnn[,nnnn...]]\\n-----------\\n-----------\\nDetecting compiler via: `icl \"\"` -> [WinError 2] The system cannot find the file specified\\n-----------\\nDetecting compiler via: `cl /?` -> 0\\nstdout:\\nC/C++ COMPILER OPTIONS\\n\\n\\n                              -OPTIMIZATION-', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='-OPTIMIZATION-\\n\\n/O1 maximum optimizations (favor space) /O2 maximum optimizations (favor speed)\\n/Ob<n> inline expansion (default n=0)   /Od disable optimizations (default)\\n/Og enable global optimization          /Oi[-] enable intrinsic functions\\n/Os favor code space                    /Ot favor code speed\\n/Ox optimizations (favor speed)         \\n/favor:<blend|AMD64|INTEL64|ATOM> select processor to optimize for, one of:\\n    blend - a combination of optimizations for several different x64 processors\\n    AMD64 - 64-bit AMD processors\\n    INTEL64 - Intel(R)64 architecture processors\\n    ATOM - Intel(R) Atom(TM) processors\\n\\n                             -CODE GENERATION-', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='/Gu[-] ensure distinct functions have distinct addresses\\n/Gw[-] separate global variables for linker\\n/GF enable read-only string pooling     /Gm[-] enable minimal rebuild\\n/Gy[-] separate functions for linker    /GS[-] enable security checks\\n/GR[-] enable C++ RTTI                  /GX[-] enable C++ EH (same as /EHsc)\\n/guard:cf[-] enable CFG (control flow guard)\\n/guard:ehcont[-] enable EH continuation metadata (CET)\\n/EHs enable C++ EH (no SEH exceptions)  /EHa enable C++ EH (w/ SEH exceptions)\\n/EHc extern \"C\" defaults to nothrow     \\n/EHr always generate noexcept runtime termination checks\\n/fp:<contract|except[-]|fast|precise|strict> choose floating-point model:\\n    contract - consider floating-point contractions when generating code\\n    except[-] - consider floating-point exceptions when generating code\\n    fast - \"fast\" floating-point model; results are less predictable\\n    precise - \"precise\" floating-point model; results are predictable\\n    strict - \"strict\" floating-point model (implies /fp:except)\\n/Qfast_transcendentals generate inline FP intrinsics even with /fp:except\\n/Qspectre[-] enable mitigations for CVE 2017-5753\\n/Qpar[-] enable parallel code generation\\n/Qpar-report:1 auto-parallelizer diagnostic; indicate parallelized loops\\n/Qpar-report:2 auto-parallelizer diagnostic; indicate loops not parallelized\\n/Qvec-report:1 auto-vectorizer diagnostic; indicate vectorized loops\\n/Qvec-report:2 auto-vectorizer diagnostic; indicate loops not vectorized\\n/GL[-] enable link-time code generation \\n/volatile:<iso|ms> choose volatile model:\\n    iso - Acquire/release semantics not guaranteed on volatile accesses\\n    ms  - Acquire/release semantics guaranteed on volatile accesses', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='/Qpar-report:2 auto-parallelizer diagnostic; indicate loops not parallelized\\n/Qvec-report:1 auto-vectorizer diagnostic; indicate vectorized loops\\n/Qvec-report:2 auto-vectorizer diagnostic; indicate loops not vectorized\\n/GL[-] enable link-time code generation \\n/volatile:<iso|ms> choose volatile model:\\n    iso - Acquire/release semantics not guaranteed on volatile accesses\\n    ms  - Acquire/release semantics guaranteed on volatile accesses\\n/GA optimize for Windows Application    /Ge force stack checking for all funcs\\n/Gs[num] control stack checking calls   /Gh enable _penter function call\\n/GH enable _pexit function call         /GT generate fiber-safe TLS accesses\\n/RTC1 Enable fast checks (/RTCsu)       /RTCc Convert to smaller type checks\\n/RTCs Stack Frame runtime checking      /RTCu Uninitialized local usage checks\\n/clr[:option] compile for common language runtime, where option is:\\n    pure : produce IL-only output file (no native executable code)\\n    safe : produce IL-only verifiable output file\\n    netcore : produce assemblies targeting .NET Core runtime\\n    noAssembly : do not produce an assembly\\n    nostdlib : ignore the system .NET framework directory when searching for assemblies\\n    nostdimport : do not import any required assemblies implicitly\\n    initialAppDomain : enable initial AppDomain behavior of Visual C++ 2002\\n    implicitKeepAlive- : turn off implicit emission of System::GC::KeepAlive(this)\\n/fsanitize=address Enable address sanitizer codegen\\n/homeparams Force parameters passed in registers to be written to the stack\\n/GZ Enable stack checks (/RTCs)         /Gv __vectorcall calling convention\\n/arch:<AVX|AVX2|AVX512> minimum CPU architecture requirements, one of:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='initialAppDomain : enable initial AppDomain behavior of Visual C++ 2002\\n    implicitKeepAlive- : turn off implicit emission of System::GC::KeepAlive(this)\\n/fsanitize=address Enable address sanitizer codegen\\n/homeparams Force parameters passed in registers to be written to the stack\\n/GZ Enable stack checks (/RTCs)         /Gv __vectorcall calling convention\\n/arch:<AVX|AVX2|AVX512> minimum CPU architecture requirements, one of:\\n   AVX - enable use of instructions available with AVX-enabled CPUs\\n   AVX2 - enable use of instructions available with AVX2-enabled CPUs\\n   AVX512 - enable use of instructions available with AVX-512-enabled CPUs\\n/QIntel-jcc-erratum enable mitigations for Intel JCC erratum\\n/Qspectre-load Enable spectre mitigations for all instructions which load memory\\n/Qspectre-load-cf Enable spectre mitigations for all control-flow instructions which load memory\\n/Qspectre-jmp[-] Enable spectre mitigations for unconditional jump instructions\\n/fpcvt:<IA|BC> FP to unsigned integer conversion compatibility\\n   IA - results compatible with VCVTTSD2USI instruction\\n   BC - results compatible with VS2017 and earlier compiler\\n/jumptablerdata Place jump tables for switch case statements in .rdata section', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='-OUTPUT FILES-\\n\\n/Fa[file] name assembly listing file    /FA[scu] configure assembly listing\\n/Fd[file] name .PDB file                /Fe<file> name executable file\\n/Fm[file] name map file                 /Fo<file> name object file\\n/Fp<file> name precompiled header file  /Fr[file] name source browser file\\n/FR[file] name extended .SBR file       /Fi[file] name preprocessed file\\n/Fd: <file> name .PDB file              /Fe: <file> name executable file\\n/Fm: <file> name map file               /Fo: <file> name object file\\n/Fp: <file> name .PCH file              /FR: <file> name extended .SBR file\\n/Fi: <file> name preprocessed file      \\n/Ft<dir> location of the header files generated for #import\\n/doc[file] process XML documentation comments and optionally name the .xdc file\\n\\n                              -PREPROCESSOR-\\n\\n/AI<dir> add to assembly search path    /FU<file> import .NET assembly/module\\n/FU:asFriend<file> import .NET assembly/module as friend\\n/C don\\'t strip comments                 /D<name>{=|#}<text> define macro\\n/E preprocess to stdout                 /EP preprocess to stdout, no #line\\n/P preprocess to file                   /Fx merge injected code to file\\n/FI<file> name forced include file      /U<name> remove predefined macro\\n/u remove all predefined macros         /I<dir> add to include search path\\n/X ignore \"standard places\"             \\n/PH generate #pragma file_hash when preprocessing\\n/PD print all macro definitions         \\n\\n                                -LANGUAGE-', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='/std:<c++14|c++17|c++20|c++latest> C++ standard version\\n    c++14 - ISO/IEC 14882:2014 (default)\\n    c++17 - ISO/IEC 14882:2017\\n    c++20 - ISO/IEC 14882:2020\\n    c++latest - latest draft standard (feature set subject to change)\\n/std:<c11|c17|clatest> C standard version\\n    c11 - ISO/IEC 9899:2011\\n    c17 - ISO/IEC 9899:2018\\n    clatest - latest draft standard (feature set subject to change)\\n/permissive[-] enable some nonconforming code to compile\\n               (feature set subject to change) (off by default in C++20 and later)\\n/Ze (deprecated) enable extensions (default)\\n/Za disable extensions (not recommended for C++)\\n/ZW enable WinRT language extensions    /Zs syntax check only\\n/await enable resumable functions extension\\n/await:strict enable standard C++20 coroutine support with earlier language versions\\n/constexpr:depth<N>     recursion depth limit for constexpr evaluation (default: 512)\\n/constexpr:backtrace<N> show N constexpr evaluations in diagnostics (default: 10)\\n/constexpr:steps<N>     terminate constexpr evaluation after N steps (default: 1048576)\\n/Zi enable debugging information        /Z7 enable old-style debug info\\n/Zo[-] generate richer debugging information for optimized code (on by default)\\n/ZH:[MD5|SHA1|SHA_256] hash algorithm for calculation of file checksum in debug info (default: SHA_256)\\n/Zp[n] pack structs on n-byte boundary  /Zl omit default library name in .OBJ\\n/vd{0|1|2} disable/enable vtordisp      /vm<x> type of pointers to members\\n/Zc:arg1[,arg2] language conformance, where arguments can be:\\n  forScope[-]           enforce Standard C++ for scoping rules\\n  wchar_t[-]            wchar_t is the native type, not a typedef', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='/ZH:[MD5|SHA1|SHA_256] hash algorithm for calculation of file checksum in debug info (default: SHA_256)\\n/Zp[n] pack structs on n-byte boundary  /Zl omit default library name in .OBJ\\n/vd{0|1|2} disable/enable vtordisp      /vm<x> type of pointers to members\\n/Zc:arg1[,arg2] language conformance, where arguments can be:\\n  forScope[-]           enforce Standard C++ for scoping rules\\n  wchar_t[-]            wchar_t is the native type, not a typedef\\n  auto[-]               enforce the new Standard C++ meaning for auto\\n  trigraphs[-]          enable trigraphs (off by default)\\n  rvalueCast[-]         enforce Standard C++ explicit type conversion rules\\n                        (on by default in C++20 or later, implied by /permissive-)\\n  strictStrings[-]      disable string-literal to [char|wchar_t]*\\n                        conversion (on by default in C++20 or later, implied by /permissive-)\\n  implicitNoexcept[-]   enable implicit noexcept on required functions\\n  threadSafeInit[-]     enable thread-safe local static initialization\\n  inline[-]             remove unreferenced function or data if it is\\n                        COMDAT or has internal linkage only (off by default)\\n  sizedDealloc[-]       enable C++14 global sized deallocation\\n                        functions (on by default)\\n  throwingNew[-]        assume operator new throws on failure (off by default)\\n  referenceBinding[-]   a temporary will not bind to a non-const\\n                        lvalue reference (on by default in C++20 or later, implied by /permissive-)\\n  twoPhase-             disable two-phase name lookup\\n  ternary[-]            enforce C++11 rules for conditional operator', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='sizedDealloc[-]       enable C++14 global sized deallocation\\n                        functions (on by default)\\n  throwingNew[-]        assume operator new throws on failure (off by default)\\n  referenceBinding[-]   a temporary will not bind to a non-const\\n                        lvalue reference (on by default in C++20 or later, implied by /permissive-)\\n  twoPhase-             disable two-phase name lookup\\n  ternary[-]            enforce C++11 rules for conditional operator\\n                        (on by default in C++20 or later, implied by /permissive-)\\n  noexceptTypes[-]      enforce C++17 noexcept rules (on by default in C++17 or later)\\n  alignedNew[-]         enable C++17 alignment of dynamically allocated objects (on by default)\\n  hiddenFriend[-]       enforce Standard C++ hidden friend rules\\n                        (on by default in C++20 or later, implied by /permissive-)\\n  externC[-]            enforce Standard C++ rules for \\'extern \"C\"\\' functions\\n                        (on by default in C++20 or later, implied by /permissive-)\\n  lambda[-]             better lambda support by using the newer lambda processor\\n                        (on by default in C++20 or later, implied by /permissive-)\\n  tlsGuards[-]          generate runtime checks for TLS variable initialization (on by default)\\n  zeroSizeArrayNew[-]   call member new/delete for 0-size arrays of objects (on by default)\\n  static_assert[-]      strict handling of \\'static_assert\\' (on by default in C++20 or later,\\n                        implied by /permissive-)\\n  gotoScope[-]          cannot jump past the initialization of a variable (implied by /permissive-)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"tlsGuards[-]          generate runtime checks for TLS variable initialization (on by default)\\n  zeroSizeArrayNew[-]   call member new/delete for 0-size arrays of objects (on by default)\\n  static_assert[-]      strict handling of 'static_assert' (on by default in C++20 or later,\\n                        implied by /permissive-)\\n  gotoScope[-]          cannot jump past the initialization of a variable (implied by /permissive-)\\n  templateScope[-]      enforce Standard C++ template parameter shadowing rules\\n  enumTypes[-]          enable Standard C++ underlying enum types (off by default)\\n  checkGwOdr[-]         enforce Standard C++ one definition rule violations\\n                        when /Gw has been enabled (off by default)\\n  nrvo[-]               enable optional copy and move elision (on by default in C++20 or later,\\n                        implied by /permissive- or /O2)\\n  __STDC__              define __STDC__ to 1 in C\\n  __cplusplus[-]        __cplusplus macro reports the supported C++ standard (off by default)\\n  char8_t[-]            enable C++20 native `u8` literal support as `const char8_t`\\n                        (on by default in C++20 or later)\\n  externConstexpr[-]    enable external linkage for constexpr variables in C++\\n                        (on by default in C++20 or later, implied by /permissive-)\\n  preprocessor[-]       enable standard conforming preprocessor in C/C++\\n                        (on by default in C11 or later)\\n/ZI enable Edit and Continue debug info \\n/openmp enable OpenMP 2.0 language extensions\\n/openmp:experimental enable OpenMP 2.0 language extensions plus select OpenMP 3.0+ language extensions\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='externConstexpr[-]    enable external linkage for constexpr variables in C++\\n                        (on by default in C++20 or later, implied by /permissive-)\\n  preprocessor[-]       enable standard conforming preprocessor in C/C++\\n                        (on by default in C11 or later)\\n/ZI enable Edit and Continue debug info \\n/openmp enable OpenMP 2.0 language extensions\\n/openmp:experimental enable OpenMP 2.0 language extensions plus select OpenMP 3.0+ language extensions\\n/openmp:llvm OpenMP language extensions using LLVM runtime', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"-MISCELLANEOUS-\\n\\n@<file> options response file           /?, /help print this help message\\n/bigobj generate extended object format /c compile only, no link\\n/errorReport:option deprecated. Report internal compiler errors to Microsoft\\n    none - do not send report\\n    prompt - prompt to immediately send report\\n    queue - at next admin logon, prompt to send report (default)\\n    send - send report automatically\\n/FC use full pathnames in diagnostics   /H<num> max external name length\\n/J default char type is unsigned        \\n/MP[n] use up to 'n' processes for compilation\\n/nologo suppress copyright message      /showIncludes show include file names\\n/Tc<source file> compile file as .c     /Tp<source file> compile file as .cpp\\n/TC compile all files as .c             /TP compile all files as .cpp\\n/V<string> set version string           /Yc[file] create .PCH file\\n/Yd put debug info in every .OBJ        /Yl[sym] inject .PCH ref for debug lib\\n/Yu[file] use .PCH file                 /Y- disable all PCH options\\n/Zm<n> max memory alloc (% of default)  /FS force to use MSPDBSRV.EXE\\n/source-charset:<iana-name>|.nnnn set source character set\\n/execution-charset:<iana-name>|.nnnn set execution character set\\n/utf-8 set source and execution character set to UTF-8\\n/validate-charset[-] validate UTF-8 files for only legal characters\\n/fastfail[-] enable fast-fail mode      /JMC[-] enable native just my code\\n/presetPadding[-] zero initialize padding for stack based class types\\n/volatileMetadata[-] generate metadata on volatile memory accesses\\n/sourcelink [file] file containing source link information\\n\\n                                -LINKING-\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='-LINKING-\\n\\n/LD Create .DLL                         /LDd Create .DLL debug library\\n/LN Create a .netmodule                 /F<num> set stack size\\n/link [linker options and libraries]    /MD link with MSVCRT.LIB\\n/MT link with LIBCMT.LIB                /MDd link with MSVCRTD.LIB debug lib\\n/MTd link with LIBCMTD.LIB debug lib    \\n\\n                              -CODE ANALYSIS-\\n\\n/analyze[-] Enable native analysis      /analyze:quiet[-] No warning to console\\n/analyze:log<name> Warnings to file     /analyze:autolog Log to *.pftlog\\n/analyze:autolog:ext<ext> Log to *.<ext>/analyze:autolog- No log file\\n/analyze:WX- Warnings not fatal         /analyze:stacksize<num> Max stack frame\\n/analyze:max_paths<num> Max paths       /analyze:only Analyze, no code gen\\n\\n                              -DIAGNOSTICS-', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='-DIAGNOSTICS-\\n\\n/diagnostics:<args,...> controls the format of diagnostic messages:\\n             classic   - retains prior format\\n             column[-] - prints column information\\n             caret[-]  - prints column and the indicated line of source\\n/Wall enable all warnings               /w   disable all warnings\\n/W<n> set warning level (default n=1)   \\n/Wv:xx[.yy[.zzzzz]] disable warnings introduced after version xx.yy.zzzzz\\n/WX treat warnings as errors            /WL enable one line diagnostics\\n/wd<n> disable warning n                /we<n> treat warning n as an error\\n/wo<n> issue warning n once             /w<l><n> set warning level 1-4 for n\\n/external:I <path>      - location of external headers\\n/external:env:<var>     - environment variable with locations of external headers\\n/external:anglebrackets - treat all headers included via <> as external\\n/external:W<n>          - warning level for external headers\\n/external:templates[-]  - evaluate warning level across template instantiation chain\\n/sdl enable additional security features and warnings\\n/options:strict unrecognized compiler options are an error\\n-----------\\nstderr:\\nMicrosoft (R) C/C++ Optimizing Compiler Version 19.39.33521 for x64\\nCopyright (C) Microsoft Corporation.  All rights reserved.\\n-----------\\nSanity testing C++ compiler: cl\\nIs cross compiler: False.\\nSanity check compiler command line: cl sanitycheckcpp.cc /Fesanitycheckcpp.exe /MD /nologo /showIncludes /utf-8 /Zc:__cplusplus /link\\nSanity check compile stdout:\\nsanitycheckcpp.cc\\n\\n-----\\nSanity check compile stderr:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='-----\\nSanity check compile stderr:\\n\\n-----\\nRunning test binary command:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\sanitycheckcpp.exe\\nC++ compiler for the build machine: cl (msvc 19.39.33521 \"Microsoft (R) C/C++ Optimizing Compiler Version 19.39.33521 for x64\")\\nC++ linker for the build machine: link link 14.39.33521.0\\n-----------\\nDetecting compiler via: `cython -V` -> 0\\nstdout:\\nCython version 3.0.9\\n-----------\\nUsing cached compile:\\nCached command line:  cython C:\\\\Users\\\\Aayush\\\\AppData\\\\Local\\\\Temp\\\\tmpmvepxyuc\\\\testfile.pyx -o C:\\\\Users\\\\Aayush\\\\AppData\\\\Local\\\\Temp\\\\tmpmvepxyuc\\\\output.exe --fast-fail \\n\\nCode:\\n print(\"hello world\")\\nCached compiler stdout:\\n \\nCached compiler stderr:\\n C:\\\\Users\\\\Aayush\\\\AppData\\\\Local\\\\Temp\\\\pip-build-env-u9xp56_3\\\\overlay\\\\Lib\\\\site-packages\\\\Cython\\\\Compiler\\\\Main.py:381: FutureWarning: Cython directive \\'language_level\\' not set, using \\'3str\\' for now (Py3). This has changed from earlier releases! File: C:\\\\Users\\\\Aayush\\\\AppData\\\\Local\\\\Temp\\\\tmpmvepxyuc\\\\testfile.pyx\\n  tree = Parsing.p_module(s, pxd, full_module_name)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='Cython compiler for the build machine: cython (cython 3.0.9)\\nBuild machine cpu family: x86_64\\nBuild machine cpu: x86_64\\nHost machine cpu family: x86_64\\nHost machine cpu: x86_64\\nTarget machine cpu family: x86_64\\nTarget machine cpu: x86_64\\n\\'utf-8\\' codec can\\'t decode byte 0x90 in position 2: invalid start byte\\nUnusable script \\'C:\\\\\\\\ProgramData\\\\\\\\miniconda3\\\\\\\\python.exe\\'\\nProgram python found: YES (C:\\\\ProgramData\\\\miniconda3\\\\python.exe)\\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpi75j082n\\nCode:\\n \\n        #ifdef __has_include\\n         #if !__has_include(\"Python.h\")\\n          #error \"Header \\'Python.h\\' could not be found\"\\n         #endif\\n        #else\\n         #include <Python.h>\\n        #endif\\n-----------\\nCommand line: `cl -IC:\\\\ProgramData\\\\miniconda3\\\\Include C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpi75j082n\\\\testfile.cpp /nologo /showIncludes /utf-8 /Zc:__cplusplus /EP /nologo /showIncludes /utf-8 /Zc:__cplusplus /EP /Od /Oi-` -> 0\\nstderr:\\ntestfile.cpp\\n-----------\\nRun-time dependency python found: YES 3.12\\nProgram cython found: YES (C:\\\\Users\\\\Aayush\\\\AppData\\\\Local\\\\Temp\\\\pip-build-env-u9xp56_3\\\\overlay\\\\Scripts\\\\cython.EXE)\\nLibrary m found: NO\\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpz00_gubk\\nCode:\\n extern int i;\\nint i;', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpz00_gubk\\\\testfile.c /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpz00_gubk\\\\output.obj /nologo /showIncludes /utf-8 /c /nologo /showIncludes /utf-8 /c /Od /Oi- -Wmaybe-uninitialized -Wno-maybe-uninitialized` -> 2\\nstderr:\\ncl : Command line error D8021 : invalid numeric argument '/Wmaybe-uninitialized'\\n-----------\\nCompiler for C supports arguments -Wno-maybe-uninitialized: NO \\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpsyv_ryxz\\nCode:\\n extern int i;\\nint i;\\n\\n-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpsyv_ryxz\\\\testfile.c /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpsyv_ryxz\\\\output.obj /nologo /showIncludes /utf-8 /c /nologo /showIncludes /utf-8 /c /Od /Oi- -Wdiscarded-qualifiers -Wno-discarded-qualifiers` -> 2\\nstderr:\\ncl : Command line error D8021 : invalid numeric argument '/Wdiscarded-qualifiers'\\n-----------\\nCompiler for C supports arguments -Wno-discarded-qualifiers: NO \\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpegt_kc7f\\nCode:\\n extern int i;\\nint i;\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpegt_kc7f\\\\testfile.c /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpegt_kc7f\\\\output.obj /nologo /showIncludes /utf-8 /c /nologo /showIncludes /utf-8 /c /Od /Oi- -Wempty-body -Wno-empty-body` -> 2\\nstderr:\\ncl : Command line error D8021 : invalid numeric argument '/Wempty-body'\\n-----------\\nCompiler for C supports arguments -Wno-empty-body: NO \\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpznzes4bi\\nCode:\\n extern int i;\\nint i;\\n\\n-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpznzes4bi\\\\testfile.c /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpznzes4bi\\\\output.obj /nologo /showIncludes /utf-8 /c /nologo /showIncludes /utf-8 /c /Od /Oi- -Wimplicit-function-declaration -Wno-implicit-function-declaration` -> 2\\nstderr:\\ncl : Command line error D8021 : invalid numeric argument '/Wimplicit-function-declaration'\\n-----------\\nCompiler for C supports arguments -Wno-implicit-function-declaration: NO \\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpz_zoc1es\\nCode:\\n extern int i;\\nint i;\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpz_zoc1es\\\\testfile.c /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpz_zoc1es\\\\output.obj /nologo /showIncludes /utf-8 /c /nologo /showIncludes /utf-8 /c /Od /Oi- -Wparentheses -Wno-parentheses` -> 2\\nstderr:\\ncl : Command line error D8021 : invalid numeric argument '/Wparentheses'\\n-----------\\nCompiler for C supports arguments -Wno-parentheses: NO \\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpe4by1c1m\\nCode:\\n extern int i;\\nint i;\\n\\n-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpe4by1c1m\\\\testfile.c /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpe4by1c1m\\\\output.obj /nologo /showIncludes /utf-8 /c /nologo /showIncludes /utf-8 /c /Od /Oi- -Wswitch -Wno-switch` -> 2\\nstderr:\\ncl : Command line error D8021 : invalid numeric argument '/Wswitch'\\n-----------\\nCompiler for C supports arguments -Wno-switch: NO \\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpq0fey7zi\\nCode:\\n extern int i;\\nint i;\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpq0fey7zi\\\\testfile.c /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpq0fey7zi\\\\output.obj /nologo /showIncludes /utf-8 /c /nologo /showIncludes /utf-8 /c /Od /Oi- -Wunused-label -Wno-unused-label` -> 2\\nstderr:\\ncl : Command line error D8021 : invalid numeric argument '/Wunused-label'\\n-----------\\nCompiler for C supports arguments -Wno-unused-label: NO \\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpn8fzv1or\\nCode:\\n extern int i;\\nint i;\\n\\n-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpn8fzv1or\\\\testfile.c /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpn8fzv1or\\\\output.obj /nologo /showIncludes /utf-8 /c /nologo /showIncludes /utf-8 /c /Od /Oi- -Wunused-variable -Wno-unused-variable` -> 2\\nstderr:\\ncl : Command line error D8021 : invalid numeric argument '/Wunused-variable'\\n-----------\\nCompiler for C supports arguments -Wno-unused-variable: NO \\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmp41fb8c7c\\nCode:\\n extern int i;\\nint i;\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmp41fb8c7c\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmp41fb8c7c\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wcpp -Wno-cpp` -> 2\\nstderr:\\ncl : Command line error D8021 : invalid numeric argument '/Wcpp'\\n-----------\\nCompiler for C++ supports arguments -Wno-cpp: NO \\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpbh959m79\\nCode:\\n extern int i;\\nint i;\\n\\n-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpbh959m79\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpbh959m79\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wdeprecated-declarations -Wno-deprecated-declarations` -> 2\\nstderr:\\ncl : Command line error D8021 : invalid numeric argument '/Wdeprecated-declarations'\\n-----------\\nCompiler for C++ supports arguments -Wno-deprecated-declarations: NO \\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmp83oubkwh\\nCode:\\n extern int i;\\nint i;\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmp83oubkwh\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmp83oubkwh\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wclass-memaccess -Wno-class-memaccess` -> 2\\nstderr:\\ncl : Command line error D8021 : invalid numeric argument '/Wclass-memaccess'\\n-----------\\nCompiler for C++ supports arguments -Wno-class-memaccess: NO \\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmp2x7mc4kd\\nCode:\\n extern int i;\\nint i;\\n\\n-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmp2x7mc4kd\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmp2x7mc4kd\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wformat-truncation -Wno-format-truncation` -> 2\\nstderr:\\ncl : Command line error D8021 : invalid numeric argument '/Wformat-truncation'\\n-----------\\nCompiler for C++ supports arguments -Wno-format-truncation: NO \\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmptrz4q7t2\\nCode:\\n extern int i;\\nint i;\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmptrz4q7t2\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmptrz4q7t2\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wformat-extra-args -Wno-format-extra-args` -> 2\\nstderr:\\ncl : Command line error D8021 : invalid numeric argument '/Wformat-extra-args'\\n-----------\\nCompiler for C++ supports arguments -Wno-format-extra-args: NO \\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmplpnbjzla\\nCode:\\n extern int i;\\nint i;\\n\\n-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmplpnbjzla\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmplpnbjzla\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wformat -Wno-format` -> 2\\nstderr:\\ncl : Command line error D8021 : invalid numeric argument '/Wformat'\\n-----------\\nCompiler for C++ supports arguments -Wno-format: NO \\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmp71su_2za\\nCode:\\n extern int i;\\nint i;\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmp71su_2za\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmp71su_2za\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wnon-virtual-dtor -Wno-non-virtual-dtor` -> 2\\nstderr:\\ncl : Command line error D8021 : invalid numeric argument '/Wnon-virtual-dtor'\\n-----------\\nCompiler for C++ supports arguments -Wno-non-virtual-dtor: NO \\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpw7j8g4bx\\nCode:\\n extern int i;\\nint i;\\n\\n-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpw7j8g4bx\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpw7j8g4bx\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wsign-compare -Wno-sign-compare` -> 2\\nstderr:\\ncl : Command line error D8021 : invalid numeric argument '/Wsign-compare'\\n-----------\\nCompiler for C++ supports arguments -Wno-sign-compare: NO \\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmprlv_rd14\\nCode:\\n extern int i;\\nint i;\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmprlv_rd14\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmprlv_rd14\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wswitch -Wno-switch` -> 2\\nstderr:\\ncl : Command line error D8021 : invalid numeric argument '/Wswitch'\\n-----------\\nCompiler for C++ supports arguments -Wno-switch: NO \\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmp3oilyywc\\nCode:\\n extern int i;\\nint i;\\n\\n-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmp3oilyywc\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmp3oilyywc\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wterminate -Wno-terminate` -> 2\\nstderr:\\ncl : Command line error D8021 : invalid numeric argument '/Wterminate'\\n-----------\\nCompiler for C++ supports arguments -Wno-terminate: NO \\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpz9j42j78\\nCode:\\n extern int i;\\nint i;\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpz9j42j78\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpz9j42j78\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wunused-but-set-variable -Wno-unused-but-set-variable` -> 2\\nstderr:\\ncl : Command line error D8021 : invalid numeric argument '/Wunused-but-set-variable'\\n-----------\\nCompiler for C++ supports arguments -Wno-unused-but-set-variable: NO \\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmp6vx0h9b6\\nCode:\\n extern int i;\\nint i;\\n\\n-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmp6vx0h9b6\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmp6vx0h9b6\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wunused-function -Wno-unused-function` -> 2\\nstderr:\\ncl : Command line error D8021 : invalid numeric argument '/Wunused-function'\\n-----------\\nCompiler for C++ supports arguments -Wno-unused-function: NO \\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpidnpu776\\nCode:\\n extern int i;\\nint i;\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpidnpu776\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpidnpu776\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wunused-local-typedefs -Wno-unused-local-typedefs` -> 2\\nstderr:\\ncl : Command line error D8021 : invalid numeric argument '/Wunused-local-typedefs'\\n-----------\\nCompiler for C++ supports arguments -Wno-unused-local-typedefs: NO \\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmp00bsfscp\\nCode:\\n extern int i;\\nint i;\\n\\n-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmp00bsfscp\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmp00bsfscp\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wunused-variable -Wno-unused-variable` -> 2\\nstderr:\\ncl : Command line error D8021 : invalid numeric argument '/Wunused-variable'\\n-----------\\nCompiler for C++ supports arguments -Wno-unused-variable: NO \\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpvn7hyiuv\\nCode:\\n extern int i;\\nint i;\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpvn7hyiuv\\\\testfile.cpp /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpvn7hyiuv\\\\output.obj /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /nologo /showIncludes /utf-8 /Zc:__cplusplus /c /Od /Oi- -Wint-in-bool-context -Wno-int-in-bool-context` -> 2\\nstderr:\\ncl : Command line error D8021 : invalid numeric argument \\'/Wint-in-bool-context\\'\\n-----------\\nCompiler for C++ supports arguments -Wno-int-in-bool-context: NO \\nRunning command: C:\\\\ProgramData\\\\miniconda3\\\\python.exe -c \"\\nimport numpy as np\\ntry:\\n  incdir = os.path.relpath(np.get_include())\\nexcept Exception:\\n  incdir = np.get_include()\\nprint(incdir)\\n  \"\\n--- stdout ---\\nC:\\\\Users\\\\Aayush\\\\AppData\\\\Local\\\\Temp\\\\pip-build-env-u9xp56_3\\\\overlay\\\\Lib\\\\site-packages\\\\numpy\\\\core\\\\include\\n\\n--- stderr ---\\n\\n\\nRunning command: C:\\\\ProgramData\\\\miniconda3\\\\python.exe -c \"import os; os.chdir(\\\\\"..\\\\\"); import numpy; print(numpy.get_include())\"\\n--- stdout ---\\nC:\\\\Users\\\\Aayush\\\\AppData\\\\Local\\\\Temp\\\\pip-build-env-u9xp56_3\\\\overlay\\\\Lib\\\\site-packages\\\\numpy\\\\core\\\\include\\n\\n--- stderr ---', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='Running compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\AppData\\\\Local\\\\Temp\\\\tmpvk5kdw9q\\nCode:\\n \\n        \\n        #ifndef _OPENMP\\n        # define _OPENMP \"MESON_GET_DEFINE_UNDEFINED_SENTINEL\"\\n        #endif\\n        \"MESON_GET_DEFINE_DELIMITER_START\"\\n_OPENMP\\n\"MESON_GET_DEFINE_DELIMITER_END\"\\n-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\AppData\\\\Local\\\\Temp\\\\tmpvk5kdw9q\\\\testfile.cpp /nologo /showIncludes /utf-8 /Zc:__cplusplus /EP /nologo /showIncludes /utf-8 /Zc:__cplusplus /EP /Od /Oi- /openmp` -> 0\\nstdout:\\n\"MESON_GET_DEFINE_DELIMITER_START\"\\n200203\\n\"MESON_GET_DEFINE_DELIMITER_END\"\\n-----------\\nstderr:\\ntestfile.cpp\\n-----------\\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmp6hb0oyf6\\nCode:\\n \\n        #ifdef __has_include\\n         #if !__has_include(\"omp.h\")\\n          #error \"Header \\'omp.h\\' could not be found\"\\n         #endif\\n        #else\\n         #include <omp.h>\\n        #endif\\n-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmp6hb0oyf6\\\\testfile.cpp /nologo /showIncludes /utf-8 /Zc:__cplusplus /EP /nologo /showIncludes /utf-8 /Zc:__cplusplus /EP /Od /Oi-` -> 0\\nstderr:\\ntestfile.cpp\\n-----------\\nRun-time dependency OpenMP found: YES 2.0\\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmplrrovt0f\\nCode:\\n \\n#if defined(__GNUC__)\\n# if !defined(__amd64__) && !defined(__x86_64__)\\n#   error \"SSE2 intrinsics are only available on x86_64\"\\n# endif\\n#elif defined (_MSC_VER) && !defined (_M_X64) && !defined (_M_AMD64)\\n# error \"SSE2 intrinsics not supported on x86 MSVC builds\"\\n#endif\\n#if defined(__SSE__) || (_M_X64 > 0)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='-----------\\nRun-time dependency OpenMP found: YES 2.0\\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmplrrovt0f\\nCode:\\n \\n#if defined(__GNUC__)\\n# if !defined(__amd64__) && !defined(__x86_64__)\\n#   error \"SSE2 intrinsics are only available on x86_64\"\\n# endif\\n#elif defined (_MSC_VER) && !defined (_M_X64) && !defined (_M_AMD64)\\n# error \"SSE2 intrinsics not supported on x86 MSVC builds\"\\n#endif\\n#if defined(__SSE__) || (_M_X64 > 0)\\n# include <mmintrin.h>\\n# include <xmmintrin.h>\\n# include <emmintrin.h>\\n#else\\n# error \"No SSE intrinsics available\"\\n#endif\\nint main () {\\n    __m128i a = _mm_set1_epi32 (0), b = _mm_set1_epi32 (0), c;\\n    c = _mm_xor_si128 (a, b);\\n    return 0;\\n}\\n-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmplrrovt0f\\\\testfile.c /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmplrrovt0f\\\\output.obj /nologo /showIncludes /utf-8 /c /nologo /showIncludes /utf-8 /c /Od /Oi- /arch:SSE2` -> 0\\nstdout:\\ntestfile.c\\nNote: including file: C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\mmintrin.h\\nNote: including file: C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\xmmintrin.h\\nNote: including file:  C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\malloc.h\\nNote: including file:   C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\corecrt.h\\nNote: including file:    C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\vcruntime.h', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='Note: including file: C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\xmmintrin.h\\nNote: including file:  C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\malloc.h\\nNote: including file:   C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\corecrt.h\\nNote: including file:    C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\vcruntime.h\\nNote: including file:     C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\sal.h\\nNote: including file:      C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\concurrencysal.h\\nNote: including file:     C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\vadefs.h\\nNote: including file:   C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\corecrt_malloc.h\\nNote: including file: C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\emmintrin.h\\n-----------\\nstderr:\\ncl : Command line warning D9002 : ignoring unknown option \\'/arch:SSE2\\'\\n-----------\\nChecking if \"SSE intrinsics\" compiles: YES \\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmp1bz4l_bx\\nCode:\\n #include <immintrin.h>\\n                      int main (int argc, char ** argv) {\\n                        static __m256 mtest;\\n                        mtest = _mm256_setzero_ps();\\n                        return *((unsigned char *) &mtest) != 0;\\n                      }\\n-----------', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='-----------\\nChecking if \"SSE intrinsics\" compiles: YES \\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmp1bz4l_bx\\nCode:\\n #include <immintrin.h>\\n                      int main (int argc, char ** argv) {\\n                        static __m256 mtest;\\n                        mtest = _mm256_setzero_ps();\\n                        return *((unsigned char *) &mtest) != 0;\\n                      }\\n-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmp1bz4l_bx\\\\testfile.c /FeC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmp1bz4l_bx\\\\output.exe /nologo /showIncludes /utf-8 /MD /nologo /showIncludes /utf-8 /Od /Oi-` -> 0\\nstdout:\\ntestfile.c\\nNote: including file: C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\immintrin.h\\nNote: including file:  C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\wmmintrin.h\\nNote: including file:   C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\nmmintrin.h\\nNote: including file:    C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\smmintrin.h\\nNote: including file:     C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\tmmintrin.h\\nNote: including file:      C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\pmmintrin.h\\nNote: including file:       C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\emmintrin.h', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='Note: including file:     C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\tmmintrin.h\\nNote: including file:      C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\pmmintrin.h\\nNote: including file:       C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\emmintrin.h\\nNote: including file:        C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\xmmintrin.h\\nNote: including file:         C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\mmintrin.h\\nNote: including file:         C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\malloc.h\\nNote: including file:          C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\corecrt.h\\nNote: including file:           C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\vcruntime.h\\nNote: including file:            C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\sal.h\\nNote: including file:             C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\concurrencysal.h\\nNote: including file:            C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\vadefs.h\\nNote: including file:          C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\corecrt_malloc.h\\nNote: including file:  C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\zmmintrin.h\\n-----------\\nChecking if \"compiler supports AVX intrinsics\" : links: YES', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='Note: including file:            C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\vadefs.h\\nNote: including file:          C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\corecrt_malloc.h\\nNote: including file:  C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\zmmintrin.h\\n-----------\\nChecking if \"compiler supports AVX intrinsics\" : links: YES \\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmp4sokhvf_\\nCode:\\n #include <immintrin.h>\\n                      int main (int argc, char ** argv) {\\n                        static __m256i mtest;\\n                        mtest = _mm256_setzero_si256();\\n                        return *((unsigned char *) &mtest) != 0;\\n                      }\\n-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmp4sokhvf_\\\\testfile.c /FeC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmp4sokhvf_\\\\output.exe /nologo /showIncludes /utf-8 /MD /nologo /showIncludes /utf-8 /Od /Oi-` -> 0\\nstdout:\\ntestfile.c\\nNote: including file: C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\immintrin.h\\nNote: including file:  C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\wmmintrin.h\\nNote: including file:   C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\nmmintrin.h\\nNote: including file:    C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\smmintrin.h', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='Note: including file:  C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\wmmintrin.h\\nNote: including file:   C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\nmmintrin.h\\nNote: including file:    C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\smmintrin.h\\nNote: including file:     C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\tmmintrin.h\\nNote: including file:      C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\pmmintrin.h\\nNote: including file:       C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\emmintrin.h\\nNote: including file:        C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\xmmintrin.h\\nNote: including file:         C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\mmintrin.h\\nNote: including file:         C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\malloc.h\\nNote: including file:          C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\corecrt.h\\nNote: including file:           C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\vcruntime.h\\nNote: including file:            C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\sal.h\\nNote: including file:             C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\concurrencysal.h', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='Note: including file:           C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\vcruntime.h\\nNote: including file:            C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\sal.h\\nNote: including file:             C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\concurrencysal.h\\nNote: including file:            C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\vadefs.h\\nNote: including file:          C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\corecrt_malloc.h\\nNote: including file:  C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\zmmintrin.h\\n-----------\\nChecking if \"compiler supports AVX2 intrinsics\" : links: YES \\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpsg7j51mp\\nCode:\\n #include <immintrin.h>\\n                      int main (int argc, char ** argv) {\\n                        static __m512 mtest;\\n                        mtest = _mm512_setzero_si512();\\n                        return *((unsigned char *) &mtest) != 0;\\n                      }\\n-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpsg7j51mp\\\\testfile.c /FeC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpsg7j51mp\\\\output.exe /nologo /showIncludes /utf-8 /MD /nologo /showIncludes /utf-8 /Od /Oi-` -> 2\\nstdout:\\ntestfile.c\\nNote: including file: C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\immintrin.h', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='}\\n-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpsg7j51mp\\\\testfile.c /FeC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpsg7j51mp\\\\output.exe /nologo /showIncludes /utf-8 /MD /nologo /showIncludes /utf-8 /Od /Oi-` -> 2\\nstdout:\\ntestfile.c\\nNote: including file: C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\immintrin.h\\nNote: including file:  C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\wmmintrin.h\\nNote: including file:   C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\nmmintrin.h\\nNote: including file:    C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\smmintrin.h\\nNote: including file:     C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\tmmintrin.h\\nNote: including file:      C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\pmmintrin.h\\nNote: including file:       C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\emmintrin.h\\nNote: including file:        C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\xmmintrin.h\\nNote: including file:         C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\mmintrin.h\\nNote: including file:         C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\malloc.h\\nNote: including file:          C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\corecrt.h', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='Note: including file:        C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\xmmintrin.h\\nNote: including file:         C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\mmintrin.h\\nNote: including file:         C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\malloc.h\\nNote: including file:          C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\corecrt.h\\nNote: including file:           C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\vcruntime.h\\nNote: including file:            C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\sal.h\\nNote: including file:             C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\concurrencysal.h\\nNote: including file:            C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\vadefs.h\\nNote: including file:          C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\include\\\\10.0.22621.0\\\\ucrt\\\\corecrt_malloc.h\\nNote: including file:  C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\zmmintrin.h\\nC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpsg7j51mp\\\\testfile.c(4): error C2440: \\'=\\': cannot convert from \\'__m512i\\' to \\'__m512\\'\\n-----------\\nChecking if \"compiler supports AVX512 intrinsics\" : links: NO \\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmprz2h_5gy\\nCode:\\n \\n#if !defined (_MSC_VER) || defined (__clang__)\\n# if !defined (_M_ARM64) && !defined (__aarch64__)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmpsg7j51mp\\\\testfile.c(4): error C2440: \\'=\\': cannot convert from \\'__m512i\\' to \\'__m512\\'\\n-----------\\nChecking if \"compiler supports AVX512 intrinsics\" : links: NO \\nRunning compile:\\nWorking directory:  C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmprz2h_5gy\\nCode:\\n \\n#if !defined (_MSC_VER) || defined (__clang__)\\n# if !defined (_M_ARM64) && !defined (__aarch64__)\\n#  ifndef __ARM_EABI__\\n#   error \"EABI is required (to be sure that calling conventions are compatible)\"\\n#  endif\\n#   ifndef __ARM_NEON__\\n#    error \"No ARM NEON instructions available\"\\n#   endif\\n# endif\\n#endif\\n#if defined (_MSC_VER) && (_MSC_VER < 1920) && defined (_M_ARM64)\\n# include <arm64_neon.h>\\n#else\\n# include <arm_neon.h>\\n#endif\\nint main () {\\n    const float32_t __v[4] = { 1, 2, 3, 4 }; \\\\\\n    const unsigned int __umask[4] = { \\\\\\n      0x80000000, \\\\\\n      0x80000000, \\\\\\n      0x80000000, \\\\\\n      0x80000000 \\\\\\n    }; \\\\\\n    const uint32x4_t __mask = vld1q_u32 (__umask); \\\\\\n    float32x4_t s = vld1q_f32 (__v); \\\\\\n    float32x4_t c = vreinterpretq_f32_u32 (veorq_u32 (vreinterpretq_u32_f32 (s), __mask)); \\\\\\n    return 0;\\n}\\n-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmprz2h_5gy\\\\testfile.c /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmprz2h_5gy\\\\output.obj /nologo /showIncludes /utf-8 /c /nologo /showIncludes /utf-8 /c /Od /Oi-` -> 2\\nstdout:\\ntestfile.c\\nNote: including file: C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\arm_neon.h', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='return 0;\\n}\\n-----------\\nCommand line: `cl C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmprz2h_5gy\\\\testfile.c /FoC:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-private\\\\tmprz2h_5gy\\\\output.obj /nologo /showIncludes /utf-8 /c /nologo /showIncludes /utf-8 /c /Od /Oi-` -> 2\\nstdout:\\ntestfile.c\\nNote: including file: C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\arm_neon.h\\nNote: including file:  C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\stdint.h\\nNote: including file:   C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\vcruntime.h\\nNote: including file:    C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\sal.h\\nNote: including file:     C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\concurrencysal.h\\nNote: including file:    C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\vadefs.h\\nC:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\include\\\\arm_neon.h(21): fatal error C1189: #error:  This header is specific to ARM targets\\n-----------\\nChecking if \"ARM NEON intrinsics\" compiles: NO \\nBuild targets in project: 45', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content='dipy 1.10.0dev\\n\\n  User defined options\\n    Native files: C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\DiPY\\\\dipy\\\\build\\\\cp312\\\\meson-python-native-file.ini\\n    buildtype   : release\\n    b_ndebug    : if-release\\n    b_vscrt     : md\\n\\nFailed to guess install tag for c:\\\\Lib\\\\site-packages\\\\dipy\\\\version.py\\nFailed to guess install tag for c:\\\\share\\\\doc\\\\dipy\\\\dummy\\nFound ninja.EXE-1.11.1.git.kitware.jobserver-1 at C:\\\\Users\\\\Aayush\\\\AppData\\\\Local\\\\Temp\\\\pip-build-env-u9xp56_3\\\\normal\\\\Scripts\\\\ninja.EXE\\nFailed to guess install tag for c:\\\\Lib\\\\site-packages\\\\dipy\\\\version.py\\nFailed to guess install tag for c:\\\\share\\\\doc\\\\dipy\\\\dummy\\nFailed to guess install tag for c:\\\\Lib\\\\site-packages\\\\dipy\\\\version.py\\nFailed to guess install tag for c:\\\\share\\\\doc\\\\dipy\\\\dummy', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-logs\\\\meson-log.txt.txt'}),\n",
       " Document(page_content=\"[options]\\nbuildtype = release\\nb_ndebug = if-release\\nb_vscrt = md\\n\\n[properties]\\nnative_file = ['C:\\\\\\\\Users\\\\\\\\Aayush\\\\\\\\OneDrive\\\\\\\\Desktop\\\\\\\\DiPY\\\\\\\\dipy\\\\\\\\build\\\\\\\\cp312\\\\\\\\meson-python-native-file.ini']\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-private\\\\cmd_line.txt.txt'}),\n",
       " Document(page_content='# Copyright 2016 The Meson development team\\n\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\n# ignore all lints for this file, since it is run by python2 as well\\n\\n# type: ignore\\n# pylint: disable=deprecated-module\\n\\nimport json, os, subprocess, sys\\nfrom compileall import compile_file\\n\\nquiet = int(os.environ.get(\\'MESON_INSTALL_QUIET\\', 0))\\n\\ndef compileall(files):\\n    for f in files:\\n        # f is prefixed by {py_xxxxlib}, both variants are 12 chars\\n        # the key is the middle 10 chars of the prefix\\n        key = f[1:11].upper()\\n        f = f[12:]\\n\\n        ddir = None\\n        fullpath = absf = os.environ[\\'MESON_INSTALL_DESTDIR_\\'+key] + f\\n        f = os.environ[\\'MESON_INSTALL_\\'+key] + f\\n\\n        if absf != f:\\n            ddir = os.path.dirname(f)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-private\\\\pycompile.py.txt'}),\n",
       " Document(page_content=\"quiet = int(os.environ.get('MESON_INSTALL_QUIET', 0))\\n\\ndef compileall(files):\\n    for f in files:\\n        # f is prefixed by {py_xxxxlib}, both variants are 12 chars\\n        # the key is the middle 10 chars of the prefix\\n        key = f[1:11].upper()\\n        f = f[12:]\\n\\n        ddir = None\\n        fullpath = absf = os.environ['MESON_INSTALL_DESTDIR_'+key] + f\\n        f = os.environ['MESON_INSTALL_'+key] + f\\n\\n        if absf != f:\\n            ddir = os.path.dirname(f)\\n\\n        if os.path.isdir(absf):\\n            for root, _, files in os.walk(absf):\\n                if ddir is not None:\\n                    ddir = root.replace(absf, f, 1)\\n                for dirf in files:\\n                    if dirf.endswith('.py'):\\n                        fullpath = os.path.join(root, dirf)\\n                        compile_file(fullpath, ddir, force=True, quiet=quiet)\\n        else:\\n            compile_file(fullpath, ddir, force=True, quiet=quiet)\\n\\ndef run(manifest):\\n    data_file = os.path.join(os.path.dirname(__file__), manifest)\\n    with open(data_file, 'rb') as f:\\n        dat = json.load(f)\\n    compileall(dat)\\n\\nif __name__ == '__main__':\\n    manifest = sys.argv[1]\\n    run(manifest)\\n    if len(sys.argv) > 2:\\n        optlevel = int(sys.argv[2])\\n        # python2 only needs one or the other\\n        if optlevel == 1 or (sys.version_info >= (3,) and optlevel > 0):\\n            subprocess.check_call([sys.executable, '-O'] + sys.argv[:2])\\n        if optlevel == 2:\\n            subprocess.check_call([sys.executable, '-OO'] + sys.argv[:2])\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\build\\\\cp312\\\\meson-private\\\\pycompile.py.txt'}),\n",
       " Document(page_content=\"# This is an ini file that may contain information about the code state\\n[commit hash]\\n# The line below may contain a valid hash if it has been substituted during 'git archive'\\narchive_subst_hash=$Format:%h$\\n# This line may be modified by the install process\\ninstall_hash=\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\COMMIT_INFO.txt.txt'}),\n",
       " Document(page_content='\"\"\"pytest initialization.\"\"\"\\nimport importlib\\nimport re\\nimport warnings\\n\\nimport numpy as np\\nimport pytest\\n\\n\\n\"\"\" Set numpy print options to \"legacy\" for new versions of numpy\\n If imported into a file, pytest will run this before any doctests.\\n\\nReferences\\n----------\\nhttps://github.com/numpy/numpy/commit/710e0327687b9f7653e5ac02d222ba62c657a718\\nhttps://github.com/numpy/numpy/commit/734b907fc2f7af6e40ec989ca49ee6d87e21c495\\nhttps://github.com/nipy/nibabel/pull/556\\n\"\"\"\\nnp.set_printoptions(legacy=\\'1.13\\')\\n\\nwarnings.simplefilter(action=\"default\", category=FutureWarning)\\nwarnings.simplefilter(\"always\", category=UserWarning)\\n# List of files that pytest should ignore\\ncollect_ignore = [\"testing/decorators.py\", \"bench*.py\", \"**/benchmarks/*\"]\\n\\n\\ndef pytest_collect_file(parent, file_path):\\n    if file_path.suffix in [\".pyx\", \".so\"] and  \\\\\\n       file_path.name.startswith(\"test\"):\\n        return PyxFile.from_parent(parent, path=file_path)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\conftest.py.txt'}),\n",
       " Document(page_content='warnings.simplefilter(action=\"default\", category=FutureWarning)\\nwarnings.simplefilter(\"always\", category=UserWarning)\\n# List of files that pytest should ignore\\ncollect_ignore = [\"testing/decorators.py\", \"bench*.py\", \"**/benchmarks/*\"]\\n\\n\\ndef pytest_collect_file(parent, file_path):\\n    if file_path.suffix in [\".pyx\", \".so\"] and  \\\\\\n       file_path.name.startswith(\"test\"):\\n        return PyxFile.from_parent(parent, path=file_path)\\n\\n\\nclass PyxFile(pytest.File):\\n    def collect(self):\\n        try:\\n            match = re.search(r\\'(dipy/[^\\\\/]+/tests/test_\\\\w+)\\', str(self.path))\\n            mod_name = match.group(1) if match else None\\n            if mod_name is None:\\n                raise PyxException(\"Could not find test module for \"\\n                                   f\"{self.path}.\")\\n            mod_name = mod_name.replace(\"/\", \".\")\\n            mod = importlib.import_module(mod_name)\\n            for name in dir(mod):\\n                item = getattr(mod, name)\\n                if callable(item) and name.startswith(\"test_\"):\\n                    yield PyxItem.from_parent(self, name=name, test_func=item,\\n                                              mod=mod)\\n        except ImportError:\\n            msg = (f\"Import failed for {self.path}. Make sure you cython file \"\\n                   \"has been compiled.\")\\n            raise PyxException(msg, self.path, 0)\\n\\n\\nclass PyxItem(pytest.Item):\\n    def __init__(self, *, test_func, mod, **kwargs):\\n        super().__init__(**kwargs)\\n        self.mod = mod\\n        self.test_func = test_func\\n\\n    def runtest(self):\\n        \"\"\"Called to execute the test item.\"\"\"\\n        self.test_func()', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\conftest.py.txt'}),\n",
       " Document(page_content='class PyxItem(pytest.Item):\\n    def __init__(self, *, test_func, mod, **kwargs):\\n        super().__init__(**kwargs)\\n        self.mod = mod\\n        self.test_func = test_func\\n\\n    def runtest(self):\\n        \"\"\"Called to execute the test item.\"\"\"\\n        self.test_func()\\n\\n    def repr_failure(self, excinfo):\\n        \"\"\"Called when self.runtest() raises an exception.\"\"\"\\n        return excinfo.value.args[0]\\n\\n    def reportinfo(self):\\n        return self.path, 0, f\"test: {self.name}\"\\n\\n\\nclass PyxException(Exception):\\n    \"\"\"Custom exception for error reporting.\"\"\"', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\conftest.py.txt'}),\n",
       " Document(page_content=\"# Platform detection\\nis_windows = host_machine.system() == 'windows'\\nis_mingw = is_windows and cc.get_id() == 'gcc'\\n\\n\\n# ------------------------------------------------------------------------\\n# Preprocessor flags\\n# ------------------------------------------------------------------------\\n\\nnumpy_nodepr_api_1_9 = '-DNPY_NO_DEPRECATED_API=NPY_1_9_API_VERSION'\\nnumpy_nodepr_api_1_7 = '-DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION'\\n\\n# ------------------------------------------------------------------------\\n# Compiler flags\\n# ------------------------------------------------------------------------\\n\\n# C warning flags\\nWno_maybe_uninitialized = cc.get_supported_arguments('-Wno-maybe-uninitialized')\\nWno_discarded_qualifiers = cc.get_supported_arguments('-Wno-discarded-qualifiers')\\nWno_empty_body = cc.get_supported_arguments('-Wno-empty-body')\\nWno_implicit_function_declaration = cc.get_supported_arguments('-Wno-implicit-function-declaration')\\nWno_parentheses = cc.get_supported_arguments('-Wno-parentheses')\\nWno_switch = cc.get_supported_arguments('-Wno-switch')\\nWno_unused_label = cc.get_supported_arguments('-Wno-unused-label')\\nWno_unused_variable = cc.get_supported_arguments('-Wno-unused-variable')\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\meson.build.txt'}),\n",
       " Document(page_content=\"# C++ warning flags\\n_cpp_Wno_cpp = cpp.get_supported_arguments('-Wno-cpp')\\n_cpp_Wno_deprecated_declarations = cpp.get_supported_arguments('-Wno-deprecated-declarations')\\n_cpp_Wno_class_memaccess = cpp.get_supported_arguments('-Wno-class-memaccess')\\n_cpp_Wno_format_truncation = cpp.get_supported_arguments('-Wno-format-truncation')\\n_cpp_Wno_format_extra_args = cpp.get_supported_arguments('-Wno-format-extra-args')\\n_cpp_Wno_format = cpp.get_supported_arguments('-Wno-format')\\n_cpp_Wno_non_virtual_dtor = cpp.get_supported_arguments('-Wno-non-virtual-dtor')\\n_cpp_Wno_sign_compare = cpp.get_supported_arguments('-Wno-sign-compare')\\n_cpp_Wno_switch = cpp.get_supported_arguments('-Wno-switch')\\n_cpp_Wno_terminate = cpp.get_supported_arguments('-Wno-terminate')\\n_cpp_Wno_unused_but_set_variable = cpp.get_supported_arguments('-Wno-unused-but-set-variable')\\n_cpp_Wno_unused_function = cpp.get_supported_arguments('-Wno-unused-function')\\n_cpp_Wno_unused_local_typedefs = cpp.get_supported_arguments('-Wno-unused-local-typedefs')\\n_cpp_Wno_unused_variable = cpp.get_supported_arguments('-Wno-unused-variable')\\n_cpp_Wno_int_in_bool_context = cpp.get_supported_arguments('-Wno-int-in-bool-context')\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\meson.build.txt'}),\n",
       " Document(page_content='cython_c_args = []\\nif is_windows\\n  # For mingw-w64, link statically against the UCRT.\\n  # automatic detect lto for now due to some issues. \\'-fno-use-linker-plugin\\'\\n  gcc_link_args = [\\'-lucrt\\', \\'-static\\']\\n  if is_mingw\\n    add_project_link_arguments(gcc_link_args, language: [\\'c\\', \\'cpp\\'])\\n    # Force gcc to float64 long doubles for compatibility with MSVC\\n    # builds, for C only.\\n    add_project_arguments(\\'-mlong-double-64\\', language: \\'c\\')\\n    # Make fprintf(\"%zd\") work (see https://github.com/rgommers/scipy/issues/118)\\n    add_project_arguments(\\'-D__USE_MINGW_ANSI_STDIO=1\\', language: [\\'c\\', \\'cpp\\'])\\n    # Manual add of MS_WIN64 macro when not using MSVC.\\n    # https://bugs.python.org/issue28267\\n    if target_machine.cpu_family().to_lower().contains(\\'64\\')\\n      add_project_arguments(\\'-DMS_WIN64\\', language: [\\'c\\', \\'cpp\\'])\\n    endif\\n    # Silence warnings emitted by PyOS_snprintf for (%zd), see\\n    # https://github.com/rgommers/scipy/issues/118.\\n    # Use as c_args for extensions containing Cython code\\n    cython_c_args += [_cpp_Wno_format_extra_args, _cpp_Wno_format]\\n  endif\\nendif\\n\\n\\n# Deal with M_PI & friends; add `use_math_defines` to c_args\\n# Cython doesn\\'t always get this correctly itself\\n# explicitly add the define as a compiler flag for Cython-generated code.\\nif is_windows\\n  use_math_defines = [\\'-D_USE_MATH_DEFINES\\']\\nelse\\n  use_math_defines = []\\nendif\\n\\n# Suppress warning for deprecated Numpy API.\\n# (Suppress warning messages emitted by #warning directives).\\n# Replace with numpy_nodepr_api after Cython 3.0 is out\\ncython_c_args += [_cpp_Wno_cpp, use_math_defines]\\ncython_cpp_args = cython_c_args', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\meson.build.txt'}),\n",
       " Document(page_content=\"# Suppress warning for deprecated Numpy API.\\n# (Suppress warning messages emitted by #warning directives).\\n# Replace with numpy_nodepr_api after Cython 3.0 is out\\ncython_c_args += [_cpp_Wno_cpp, use_math_defines]\\ncython_cpp_args = cython_c_args\\n\\n# ------------------------------------------------------------------------\\n# NumPy include directory - needed in all submodules\\n# ------------------------------------------------------------------------\\n\\n# The chdir is needed because within numpy there's an `import signal`\\n# statement, and we don't want that to pick up scipy's signal module rather\\n# than the stdlib module. The try-except is needed because when things are\\n# split across drives on Windows, there is no relative path and an exception\\n# gets raised. There may be other such cases, so add a catch-all and switch to\\n# an absolute path. Relative paths are needed when for example a virtualenv is\\n# placed inside the source tree; Meson rejects absolute paths to places inside\\n# the source tree.\\n# For cross-compilation it is often not possible to run the Python interpreter\\n# in order to retrieve numpy's include directory. It can be specified in the\\n# cross file instead:\\n#   [properties]\\n#   numpy-include-dir = /abspath/to/host-pythons/site-packages/numpy/core/include\\n#\\n# This uses the path as is, and avoids running the interpreter.\\nincdir_numpy = meson.get_external_property('numpy-include-dir', 'not-given')\\nif incdir_numpy == 'not-given'\\n  incdir_numpy = run_command(py3,\\n    [\\n      '-c',\\n      '''\\nimport numpy as np\\ntry:\\n  incdir = os.path.relpath(np.get_include())\\nexcept Exception:\\n  incdir = np.get_include()\\nprint(incdir)\\n  '''\\n    ],\\n    check: true\\n  ).stdout().strip()\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\meson.build.txt'}),\n",
       " Document(page_content='# We do need an absolute path to feed to `cc.find_library` below\\n  _incdir_numpy_abs = run_command(py3,\\n    [\\'-c\\', \\'import os; os.chdir(\"..\"); import numpy; print(numpy.get_include())\\'],\\n    check: true\\n  ).stdout().strip()\\nelse\\n  _incdir_numpy_abs = incdir_numpy\\nendif\\ninc_np = include_directories(incdir_numpy)\\nnp_dep = declare_dependency(include_directories: inc_np)\\n\\n\\n# npymath_path = _incdir_numpy_abs / \\'..\\' / \\'lib\\'\\n# npyrandom_path = _incdir_numpy_abs / \\'..\\' / \\'..\\' / \\'random\\' / \\'lib\\'\\n# npymath_lib = cc.find_library(\\'npymath\\', dirs: npymath_path)\\n# npyrandom_lib = cc.find_library(\\'npyrandom\\', dirs: npyrandom_path)\\n\\n# ------------------------------------------------------------------------\\n# Define Optimisation for cython extensions\\n# ------------------------------------------------------------------------\\nomp = dependency(\\'openmp\\', required: false)\\nif not omp.found() and meson.get_compiler(\\'c\\').get_id() == \\'clang\\'\\n  # Check for libomp (OpenMP) using Homebrew\\n  brew = find_program(\\'brew\\', required : false)\\n  if brew.found()\\n    output = run_command(brew, \\'list\\', \\'libomp\\', check: true)\\n    output = output.stdout().strip()\\n    if output.contains(\\'/libomp/\\')\\n      omp_prefix = fs.parent(output.split(\\'\\\\n\\')[0])\\n      message(\\'OpenMP Found: YES (Manual search) - \\', omp_prefix)\\n      omp = declare_dependency(compile_args : [\\'-Xpreprocessor\\', \\'-fopenmp\\'],\\n                               link_args : [\\'-L\\' + omp_prefix + \\'/lib\\', \\'-lomp\\'],\\n                               include_directories : include_directories(omp_prefix / \\'include\\')\\n                              )\\n    endif\\n  endif\\nendif', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\meson.build.txt'}),\n",
       " Document(page_content='# SSE intrinsics\\nsse2_cflags = []\\nsse_prog = \\'\\'\\'\\n#if defined(__GNUC__)\\n# if !defined(__amd64__) && !defined(__x86_64__)\\n#   error \"SSE2 intrinsics are only available on x86_64\"\\n# endif\\n#elif defined (_MSC_VER) && !defined (_M_X64) && !defined (_M_AMD64)\\n# error \"SSE2 intrinsics not supported on x86 MSVC builds\"\\n#endif\\n#if defined(__SSE__) || (_M_X64 > 0)\\n# include <mmintrin.h>\\n# include <xmmintrin.h>\\n# include <emmintrin.h>\\n#else\\n# error \"No SSE intrinsics available\"\\n#endif\\nint main () {\\n    __m128i a = _mm_set1_epi32 (0), b = _mm_set1_epi32 (0), c;\\n    c = _mm_xor_si128 (a, b);\\n    return 0;\\n}\\'\\'\\'\\n\\nif cc.get_id() != \\'msvc\\'\\n  test_sse2_cflags = [\\'-mfpmath=sse\\', \\'-msse\\', \\'-msse2\\']\\n  # might need to check the processor type here\\n  # arm neon flag: -mfpu=neon -mfloat-abi=softfp  # see test below\\n  # freescale altivec flag: -maltivec -mabi=altivec\\nelse\\n  test_sse2_cflags = [\\'/arch:SSE2\\']  # SSE2 support is only available in 32 bit mode.\\nendif\\n\\nif cc.compiles(sse_prog, args: test_sse2_cflags, name: \\'SSE intrinsics\\')\\n  sse2_cflags = test_sse2_cflags\\n  cython_c_args += test_sse2_cflags\\n  cython_cpp_args = cython_c_args\\nendif', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\meson.build.txt'}),\n",
       " Document(page_content=\"if cc.compiles(sse_prog, args: test_sse2_cflags, name: 'SSE intrinsics')\\n  sse2_cflags = test_sse2_cflags\\n  cython_c_args += test_sse2_cflags\\n  cython_cpp_args = cython_c_args\\nendif\\n\\nif host_cpu_family in ['x86', 'x86_64']\\n  x86_intrinsics = []\\n  if cc.get_id() == 'msvc'\\n    x86_intrinsics = [\\n      [ 'AVX', 'immintrin.h', '__m256', '_mm256_setzero_ps()', ['/ARCH:AVX'] ],\\n      [ 'AVX2', 'immintrin.h', '__m256i', '_mm256_setzero_si256()', ['/ARCH:AVX2'] ],\\n      [ 'AVX512', 'immintrin.h', '__m512', '_mm512_setzero_si512()', ['/ARCH:AVX512'] ],\\n    ]\\n  else\\n    x86_intrinsics = [\\n      # [ 'SSE', 'xmmintrin.h', '__m128', '_mm_setzero_ps()', ['-msse'] ],\\n      # [ 'SSE2', 'emmintrin.h', '__m128i', '_mm_setzero_si128()', ['-msse2'] ],\\n      [ 'SSE4.1', 'smmintrin.h', '__m128i', '_mm_setzero_si128(); mtest = _mm_cmpeq_epi64(mtest, mtest)', ['-msse4.1'] ],\\n      [ 'AVX', 'immintrin.h', '__m256', '_mm256_setzero_ps()', ['-mavx'] ],\\n    ]\\n  endif\\n\\n  foreach intrin : x86_intrinsics\\n    intrin_check = '''#include <@0@>\\n                      int main (int argc, char ** argv) {\\n                        static @1@ mtest;\\n                        mtest = @2@;\\n                        return *((unsigned char *) &mtest) != 0;\\n                      }'''.format(intrin[1],intrin[2],intrin[3])\\n    intrin_name = intrin[0]\\n    if cc.links(intrin_check, name : 'compiler supports @0@ intrinsics'.format(intrin_name))\\n      cython_c_args +=  intrin[4]\\n      cython_cpp_args = cython_c_args\\n    endif\\n  endforeach\\nendif\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\meson.build.txt'}),\n",
       " Document(page_content='# ARM NEON intrinsics\\nneon_prog = \\'\\'\\'\\n#if !defined (_MSC_VER) || defined (__clang__)\\n# if !defined (_M_ARM64) && !defined (__aarch64__)\\n#  ifndef __ARM_EABI__\\n#   error \"EABI is required (to be sure that calling conventions are compatible)\"\\n#  endif\\n#   ifndef __ARM_NEON__\\n#    error \"No ARM NEON instructions available\"\\n#   endif\\n# endif\\n#endif\\n#if defined (_MSC_VER) && (_MSC_VER < 1920) && defined (_M_ARM64)\\n# include <arm64_neon.h>\\n#else\\n# include <arm_neon.h>\\n#endif\\nint main () {\\n    const float32_t __v[4] = { 1, 2, 3, 4 }; \\\\\\n    const unsigned int __umask[4] = { \\\\\\n      0x80000000, \\\\\\n      0x80000000, \\\\\\n      0x80000000, \\\\\\n      0x80000000 \\\\\\n    }; \\\\\\n    const uint32x4_t __mask = vld1q_u32 (__umask); \\\\\\n    float32x4_t s = vld1q_f32 (__v); \\\\\\n    float32x4_t c = vreinterpretq_f32_u32 (veorq_u32 (vreinterpretq_u32_f32 (s), __mask)); \\\\\\n    return 0;\\n}\\'\\'\\'\\n\\ntest_neon_cflags = []\\n\\nif cc.get_id() != \\'msvc\\' and host_cpu_family != \\'aarch64\\'\\n  test_neon_cflags += [\\'-mfpu=neon\\']\\nendif\\n\\nif host_system == \\'android\\'  # dipy not in android but I keep it just in case\\n  test_neon_cflags += [\\'-mfloat-abi=softfp\\']\\nendif\\n\\nif cc.compiles(neon_prog, args: test_neon_cflags, name: \\'ARM NEON intrinsics\\')\\n  neon_cflags = test_neon_cflags\\n  cython_c_args += neon_cflags\\n  cython_cpp_args = cython_c_args\\nendif\\n\\n\\n# ------------------------------------------------------------------------\\n#  include openmp\\n# Copy the main __init__.py and pxd files to the build dir.\\n# Needed to trick Cython, it won\\'t do a relative import outside a package\\n# ------------------------------------------------------------------------', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\meson.build.txt'}),\n",
       " Document(page_content=\"if cc.compiles(neon_prog, args: test_neon_cflags, name: 'ARM NEON intrinsics')\\n  neon_cflags = test_neon_cflags\\n  cython_c_args += neon_cflags\\n  cython_cpp_args = cython_c_args\\nendif\\n\\n\\n# ------------------------------------------------------------------------\\n#  include openmp\\n# Copy the main __init__.py and pxd files to the build dir.\\n# Needed to trick Cython, it won't do a relative import outside a package\\n# ------------------------------------------------------------------------\\n\\n_cython_tree = [\\n  fs.copyfile('__init__.py'),\\n  fs.copyfile('../src/conditional_omp.h'),\\n  fs.copyfile('../src/cythonutils.h'),\\n  fs.copyfile('../src/dpy_math.h'),\\n  fs.copyfile('../src/safe_openmp.pxd'),\\n]\\n\\n# include some local folder\\n# Todo: need more explicit name\\nincdir_local = meson.current_build_dir()\\ninc_local = include_directories('.')\\n\\n\\n# ------------------------------------------------------------------------\\n# Manage version file\\n# ------------------------------------------------------------------------\\ndipy_dir = py3.get_install_dir() / 'dipy'\\n\\ngenerate_version = custom_target(\\n  'generate-version',\\n  install: true,\\n  build_always_stale: true,\\n  build_by_default: true,\\n  output: 'version.py',\\n  input: '../tools/version_utils.py',\\n  command: [py3, '@INPUT@', '--source-root', '@SOURCE_ROOT@'],\\n  install_dir: dipy_dir\\n)\\n\\n# ------------------------------------------------------------------------\\n# Include Python Sources\\n# ------------------------------------------------------------------------\\npython_sources = [\\n  '__init__.py',\\n  'conftest.py',\\n  'pkg_info.py'\\n]\\n\\npy3.install_sources(\\n  python_sources,\\n  pure: false,\\n  subdir: 'dipy'\\n)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\meson.build.txt'}),\n",
       " Document(page_content=\"# ------------------------------------------------------------------------\\n# Include Python Sources\\n# ------------------------------------------------------------------------\\npython_sources = [\\n  '__init__.py',\\n  'conftest.py',\\n  'pkg_info.py'\\n]\\n\\npy3.install_sources(\\n  python_sources,\\n  pure: false,\\n  subdir: 'dipy'\\n)\\n\\n# ------------------------------------------------------------------------\\n# Manage datafiles\\n# ------------------------------------------------------------------------\\n\\ndata_install_dir = join_paths(get_option('datadir'), 'doc', meson.project_name())\\nex_file_excludes = ['_valid_examples.toml', '.gitignore', 'README.md']\\ninstall_subdir('../doc/examples',\\n  install_dir: data_install_dir,\\n  exclude_files: ex_file_excludes,\\n)\\n\\n# ------------------------------------------------------------------------\\n# Custom Meson Command line tools\\n# ------------------------------------------------------------------------\\n\\ncython_args = ['-3', '--fast-fail', '--warning-errors', '@EXTRA_ARGS@',\\n               '--output-file', '@OUTPUT@', '--include-dir', incdir_local,\\n               '@INPUT@']\\ncython_cplus_args = ['--cplus'] + cython_args\\n\\ncython_gen = generator(cython,\\n  arguments : cython_args,\\n  output : '@BASENAME@.c',\\n  depends : _cython_tree)\\n\\ncython_gen_cpp = generator(cython,\\n  arguments : cython_cplus_args,\\n  output : '@BASENAME@.cpp',\\n  depends : [_cython_tree])\\n\\n\\n# ------------------------------------------------------------------------\\n# Add subfolders\\n# ------------------------------------------------------------------------\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\meson.build.txt'}),\n",
       " Document(page_content=\"cython_gen = generator(cython,\\n  arguments : cython_args,\\n  output : '@BASENAME@.c',\\n  depends : _cython_tree)\\n\\ncython_gen_cpp = generator(cython,\\n  arguments : cython_cplus_args,\\n  output : '@BASENAME@.cpp',\\n  depends : [_cython_tree])\\n\\n\\n# ------------------------------------------------------------------------\\n# Add subfolders\\n# ------------------------------------------------------------------------\\n\\nsubdir('align')\\nsubdir('core')\\nsubdir('data')\\nsubdir('denoise')\\nsubdir('direction')\\nsubdir('io')\\nsubdir('nn')\\nsubdir('reconst')\\nsubdir('segment')\\nsubdir('sims')\\nsubdir('stats')\\nsubdir('testing')\\nsubdir('tests')\\nsubdir('tracking')\\nsubdir('utils')\\nsubdir('viz')\\nsubdir('workflows')\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\meson.build.txt'}),\n",
       " Document(page_content='import os\\nimport sys\\nimport subprocess\\n\\nimport configparser\\n\\nCOMMIT_INFO_FNAME = \\'COMMIT_INFO.txt\\'\\n\\n\\ndef pkg_commit_hash(pkg_path):\\n    \"\"\" Get short form of commit hash given directory `pkg_path`\\n\\n    There should be a file called \\'COMMIT_INFO.txt\\' in `pkg_path`.  This is a\\n    file in INI file format, with at least one section: ``commit hash``, and\\n    two variables ``archive_subst_hash`` and ``install_hash``.  The first has\\n    a substitution pattern in it which may have been filled by the execution\\n    of ``git archive`` if this is an archive generated that way.  The second\\n    is filled in by the installation, if the installation is from a git\\n    archive.\\n\\n    We get the commit hash from (in order of preference):\\n\\n    * A substituted value in ``archive_subst_hash``\\n    * A written commit hash value in ``install_hash`\\n    * git\\'s output, if we are in a git repository\\n\\n    If all these fail, we return a not-found placeholder tuple\\n\\n    Parameters\\n    ----------\\n    pkg_path : str\\n       directory containing package', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\pkg_info.py.txt'}),\n",
       " Document(page_content='We get the commit hash from (in order of preference):\\n\\n    * A substituted value in ``archive_subst_hash``\\n    * A written commit hash value in ``install_hash`\\n    * git\\'s output, if we are in a git repository\\n\\n    If all these fail, we return a not-found placeholder tuple\\n\\n    Parameters\\n    ----------\\n    pkg_path : str\\n       directory containing package\\n\\n    Returns\\n    -------\\n    hash_from : str\\n       Where we got the hash from - description\\n    hash_str : str\\n       short form of hash\\n    \"\"\"\\n    # Try and get commit from written commit text file\\n    pth = os.path.join(pkg_path, COMMIT_INFO_FNAME)\\n    if not os.path.isfile(pth):\\n        raise OSError(\\'Missing commit info file %s\\' % pth)\\n    cfg_parser = configparser.RawConfigParser()\\n    cfg_parser.read(pth)\\n    archive_subst = cfg_parser.get(\\'commit hash\\', \\'archive_subst_hash\\')\\n    if not archive_subst.startswith(\\'$Format\\'): # it has been substituted\\n        return \\'archive substitution\\', archive_subst\\n    install_subst = cfg_parser.get(\\'commit hash\\', \\'install_hash\\')\\n    if install_subst != \\'\\':\\n        return \\'installation\\', install_subst\\n    # maybe we are in a repository\\n    proc = subprocess.Popen(\\'git rev-parse --short HEAD\\',\\n                            stdout=subprocess.PIPE,\\n                            stderr=subprocess.PIPE,\\n                            cwd=pkg_path, shell=True)\\n    repo_commit, _ = proc.communicate()\\n    if repo_commit:\\n        return \\'repository\\', repo_commit.strip()\\n    return \\'(none found)\\', \\'<not found>\\'\\n\\n\\ndef get_pkg_info(pkg_path):\\n    \"\"\" Return dict describing the context of this package\\n\\n    Parameters\\n    ----------\\n    pkg_path : str\\n       path containing __init__.py for package', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\pkg_info.py.txt'}),\n",
       " Document(page_content='def get_pkg_info(pkg_path):\\n    \"\"\" Return dict describing the context of this package\\n\\n    Parameters\\n    ----------\\n    pkg_path : str\\n       path containing __init__.py for package\\n\\n    Returns\\n    -------\\n    context : dict\\n       with named parameters of interest\\n    \"\"\"\\n    src, hsh = pkg_commit_hash(pkg_path)\\n    import numpy\\n    import dipy\\n    return dict(\\n        pkg_path=pkg_path,\\n        commit_source=src,\\n        commit_hash=hsh,\\n        sys_version=sys.version,\\n        sys_executable=sys.executable,\\n        sys_platform=sys.platform,\\n        np_version=numpy.__version__,\\n        dipy_version=dipy.__version__)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\pkg_info.py.txt'}),\n",
       " Document(page_content='\"\"\"\\nDiffusion Imaging in Python\\n============================\\n\\nFor more information, please visit https://dipy.org\\n\\nSubpackages\\n-----------\\n::\\n\\n align         -- Registration, streamline alignment, volume resampling\\n core          -- Spheres, gradient tables\\n core.geometry -- Spherical geometry, coordinate and vector manipulation\\n core.meshes   -- Point distributions on the sphere\\n data          -- Small testing datasets\\n denoise       -- Denoising algorithms\\n direction     -- Manage peaks and tracking\\n io            -- Loading/saving of dpy datasets\\n nn            -- Neural networks algorithms\\n reconst       -- Signal reconstruction modules (tensor, spherical harmonics,\\n                  diffusion spectrum, etc.)\\n segment       -- Tractography segmentation\\n sims          -- MRI phantom signal simulation\\n stats         -- Tractometry\\n tracking      -- Tractography, metrics for streamlines\\n viz           -- Visualization and GUIs\\n workflows      -- Predefined Command line for common tasks\\n\\nUtilities\\n---------\\n::\\n\\n test          -- Run unittests\\n __version__   -- Dipy version\\n\\n\"\"\"\\nimport sys\\n\\nfrom dipy.version import version as __version__\\n\\n# Plumb in version etc info stuff\\nfrom .pkg_info import get_pkg_info as _get_pkg_info\\n\\n\\ndef get_info():\\n    from os.path import dirname\\n    return _get_pkg_info(dirname(__file__))\\n\\n\\ndel sys\\n\\nsubmodules = [\\n    \\'align\\',\\n    \\'core\\',\\n    \\'data\\',\\n    \\'denoise\\',\\n    \\'direction\\',\\n    \\'io\\',\\n    \\'nn\\',\\n    \\'reconst\\',\\n    \\'segment\\',\\n    \\'sims\\',\\n    \\'stats\\',\\n    \\'tracking\\',\\n    \\'utils\\',\\n    \\'viz\\',\\n    \\'workflows\\',\\n    \\'tests\\',\\n    \\'testing\\'\\n]\\n\\n__all__ = submodules + [\\'__version__\\', \\'setup_test\\', \\'get_info\\']', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\__init__.py.txt'}),\n",
       " Document(page_content='#!python\\n#cython: boundscheck=False\\n#cython: wraparound=False\\n#cython: cdivision=True\\n\\nimport numpy as np\\ncimport numpy as cnp\\n\\ncimport safe_openmp as openmp\\nfrom safe_openmp cimport have_openmp\\n\\nfrom cython.parallel import prange\\nfrom libc.stdlib cimport malloc, free\\nfrom libc.math cimport sqrt\\n\\nfrom dipy.utils.omp import determine_num_threads\\nfrom dipy.utils.omp cimport set_num_threads, restore_default_num_threads\\n\\ncdef cnp.dtype f64_dt = np.dtype(np.float64)\\n\\n\\ncdef double min_direct_flip_dist(double *a,double *b,\\n                                 cnp.npy_intp rows) noexcept nogil:\\n    r\"\"\" Minimum of direct and flip average (MDF) distance [Garyfallidis12]\\n    between two streamlines.\\n\\n    Parameters\\n    ----------\\n    a : double pointer\\n        first streamline\\n    b : double pointer\\n        second streamline\\n    rows : number of points of the streamline\\n        both tracks need to have the same number of points\\n\\n    Returns\\n    -------\\n    out : double\\n        minimum of direct and flipped average distances\\n\\n    References\\n    ----------\\n    .. [Garyfallidis12] Garyfallidis E. et al., QuickBundles a method for\\n                        tractography simplification, Frontiers in Neuroscience,\\n                        vol 6, no 175, 2012.\\n    \"\"\"\\n\\n    cdef:\\n        cnp.npy_intp i=0, j=0\\n        double sub=0, subf=0, distf=0, dist=0, tmprow=0, tmprowf=0\\n\\n\\n    for i in range(rows):\\n        tmprow = 0\\n        tmprowf = 0\\n        for j in range(3):\\n            sub = a[i * 3 + j] - b[i * 3 + j]\\n            subf = a[i * 3 + j] - b[(rows - 1 - i) * 3 + j]\\n            tmprow += sub * sub\\n            tmprowf += subf * subf\\n        dist += sqrt(tmprow)\\n        distf += sqrt(tmprowf)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\bundlemin.pyx.txt'}),\n",
       " Document(page_content='cdef:\\n        cnp.npy_intp i=0, j=0\\n        double sub=0, subf=0, distf=0, dist=0, tmprow=0, tmprowf=0\\n\\n\\n    for i in range(rows):\\n        tmprow = 0\\n        tmprowf = 0\\n        for j in range(3):\\n            sub = a[i * 3 + j] - b[i * 3 + j]\\n            subf = a[i * 3 + j] - b[(rows - 1 - i) * 3 + j]\\n            tmprow += sub * sub\\n            tmprowf += subf * subf\\n        dist += sqrt(tmprow)\\n        distf += sqrt(tmprowf)\\n\\n    dist = dist / <double>rows\\n    distf = distf / <double>rows\\n\\n    if dist <= distf:\\n        return dist\\n    return distf\\n\\n\\ndef _bundle_minimum_distance_matrix(double [:, ::1] static,\\n                                    double [:, ::1] moving,\\n                                    cnp.npy_intp static_size,\\n                                    cnp.npy_intp moving_size,\\n                                    cnp.npy_intp rows,\\n                                    double [:, ::1] D,\\n                                    num_threads=None):\\n    \"\"\" MDF-based pairwise distance optimization function\\n\\n    We minimize the distance between moving streamlines of the same number of\\n    points as they align with the static streamlines.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\bundlemin.pyx.txt'}),\n",
       " Document(page_content='We minimize the distance between moving streamlines of the same number of\\n    points as they align with the static streamlines.\\n\\n    Parameters\\n    ----------\\n    static: array\\n        Static streamlines\\n    moving: array\\n        Moving streamlines\\n    static_size : int\\n        Number of static streamlines\\n    moving_size : int\\n        Number of moving streamlines\\n    rows : int\\n        Number of points per streamline\\n    D : 2D array\\n        Distance matrix\\n    num_threads : int, optional\\n        Number of threads to be used for OpenMP parallelization. If None\\n        (default) the value of OMP_NUM_THREADS environment variable is used\\n        if it is set, otherwise all available threads are used. If < 0 the\\n        maximal number of threads minus |num_threads + 1| is used (enter -1 to\\n        use as many threads as possible). 0 raises an error.\\n\\n    Returns\\n    -------\\n    cost : double\\n    \"\"\"\\n\\n    cdef:\\n        cnp.npy_intp i=0, j=0, mov_i=0, mov_j=0\\n        int threads_to_use = -1\\n\\n    threads_to_use = determine_num_threads(num_threads)\\n    set_num_threads(threads_to_use)\\n\\n    with nogil:\\n\\n        for i in prange(static_size):\\n            for j in prange(moving_size):\\n\\n                D[i, j] = min_direct_flip_dist(&static[i * rows, 0],\\n                                               &moving[j * rows, 0],\\n                                               rows)\\n\\n    if num_threads is not None:\\n        restore_default_num_threads()\\n\\n    return np.asarray(D)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\bundlemin.pyx.txt'}),\n",
       " Document(page_content='threads_to_use = determine_num_threads(num_threads)\\n    set_num_threads(threads_to_use)\\n\\n    with nogil:\\n\\n        for i in prange(static_size):\\n            for j in prange(moving_size):\\n\\n                D[i, j] = min_direct_flip_dist(&static[i * rows, 0],\\n                                               &moving[j * rows, 0],\\n                                               rows)\\n\\n    if num_threads is not None:\\n        restore_default_num_threads()\\n\\n    return np.asarray(D)\\n\\n\\ndef _bundle_minimum_distance(double [:, ::1] static,\\n                             double [:, ::1] moving,\\n                             cnp.npy_intp static_size,\\n                             cnp.npy_intp moving_size,\\n                             cnp.npy_intp rows,\\n                             num_threads=None):\\n    \"\"\" MDF-based pairwise distance optimization function\\n\\n    We minimize the distance between moving streamlines of the same number of\\n    points as they align with the static streamlines.\\n\\n    Parameters\\n    ----------\\n    static : array\\n        Static streamlines\\n    moving : array\\n        Moving streamlines\\n    static_size : int\\n        Number of static streamlines\\n    moving_size : int\\n        Number of moving streamlines\\n    rows : int\\n        Number of points per streamline\\n    num_threads : int, optional\\n        Number of threads to be used for OpenMP parallelization. If None\\n        (default) the value of OMP_NUM_THREADS environment variable is used\\n        if it is set, otherwise all available threads are used. If < 0 the\\n        maximal number of threads minus |num_threads + 1| is used (enter -1 to\\n        use as many threads as possible). 0 raises an error.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\bundlemin.pyx.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    cost : double\\n\\n    Notes\\n    -----\\n    The difference with ``_bundle_minimum_distance_matrix`` is that it does not\\n    save the full distance matrix and therefore needs much less memory.\\n    \"\"\"\\n\\n    cdef:\\n        cnp.npy_intp i=0, j=0\\n        double sum_i=0, sum_j=0, tmp=0\\n        double inf = np.finfo(\\'f8\\').max\\n        double dist=0\\n        double * min_j\\n        double * min_i\\n        openmp.omp_lock_t lock\\n        int threads_to_use = -1\\n\\n    threads_to_use = determine_num_threads(num_threads)\\n    set_num_threads(threads_to_use)\\n\\n    with nogil:\\n\\n        if have_openmp:\\n            openmp.omp_init_lock(&lock)\\n\\n        min_j = <double *> malloc(static_size * sizeof(double))\\n        min_i = <double *> malloc(moving_size * sizeof(double))\\n\\n        for i in range(static_size):\\n            min_j[i] = inf\\n\\n        for j in range(moving_size):\\n            min_i[j] = inf\\n\\n        for i in prange(static_size):\\n\\n            for j in range(moving_size):\\n\\n                tmp = min_direct_flip_dist(&static[i * rows, 0],\\n                                       &moving[j * rows, 0], rows)\\n\\n                if have_openmp:\\n                    openmp.omp_set_lock(&lock)\\n                if tmp < min_j[i]:\\n                    min_j[i] = tmp\\n\\n                if tmp < min_i[j]:\\n                    min_i[j] = tmp\\n                if have_openmp:\\n                    openmp.omp_unset_lock(&lock)\\n\\n        if have_openmp:\\n            openmp.omp_destroy_lock(&lock)\\n\\n        for i in range(static_size):\\n            sum_i += min_j[i]\\n\\n        for j in range(moving_size):\\n            sum_j += min_i[j]\\n\\n        free(min_j)\\n        free(min_i)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\bundlemin.pyx.txt'}),\n",
       " Document(page_content='if tmp < min_i[j]:\\n                    min_i[j] = tmp\\n                if have_openmp:\\n                    openmp.omp_unset_lock(&lock)\\n\\n        if have_openmp:\\n            openmp.omp_destroy_lock(&lock)\\n\\n        for i in range(static_size):\\n            sum_i += min_j[i]\\n\\n        for j in range(moving_size):\\n            sum_j += min_i[j]\\n\\n        free(min_j)\\n        free(min_i)\\n\\n        dist = (sum_i / <double>static_size + sum_j / <double>moving_size)\\n\\n        dist = 0.25 * dist * dist\\n\\n    if num_threads is not None:\\n        restore_default_num_threads()\\n\\n    return dist\\n\\n\\n\\ndef _bundle_minimum_distance_asymmetric(double [:, ::1] static,\\n                                        double [:, ::1] moving,\\n                                        cnp.npy_intp static_size,\\n                                        cnp.npy_intp moving_size,\\n                                        cnp.npy_intp rows):\\n    \"\"\" MDF-based pairwise distance optimization function\\n\\n    We minimize the distance between moving streamlines of the same number of\\n    points as they align with the static streamlines.\\n\\n    Parameters\\n    ----------\\n    static : array\\n        Static streamlines\\n    moving : array\\n        Moving streamlines\\n    static_size : int\\n        Number of static streamlines\\n    moving_size : int\\n        Number of moving streamlines\\n    rows : int\\n        Number of points per streamline\\n\\n    Returns\\n    -------\\n    cost : double', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\bundlemin.pyx.txt'}),\n",
       " Document(page_content='We minimize the distance between moving streamlines of the same number of\\n    points as they align with the static streamlines.\\n\\n    Parameters\\n    ----------\\n    static : array\\n        Static streamlines\\n    moving : array\\n        Moving streamlines\\n    static_size : int\\n        Number of static streamlines\\n    moving_size : int\\n        Number of moving streamlines\\n    rows : int\\n        Number of points per streamline\\n\\n    Returns\\n    -------\\n    cost : double\\n\\n    Notes\\n    -----\\n    The difference with ``_bundle_minimum_distance`` is that we sum the\\n    minimum values only for the static. Therefore, this is an asymmetric\\n    distance metric. This means that we are weighting only one direction of the\\n    registration. Not both directions. This can be very useful when we want\\n    to register a big set of bundles to a small set of bundles.\\n    See [Wanyan17]_.\\n\\n    References\\n    ----------\\n    .. [Wanyan17] Wanyan and Garyfallidis, Important new insights for the\\n    reduction of false positives in tractograms emerge from streamline-based\\n    registration and pruning, International Society for Magnetic Resonance in\\n    Medicine, Honolulu, Hawai, 2017.\\n\\n    \"\"\"\\n\\n    cdef:\\n        cnp.npy_intp i=0, j=0\\n        double sum_i=0, sum_j=0, tmp=0\\n        double inf = np.finfo(\\'f8\\').max\\n        double dist=0\\n        double * min_j\\n        openmp.omp_lock_t lock\\n\\n    with nogil:\\n\\n        if have_openmp:\\n            openmp.omp_init_lock(&lock)\\n\\n        min_j = <double *> malloc(static_size * sizeof(double))\\n\\n        for sz_i in range(static_size):\\n            min_j[sz_i] = inf\\n\\n        for i in prange(static_size):\\n\\n            for j in range(moving_size):', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\bundlemin.pyx.txt'}),\n",
       " Document(page_content='cdef:\\n        cnp.npy_intp i=0, j=0\\n        double sum_i=0, sum_j=0, tmp=0\\n        double inf = np.finfo(\\'f8\\').max\\n        double dist=0\\n        double * min_j\\n        openmp.omp_lock_t lock\\n\\n    with nogil:\\n\\n        if have_openmp:\\n            openmp.omp_init_lock(&lock)\\n\\n        min_j = <double *> malloc(static_size * sizeof(double))\\n\\n        for sz_i in range(static_size):\\n            min_j[sz_i] = inf\\n\\n        for i in prange(static_size):\\n\\n            for j in range(moving_size):\\n\\n                tmp = min_direct_flip_dist(&static[i * rows, 0],\\n                                           &moving[j * rows, 0], rows)\\n\\n                if have_openmp:\\n                    openmp.omp_set_lock(&lock)\\n                if tmp < min_j[i]:\\n                    min_j[i] = tmp\\n\\n                if have_openmp:\\n                    openmp.omp_unset_lock(&lock)\\n\\n        if have_openmp:\\n            openmp.omp_destroy_lock(&lock)\\n\\n        for i in range(static_size):\\n            sum_i += min_j[i]\\n\\n        free(min_j)\\n\\n        dist = sum_i / <double>static_size\\n\\n    return dist\\n\\n\\ndef distance_matrix_mdf(streamlines_a, streamlines_b):\\n    r\"\"\" Minimum direct flipped distance matrix between two streamline sets\\n\\n    All streamlines need to have the same number of points\\n\\n    Parameters\\n    ----------\\n    streamlines_a : sequence\\n       of streamlines as arrays, [(N, 3) .. (N, 3)]\\n    streamlines_b : sequence\\n       of streamlines as arrays, [(N, 3) .. (N, 3)]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\bundlemin.pyx.txt'}),\n",
       " Document(page_content='free(min_j)\\n\\n        dist = sum_i / <double>static_size\\n\\n    return dist\\n\\n\\ndef distance_matrix_mdf(streamlines_a, streamlines_b):\\n    r\"\"\" Minimum direct flipped distance matrix between two streamline sets\\n\\n    All streamlines need to have the same number of points\\n\\n    Parameters\\n    ----------\\n    streamlines_a : sequence\\n       of streamlines as arrays, [(N, 3) .. (N, 3)]\\n    streamlines_b : sequence\\n       of streamlines as arrays, [(N, 3) .. (N, 3)]\\n\\n    Returns\\n    -------\\n    DM : array, shape (len(streamlines_a), len(streamlines_b))\\n        distance matrix\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp i, j, lentA, lentB\\n    # preprocess tracks\\n    cdef:\\n        cnp.npy_intp longest_track_len = 0, track_len\\n        longest_track_lenA, longest_track_lenB\\n        cnp.ndarray[object, ndim=1] tracksA64\\n        cnp.ndarray[object, ndim=1] tracksB64\\n        cnp.ndarray[cnp.double_t, ndim=2] DM', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\bundlemin.pyx.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    DM : array, shape (len(streamlines_a), len(streamlines_b))\\n        distance matrix\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp i, j, lentA, lentB\\n    # preprocess tracks\\n    cdef:\\n        cnp.npy_intp longest_track_len = 0, track_len\\n        longest_track_lenA, longest_track_lenB\\n        cnp.ndarray[object, ndim=1] tracksA64\\n        cnp.ndarray[object, ndim=1] tracksB64\\n        cnp.ndarray[cnp.double_t, ndim=2] DM\\n\\n    lentA = len(streamlines_a)\\n    lentB = len(streamlines_b)\\n    tracksA64 = np.zeros((lentA,), dtype=object)\\n    tracksB64 = np.zeros((lentB,), dtype=object)\\n    DM = np.zeros((lentA,lentB), dtype=np.double)\\n    if streamlines_a[0].shape[0] != streamlines_b[0].shape[0]:\\n        msg = \\'Streamlines should have the same number of points as required\\'\\n        msg += \\'by the MDF distance\\'\\n        raise ValueError(msg)\\n    # process tracks to predictable memory layout\\n    for i in range(lentA):\\n        tracksA64[i] = np.ascontiguousarray(streamlines_a[i], dtype=f64_dt)\\n    for i in range(lentB):\\n        tracksB64[i] = np.ascontiguousarray(streamlines_b[i], dtype=f64_dt)\\n    # preallocate buffer array for track distance calculations\\n    cdef:\\n        cnp.float64_t *t1_ptr\\n        cnp.float64_t *t2_ptr\\n        cnp.float64_t *min_buffer\\n    # cycle over tracks\\n    cdef:\\n        cnp.ndarray [cnp.float64_t, ndim=2] t1, t2\\n        cnp.npy_intp t1_len, t2_len\\n        double d[2]\\n    t_len = tracksA64[0].shape[0]\\n\\n    for i from 0 <= i < lentA:\\n        t1 = tracksA64[i]\\n        t1_ptr = <cnp.float64_t *> cnp.PyArray_DATA(t1)\\n        for j from 0 <= j < lentB:\\n            t2 = tracksB64[j]\\n            t2_ptr = <cnp.float64_t *> cnp.PyArray_DATA(t2)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\bundlemin.pyx.txt'}),\n",
       " Document(page_content='for i from 0 <= i < lentA:\\n        t1 = tracksA64[i]\\n        t1_ptr = <cnp.float64_t *> cnp.PyArray_DATA(t1)\\n        for j from 0 <= j < lentB:\\n            t2 = tracksB64[j]\\n            t2_ptr = <cnp.float64_t *> cnp.PyArray_DATA(t2)\\n\\n            DM[i, j] = min_direct_flip_dist(t1_ptr, t2_ptr,t_len)\\n\\n    return DM', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\bundlemin.pyx.txt'}),\n",
       " Document(page_content='\"\"\"\\n\\nNote\\n----\\n\\nThis file is copied (possibly with major modifications) from the\\nsources of the pycpd project - https://github.com/siavashk/pycpd.\\nIt remains licensed as the rest of PyCPD (MIT license as of October 2010).\\n\\n# ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##\\n#\\n#   See COPYING file distributed along with the PyCPD package for the\\n#   copyright and license terms.\\n#\\n# ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##\\n\"\"\"\\n\\nimport numpy as np\\nimport numbers\\nfrom warnings import warn\\n\\n\\ndef gaussian_kernel(X, beta, Y=None):\\n    if Y is None:\\n        Y = X\\n    diff = X[:, None, :] - Y[None, :, :]\\n    diff = np.square(diff)\\n    diff = np.sum(diff, 2)\\n    return np.exp(-diff / (2 * beta**2))\\n\\n\\ndef low_rank_eigen(G, num_eig):\\n    \"\"\"Calculate num_eig eigenvectors and eigenvalues of gaussian matrix G.\\n\\n    Enables lower dimensional solving.\\n\\n    \"\"\"\\n    S, Q = np.linalg.eigh(G)\\n    eig_indices = list(np.argsort(np.abs(S))[::-1][:num_eig])\\n    Q = Q[:, eig_indices]  # eigenvectors\\n    S = S[eig_indices]  # eigenvalues.\\n    return Q, S\\n\\n\\ndef initialize_sigma2(X, Y):\\n    \"\"\"Initialize the variance (sigma2).\\n\\n    Parameters\\n    ----------\\n    X: numpy array\\n        NxD array of points for target.\\n\\n    Y: numpy array\\n        MxD array of points for source.\\n\\n    Returns\\n    -------\\n    sigma2: float\\n        Initial variance.\\n    \"\"\"\\n    (N, D) = X.shape\\n    (M, _) = Y.shape\\n    diff = X[None, :, :] - Y[:, None, :]\\n    err = diff ** 2\\n    return np.sum(err) / (D * M * N)\\n\\n\\ndef lowrankQS(G, beta, num_eig, eig_fgt=False):\\n    \"\"\"Calculate eigenvectors and eigenvalues of gaussian matrix G.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\cpd.py.txt'}),\n",
       " Document(page_content='Parameters\\n    ----------\\n    X: numpy array\\n        NxD array of points for target.\\n\\n    Y: numpy array\\n        MxD array of points for source.\\n\\n    Returns\\n    -------\\n    sigma2: float\\n        Initial variance.\\n    \"\"\"\\n    (N, D) = X.shape\\n    (M, _) = Y.shape\\n    diff = X[None, :, :] - Y[:, None, :]\\n    err = diff ** 2\\n    return np.sum(err) / (D * M * N)\\n\\n\\ndef lowrankQS(G, beta, num_eig, eig_fgt=False):\\n    \"\"\"Calculate eigenvectors and eigenvalues of gaussian matrix G.\\n\\n    !!!\\n    This function is a placeholder for implementing the fast\\n    gauss transform. It is not yet implemented.\\n    !!!\\n\\n    Parameters\\n    ----------\\n    G: numpy array\\n        Gaussian kernel matrix.\\n\\n    beta: float\\n        Width of the Gaussian kernel.\\n\\n    num_eig: int\\n        Number of eigenvectors to use in lowrank calculation of G\\n\\n    eig_fgt: bool\\n        If True, use fast gauss transform method to speed up.\\n\\n    \"\"\"\\n    # if we do not use FGT we construct affinity matrix G and find the\\n    # first eigenvectors/values directly\\n\\n    if eig_fgt is False:\\n        S, Q = np.linalg.eigh(G)\\n        eig_indices = list(np.argsort(np.abs(S))[::-1][:num_eig])\\n        Q = Q[:, eig_indices]  # eigenvectors\\n        S = S[eig_indices]  # eigenvalues.\\n\\n        return Q, S\\n\\n    elif eig_fgt is True:\\n        raise Exception(\\'Fast Gauss Transform Not Implemented!\\')\\n\\n\\nclass DeformableRegistration:\\n    \"\"\"\\n    Deformable point cloud registration.\\n\\n    Attributes\\n    ----------\\n    X: numpy array\\n        NxD array of target points.\\n\\n    Y: numpy array\\n        MxD array of source points.\\n\\n    TY: numpy array\\n        MxD array of transformed source points.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\cpd.py.txt'}),\n",
       " Document(page_content='return Q, S\\n\\n    elif eig_fgt is True:\\n        raise Exception(\\'Fast Gauss Transform Not Implemented!\\')\\n\\n\\nclass DeformableRegistration:\\n    \"\"\"\\n    Deformable point cloud registration.\\n\\n    Attributes\\n    ----------\\n    X: numpy array\\n        NxD array of target points.\\n\\n    Y: numpy array\\n        MxD array of source points.\\n\\n    TY: numpy array\\n        MxD array of transformed source points.\\n\\n    sigma2: float (positive)\\n        Initial variance of the Gaussian mixture model.\\n\\n    N: int\\n        Number of target points.\\n\\n    M: int\\n        Number of source points.\\n\\n    D: int\\n        Dimensionality of source and target points\\n\\n    iteration: int\\n        The current iteration throughout registration.\\n\\n    max_iterations: int\\n        Registration will terminate once the algorithm has taken this\\n        many iterations.\\n\\n    tolerance: float (positive)\\n        Registration will terminate once the difference between\\n        consecutive objective function values falls within this tolerance.\\n\\n    w: float (between 0 and 1)\\n        Contribution of the uniform distribution to account for outliers.\\n        Valid values span 0 (inclusive) and 1 (exclusive).\\n\\n    q: float\\n        The objective function value that represents the misalignment between\\n        source and target point clouds.\\n\\n    diff: float (positive)\\n        The absolute difference between the current and previous objective\\n        function values.\\n\\n    P: numpy array\\n        MxN array of probabilities.\\n        P[m, n] represents the probability that the m-th source point\\n        corresponds to the n-th target point.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\cpd.py.txt'}),\n",
       " Document(page_content='q: float\\n        The objective function value that represents the misalignment between\\n        source and target point clouds.\\n\\n    diff: float (positive)\\n        The absolute difference between the current and previous objective\\n        function values.\\n\\n    P: numpy array\\n        MxN array of probabilities.\\n        P[m, n] represents the probability that the m-th source point\\n        corresponds to the n-th target point.\\n\\n    Pt1: numpy array\\n        Nx1 column array. Multiplication result between the transpose of P\\n        and a column vector of all 1s.\\n\\n    P1: numpy array\\n        Mx1 column array.\\n        Multiplication result between P and a column vector of all 1s.\\n\\n    Np: float (positive)\\n        The sum of all elements in P.\\n\\n    alpha: float (positive)\\n        Represents the trade-off between the goodness of maximum likelihoo\\n        fit and regularization.\\n\\n    beta: float(positive)\\n        Width of the Gaussian kernel.\\n\\n    low_rank: bool\\n        Whether to use low rank approximation.\\n\\n    num_eig: int\\n        Number of eigenvectors to use in lowrank calculation.\\n    \"\"\"\\n\\n    def __init__(self, X, Y, sigma2=None, alpha=None, beta=None,\\n                 low_rank=False, num_eig=100, max_iterations=None,\\n                 tolerance=None,  w=None, *args, **kwargs):\\n        if not isinstance(X, np.ndarray) or X.ndim != 2:\\n            raise ValueError(\\n                \"The target point cloud (X) must be at a 2D numpy array.\")\\n\\n        if not isinstance(Y, np.ndarray) or Y.ndim != 2:\\n            raise ValueError(\\n                \"The source point cloud (Y) must be a 2D numpy array.\")', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\cpd.py.txt'}),\n",
       " Document(page_content='if not isinstance(Y, np.ndarray) or Y.ndim != 2:\\n            raise ValueError(\\n                \"The source point cloud (Y) must be a 2D numpy array.\")\\n\\n        if X.shape[1] != Y.shape[1]:\\n            msg = \"Both point clouds need to have the same number \"\\n            msg += \"of dimensions.\"\\n            raise ValueError(msg)\\n\\n        if sigma2 is not None and (not isinstance(sigma2, numbers.Number)\\n                                   or sigma2 <= 0):\\n            msg = f\"Expected a positive value for sigma2 instead got: {sigma2}\"\\n            raise ValueError(msg)\\n\\n        if max_iterations is not None and (not isinstance(max_iterations,\\n                                                          numbers.Number)\\n                                           or max_iterations < 0):\\n            msg = \"Expected a positive integer for max_iterations \"\\n            msg += f\"instead got: {max_iterations}\"\\n            raise ValueError(msg)\\n        elif isinstance(max_iterations, numbers.Number) and \\\\\\n                not isinstance(max_iterations, int):\\n            msg = \"Received a non-integer value for max_iterations: \"\\n            msg += f\"{max_iterations}. Casting to integer.\"\\n            warn(msg)\\n            max_iterations = int(max_iterations)\\n\\n        if tolerance is not None and (not isinstance(tolerance, numbers.Number)\\n                                      or tolerance < 0):\\n            msg = \"Expected a positive float for tolerance \"\\n            msg += f\"instead got: {tolerance}\"\\n            raise ValueError(msg)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\cpd.py.txt'}),\n",
       " Document(page_content='if tolerance is not None and (not isinstance(tolerance, numbers.Number)\\n                                      or tolerance < 0):\\n            msg = \"Expected a positive float for tolerance \"\\n            msg += f\"instead got: {tolerance}\"\\n            raise ValueError(msg)\\n\\n        if w is not None and (not isinstance(w, numbers.Number)\\n                              or w < 0 or w >= 1):\\n            msg = \"Expected a value between 0 (inclusive) and 1 (exclusive) \"\\n            msg += f\"for w instead got: {w}\"\\n            raise ValueError(msg)\\n\\n        self.X = X\\n        self.Y = Y\\n        self.TY = Y\\n        self.sigma2 = initialize_sigma2(X, Y) if sigma2 is None else sigma2\\n        (self.N, self.D) = self.X.shape\\n        (self.M, _) = self.Y.shape\\n        self.tolerance = 0.001 if tolerance is None else tolerance\\n        self.w = 0.0 if w is None else w\\n        self.max_iterations = 100 if max_iterations is None else max_iterations\\n        self.iteration = 0\\n        self.diff = np.inf\\n        self.q = np.inf\\n        self.P = np.zeros((self.M, self.N))\\n        self.Pt1 = np.zeros((self.N, ))\\n        self.P1 = np.zeros((self.M, ))\\n        self.PX = np.zeros((self.M, self.D))\\n        self.Np = 0\\n\\n        if alpha is not None and (not isinstance(alpha, numbers.Number)\\n                                  or alpha <= 0):\\n            msg = \"Expected a positive value for regularization parameter \"\\n            msg += f\"alpha. Instead got: {alpha}\"\\n            raise ValueError(msg)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\cpd.py.txt'}),\n",
       " Document(page_content='if alpha is not None and (not isinstance(alpha, numbers.Number)\\n                                  or alpha <= 0):\\n            msg = \"Expected a positive value for regularization parameter \"\\n            msg += f\"alpha. Instead got: {alpha}\"\\n            raise ValueError(msg)\\n\\n        if beta is not None and (not isinstance(beta, numbers.Number)\\n                                 or beta <= 0):\\n            msg = \"Expected a positive value for the width of the coherent \"\\n            msg += f\"Gaussian kernel. Instead got: {beta}\"\\n\\n        self.alpha = 2 if alpha is None else alpha\\n        self.beta = 2 if beta is None else beta\\n        self.W = np.zeros((self.M, self.D))\\n        self.G = gaussian_kernel(self.Y, self.beta)\\n        self.low_rank = low_rank\\n        self.num_eig = num_eig\\n        if self.low_rank is True:\\n            self.Q, self.S = low_rank_eigen(self.G, self.num_eig)\\n            self.inv_S = np.diag(1./self.S)\\n            self.S = np.diag(self.S)\\n            self.E = 0.\\n\\n    def register(self, callback=lambda **kwargs: None):\\n        \"\"\"\\n        Perform the EM registration.\\n\\n        Parameters\\n        ----------\\n        callback: function\\n            A function that will be called after each iteration.\\n            Can be used to visualize the registration process.\\n\\n        Returns\\n        -------\\n        self.TY: numpy array\\n            MxD array of transformed source points.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\cpd.py.txt'}),\n",
       " Document(page_content='def register(self, callback=lambda **kwargs: None):\\n        \"\"\"\\n        Perform the EM registration.\\n\\n        Parameters\\n        ----------\\n        callback: function\\n            A function that will be called after each iteration.\\n            Can be used to visualize the registration process.\\n\\n        Returns\\n        -------\\n        self.TY: numpy array\\n            MxD array of transformed source points.\\n\\n        registration_parameters:\\n            Returned params dependent on registration method used.\\n        \"\"\"\\n        self.transform_point_cloud()\\n        while self.iteration < self.max_iterations and \\\\\\n                self.diff > self.tolerance:\\n            self.iterate()\\n            if callable(callback):\\n                kwargs = {\\'iteration\\': self.iteration,\\n                          \\'error\\': self.q, \\'X\\': self.X, \\'Y\\': self.TY}\\n                callback(**kwargs)\\n\\n        return self.TY, self.get_registration_parameters()\\n\\n    def update_transform(self):\\n        \"\"\"\\n        Calculate a new estimate of the deformable transformation.\\n        See Eq. 22 of https://arxiv.org/pdf/0905.2635.pdf.\\n\\n        \"\"\"\\n        if self.low_rank is False:\\n            A = np.dot(np.diag(self.P1), self.G) + \\\\\\n                self.alpha * self.sigma2 * np.eye(self.M)\\n            B = self.PX - np.dot(np.diag(self.P1), self.Y)\\n            self.W = np.linalg.solve(A, B)\\n\\n        elif self.low_rank is True:\\n            # Matlab code equivalent can be found here:\\n            # https://github.com/markeroon/matlab-computer-vision-routines/tree/master/third_party/CoherentPointDrift\\n            dP = np.diag(self.P1)\\n            dPQ = np.matmul(dP, self.Q)\\n            F = self.PX - np.matmul(dP, self.Y)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\cpd.py.txt'}),\n",
       " Document(page_content='elif self.low_rank is True:\\n            # Matlab code equivalent can be found here:\\n            # https://github.com/markeroon/matlab-computer-vision-routines/tree/master/third_party/CoherentPointDrift\\n            dP = np.diag(self.P1)\\n            dPQ = np.matmul(dP, self.Q)\\n            F = self.PX - np.matmul(dP, self.Y)\\n\\n            self.W = 1 / (self.alpha * self.sigma2) * (F - np.matmul(dPQ, (\\n                np.linalg.solve((self.alpha * self.sigma2 * self.inv_S + np.matmul(self.Q.T, dPQ)),\\n                                (np.matmul(self.Q.T, F))))))\\n            QtW = np.matmul(self.Q.T, self.W)\\n            self.E = self.E + self.alpha / 2 * np.trace(np.matmul(QtW.T, np.matmul(self.S, QtW)))\\n\\n    def transform_point_cloud(self, Y=None):\\n        \"\"\"Update a point cloud using the new estimate of the deformable\\n        transformation.\\n\\n        Parameters\\n        ----------\\n        Y: numpy array, optional\\n            Array of points to transform - use to predict on new set of points.\\n            Best for predicting on new points not used to run initial\\n            registration. If None, self.Y used.\\n\\n        Returns\\n        -------\\n        If Y is None, returns None.\\n        Otherwise, returns the transformed Y.\\n\\n\\n        \"\"\"\\n        if Y is not None:\\n            G = gaussian_kernel(X=Y, beta=self.beta, Y=self.Y)\\n            return Y + np.dot(G, self.W)\\n        else:\\n            if self.low_rank is False:\\n                self.TY = self.Y + np.dot(self.G, self.W)\\n\\n            elif self.low_rank is True:\\n                self.TY = self.Y + np.matmul(\\n                    self.Q,\\n                    np.matmul(self.S, np.matmul(self.Q.T, self.W)))\\n                return', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\cpd.py.txt'}),\n",
       " Document(page_content='\"\"\"\\n        if Y is not None:\\n            G = gaussian_kernel(X=Y, beta=self.beta, Y=self.Y)\\n            return Y + np.dot(G, self.W)\\n        else:\\n            if self.low_rank is False:\\n                self.TY = self.Y + np.dot(self.G, self.W)\\n\\n            elif self.low_rank is True:\\n                self.TY = self.Y + np.matmul(\\n                    self.Q,\\n                    np.matmul(self.S, np.matmul(self.Q.T, self.W)))\\n                return\\n\\n    def update_variance(self):\\n        \"\"\"Update the variance of the mixture model.\\n\\n        This is using the new estimate of the deformable transformation.\\n        See the update rule for sigma2 in\\n        Eq. 23 of of https://arxiv.org/pdf/0905.2635.pdf.\\n\\n        \"\"\"\\n        qprev = self.sigma2\\n\\n        # The original CPD paper does not explicitly calculate the objective\\n        # functional. This functional will include terms from both the negative\\n        # log-likelihood and the Gaussian kernel used for regularization.\\n        self.q = np.inf\\n\\n        xPx = np.dot(np.transpose(self.Pt1), np.sum(\\n            np.multiply(self.X, self.X), axis=1))\\n        yPy = np.dot(np.transpose(self.P1),  np.sum(\\n            np.multiply(self.TY, self.TY), axis=1))\\n        trPXY = np.sum(np.multiply(self.TY, self.PX))\\n\\n        self.sigma2 = (xPx - 2 * trPXY + yPy) / (self.Np * self.D)\\n\\n        if self.sigma2 <= 0:\\n            self.sigma2 = self.tolerance / 10\\n\\n        # Here we use the difference between the current and previous\\n        # estimate of the variance as a proxy to test for convergence.\\n        self.diff = np.abs(self.sigma2 - qprev)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\cpd.py.txt'}),\n",
       " Document(page_content='self.sigma2 = (xPx - 2 * trPXY + yPy) / (self.Np * self.D)\\n\\n        if self.sigma2 <= 0:\\n            self.sigma2 = self.tolerance / 10\\n\\n        # Here we use the difference between the current and previous\\n        # estimate of the variance as a proxy to test for convergence.\\n        self.diff = np.abs(self.sigma2 - qprev)\\n\\n    def get_registration_parameters(self):\\n        \"\"\"Return the current estimate of the deformable transformation\\n        parameters.\\n\\n        Returns\\n        -------\\n        self.G: numpy array\\n            Gaussian kernel matrix.\\n\\n        self.W: numpy array\\n            Deformable transformation matrix.\\n        \"\"\"\\n        return self.G, self.W\\n\\n    def iterate(self):\\n        \"\"\"Perform one iteration of the EM algorithm.\"\"\"\\n        self.expectation()\\n        self.maximization()\\n        self.iteration += 1\\n\\n    def expectation(self):\\n        \"\"\"Compute the expectation step of the EM algorithm.\"\"\"\\n        # (M, N)\\n        P = np.sum((self.X[None, :, :] - self.TY[:, None, :])**2, axis=2)\\n        P = np.exp(-P/(2*self.sigma2))\\n        c = (2*np.pi*self.sigma2)**(self.D/2)*self.w/(1. - self.w)*self.M/self.N\\n\\n        den = np.sum(P, axis=0, keepdims=True)  # (1, N)\\n        den = np.clip(den, np.finfo(self.X.dtype).eps, None) + c\\n\\n        self.P = np.divide(P, den)\\n        self.Pt1 = np.sum(self.P, axis=0)\\n        self.P1 = np.sum(self.P, axis=1)\\n        self.Np = np.sum(self.P1)\\n        self.PX = np.matmul(self.P, self.X)\\n\\n    def maximization(self):\\n        \"\"\"Compute the maximization step of the EM algorithm.\"\"\"\\n        self.update_transform()\\n        self.transform_point_cloud()\\n        self.update_variance()', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\cpd.py.txt'}),\n",
       " Document(page_content='\"\"\" Utility functions used by the Cross Correlation (CC) metric \"\"\"\\n\\nimport numpy as np\\nfrom dipy.align.fused_types cimport floating\\ncimport cython\\ncimport numpy as cnp\\n\\n\\ncdef inline int _int_max(int a, int b) noexcept nogil:\\n    r\"\"\"\\n    Returns the maximum of a and b\\n    \"\"\"\\n    return a if a >= b else b\\n\\n\\ncdef inline int _int_min(int a, int b) noexcept nogil:\\n    r\"\"\"\\n    Returns the minimum of a and b\\n    \"\"\"\\n    return a if a <= b else b\\n\\n\\ncdef enum:\\n    SI = 0\\n    SI2 = 1\\n    SJ = 2\\n    SJ2 = 3\\n    SIJ = 4\\n    CNT = 5\\n\\n\\n@cython.boundscheck(False)\\n@cython.wraparound(False)\\n@cython.cdivision(True)\\ncdef inline int _wrap(int x, int m) noexcept nogil:\\n    r\"\"\" Auxiliary function to `wrap` an array around its low-end side.\\n    Negative indices are mapped to last coordinates so that no extra memory\\n    is required to account for local rectangular windows that exceed the\\n    array\\'s low-end boundary.\\n\\n    Parameters\\n    ----------\\n    x : int\\n        the array position to be wrapped\\n    m : int\\n        array length\\n    \"\"\"\\n    if x < 0:\\n        return x + m\\n    return x\\n\\n\\n@cython.boundscheck(False)\\n@cython.wraparound(False)\\n@cython.cdivision(True)\\ncdef inline void _update_factors(double[:, :, :, :] factors,\\n                                 floating[:, :, :] moving,\\n                                 floating[:, :, :] static,\\n                                 cnp.npy_intp ss, cnp.npy_intp rr, cnp.npy_intp cc,\\n                                 cnp.npy_intp s, cnp.npy_intp r, cnp.npy_intp c, int operation)noexcept nogil:\\n    r\"\"\"Updates the precomputed CC factors of a rectangular window', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\crosscorr.pyx.txt'}),\n",
       " Document(page_content='Updates the precomputed CC factors of the rectangular window centered\\n    at (`ss`, `rr`, `cc`) by adding the factors corresponding to voxel\\n    (`s`, `r`, `c`) of input images `moving` and `static`.\\n\\n    Parameters\\n    ----------\\n    factors : array, shape (S, R, C, 5)\\n        array containing the current precomputed factors to be updated\\n    moving : array, shape (S, R, C)\\n        the moving volume (notice that both images must already be in a common\\n        reference domain, in particular, they must have the same shape)\\n    static : array, shape (S, R, C)\\n        the static volume, which also defines the reference registration domain\\n    ss : int\\n        first coordinate of the rectangular window to be updated\\n    rr : int\\n        second coordinate of the rectangular window to be updated\\n    cc : int\\n        third coordinate of the rectangular window to be updated\\n    s: int\\n        first coordinate of the voxel the local window should be updated with\\n    r: int\\n        second coordinate of the voxel the local window should be updated with\\n    c: int\\n        third coordinate of the voxel the local window should be updated with\\n    operation : int, either -1, 0 or 1\\n        indicates whether the factors of voxel (`s`, `r`, `c`) should be\\n        added to (`operation`=1), subtracted from (`operation`=-1), or set as\\n        (`operation`=0) the current factors for the rectangular window centered\\n        at (`ss`, `rr`, `cc`).', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\crosscorr.pyx.txt'}),\n",
       " Document(page_content='\"\"\"\\n    cdef:\\n        double sval\\n        double mval\\n    if s >= moving.shape[0] or r >= moving.shape[1] or c >= moving.shape[2]:\\n        if operation == 0:\\n            factors[ss, rr, cc, SI] = 0\\n            factors[ss, rr, cc, SI2] = 0\\n            factors[ss, rr, cc, SJ] = 0\\n            factors[ss, rr, cc, SJ2] = 0\\n            factors[ss, rr, cc, SIJ] = 0\\n    else:\\n        sval = static[s, r, c]\\n        mval = moving[s, r, c]\\n        if operation == 0:\\n            factors[ss, rr, cc, SI] = sval\\n            factors[ss, rr, cc, SI2] = sval*sval\\n            factors[ss, rr, cc, SJ] = mval\\n            factors[ss, rr, cc, SJ2] = mval*mval\\n            factors[ss, rr, cc, SIJ] = sval*mval\\n        elif operation == -1:\\n            factors[ss, rr, cc, SI] -= sval\\n            factors[ss, rr, cc, SI2] -= sval*sval\\n            factors[ss, rr, cc, SJ] -= mval\\n            factors[ss, rr, cc, SJ2] -= mval*mval\\n            factors[ss, rr, cc, SIJ] -= sval*mval\\n        elif operation == 1:\\n            factors[ss, rr, cc, SI] += sval\\n            factors[ss, rr, cc, SI2] += sval*sval\\n            factors[ss, rr, cc, SJ] += mval\\n            factors[ss, rr, cc, SJ2] += mval*mval\\n            factors[ss, rr, cc, SIJ] += sval*mval\\n\\n\\n@cython.boundscheck(False)\\n@cython.wraparound(False)\\n@cython.cdivision(True)\\ndef precompute_cc_factors_3d(floating[:, :, :] static,\\n                             floating[:, :, :] moving,\\n                             cnp.npy_intp radius, num_threads=None):\\n    r\"\"\"Precomputations to quickly compute the gradient of the CC Metric', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\crosscorr.pyx.txt'}),\n",
       " Document(page_content='@cython.boundscheck(False)\\n@cython.wraparound(False)\\n@cython.cdivision(True)\\ndef precompute_cc_factors_3d(floating[:, :, :] static,\\n                             floating[:, :, :] moving,\\n                             cnp.npy_intp radius, num_threads=None):\\n    r\"\"\"Precomputations to quickly compute the gradient of the CC Metric\\n\\n    Pre-computes the separate terms of the cross correlation metric and image\\n    norms at each voxel considering a neighborhood of the given radius to\\n    efficiently compute the gradient of the metric with respect to the\\n    deformation field [Ocegueda2016]_ [Avants2008]_ [Avants2011]_.\\n\\n    Parameters\\n    ----------\\n    static : array, shape (S, R, C)\\n        the static volume, which also defines the reference registration domain\\n    moving : array, shape (S, R, C)\\n        the moving volume (notice that both images must already be in a common\\n        reference domain, i.e. the same S, R, C)\\n    radius : the radius of the neighborhood (cube of (2 * radius + 1)^3 voxels)\\n\\n    Returns\\n    -------\\n    factors : array, shape (S, R, C, 5)\\n        the precomputed cross correlation terms:\\n        factors[:,:,:,0] : static minus its mean value along the neighborhood\\n        factors[:,:,:,1] : moving minus its mean value along the neighborhood\\n        factors[:,:,:,2] : sum of the pointwise products of static and moving\\n                           along the neighborhood\\n        factors[:,:,:,3] : sum of sq. values of static along the neighborhood\\n        factors[:,:,:,4] : sum of sq. values of moving along the neighborhood', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\crosscorr.pyx.txt'}),\n",
       " Document(page_content='References\\n    ----------\\n    .. [Ocegueda2016]_ Ocegueda, O., Dalmau, O., Garyfallidis, E., Descoteaux,\\n        M., & Rivera, M. (2016). On the computation of integrals over\\n        fixed-size rectangles of arbitrary dimension, Pattern Recognition\\n        Letters. doi:10.1016/j.patrec.2016.05.008\\n    .. [Avants2008]_ Avants, B. B., Epstein, C. L., Grossman, M., & Gee, J. C.\\n        (2008). Symmetric Diffeomorphic Image Registration with\\n        Cross-Correlation: Evaluating Automated Labeling of Elderly and\\n        Neurodegenerative Brain, Med Image Anal. 12(1), 26-41.\\n    .. [Avants2011]_ Avants, B. B., Tustison, N., & Song, G. (2011). Advanced\\n        Normalization Tools (ANTS), 1-35.\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp ns = static.shape[0]\\n        cnp.npy_intp nr = static.shape[1]\\n        cnp.npy_intp nc = static.shape[2]\\n        cnp.npy_intp side = 2 * radius + 1\\n        cnp.npy_intp firstc, lastc, firstr, lastr, firsts, lasts\\n        cnp.npy_intp s, r, c, it, sides, sider, sidec\\n        double cnt\\n        cnp.npy_intp ssss, sss, ss, rr, cc, prev_ss, prev_rr, prev_cc\\n        double Imean, Jmean, IJprods, Isq, Jsq\\n        double[:, :, :, :] temp = np.zeros((2, nr, nc, 5), dtype=np.float64)\\n        floating[:, :, :, :] factors = np.zeros((ns, nr, nc, 5),\\n                                                dtype=np.asarray(static).dtype)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\crosscorr.pyx.txt'}),\n",
       " Document(page_content='with nogil:\\n        sss = 1\\n        for s in range(ns+radius):\\n            ss = _wrap(s - radius, ns)\\n            sss = 1 - sss\\n            firsts = _int_max(0, ss - radius)\\n            lasts = _int_min(ns - 1, ss + radius)\\n            sides = (lasts - firsts + 1)\\n            for r in range(nr+radius):\\n                rr = _wrap(r - radius, nr)\\n                firstr = _int_max(0, rr - radius)\\n                lastr = _int_min(nr - 1, rr + radius)\\n                sider = (lastr - firstr + 1)\\n                for c in range(nc+radius):\\n                    cc = _wrap(c - radius, nc)\\n                    # New corner\\n                    _update_factors(temp, moving, static,\\n                                    sss, rr, cc, s, r, c, 0)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\crosscorr.pyx.txt'}),\n",
       " Document(page_content='# Add signed sub-volumes\\n                    if s > 0:\\n                        prev_ss = 1 - sss\\n                        for it in range(5):\\n                            temp[sss, rr, cc, it] += temp[prev_ss, rr, cc, it]\\n                        if r > 0:\\n                            prev_rr = _wrap(rr-1, nr)\\n                            for it in range(5):\\n                                temp[sss, rr, cc, it] -= \\\\\\n                                    temp[prev_ss, prev_rr, cc, it]\\n                            if c > 0:\\n                                prev_cc = _wrap(cc-1, nc)\\n                                for it in range(5):\\n                                    temp[sss, rr, cc, it] += \\\\\\n                                        temp[prev_ss, prev_rr, prev_cc, it]\\n                        if c > 0:\\n                            prev_cc = _wrap(cc-1, nc)\\n                            for it in range(5):\\n                                temp[sss, rr, cc, it] -= \\\\\\n                                    temp[prev_ss, rr, prev_cc, it]\\n                    if r > 0:\\n                        prev_rr = _wrap(rr-1, nr)\\n                        for it in range(5):\\n                            temp[sss, rr, cc, it] += \\\\\\n                                temp[sss, prev_rr, cc, it]\\n                        if c > 0:\\n                            prev_cc = _wrap(cc-1, nc)\\n                            for it in range(5):\\n                                temp[sss, rr, cc, it] -= \\\\\\n                                    temp[sss, prev_rr, prev_cc, it]\\n                    if c > 0:\\n                        prev_cc = _wrap(cc-1, nc)\\n                        for it in range(5):', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\crosscorr.pyx.txt'}),\n",
       " Document(page_content='temp[sss, prev_rr, cc, it]\\n                        if c > 0:\\n                            prev_cc = _wrap(cc-1, nc)\\n                            for it in range(5):\\n                                temp[sss, rr, cc, it] -= \\\\\\n                                    temp[sss, prev_rr, prev_cc, it]\\n                    if c > 0:\\n                        prev_cc = _wrap(cc-1, nc)\\n                        for it in range(5):\\n                            temp[sss, rr, cc, it] += temp[sss, rr, prev_cc, it]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\crosscorr.pyx.txt'}),\n",
       " Document(page_content='# Add signed corners\\n                    if s >= side:\\n                        _update_factors(temp, moving, static,\\n                                        sss, rr, cc, s-side, r, c, -1)\\n                        if r >= side:\\n                            _update_factors(temp, moving, static,\\n                                            sss, rr, cc, s-side, r-side, c, 1)\\n                            if c >= side:\\n                                _update_factors(temp, moving, static, sss, rr,\\n                                                cc, s-side, r-side, c-side, -1)\\n                        if c >= side:\\n                            _update_factors(temp, moving, static,\\n                                            sss, rr, cc, s-side, r, c-side, 1)\\n                    if r >= side:\\n                        _update_factors(temp, moving, static,\\n                                        sss, rr, cc, s, r-side, c, -1)\\n                        if c >= side:\\n                            _update_factors(temp, moving, static,\\n                                            sss, rr, cc, s, r-side, c-side, 1)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\crosscorr.pyx.txt'}),\n",
       " Document(page_content='if c >= side:\\n                        _update_factors(temp, moving, static,\\n                                        sss, rr, cc, s, r, c-side, -1)\\n                    # Compute final factors\\n                    if s >= radius and r >= radius and c >= radius:\\n                        firstc = _int_max(0, cc - radius)\\n                        lastc = _int_min(nc - 1, cc + radius)\\n                        sidec = (lastc - firstc + 1)\\n                        cnt = sides*sider*sidec\\n                        Imean = temp[sss, rr, cc, SI] / cnt\\n                        Jmean = temp[sss, rr, cc, SJ] / cnt\\n                        IJprods = (temp[sss, rr, cc, SIJ] -\\n                                   Jmean * temp[sss, rr, cc, SI] -\\n                                   Imean * temp[sss, rr, cc, SJ] +\\n                                   cnt * Jmean * Imean)\\n                        Isq = (temp[sss, rr, cc, SI2] -\\n                               Imean * temp[sss, rr, cc, SI] -\\n                               Imean * temp[sss, rr, cc, SI] +\\n                               cnt * Imean * Imean)\\n                        Jsq = (temp[sss, rr, cc, SJ2] -\\n                               Jmean * temp[sss, rr, cc, SJ] -\\n                               Jmean * temp[sss, rr, cc, SJ] +\\n                               cnt * Jmean * Jmean)\\n                        factors[ss, rr, cc, 0] = static[ss, rr, cc] - Imean\\n                        factors[ss, rr, cc, 1] = moving[ss, rr, cc] - Jmean\\n                        factors[ss, rr, cc, 2] = IJprods\\n                        factors[ss, rr, cc, 3] = Isq\\n                        factors[ss, rr, cc, 4] = Jsq\\n    return factors', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\crosscorr.pyx.txt'}),\n",
       " Document(page_content='@cython.boundscheck(False)\\n@cython.wraparound(False)\\n@cython.cdivision(True)\\ndef precompute_cc_factors_3d_test(floating[:, :, :] static,\\n                                  floating[:, :, :] moving, int radius):\\n    r\"\"\"Precomputations to quickly compute the gradient of the CC Metric\\n\\n    This version of precompute_cc_factors_3d is for testing purposes, it\\n    directly computes the local cross-correlation factors without any\\n    optimization, so it is less error-prone than the accelerated version.\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp ns = static.shape[0]\\n        cnp.npy_intp nr = static.shape[1]\\n        cnp.npy_intp nc = static.shape[2]\\n        cnp.npy_intp s, r, c, k, i, j, t\\n        cnp.npy_intp firstc, lastc, firstr, lastr, firsts, lasts\\n        double Imean, Jmean\\n        floating[:, :, :, :] factors = np.zeros((ns, nr, nc, 5),\\n                                                dtype=np.asarray(static).dtype)\\n        double[:] sums = np.zeros((6,), dtype=np.float64)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\crosscorr.pyx.txt'}),\n",
       " Document(page_content='with nogil:\\n        for s in range(ns):\\n            firsts = _int_max(0, s - radius)\\n            lasts = _int_min(ns - 1, s + radius)\\n            for r in range(nr):\\n                firstr = _int_max(0, r - radius)\\n                lastr = _int_min(nr - 1, r + radius)\\n                for c in range(nc):\\n                    firstc = _int_max(0, c - radius)\\n                    lastc = _int_min(nc - 1, c + radius)\\n                    for t in range(6):\\n                        sums[t] = 0\\n                    for k in range(firsts, 1 + lasts):\\n                        for i in range(firstr, 1 + lastr):\\n                            for j in range(firstc, 1 + lastc):\\n                                sums[SI] += static[k, i, j]\\n                                sums[SI2] += static[k, i, j]**2\\n                                sums[SJ] += moving[k, i, j]\\n                                sums[SJ2] += moving[k, i, j]**2\\n                                sums[SIJ] += static[k, i, j]*moving[k, i, j]\\n                                sums[CNT] += 1\\n                    Imean = sums[SI] / sums[CNT]\\n                    Jmean = sums[SJ] / sums[CNT]\\n                    factors[s, r, c, 0] = static[s, r, c] - Imean\\n                    factors[s, r, c, 1] = moving[s, r, c] - Jmean\\n                    factors[s, r, c, 2] = (sums[SIJ] - Jmean * sums[SI] -\\n                                           Imean * sums[SJ] +\\n                                           sums[CNT] * Jmean * Imean)\\n                    factors[s, r, c, 3] = (sums[SI2] - Imean * sums[SI] -\\n                                           Imean * sums[SI] +\\n                                           sums[CNT] * Imean * Imean)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\crosscorr.pyx.txt'}),\n",
       " Document(page_content='factors[s, r, c, 1] = moving[s, r, c] - Jmean\\n                    factors[s, r, c, 2] = (sums[SIJ] - Jmean * sums[SI] -\\n                                           Imean * sums[SJ] +\\n                                           sums[CNT] * Jmean * Imean)\\n                    factors[s, r, c, 3] = (sums[SI2] - Imean * sums[SI] -\\n                                           Imean * sums[SI] +\\n                                           sums[CNT] * Imean * Imean)\\n                    factors[s, r, c, 4] = (sums[SJ2] - Jmean * sums[SJ] -\\n                                           Jmean * sums[SJ] +\\n                                           sums[CNT] * Jmean * Jmean)\\n    return np.asarray(factors)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\crosscorr.pyx.txt'}),\n",
       " Document(page_content='@cython.boundscheck(False)\\n@cython.wraparound(False)\\n@cython.cdivision(True)\\ndef compute_cc_forward_step_3d(floating[:, :, :, :] grad_static,\\n                               floating[:, :, :, :] factors,\\n                               cnp.npy_intp radius):\\n    r\"\"\"Gradient of the CC Metric w.r.t. the forward transformation\\n\\n    Computes the gradient of the Cross Correlation metric for symmetric\\n    registration (SyN) [Avants2008]_ w.r.t. the displacement associated to\\n    the moving volume (\\'forward\\' step) as in [Avants2011]_\\n\\n    Parameters\\n    ----------\\n    grad_static : array, shape (S, R, C, 3)\\n        the gradient of the static volume\\n    factors : array, shape (S, R, C, 5)\\n        the precomputed cross correlation terms obtained via\\n        precompute_cc_factors_3d\\n    radius : int\\n        the radius of the neighborhood used for the CC metric when\\n        computing the factors. The returned vector field will be\\n        zero along a boundary of width radius voxels.\\n\\n    Returns\\n    -------\\n    out : array, shape (S, R, C, 3)\\n        the gradient of the cross correlation metric with respect to the\\n        displacement associated to the moving volume\\n    energy : the cross correlation energy (data term) at this iteration', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\crosscorr.pyx.txt'}),\n",
       " Document(page_content='References\\n    ----------\\n    .. [Avants2008]_ Avants, B. B., Epstein, C. L., Grossman, M., & Gee, J. C.\\n        (2008). Symmetric Diffeomorphic Image Registration with\\n        Cross-Correlation: Evaluating Automated Labeling of Elderly and\\n        Neurodegenerative Brain, Med Image Anal. 12(1), 26-41.\\n    .. [Avants2011]_ Avants, B. B., Tustison, N., & Song, G. (2011). Advanced\\n        Normalization Tools (ANTS), 1-35.\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp ns = grad_static.shape[0]\\n        cnp.npy_intp nr = grad_static.shape[1]\\n        cnp.npy_intp nc = grad_static.shape[2]\\n        double energy = 0\\n        cnp.npy_intp s, r, c\\n        double Ii, Ji, sfm, sff, smm, localCorrelation, temp\\n        floating[:, :, :, :] out =\\\\\\n            np.zeros((ns, nr, nc, 3), dtype=np.asarray(grad_static).dtype)\\n    with nogil:\\n        for s in range(radius, ns-radius):\\n            for r in range(radius, nr-radius):\\n                for c in range(radius, nc-radius):\\n                    Ii = factors[s, r, c, 0]\\n                    Ji = factors[s, r, c, 1]\\n                    sfm = factors[s, r, c, 2]\\n                    sff = factors[s, r, c, 3]\\n                    smm = factors[s, r, c, 4]\\n                    if sff == 0.0 or smm == 0.0:\\n                        continue\\n                    localCorrelation = 0\\n                    if sff * smm > 1e-5:\\n                        localCorrelation = sfm * sfm / (sff * smm)\\n                    if localCorrelation < 1:  # avoid bad values...\\n                        energy -= localCorrelation\\n                    temp = 2.0 * sfm / (sff * smm) * (Ji - sfm / sff * Ii)\\n                    out[s, r, c, 0] -= temp * grad_static[s, r, c, 0]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\crosscorr.pyx.txt'}),\n",
       " Document(page_content='if sff == 0.0 or smm == 0.0:\\n                        continue\\n                    localCorrelation = 0\\n                    if sff * smm > 1e-5:\\n                        localCorrelation = sfm * sfm / (sff * smm)\\n                    if localCorrelation < 1:  # avoid bad values...\\n                        energy -= localCorrelation\\n                    temp = 2.0 * sfm / (sff * smm) * (Ji - sfm / sff * Ii)\\n                    out[s, r, c, 0] -= temp * grad_static[s, r, c, 0]\\n                    out[s, r, c, 1] -= temp * grad_static[s, r, c, 1]\\n                    out[s, r, c, 2] -= temp * grad_static[s, r, c, 2]\\n    return np.asarray(out), energy', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\crosscorr.pyx.txt'}),\n",
       " Document(page_content='@cython.boundscheck(False)\\n@cython.wraparound(False)\\n@cython.cdivision(True)\\ndef compute_cc_backward_step_3d(floating[:, :, :, :] grad_moving,\\n                                floating[:, :, :, :] factors,\\n                                cnp.npy_intp radius):\\n    r\"\"\"Gradient of the CC Metric w.r.t. the backward transformation\\n\\n    Computes the gradient of the Cross Correlation metric for symmetric\\n    registration (SyN) [Avants08]_ w.r.t. the displacement associated to\\n    the static volume (\\'backward\\' step) as in [Avants11]_\\n\\n    Parameters\\n    ----------\\n    grad_moving : array, shape (S, R, C, 3)\\n        the gradient of the moving volume\\n    factors : array, shape (S, R, C, 5)\\n        the precomputed cross correlation terms obtained via\\n        precompute_cc_factors_3d\\n    radius : int\\n        the radius of the neighborhood used for the CC metric when\\n        computing the factors. The returned vector field will be\\n        zero along a boundary of width radius voxels.\\n\\n    Returns\\n    -------\\n    out : array, shape (S, R, C, 3)\\n        the gradient of the cross correlation metric with respect to the\\n        displacement associated to the static volume\\n    energy : the cross correlation energy (data term) at this iteration', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\crosscorr.pyx.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    out : array, shape (S, R, C, 3)\\n        the gradient of the cross correlation metric with respect to the\\n        displacement associated to the static volume\\n    energy : the cross correlation energy (data term) at this iteration\\n\\n    References\\n    ----------\\n    [Avants08]_ Avants, B. B., Epstein, C. L., Grossman, M., & Gee, J. C. (2008)\\n               Symmetric Diffeomorphic Image Registration with\\n               Cross-Correlation: Evaluating Automated Labeling of Elderly and\\n               Neurodegenerative Brain, Med Image Anal. 12(1), 26-41.\\n    [Avants11]_ Avants, B. B., Tustison, N., & Song, G. (2011).\\n               Advanced Normalization Tools (ANTS), 1-35.\\n    \"\"\"\\n    ftype = np.asarray(grad_moving).dtype\\n    cdef:\\n        cnp.npy_intp ns = grad_moving.shape[0]\\n        cnp.npy_intp nr = grad_moving.shape[1]\\n        cnp.npy_intp nc = grad_moving.shape[2]\\n        cnp.npy_intp s, r, c\\n        double energy = 0\\n        double Ii, Ji, sfm, sff, smm, localCorrelation, temp\\n        floating[:, :, :, :] out = np.zeros((ns, nr, nc, 3), dtype=ftype)\\n\\n    with nogil:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\crosscorr.pyx.txt'}),\n",
       " Document(page_content='with nogil:\\n\\n        for s in range(radius, ns-radius):\\n            for r in range(radius, nr-radius):\\n                for c in range(radius, nc-radius):\\n                    Ii = factors[s, r, c, 0]\\n                    Ji = factors[s, r, c, 1]\\n                    sfm = factors[s, r, c, 2]\\n                    sff = factors[s, r, c, 3]\\n                    smm = factors[s, r, c, 4]\\n                    if sff == 0.0 or smm == 0.0:\\n                        continue\\n                    localCorrelation = 0\\n                    if sff * smm > 1e-5:\\n                        localCorrelation = sfm * sfm / (sff * smm)\\n                    if localCorrelation < 1:  # avoid bad values...\\n                        energy -= localCorrelation\\n                    temp = 2.0 * sfm / (sff * smm) * (Ii - sfm / smm * Ji)\\n                    out[s, r, c, 0] -= temp * grad_moving[s, r, c, 0]\\n                    out[s, r, c, 1] -= temp * grad_moving[s, r, c, 1]\\n                    out[s, r, c, 2] -= temp * grad_moving[s, r, c, 2]\\n    return np.asarray(out), energy\\n\\n\\n@cython.boundscheck(False)\\n@cython.wraparound(False)\\n@cython.cdivision(True)\\ndef precompute_cc_factors_2d(floating[:, :] static, floating[:, :] moving,\\n                             cnp.npy_intp radius):\\n    r\"\"\"Precomputations to quickly compute the gradient of the CC Metric\\n\\n    Pre-computes the separate terms of the cross correlation metric\\n    [Avants2008]_ and image norms at each voxel considering a neighborhood of\\n    the given radius to efficiently [Avants2011]_ compute the gradient of the\\n    metric with respect to the deformation field.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\crosscorr.pyx.txt'}),\n",
       " Document(page_content='Pre-computes the separate terms of the cross correlation metric\\n    [Avants2008]_ and image norms at each voxel considering a neighborhood of\\n    the given radius to efficiently [Avants2011]_ compute the gradient of the\\n    metric with respect to the deformation field.\\n\\n    Parameters\\n    ----------\\n    static : array, shape (R, C)\\n        the static volume, which also defines the reference registration domain\\n    moving : array, shape (R, C)\\n        the moving volume (notice that both images must already be in a common\\n        reference domain, i.e. the same R, C)\\n    radius : the radius of the neighborhood(square of (2*radius + 1)^2 voxels)\\n\\n    Returns\\n    -------\\n    factors : array, shape (R, C, 5)\\n        the precomputed cross correlation terms:\\n        factors[:,:,0] : static minus its mean value along the neighborhood\\n        factors[:,:,1] : moving minus its mean value along the neighborhood\\n        factors[:,:,2] : sum of the pointwise products of static and moving\\n                           along the neighborhood\\n        factors[:,:,3] : sum of sq. values of static along the neighborhood\\n        factors[:,:,4] : sum of sq. values of moving along the neighborhood', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\crosscorr.pyx.txt'}),\n",
       " Document(page_content='References\\n    ----------\\n    .. [Avants2008]_ Avants, B. B., Epstein, C. L., Grossman, M., & Gee, J. C.\\n        (2008). Symmetric Diffeomorphic Image Registration with\\n        Cross-Correlation: Evaluating Automated Labeling of Elderly and\\n        Neurodegenerative Brain, Med Image Anal. 12(1), 26-41.\\n    .. [Avants2011]_ Avants, B. B., Tustison, N., & Song, G. (2011). Advanced\\n        Normalization Tools (ANTS), 1-35.\\n    \"\"\"\\n    ftype = np.asarray(static).dtype\\n    cdef:\\n        cnp.npy_intp side = 2 * radius + 1\\n        cnp.npy_intp nr = static.shape[0]\\n        cnp.npy_intp nc = static.shape[1]\\n        cnp.npy_intp r, c, i, j, t, q, qq, firstc, lastc\\n        double Imean, Jmean\\n        floating[:, :, :] factors = np.zeros((nr, nc, 5), dtype=ftype)\\n        double[:, :] lines = np.zeros((6, side), dtype=np.float64)\\n        double[:] sums = np.zeros((6,), dtype=np.float64)\\n\\n    with nogil:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\crosscorr.pyx.txt'}),\n",
       " Document(page_content='with nogil:\\n\\n        for c in range(nc):\\n            firstc = _int_max(0, c - radius)\\n            lastc = _int_min(nc - 1, c + radius)\\n            # compute factors for row [:,c]\\n            for t in range(6):\\n                for q in range(side):\\n                    lines[t, q] = 0\\n            # Compute all rows and set the sums on the fly\\n            # compute row [i, j = {c-radius, c + radius}]\\n            for i in range(nr):\\n                q = i % side\\n                for t in range(6):\\n                    lines[t, q] = 0\\n                for j in range(firstc, lastc + 1):\\n                    lines[SI, q] += static[i, j]\\n                    lines[SI2, q] += static[i, j] * static[i, j]\\n                    lines[SJ, q] += moving[i, j]\\n                    lines[SJ2, q] += moving[i, j] * moving[i, j]\\n                    lines[SIJ, q] += static[i, j] * moving[i, j]\\n                    lines[CNT, q] += 1', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\crosscorr.pyx.txt'}),\n",
       " Document(page_content='for t in range(6):\\n                    sums[t] = 0\\n                    for qq in range(side):\\n                        sums[t] += lines[t, qq]\\n                if i >= radius:\\n                    # r is the pixel that is affected by the cube with slices\\n                    # [r - radius.. r + radius, :]\\n                    r = i - radius\\n                    Imean = sums[SI] / sums[CNT]\\n                    Jmean = sums[SJ] / sums[CNT]\\n                    factors[r, c, 0] = static[r, c] - Imean\\n                    factors[r, c, 1] = moving[r, c] - Jmean\\n                    factors[r, c, 2] = (sums[SIJ] - Jmean * sums[SI] -\\n                                        Imean * sums[SJ] +\\n                                        sums[CNT] * Jmean * Imean)\\n                    factors[r, c, 3] = (sums[SI2] - Imean * sums[SI] -\\n                                        Imean * sums[SI] +\\n                                        sums[CNT] * Imean * Imean)\\n                    factors[r, c, 4] = (sums[SJ2] - Jmean * sums[SJ] -\\n                                        Jmean * sums[SJ] +\\n                                        sums[CNT] * Jmean * Jmean)\\n            # Finally set the values at the end of the line\\n            for r in range(nr - radius, nr):\\n                # this would be the last slice to be processed for pixel\\n                # [r, c], if it existed\\n                i = r + radius\\n                q = i % side\\n                for t in range(6):\\n                    sums[t] -= lines[t, q]\\n                Imean = sums[SI] / sums[CNT]\\n                Jmean = sums[SJ] / sums[CNT]\\n                factors[r, c, 0] = static[r, c] - Imean', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\crosscorr.pyx.txt'}),\n",
       " Document(page_content='for r in range(nr - radius, nr):\\n                # this would be the last slice to be processed for pixel\\n                # [r, c], if it existed\\n                i = r + radius\\n                q = i % side\\n                for t in range(6):\\n                    sums[t] -= lines[t, q]\\n                Imean = sums[SI] / sums[CNT]\\n                Jmean = sums[SJ] / sums[CNT]\\n                factors[r, c, 0] = static[r, c] - Imean\\n                factors[r, c, 1] = moving[r, c] - Jmean\\n                factors[r, c, 2] = (sums[SIJ] - Jmean * sums[SI] -\\n                                    Imean * sums[SJ] +\\n                                    sums[CNT] * Jmean * Imean)\\n                factors[r, c, 3] = (sums[SI2] - Imean * sums[SI] -\\n                                    Imean * sums[SI] +\\n                                    sums[CNT] * Imean * Imean)\\n                factors[r, c, 4] = (sums[SJ2] - Jmean * sums[SJ] -\\n                                    Jmean * sums[SJ] +\\n                                    sums[CNT] * Jmean * Jmean)\\n    return np.asarray(factors)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\crosscorr.pyx.txt'}),\n",
       " Document(page_content='@cython.boundscheck(False)\\n@cython.wraparound(False)\\n@cython.cdivision(True)\\ndef precompute_cc_factors_2d_test(floating[:, :] static, floating[:, :] moving,\\n                                  cnp.npy_intp radius):\\n    r\"\"\"Precomputations to quickly compute the gradient of the CC Metric\\n\\n    This version of precompute_cc_factors_2d is for testing purposes, it\\n    directly computes the local cross-correlation without any optimization.\\n    \"\"\"\\n    ftype = np.asarray(static).dtype\\n    cdef:\\n        cnp.npy_intp nr = static.shape[0]\\n        cnp.npy_intp nc = static.shape[1]\\n        cnp.npy_intp r, c, i, j, t, firstr, lastr, firstc, lastc\\n        double Imean, Jmean\\n        floating[:, :, :] factors = np.zeros((nr, nc, 5), dtype=ftype)\\n        double[:] sums = np.zeros((6,), dtype=np.float64)\\n\\n    with nogil:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\crosscorr.pyx.txt'}),\n",
       " Document(page_content='with nogil:\\n\\n        for r in range(nr):\\n            firstr = _int_max(0, r - radius)\\n            lastr = _int_min(nr - 1, r + radius)\\n            for c in range(nc):\\n                firstc = _int_max(0, c - radius)\\n                lastc = _int_min(nc - 1, c + radius)\\n                for t in range(6):\\n                    sums[t] = 0\\n                for i in range(firstr, 1 + lastr):\\n                    for j in range(firstc, 1+lastc):\\n                        sums[SI] += static[i, j]\\n                        sums[SI2] += static[i, j]**2\\n                        sums[SJ] += moving[i, j]\\n                        sums[SJ2] += moving[i, j]**2\\n                        sums[SIJ] += static[i, j]*moving[i, j]\\n                        sums[CNT] += 1\\n                Imean = sums[SI] / sums[CNT]\\n                Jmean = sums[SJ] / sums[CNT]\\n                factors[r, c, 0] = static[r, c] - Imean\\n                factors[r, c, 1] = moving[r, c] - Jmean\\n                factors[r, c, 2] = (sums[SIJ] - Jmean * sums[SI] -\\n                                    Imean * sums[SJ] +\\n                                    sums[CNT] * Jmean * Imean)\\n                factors[r, c, 3] = (sums[SI2] - Imean * sums[SI] -\\n                                    Imean * sums[SI] +\\n                                    sums[CNT] * Imean * Imean)\\n                factors[r, c, 4] = (sums[SJ2] - Jmean * sums[SJ] -\\n                                    Jmean * sums[SJ] +\\n                                    sums[CNT] * Jmean * Jmean)\\n    return np.asarray(factors)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\crosscorr.pyx.txt'}),\n",
       " Document(page_content='@cython.boundscheck(False)\\n@cython.wraparound(False)\\n@cython.cdivision(True)\\ndef compute_cc_forward_step_2d(floating[:, :, :] grad_static,\\n                               floating[:, :, :] factors,\\n                               cnp.npy_intp radius):\\n    r\"\"\"Gradient of the CC Metric w.r.t. the forward transformation\\n\\n    Computes the gradient of the Cross Correlation metric for symmetric\\n    registration (SyN) [Avants2008]_ w.r.t. the displacement associated to\\n    the moving image (\\'backward\\' step) as in [Avants2011]_\\n\\n    Parameters\\n    ----------\\n    grad_static : array, shape (R, C, 2)\\n        the gradient of the static image\\n    factors : array, shape (R, C, 5)\\n        the precomputed cross correlation terms obtained via\\n        precompute_cc_factors_2d\\n\\n    Returns\\n    -------\\n    out : array, shape (R, C, 2)\\n        the gradient of the cross correlation metric with respect to the\\n        displacement associated to the moving image\\n    energy : the cross correlation energy (data term) at this iteration\\n\\n    Notes\\n    -----\\n    Currently, the gradient of the static image is not being used, but some\\n    authors suggest that symmetrizing the gradient by including both, the\\n    moving and static gradients may improve the registration quality. We are\\n    leaving this parameter as a placeholder for future investigation', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\crosscorr.pyx.txt'}),\n",
       " Document(page_content='Notes\\n    -----\\n    Currently, the gradient of the static image is not being used, but some\\n    authors suggest that symmetrizing the gradient by including both, the\\n    moving and static gradients may improve the registration quality. We are\\n    leaving this parameter as a placeholder for future investigation\\n\\n    References\\n    ----------\\n    .. [Avants2008]_ Avants, B. B., Epstein, C. L., Grossman, M., & Gee, J. C.\\n        (2008). Symmetric Diffeomorphic Image Registration with\\n        Cross-Correlation: Evaluating Automated Labeling of Elderly and\\n        Neurodegenerative Brain, Med Image Anal. 12(1), 26-41.\\n    .. [Avants2011]_ Avants, B. B., Tustison, N., & Song, G. (2011). Advanced\\n        Normalization Tools (ANTS), 1-35.\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp nr = grad_static.shape[0]\\n        cnp.npy_intp nc = grad_static.shape[1]\\n        double energy = 0\\n        cnp.npy_intp r, c\\n        double Ii, Ji, sfm, sff, smm, localCorrelation, temp\\n        floating[:, :, :] out = np.zeros((nr, nc, 2),\\n                                         dtype=np.asarray(grad_static).dtype)\\n    with nogil:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\crosscorr.pyx.txt'}),\n",
       " Document(page_content='for r in range(radius, nr-radius):\\n            for c in range(radius, nc-radius):\\n                Ii = factors[r, c, 0]\\n                Ji = factors[r, c, 1]\\n                sfm = factors[r, c, 2]\\n                sff = factors[r, c, 3]\\n                smm = factors[r, c, 4]\\n                if sff == 0.0 or smm == 0.0:\\n                    continue\\n                localCorrelation = 0\\n                if sff * smm > 1e-5:\\n                    localCorrelation = sfm * sfm / (sff * smm)\\n                if localCorrelation < 1:  # avoid bad values...\\n                    energy -= localCorrelation\\n                temp = 2.0 * sfm / (sff * smm) * (Ji - sfm / sff * Ii)\\n                out[r, c, 0] -= temp * grad_static[r, c, 0]\\n                out[r, c, 1] -= temp * grad_static[r, c, 1]\\n    return np.asarray(out), energy\\n\\n\\n@cython.boundscheck(False)\\n@cython.wraparound(False)\\n@cython.cdivision(True)\\ndef compute_cc_backward_step_2d(floating[:, :, :] grad_moving,\\n                                floating[:, :, :] factors,\\n                                cnp.npy_intp radius):\\n    r\"\"\"Gradient of the CC Metric w.r.t. the backward transformation\\n\\n    Computes the gradient of the Cross Correlation metric for symmetric\\n    registration (SyN) [Avants2008]_ w.r.t. the displacement associated to\\n    the static image (\\'forward\\' step) as in [Avants2011]_\\n\\n    Parameters\\n    ----------\\n    grad_moving : array, shape (R, C, 2)\\n        the gradient of the moving image\\n    factors : array, shape (R, C, 5)\\n        the precomputed cross correlation terms obtained via\\n        precompute_cc_factors_2d', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\crosscorr.pyx.txt'}),\n",
       " Document(page_content='Computes the gradient of the Cross Correlation metric for symmetric\\n    registration (SyN) [Avants2008]_ w.r.t. the displacement associated to\\n    the static image (\\'forward\\' step) as in [Avants2011]_\\n\\n    Parameters\\n    ----------\\n    grad_moving : array, shape (R, C, 2)\\n        the gradient of the moving image\\n    factors : array, shape (R, C, 5)\\n        the precomputed cross correlation terms obtained via\\n        precompute_cc_factors_2d\\n\\n    Returns\\n    -------\\n    out : array, shape (R, C, 2)\\n        the gradient of the cross correlation metric with respect to the\\n        displacement associated to the static image\\n    energy : the cross correlation energy (data term) at this iteration\\n\\n    References\\n    ----------\\n    .. [Avants2008]_ Avants, B. B., Epstein, C. L., Grossman, M., & Gee, J. C.\\n        (2008). Symmetric Diffeomorphic Image Registration with\\n        Cross-Correlation: Evaluating Automated Labeling of Elderly and\\n        Neurodegenerative Brain, Med Image Anal. 12(1), 26-41.\\n    .. [Avants2011]_ Avants, B. B., Tustison, N., & Song, G. (2011). Advanced\\n        Normalization Tools (ANTS), 1-35.\\n    \"\"\"\\n    ftype = np.asarray(grad_moving).dtype\\n    cdef:\\n        cnp.npy_intp nr = grad_moving.shape[0]\\n        cnp.npy_intp nc = grad_moving.shape[1]\\n        cnp.npy_intp r, c\\n        double energy = 0\\n        double Ii, Ji, sfm, sff, smm, localCorrelation, temp\\n        floating[:, :, :] out = np.zeros((nr, nc, 2), dtype=ftype)\\n\\n    with nogil:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\crosscorr.pyx.txt'}),\n",
       " Document(page_content='with nogil:\\n\\n        for r in range(radius, nr-radius):\\n            for c in range(radius, nc-radius):\\n                Ii = factors[r, c, 0]\\n                Ji = factors[r, c, 1]\\n                sfm = factors[r, c, 2]\\n                sff = factors[r, c, 3]\\n                smm = factors[r, c, 4]\\n                if sff == 0.0 or smm == 0.0:\\n                    continue\\n                localCorrelation = 0\\n                if sff * smm > 1e-5:\\n                    localCorrelation = sfm * sfm / (sff * smm)\\n                if localCorrelation < 1:  # avoid bad values...\\n                    energy -= localCorrelation\\n                temp = 2.0 * sfm / (sff * smm) * (Ii - sfm / smm * Ji)\\n                out[r, c, 0] -= temp * grad_moving[r, c, 0]\\n                out[r, c, 1] -= temp * grad_moving[r, c, 1]\\n    return np.asarray(out), energy', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\crosscorr.pyx.txt'}),\n",
       " Document(page_content='#!python\\n#cython: boundscheck=False\\n#cython: wraparound=False\\n#cython: cdivision=True\\n\\n\\nimport numpy as np\\ncimport cython\\ncimport numpy as cnp\\nfrom dipy.align.fused_types cimport floating\\ncdef extern from \"dpy_math.h\" nogil:\\n    int dpy_isinf(double)\\n    double floor(double)\\n\\ncdef inline int ifloor(double x) nogil:\\n    return int(floor(x))\\n\\ndef quantize_positive_2d(floating[:, :] v, int num_levels):\\n    r\"\"\"Quantizes a 2D image to num_levels quantization levels\\n\\n    Quantizes the input image at num_levels intensity levels considering <=0\\n    as a special value. Those input pixels <=0, and only those, will be\\n    assigned a quantization level of 0. The positive values are divided into\\n    the remaining num_levels-1 uniform quantization levels.\\n\\n    The following are undefined, and raise a ValueError:\\n    * Quantizing at zero levels because at least one level must be assigned\\n    * Quantizing at one level because positive values should be assigned a\\n      level different from the secial level 0 (at least 2 levels are needed)\\n\\n    Parameters\\n    ----------\\n    v : array, shape (R, C)\\n        the image to be quantized\\n    num_levels : int\\n        the number of levels', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\expectmax.pyx.txt'}),\n",
       " Document(page_content='The following are undefined, and raise a ValueError:\\n    * Quantizing at zero levels because at least one level must be assigned\\n    * Quantizing at one level because positive values should be assigned a\\n      level different from the secial level 0 (at least 2 levels are needed)\\n\\n    Parameters\\n    ----------\\n    v : array, shape (R, C)\\n        the image to be quantized\\n    num_levels : int\\n        the number of levels\\n\\n    Returns\\n    -------\\n    out : array, shape (R, C), same shape as v\\n        the quantized image\\n    levels: array, shape (num_levels,)\\n        the quantization values: levels[0]=0, and levels[i] is the mid-point\\n        of the interval of intensities that are assigned to quantization\\n        level i, i=1, ..., num_levels-1.\\n    hist: array, shape (num_levels,)\\n        histogram: the number of pixels that were assigned to each quantization\\n        level\\n    \"\"\"\\n    ftype = np.asarray(v).dtype\\n    cdef:\\n        cnp.npy_intp nrows = v.shape[0]\\n        cnp.npy_intp ncols = v.shape[1]\\n        cnp.npy_intp npix = nrows * ncols\\n        cnp.npy_intp i, j, l\\n        double epsilon, delta\\n        double min_val = -1\\n        double max_val = -1\\n        cnp.npy_int32[:] hist = np.zeros(shape=(num_levels,), dtype=np.int32)\\n        cnp.npy_int32[:, :] out = np.zeros(shape=(nrows, ncols,), dtype=np.int32)\\n        floating[:] levels = np.zeros(shape=(num_levels,), dtype=ftype)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\expectmax.pyx.txt'}),\n",
       " Document(page_content=\"#Quantizing at zero levels is undefined\\n    #Quantizing at one level is not supported because we want to make sure the\\n    #maximum level in the quantization is never greater than num_levels-1\\n    if num_levels < 2:\\n        raise ValueError('Quantization levels must be at least 2')\\n    if num_levels >= 2**31:\\n        raise ValueError('Quantization levels must be < 2**31')\\n\\n    num_levels -= 1  # zero is one of the levels\\n\\n    with nogil:\\n\\n        for i in range(nrows):\\n            for j in range(ncols):\\n                if v[i, j] > 0:\\n                    if min_val < 0 or v[i, j] < min_val:\\n                        min_val = v[i, j]\\n                    if v[i, j] > max_val:\\n                        max_val = v[i, j]\\n        epsilon = 1e-8\\n        delta = (max_val - min_val + epsilon) / num_levels\\n        # notice that we decreased num_levels, so levels[0..num_levels] are well\\n        # defined\\n        if num_levels < 2 or delta < epsilon:\\n            for i in range(nrows):\\n                for j in range(ncols):\\n                    if v[i, j] > 0:\\n                        out[i, j] = 1\\n                    else:\\n                        out[i, j] = 0\\n                        hist[0] += 1\\n            levels[0] = 0\\n            levels[1] = 0.5 * (min_val + max_val)\\n            hist[1] = npix - hist[0]\\n            with gil:\\n                return out, levels, hist\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\expectmax.pyx.txt'}),\n",
       " Document(page_content='levels[0] = 0\\n        levels[1] = min_val + delta * 0.5\\n        for i in range(2, 1 + num_levels):\\n            levels[i] = levels[i - 1] + delta\\n        for i in range(nrows):\\n            for j in range(ncols):\\n                if v[i, j] > 0:\\n                    l = ifloor((v[i, j] - min_val) / delta)\\n                    out[i, j] = l + 1\\n                    hist[l + 1] += 1\\n                else:\\n                    out[i, j] = 0\\n                    hist[0] += 1\\n\\n    return np.asarray(out), np.array(levels), np.array(hist)\\n\\n\\ndef quantize_positive_3d(floating[:, :, :] v, int num_levels):\\n    r\"\"\"Quantizes a 3D volume to num_levels quantization levels\\n\\n    Quantizes the input volume at num_levels intensity levels considering <=0\\n    as a special value. Those input voxels <=0, and only those, will be\\n    assigned a quantization level of 0. The positive values are divided into\\n    the remaining num_levels-1 uniform quantization levels.\\n\\n    The following are undefined, and raise a ValueError:\\n    * Quantizing at zero levels because at least one level must be assigned\\n    * Quantizing at one level because positive values should be assigned a\\n      level different from the secial level 0 (at least 2 levels are needed)\\n\\n    Parameters\\n    ----------\\n    v : array, shape (S, R, C)\\n        the volume to be quantized\\n    num_levels : int\\n        the number of levels', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\expectmax.pyx.txt'}),\n",
       " Document(page_content='The following are undefined, and raise a ValueError:\\n    * Quantizing at zero levels because at least one level must be assigned\\n    * Quantizing at one level because positive values should be assigned a\\n      level different from the secial level 0 (at least 2 levels are needed)\\n\\n    Parameters\\n    ----------\\n    v : array, shape (S, R, C)\\n        the volume to be quantized\\n    num_levels : int\\n        the number of levels\\n\\n    Returns\\n    -------\\n    out : array, shape (S, R, C), same shape as v\\n        the quantized volume\\n    levels: array, shape (num_levels,)\\n        the quantization values: levels[0]=0, and levels[i] is the mid-point\\n        of the interval of intensities that are assigned to quantization\\n        level i, i=1, ..., num_levels-1.\\n    hist: array, shape (num_levels,)\\n        histogram: the number of voxels that were assigned to each quantization\\n        level\\n    \"\"\"\\n    ftype = np.asarray(v).dtype\\n    cdef:\\n        cnp.npy_intp nslices = v.shape[0]\\n        cnp.npy_intp nrows = v.shape[1]\\n        cnp.npy_intp ncols = v.shape[2]\\n        cnp.npy_intp nvox = nrows * ncols * nslices\\n        cnp.npy_intp i, j, k, l\\n        double epsilon, delta\\n        double min_val = -1\\n        double max_val = -1\\n        int[:] hist = np.zeros(shape=(num_levels,), dtype=np.int32)\\n        int[:, :, :] out = np.zeros(shape=(nslices, nrows, ncols),\\n                                    dtype=np.int32)\\n        floating[:] levels = np.zeros(shape=(num_levels,), dtype=ftype)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\expectmax.pyx.txt'}),\n",
       " Document(page_content=\"#Quantizing at zero levels is undefined\\n    #Quantizing at one level is not supported because we want to make sure the\\n    #maximum level in the quantization is never greater than num_levels-1\\n    if num_levels < 2:\\n        raise ValueError('Quantization levels must be at least 2')\\n\\n    num_levels -= 1  # zero is one of the levels\\n\\n    with nogil:\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\expectmax.pyx.txt'}),\n",
       " Document(page_content='for k in range(nslices):\\n            for i in range(nrows):\\n                for j in range(ncols):\\n                    if v[k, i, j] > 0:\\n                        if min_val < 0 or v[k, i, j] < min_val:\\n                            min_val = v[k, i, j]\\n                        if v[k, i, j] > max_val:\\n                            max_val = v[k, i, j]\\n        epsilon = 1e-8\\n        delta = (max_val - min_val + epsilon) / num_levels\\n        # notice that we decreased num_levels, so levels[0..num_levels] are well\\n        # defined\\n        if num_levels < 2 or delta < epsilon:\\n            for k in range(nslices):\\n                for i in range(nrows):\\n                    for j in range(ncols):\\n                        if v[k, i, j] > 0:\\n                            out[k, i, j] = 1\\n                        else:\\n                            out[k, i, j] = 0\\n                            hist[0] += 1\\n            levels[0] = 0\\n            levels[1] = 0.5 * (min_val + max_val)\\n            hist[1] = nvox - hist[0]\\n            with gil:\\n                return out, levels, hist\\n        levels[0] = 0\\n        levels[1] = min_val + delta * 0.5\\n        for i in range(2, 1 + num_levels):\\n            levels[i] = levels[i - 1] + delta\\n        for k in range(nslices):\\n            for i in range(nrows):\\n                for j in range(ncols):\\n                    if v[k, i, j] > 0:\\n                        l = ifloor((v[k, i, j] - min_val) / delta)\\n                        out[k, i, j] = l + 1\\n                        hist[l + 1] += 1\\n                    else:\\n                        out[k, i, j] = 0\\n                        hist[0] += 1\\n    return np.asarray(out), np.asarray(levels), np.asarray(hist)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\expectmax.pyx.txt'}),\n",
       " Document(page_content='def compute_masked_class_stats_2d(int[:, :] mask, floating[:, :] v,\\n                                     int num_labels, int[:, :] labels):\\n    r\"\"\"Computes the mean and std. for each quantization level.\\n\\n    Computes the mean and standard deviation of the intensities in \\'v\\' for\\n    each corresponding label in \\'labels\\'. In other words, for each label\\n    L, it computes the mean and standard deviation of the intensities in \\'v\\'\\n    at pixels whose label in \\'labels\\' is L. This is used by the EM metric\\n    to compute statistics for each hidden variable represented by the labels.\\n\\n    Parameters\\n    ----------\\n    mask : array, shape (R, C)\\n        the mask of pixels that will be taken into account for computing the\\n        statistics. All zero pixels in mask will be ignored\\n    v : array, shape (R, C)\\n        the image which the statistics will be computed from\\n    num_labels : int\\n        the number of different labels in \\'labels\\' (equal to the\\n        number of hidden variables in the EM metric)\\n    labels : array, shape (R, C)\\n        the label assigned to each pixel', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\expectmax.pyx.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    means : array, shape (num_labels,)\\n        means[i], 0<=i<num_labels will be the mean intensity in v of all\\n        voxels labeled i, or 0 if no voxels are labeled i\\n    variances : array, shape (num_labels,)\\n        variances[i], 0<=i<num_labels will be the standard deviation of the\\n        intensities in v of all voxels labeled i, or infinite if less than 2\\n        voxels are labeled i.\\n    \"\"\"\\n    ftype=np.asarray(v).dtype\\n    cdef:\\n        cnp.npy_intp nrows = v.shape[0]\\n        cnp.npy_intp ncols = v.shape[1]\\n        cnp.npy_intp i, j\\n        double INF64 = np.inf\\n        int[:] counts = np.zeros(shape=(num_labels,), dtype=np.int32)\\n        floating diff\\n        floating[:] means = np.zeros(shape=(num_labels,), dtype=ftype)\\n        floating[:] variances = np.zeros(shape=(num_labels, ), dtype=ftype)\\n\\n    with nogil:\\n        for i in range(nrows):\\n            for j in range(ncols):\\n                if mask[i, j] != 0:\\n                    means[labels[i, j]] += v[i, j]\\n                    counts[labels[i, j]] += 1\\n        for i in range(num_labels):\\n            if counts[i] > 0:\\n                means[i] /= counts[i]\\n        for i in range(nrows):\\n            for j in range(ncols):\\n                if mask[i, j] != 0:\\n                    diff = v[i, j] - means[labels[i, j]]\\n                    variances[labels[i, j]] += diff ** 2\\n\\n        for i in range(num_labels):\\n            if counts[i] > 1:\\n                variances[i] /= counts[i]\\n            else:\\n                variances[i] = INF64\\n    return np.asarray(means), np.asarray(variances)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\expectmax.pyx.txt'}),\n",
       " Document(page_content='for i in range(num_labels):\\n            if counts[i] > 1:\\n                variances[i] /= counts[i]\\n            else:\\n                variances[i] = INF64\\n    return np.asarray(means), np.asarray(variances)\\n\\n\\ndef compute_masked_class_stats_3d(int[:, :, :] mask, floating[:, :, :] v,\\n                                      int num_labels, int[:, :, :] labels):\\n    r\"\"\"Computes the mean and std. for each quantization level.\\n\\n    Computes the mean and standard deviation of the intensities in \\'v\\' for\\n    each corresponding label in \\'labels\\'. In other words, for each label\\n    L, it computes the mean and standard deviation of the intensities in \\'v\\'\\n    at voxels whose label in \\'labels\\' is L. This is used by the EM metric\\n    to compute statistics for each hidden variable represented by the labels.\\n\\n    Parameters\\n    ----------\\n    mask : array, shape (S, R, C)\\n        the mask of voxels that will be taken into account for computing the\\n        statistics. All zero voxels in mask will be ignored\\n    v : array, shape (S, R, C)\\n        the volume which the statistics will be computed from\\n    num_labels : int\\n        the number of different labels in \\'labels\\' (equal to the\\n        number of hidden variables in the EM metric)\\n    labels : array, shape (S, R, C)\\n        the label assigned to each pixel', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\expectmax.pyx.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    means : array, shape (num_labels,)\\n        means[i], 0<=i<num_labels will be the mean intensity in v of all\\n        voxels labeled i, or 0 if no voxels are labeled i\\n    variances : array, shape (num_labels,)\\n        variances[i], 0<=i<num_labels will be the standard deviation of the\\n        intensities in v of all voxels labeled i, or infinite if less than 2\\n        voxels are labeled i.\\n    \"\"\"\\n    ftype=np.asarray(v).dtype\\n    cdef:\\n        cnp.npy_intp nslices = v.shape[0]\\n        cnp.npy_intp nrows = v.shape[1]\\n        cnp.npy_intp ncols = v.shape[2]\\n        cnp.npy_intp i, j, k\\n        double INF64 = np.inf\\n        floating diff\\n        int[:] counts = np.zeros(shape=(num_labels,), dtype=np.int32)\\n        floating[:] means = np.zeros(shape=(num_labels,), dtype=ftype)\\n        floating[:] variances = np.zeros(shape=(num_labels, ), dtype=ftype)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\expectmax.pyx.txt'}),\n",
       " Document(page_content='with nogil:\\n        for k in range(nslices):\\n            for i in range(nrows):\\n                for j in range(ncols):\\n                    if mask[k, i, j] != 0:\\n                        means[labels[k, i, j]] += v[k, i, j]\\n                        counts[labels[k, i, j]] += 1\\n        for i in range(num_labels):\\n            if counts[i] > 0:\\n                means[i] /= counts[i]\\n        for k in range(nslices):\\n            for i in range(nrows):\\n                for j in range(ncols):\\n                    if mask[k, i, j] != 0:\\n                        diff = means[labels[k, i, j]] - v[k, i, j]\\n                        variances[labels[k, i, j]] += diff ** 2\\n        for i in range(num_labels):\\n            if counts[i] > 1:\\n                variances[i] /= counts[i]\\n            else:\\n                variances[i] = INF64\\n    return np.asarray(means), np.asarray(variances)\\n\\n@cython.boundscheck(False)\\n@cython.wraparound(False)\\n@cython.cdivision(True)\\ndef compute_em_demons_step_2d(floating[:,:] delta_field,\\n                              floating[:,:] sigma_sq_field,\\n                              floating[:,:,:] gradient_moving,\\n                              double sigma_sq_x,\\n                              floating[:,:,:] out):\\n    r\"\"\"Demons step for EM metric in 2D\\n\\n    Computes the demons step [Vercauteren09] for SSD-driven registration\\n    ( eq. 4 in [Vercauteren09] ) using the EM algorithm [Arce14] to handle\\n    multi-modality images.\\n\\n    In this case, $\\\\sigma_i$ in eq. 4 of [Vercauteren] is estimated using the EM\\n    algorithm, while in the original version of diffeomorphic demons it is\\n    estimated by the difference between the image values at each pixel.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\expectmax.pyx.txt'}),\n",
       " Document(page_content=\"Computes the demons step [Vercauteren09] for SSD-driven registration\\n    ( eq. 4 in [Vercauteren09] ) using the EM algorithm [Arce14] to handle\\n    multi-modality images.\\n\\n    In this case, $\\\\sigma_i$ in eq. 4 of [Vercauteren] is estimated using the EM\\n    algorithm, while in the original version of diffeomorphic demons it is\\n    estimated by the difference between the image values at each pixel.\\n\\n    Parameters\\n    ----------\\n    delta_field : array, shape (R, C)\\n        contains, at each pixel, the difference between the moving image (warped\\n        under the current deformation s(. , .) ) J and the static image I:\\n        delta_field[i,j] = J(s(i,j)) - I(i,j). The order is important, changing\\n        to delta_field[i,j] = I(i,j) - J(s(i,j)) yields the backward demons step\\n        warping the static image towards the moving, which may not be the\\n        intended behavior unless the 'gradient_moving' passed corresponds to\\n        the gradient of the static image\\n    sigma_sq_field : array, shape (R, C)\\n        contains, at each pixel (i, j), the estimated variance (not std) of the\\n        hidden variable associated to the intensity at static[i,j] (which must\\n        have been previously quantized)\\n    gradient_moving : array, shape (R, C, 2)\\n        the gradient of the moving image\\n    sigma_sq_x : float\\n        parameter controlling the amount of regularization. It corresponds to\\n        $\\\\sigma_x^2$ in algorithm 1 of Vercauteren et al.[2]\\n    out : array, shape (R, C, 2)\\n        the resulting demons step will be written to this array\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\expectmax.pyx.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    demons_step : array, shape (R, C, 2)\\n        the demons step to be applied for updating the current displacement\\n        field\\n    energy : float\\n        the current em energy (before applying the returned demons_step)\\n\\n    References\\n    ----------\\n    [Arce14] Arce-santana, E., Campos-delgado, D. U., & Vigueras-g, F. (2014).\\n             Non-rigid Multimodal Image Registration Based on the\\n             Expectation-Maximization Algorithm, (168140), 36-47.\\n\\n    [Vercauteren09] Vercauteren, T., Pennec, X., Perchant, A., & Ayache, N.\\n                    (2009). Diffeomorphic demons: efficient non-parametric\\n                    image registration. NeuroImage, 45(1 Suppl), S61-72.\\n                    doi:10.1016/j.neuroimage.2008.10.040\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp nr = delta_field.shape[0]\\n        cnp.npy_intp nc = delta_field.shape[1]\\n        cnp.npy_intp i, j\\n        double delta, sigma_sq_i, nrm2, energy, den, prod\\n\\n    if out is None:\\n        out = np.zeros((nr, nc, 2), dtype=np.asarray(delta_field).dtype)\\n\\n    with nogil:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\expectmax.pyx.txt'}),\n",
       " Document(page_content='if out is None:\\n        out = np.zeros((nr, nc, 2), dtype=np.asarray(delta_field).dtype)\\n\\n    with nogil:\\n\\n        energy = 0\\n        for i in range(nr):\\n            for j in range(nc):\\n                sigma_sq_i = sigma_sq_field[i,j]\\n                delta = delta_field[i,j]\\n                energy += (delta**2)\\n                if dpy_isinf(sigma_sq_i) != 0:\\n                    out[i, j, 0], out[i, j, 1] = 0, 0\\n                else:\\n                    nrm2 = (gradient_moving[i, j, 0]**2 +\\n                            gradient_moving[i, j, 1]**2)\\n                    if sigma_sq_i == 0:\\n                        if nrm2 == 0:\\n                            out[i, j, 0], out[i, j, 1] = 0, 0\\n                        else:\\n                            out[i, j, 0] = (delta *\\n                                            gradient_moving[i, j, 0] / nrm2)\\n                            out[i, j, 1] = (delta *\\n                                            gradient_moving[i, j, 1] / nrm2)\\n                    else:\\n                        den = (sigma_sq_x * nrm2 + sigma_sq_i)\\n                        prod = sigma_sq_x * delta\\n                        out[i, j, 0] = prod * gradient_moving[i, j, 0] / den\\n                        out[i, j, 1] = prod * gradient_moving[i, j, 1] / den\\n    return np.asarray(out), energy', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\expectmax.pyx.txt'}),\n",
       " Document(page_content='@cython.boundscheck(False)\\n@cython.wraparound(False)\\n@cython.cdivision(True)\\ndef compute_em_demons_step_3d(floating[:,:,:] delta_field,\\n                              floating[:,:,:] sigma_sq_field,\\n                              floating[:,:,:,:] gradient_moving,\\n                              double sigma_sq_x,\\n                              floating[:,:,:,:] out):\\n    r\"\"\"Demons step for EM metric in 3D\\n\\n    Computes the demons step [Vercauteren09] for SSD-driven registration\\n    ( eq. 4 in [Vercauteren09] ) using the EM algorithm [Arce14] to handle\\n    multi-modality images.\\n\\n    In this case, $\\\\sigma_i$ in eq. 4 of [Vercauteren09] is estimated using\\n    the EM algorithm, while in the original version of diffeomorphic demons\\n    it is estimated by the difference between the image values at each pixel.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\expectmax.pyx.txt'}),\n",
       " Document(page_content=\"Computes the demons step [Vercauteren09] for SSD-driven registration\\n    ( eq. 4 in [Vercauteren09] ) using the EM algorithm [Arce14] to handle\\n    multi-modality images.\\n\\n    In this case, $\\\\sigma_i$ in eq. 4 of [Vercauteren09] is estimated using\\n    the EM algorithm, while in the original version of diffeomorphic demons\\n    it is estimated by the difference between the image values at each pixel.\\n\\n    Parameters\\n    ----------\\n    delta_field : array, shape (S, R, C)\\n        contains, at each pixel, the difference between the moving image (warped\\n        under the current deformation s ) J and the static image I:\\n        delta_field[k,i,j] = J(s(k,i,j)) - I(k,i,j). The order is important,\\n        changing to delta_field[k,i,j] = I(k,i,j) - J(s(k,i,j)) yields the\\n        backward demons step warping the static image towards the moving, which\\n        may not be the intended behavior unless the 'gradient_moving' passed\\n        corresponds to the gradient of the static image\\n    sigma_sq_field : array, shape (S, R, C)\\n        contains, at each pixel (k, i, j), the estimated variance (not std) of\\n        the hidden variable associated to the intensity at static[k,i,j] (which\\n        must have been previously quantized)\\n    gradient_moving : array, shape (S, R, C, 2)\\n        the gradient of the moving image\\n    sigma_sq_x : float\\n        parameter controlling the amount of regularization. It corresponds to\\n        $\\\\sigma_x^2$ in algorithm 1 of Vercauteren et al.[2].\\n    out : array, shape (S, R, C, 2)\\n        the resulting demons step will be written to this array\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\expectmax.pyx.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    demons_step : array, shape (S, R, C, 3)\\n        the demons step to be applied for updating the current displacement\\n        field\\n    energy : float\\n        the current em energy (before applying the returned demons_step)\\n\\n    References\\n    ----------\\n    [Arce14] Arce-santana, E., Campos-delgado, D. U., & Vigueras-g, F. (2014).\\n             Non-rigid Multimodal Image Registration Based on the\\n             Expectation-Maximization Algorithm, (168140), 36-47.\\n\\n    [Vercauteren09] Vercauteren, T., Pennec, X., Perchant, A., & Ayache, N.\\n                    (2009). Diffeomorphic demons: efficient non-parametric\\n                    image registration. NeuroImage, 45(1 Suppl), S61-72.\\n                    doi:10.1016/j.neuroimage.2008.10.040\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp ns = delta_field.shape[0]\\n        cnp.npy_intp nr = delta_field.shape[1]\\n        cnp.npy_intp nc = delta_field.shape[2]\\n        cnp.npy_intp i, j, k\\n        double delta, sigma_sq_i, nrm2, energy, den\\n\\n    if out is None:\\n        out = np.zeros((ns, nr, nc, 3), dtype=np.asarray(delta_field).dtype)\\n\\n    with nogil:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\expectmax.pyx.txt'}),\n",
       " Document(page_content='energy = 0\\n        for k in range(ns):\\n            for i in range(nr):\\n                for j in range(nc):\\n                    sigma_sq_i = sigma_sq_field[k,i,j]\\n                    delta = delta_field[k,i,j]\\n                    energy += (delta**2)\\n                    if dpy_isinf(sigma_sq_i) != 0:\\n                        out[k, i, j, 0] = 0\\n                        out[k, i, j, 1] = 0\\n                        out[k, i, j, 2] = 0\\n                    else:\\n                        nrm2 = (gradient_moving[k, i, j, 0]**2 +\\n                                gradient_moving[k, i, j, 1]**2 +\\n                                gradient_moving[k, i, j, 2]**2)\\n                        if sigma_sq_i == 0:\\n                            if nrm2 == 0:\\n                                out[k, i, j, 0] = 0\\n                                out[k, i, j, 1] = 0\\n                                out[k, i, j, 2] = 0\\n                            else:\\n                                out[k, i, j, 0] = (delta *\\n                                    gradient_moving[k, i, j, 0] / nrm2)\\n                                out[k, i, j, 1] = (delta *\\n                                    gradient_moving[k, i, j, 1] / nrm2)\\n                                out[k, i, j, 2] = (delta *\\n                                    gradient_moving[k, i, j, 2] / nrm2)\\n                        else:\\n                            den = (sigma_sq_x * nrm2 + sigma_sq_i)\\n                            out[k, i, j, 0] = (sigma_sq_x * delta *\\n                                gradient_moving[k, i, j, 0] / den)\\n                            out[k, i, j, 1] = (sigma_sq_x * delta *\\n                                gradient_moving[k, i, j, 1] / den)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\expectmax.pyx.txt'}),\n",
       " Document(page_content='out[k, i, j, 2] = (delta *\\n                                    gradient_moving[k, i, j, 2] / nrm2)\\n                        else:\\n                            den = (sigma_sq_x * nrm2 + sigma_sq_i)\\n                            out[k, i, j, 0] = (sigma_sq_x * delta *\\n                                gradient_moving[k, i, j, 0] / den)\\n                            out[k, i, j, 1] = (sigma_sq_x * delta *\\n                                gradient_moving[k, i, j, 1] / den)\\n                            out[k, i, j, 2] = (sigma_sq_x * delta *\\n                                gradient_moving[k, i, j, 2] / den)\\n    return np.asarray(out), energy', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\expectmax.pyx.txt'}),\n",
       " Document(page_content='\"\"\" Affine image registration module consisting of the following classes:\\n\\n    AffineMap: encapsulates the necessary information to perform affine\\n        transforms between two domains, defined by a `static` and a `moving`\\n        image. The `domain` of the transform is the set of points in the\\n        `static` image\\'s grid, and the `codomain` is the set of points in\\n        the `moving` image. When we call the `transform` method, `AffineMap`\\n        maps each point `x` of the domain (`static` grid) to the codomain\\n        (`moving` grid) and interpolates the `moving` image at that point\\n        to obtain the intensity value to be placed at `x` in the resulting\\n        grid. The `transform_inverse` method performs the opposite operation\\n        mapping points in the codomain to points in the domain.\\n\\n    ParzenJointHistogram: computes the marginal and joint distributions of\\n        intensities of a pair of images, using Parzen windows [Parzen62]\\n        with a cubic spline kernel, as proposed by Mattes et al. [Mattes03].\\n        It also computes the gradient of the joint histogram w.r.t. the\\n        parameters of a given transform.\\n\\n    MutualInformationMetric: computes the value and gradient of the mutual\\n        information metric the way `Optimizer` needs them. That is, given\\n        a set of transform parameters, it will use `ParzenJointHistogram`\\n        to compute the value and gradient of the joint intensity histogram\\n        evaluated at the given parameters, and evaluate the value and\\n        gradient of the histogram\\'s mutual information.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='MutualInformationMetric: computes the value and gradient of the mutual\\n        information metric the way `Optimizer` needs them. That is, given\\n        a set of transform parameters, it will use `ParzenJointHistogram`\\n        to compute the value and gradient of the joint intensity histogram\\n        evaluated at the given parameters, and evaluate the value and\\n        gradient of the histogram\\'s mutual information.\\n\\n    AffineRegistration: it runs the multi-resolution registration, putting\\n        all the pieces together. It needs to create the scale space of the\\n        images and run the multi-resolution registration by using the Metric\\n        and the Optimizer at each level of the Gaussian pyramid. At each\\n        level, it will setup the metric to compute value and gradient of the\\n        metric with the input images with different levels of smoothing.\\n\\n    References\\n    ----------\\n    [Parzen62] E. Parzen. On the estimation of a probability density\\n               function and the mode. Annals of Mathematical Statistics,\\n               33(3), 1065-1076, 1962.\\n    [Mattes03] Mattes, D., Haynor, D. R., Vesselle, H., Lewellen, T. K.,\\n               & Eubank, W. PET-CT image registration in the chest using\\n               free-form deformations. IEEE Transactions on Medical\\n               Imaging, 22(1), 120-8, 2003.\\n\\n\"\"\"\\n\\nfrom warnings import warn', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='\"\"\"\\n\\nfrom warnings import warn\\n\\nimport numpy as np\\nimport numpy.linalg as npl\\nimport scipy.ndimage as ndimage\\nfrom dipy.core.optimize import Optimizer\\nfrom dipy.core.interpolation import (interpolate_scalar_2d,\\n                                     interpolate_scalar_3d)\\nfrom dipy.align import vector_fields as vf\\nfrom dipy.align import VerbosityLevels\\nfrom dipy.align.parzenhist import (ParzenJointHistogram,\\n                                   sample_domain_regular,\\n                                   compute_parzen_mi)\\nfrom dipy.align.imwarp import (get_direction_and_spacings, ScaleSpace)\\nfrom dipy.align.scalespace import IsotropicScaleSpace\\n\\n_interp_options = [\\'nearest\\', \\'linear\\']\\n_transform_method = dict()\\n_transform_method[(2, \\'nearest\\')] = vf.transform_2d_affine_nn\\n_transform_method[(3, \\'nearest\\')] = vf.transform_3d_affine_nn\\n_transform_method[(2, \\'linear\\')] = vf.transform_2d_affine\\n_transform_method[(3, \\'linear\\')] = vf.transform_3d_affine\\n_number_dim_affine_matrix = 2\\n\\n\\nclass AffineInversionError(Exception):\\n    pass\\n\\n\\nclass AffineInvalidValuesError(Exception):\\n    pass\\n\\n\\nclass AffineMap:\\n\\n    def __init__(self, affine, domain_grid_shape=None, domain_grid2world=None,\\n                 codomain_grid_shape=None, codomain_grid2world=None):\\n        \"\"\" AffineMap.\\n\\n        Implements an affine transformation whose domain is given by\\n        `domain_grid` and `domain_grid2world`, and whose co-domain is\\n        given by `codomain_grid` and `codomain_grid2world`.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='class AffineInversionError(Exception):\\n    pass\\n\\n\\nclass AffineInvalidValuesError(Exception):\\n    pass\\n\\n\\nclass AffineMap:\\n\\n    def __init__(self, affine, domain_grid_shape=None, domain_grid2world=None,\\n                 codomain_grid_shape=None, codomain_grid2world=None):\\n        \"\"\" AffineMap.\\n\\n        Implements an affine transformation whose domain is given by\\n        `domain_grid` and `domain_grid2world`, and whose co-domain is\\n        given by `codomain_grid` and `codomain_grid2world`.\\n\\n        The actual transform is represented by the `affine` matrix, which\\n        operate in world coordinates. Therefore, to transform a moving image\\n        towards a static image, we first map each voxel (i,j,k) of the static\\n        image to world coordinates (x,y,z) by applying `domain_grid2world`.\\n        Then we apply the `affine` transform to (x,y,z) obtaining (x\\', y\\', z\\')\\n        in moving image\\'s world coordinates. Finally, (x\\', y\\', z\\') is mapped\\n        to voxel coordinates (i\\', j\\', k\\') in the moving image by multiplying\\n        (x\\', y\\', z\\') by the inverse of `codomain_grid2world`. The\\n        `codomain_grid_shape` is used analogously to transform the static\\n        image towards the moving image when calling `transform_inverse`.\\n\\n        If the domain/co-domain information is not provided (None) then the\\n        sampling information needs to be specified each time the `transform`\\n        or `transform_inverse` is called to transform images. Note that such\\n        sampling information is not necessary to transform points defined in\\n        physical space, such as stream lines.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='Parameters\\n        ----------\\n        affine : array, shape (dim + 1, dim + 1)\\n            the matrix defining the affine transform, where `dim` is the\\n            dimension of the space this map operates in (2 for 2D images,\\n            3 for 3D images). If None, then `self` represents the identity\\n            transformation.\\n        domain_grid_shape : sequence, shape (dim,), optional\\n            the shape of the default domain sampling grid. When `transform`\\n            is called to transform an image, the resulting image will have\\n            this shape, unless a different sampling information is provided.\\n            If None, then the sampling grid shape must be specified each time\\n            the `transform` method is called.\\n        domain_grid2world : array, shape (dim + 1, dim + 1), optional\\n            the grid-to-world transform associated with the domain grid.\\n            If None (the default), then the grid-to-world transform is assumed\\n            to be the identity.\\n        codomain_grid_shape : sequence of integers, shape (dim,)\\n            the shape of the default co-domain sampling grid. When\\n            `transform_inverse` is called to transform an image, the resulting\\n            image will have this shape, unless a different sampling\\n            information is provided. If None (the default), then the sampling\\n            grid shape must be specified each time the `transform_inverse`\\n            method is called.\\n        codomain_grid2world : array, shape (dim + 1, dim + 1)\\n            the grid-to-world transform associated with the co-domain grid.\\n            If None (the default), then the grid-to-world transform is assumed', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='image will have this shape, unless a different sampling\\n            information is provided. If None (the default), then the sampling\\n            grid shape must be specified each time the `transform_inverse`\\n            method is called.\\n        codomain_grid2world : array, shape (dim + 1, dim + 1)\\n            the grid-to-world transform associated with the co-domain grid.\\n            If None (the default), then the grid-to-world transform is assumed\\n            to be the identity.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='\"\"\"\\n        self.set_affine(affine)\\n        self.domain_shape = domain_grid_shape\\n        self.domain_grid2world = domain_grid2world\\n        self.codomain_shape = codomain_grid_shape\\n        self.codomain_grid2world = codomain_grid2world\\n\\n    def get_affine(self):\\n        \"\"\"Return the value of the transformation, not a reference.\\n\\n        Returns\\n        -------\\n        affine : ndarray\\n            Copy of the transform, not a reference.\\n\\n        \"\"\"\\n\\n        # returning a copy to insulate it from changes outside object\\n        return self.affine.copy()\\n\\n    def set_affine(self, affine):\\n        \"\"\"Set the affine transform (operating in physical space).\\n\\n        Also sets `self.affine_inv` - the inverse of `affine`, or None if\\n        there is no inverse.\\n\\n        Parameters\\n        ----------\\n        affine : array, shape (dim + 1, dim + 1)\\n            the matrix representing the affine transform operating in\\n            physical space. The domain and co-domain information\\n            remains unchanged. If None, then `self` represents the identity\\n            transformation.\\n\\n        \"\"\"\\n\\n        if affine is None:\\n            self.affine = None\\n            self.affine_inv = None\\n            return\\n\\n        try:\\n            affine = np.array(affine)\\n        except Exception:\\n            raise TypeError(\"Input must be type ndarray, or be convertible\"\\n                            \" to one.\")\\n\\n        if len(affine.shape) != _number_dim_affine_matrix:\\n            raise AffineInversionError(\\'Affine transform must be 2D\\')', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='\"\"\"\\n\\n        if affine is None:\\n            self.affine = None\\n            self.affine_inv = None\\n            return\\n\\n        try:\\n            affine = np.array(affine)\\n        except Exception:\\n            raise TypeError(\"Input must be type ndarray, or be convertible\"\\n                            \" to one.\")\\n\\n        if len(affine.shape) != _number_dim_affine_matrix:\\n            raise AffineInversionError(\\'Affine transform must be 2D\\')\\n\\n        if not affine.shape[0] == affine.shape[1]:\\n            raise AffineInversionError(\"Affine transform must be a square \"\\n                                       \"matrix\")\\n\\n        if not np.all(np.isfinite(affine)):\\n            raise AffineInvalidValuesError(\"Affine transform contains invalid\"\\n                                           \" elements\")\\n\\n        # checking on proper augmentation\\n        # First n-1 columns in last row in matrix contain non-zeros\\n        if not np.all(affine[-1, :-1] == 0.0):\\n            raise AffineInvalidValuesError(\"First {n_1} columns in last row\"\\n                                           \" in matrix contain non-zeros!\"\\n                                           .format(n_1=affine.shape[0] - 1))\\n\\n        # Last row, last column in matrix must be 1.0!\\n        if affine[-1, -1] != 1.0:\\n            raise AffineInvalidValuesError(\"Last row, last column in matrix\"\\n                                           \" is not 1.0!\")\\n\\n        # making a copy to insulate it from changes outside object\\n        self.affine = affine.copy()\\n\\n        try:\\n            self.affine_inv = npl.inv(affine)\\n        except npl.LinAlgError:\\n            raise AffineInversionError(\\'Affine cannot be inverted\\')', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='# Last row, last column in matrix must be 1.0!\\n        if affine[-1, -1] != 1.0:\\n            raise AffineInvalidValuesError(\"Last row, last column in matrix\"\\n                                           \" is not 1.0!\")\\n\\n        # making a copy to insulate it from changes outside object\\n        self.affine = affine.copy()\\n\\n        try:\\n            self.affine_inv = npl.inv(affine)\\n        except npl.LinAlgError:\\n            raise AffineInversionError(\\'Affine cannot be inverted\\')\\n\\n    def __str__(self):\\n        \"\"\"Printable format - relies on ndarray\\'s implementation.\"\"\"\\n\\n        return str(self.affine)\\n\\n    def __repr__(self):\\n        \"\"\"Reloadable representation - relies on ndarray\\'s implementation.\"\"\"\\n        return self.affine.__repr__()\\n\\n    def __format__(self, format_spec):\\n        \"\"\" Implementation various formatting options.\"\"\"', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='def __str__(self):\\n        \"\"\"Printable format - relies on ndarray\\'s implementation.\"\"\"\\n\\n        return str(self.affine)\\n\\n    def __repr__(self):\\n        \"\"\"Reloadable representation - relies on ndarray\\'s implementation.\"\"\"\\n        return self.affine.__repr__()\\n\\n    def __format__(self, format_spec):\\n        \"\"\" Implementation various formatting options.\"\"\"\\n\\n        if format_spec is None or self.affine is None:\\n            return str(self.affine)\\n        elif isinstance(format_spec, str):\\n            format_spec = format_spec.lower()\\n            if format_spec in [\\'\\', \\' \\', \\'f\\', \\'full\\']:\\n                return str(self.affine)\\n            # rotation part only (initial 3x3)\\n            elif format_spec in [\\'r\\', \\'rotation\\']:\\n                return str(self.affine[:-1, :-1])\\n            # translation part only (4th col)\\n            elif format_spec in [\\'t\\', \\'translation\\']:\\n                # notice unusual indexing to make it a column vector\\n                #   i.e. rows from 0 to n-1, cols from n to n\\n                return str(self.affine[:-1, -1:])\\n            else:\\n                allowed_formats_print_map = [\\'full\\', \\'f\\',\\n                                             \\'rotation\\', \\'r\\',\\n                                             \\'translation\\', \\'t\\']\\n                raise NotImplementedError(\"Format {} not recognized or\"\\n                                          \"implemented.\\\\nTry one of {}\"\\n                                          .format(format_spec,\\n                                                  allowed_formats_print_map))', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='def _apply_transform(self, image, interpolation=\\'linear\\',\\n                         image_grid2world=None, sampling_grid_shape=None,\\n                         sampling_grid2world=None, resample_only=False,\\n                         apply_inverse=False):\\n        \"\"\"Transform the input image applying this affine transform.\\n\\n        This is a generic function to transform images using either this\\n        (direct) transform or its inverse.\\n\\n        If applying the direct transform (`apply_inverse=False`):\\n            by default, the transformed image is sampled at a grid defined by\\n            `self.domain_shape` and `self.domain_grid2world`.\\n        If applying the inverse transform (`apply_inverse=True`):\\n            by default, the transformed image is sampled at a grid defined by\\n            `self.codomain_shape` and `self.codomain_grid2world`.\\n\\n        If the sampling information was not provided at initialization of this\\n        transform then `sampling_grid_shape` is mandatory.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content=\"Parameters\\n        ----------\\n        image :  2D or 3D array\\n            the image to be transformed\\n        interpolation : string, either 'linear' or 'nearest'\\n            the type of interpolation to be used, either 'linear'\\n            (for k-linear interpolation) or 'nearest' for nearest neighbor\\n        image_grid2world : array, shape (dim + 1, dim + 1), optional\\n            the grid-to-world transform associated with `image`.\\n            If None (the default), then the grid-to-world transform is assumed\\n            to be the identity.\\n        sampling_grid_shape : sequence, shape (dim,), optional\\n            the shape of the grid where the transformed image must be sampled.\\n            If None (the default), then `self.domain_shape` is used instead\\n            (which must have been set at initialization, otherwise an exception\\n            will be raised).\\n        sampling_grid2world : array, shape (dim + 1, dim + 1), optional\\n            the grid-to-world transform associated with the sampling grid\\n            (specified by `sampling_grid_shape`, or by default\\n            `self.domain_shape`). If None (the default), then the\\n            grid-to-world transform is assumed to be the identity.\\n        resample_only : Boolean, optional\\n            If False (the default) the affine transform is applied normally.\\n            If True, then the affine transform is not applied, and the input\\n            image is just re-sampled on the domain grid of this transform.\\n        apply_inverse : Boolean, optional\\n            If False (the default) the image is transformed from the codomain\\n            of this transform to its domain using the (direct) affine\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='resample_only : Boolean, optional\\n            If False (the default) the affine transform is applied normally.\\n            If True, then the affine transform is not applied, and the input\\n            image is just re-sampled on the domain grid of this transform.\\n        apply_inverse : Boolean, optional\\n            If False (the default) the image is transformed from the codomain\\n            of this transform to its domain using the (direct) affine\\n            transform. Otherwise, the image is transformed from the domain\\n            of this transform to its codomain using the (inverse) affine\\n            transform.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='Returns\\n        -------\\n        transformed : array, shape `sampling_grid_shape` or `self.domain_shape`\\n            the transformed image, sampled at the requested grid\\n\\n        \"\"\"\\n        # Verify valid interpolation requested\\n        if interpolation not in _interp_options:\\n            msg = \\'Unknown interpolation method: %s\\' % (interpolation,)\\n            raise ValueError(msg)\\n\\n        # Obtain sampling grid\\n        if sampling_grid_shape is None:\\n            if apply_inverse:\\n                sampling_grid_shape = self.codomain_shape\\n            else:\\n                sampling_grid_shape = self.domain_shape\\n        if sampling_grid_shape is None:\\n            msg = \\'Unknown sampling info. Provide a valid sampling_grid_shape\\'\\n            raise ValueError(msg)\\n\\n        dim = len(sampling_grid_shape)\\n        shape = np.array(sampling_grid_shape, dtype=np.int32)\\n\\n        # Verify valid image dimension\\n        img_dim = len(image.shape)\\n        if img_dim < 2 or img_dim > 3:\\n            raise ValueError(\\'Undefined transform for dim: %d\\' % (img_dim,))\\n\\n        # Obtain grid-to-world transform for sampling grid\\n        if sampling_grid2world is None:\\n            if apply_inverse:\\n                sampling_grid2world = self.codomain_grid2world\\n            else:\\n                sampling_grid2world = self.domain_grid2world\\n        if sampling_grid2world is None:\\n            sampling_grid2world = np.eye(dim + 1)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='# Obtain grid-to-world transform for sampling grid\\n        if sampling_grid2world is None:\\n            if apply_inverse:\\n                sampling_grid2world = self.codomain_grid2world\\n            else:\\n                sampling_grid2world = self.domain_grid2world\\n        if sampling_grid2world is None:\\n            sampling_grid2world = np.eye(dim + 1)\\n\\n        # Obtain world-to-grid transform for input image\\n        if image_grid2world is None:\\n            if apply_inverse:\\n                image_grid2world = self.domain_grid2world\\n            else:\\n                image_grid2world = self.codomain_grid2world\\n            if image_grid2world is None:\\n                image_grid2world = np.eye(dim + 1)\\n        image_world2grid = npl.inv(image_grid2world)\\n\\n        # Compute the transform from sampling grid to input image grid\\n        if apply_inverse:\\n            aff = self.affine_inv\\n        else:\\n            aff = self.affine\\n\\n        if (aff is None) or resample_only:\\n            comp = image_world2grid.dot(sampling_grid2world)\\n        else:\\n            comp = image_world2grid.dot(aff.dot(sampling_grid2world))\\n\\n        # Transform the input image\\n        if interpolation == \\'linear\\':\\n            image = image.astype(np.float64)\\n        transformed = _transform_method[(dim, interpolation)](image, shape,\\n                                                              comp)\\n        return transformed\\n\\n    def transform(self, image, interpolation=\\'linear\\', image_grid2world=None,\\n                  sampling_grid_shape=None, sampling_grid2world=None,\\n                  resample_only=False):\\n        \"\"\"Transform the input image from co-domain to domain space.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='def transform(self, image, interpolation=\\'linear\\', image_grid2world=None,\\n                  sampling_grid_shape=None, sampling_grid2world=None,\\n                  resample_only=False):\\n        \"\"\"Transform the input image from co-domain to domain space.\\n\\n        By default, the transformed image is sampled at a grid defined by\\n        `self.domain_shape` and `self.domain_grid2world`. If such\\n        information was not provided then `sampling_grid_shape` is mandatory.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content=\"Parameters\\n        ----------\\n        image :  2D or 3D array\\n            the image to be transformed\\n        interpolation : string, either 'linear' or 'nearest'\\n            the type of interpolation to be used, either 'linear'\\n            (for k-linear interpolation) or 'nearest' for nearest neighbor\\n        image_grid2world : array, shape (dim + 1, dim + 1), optional\\n            the grid-to-world transform associated with `image`.\\n            If None (the default), then the grid-to-world transform is assumed\\n            to be the identity.\\n        sampling_grid_shape : sequence, shape (dim,), optional\\n            the shape of the grid where the transformed image must be sampled.\\n            If None (the default), then `self.codomain_shape` is used instead\\n            (which must have been set at initialization, otherwise an exception\\n            will be raised).\\n        sampling_grid2world : array, shape (dim + 1, dim + 1), optional\\n            the grid-to-world transform associated with the sampling grid\\n            (specified by `sampling_grid_shape`, or by default\\n            `self.codomain_shape`). If None (the default), then the\\n            grid-to-world transform is assumed to be the identity.\\n        resample_only : Boolean, optional\\n            If False (the default) the affine transform is applied normally.\\n            If True, then the affine transform is not applied, and the input\\n            image is just re-sampled on the domain grid of this transform.\\n\\n        Returns\\n        -------\\n        transformed : array, shape `sampling_grid_shape` or\\n                      `self.codomain_shape`\\n            the transformed image, sampled at the requested grid\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='Returns\\n        -------\\n        transformed : array, shape `sampling_grid_shape` or\\n                      `self.codomain_shape`\\n            the transformed image, sampled at the requested grid\\n\\n        \"\"\"\\n        transformed = self._apply_transform(image, interpolation,\\n                                            image_grid2world,\\n                                            sampling_grid_shape,\\n                                            sampling_grid2world,\\n                                            resample_only,\\n                                            apply_inverse=False)\\n        return np.array(transformed)\\n\\n    def transform_inverse(self, image, interpolation=\\'linear\\',\\n                          image_grid2world=None, sampling_grid_shape=None,\\n                          sampling_grid2world=None, resample_only=False):\\n        \"\"\"Transform the input image from domain to co-domain space.\\n\\n        By default, the transformed image is sampled at a grid defined by\\n        `self.codomain_shape` and `self.codomain_grid2world`. If such\\n        information was not provided then `sampling_grid_shape` is mandatory.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content=\"Parameters\\n        ----------\\n        image :  2D or 3D array\\n            the image to be transformed\\n        interpolation : string, either 'linear' or 'nearest'\\n            the type of interpolation to be used, either 'linear'\\n            (for k-linear interpolation) or 'nearest' for nearest neighbor\\n        image_grid2world : array, shape (dim + 1, dim + 1), optional\\n            the grid-to-world transform associated with `image`.\\n            If None (the default), then the grid-to-world transform is assumed\\n            to be the identity.\\n        sampling_grid_shape : sequence, shape (dim,), optional\\n            the shape of the grid where the transformed image must be sampled.\\n            If None (the default), then `self.codomain_shape` is used instead\\n            (which must have been set at initialization, otherwise an exception\\n            will be raised).\\n        sampling_grid2world : array, shape (dim + 1, dim + 1), optional\\n            the grid-to-world transform associated with the sampling grid\\n            (specified by `sampling_grid_shape`, or by default\\n            `self.codomain_shape`). If None (the default), then the\\n            grid-to-world transform is assumed to be the identity.\\n        resample_only : Boolean, optional\\n            If False (the default) the affine transform is applied normally.\\n            If True, then the affine transform is not applied, and the input\\n            image is just re-sampled on the domain grid of this transform.\\n\\n        Returns\\n        -------\\n        transformed : array, shape `sampling_grid_shape` or\\n                      `self.codomain_shape`\\n            the transformed image, sampled at the requested grid\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='Returns\\n        -------\\n        transformed : array, shape `sampling_grid_shape` or\\n                      `self.codomain_shape`\\n            the transformed image, sampled at the requested grid\\n\\n        \"\"\"\\n        transformed = self._apply_transform(image, interpolation,\\n                                            image_grid2world,\\n                                            sampling_grid_shape,\\n                                            sampling_grid2world,\\n                                            resample_only,\\n                                            apply_inverse=True)\\n        return np.array(transformed)\\n\\n\\nclass MutualInformationMetric:\\n\\n    def __init__(self, nbins=32, sampling_proportion=None):\\n        r\"\"\"Initialize an instance of the Mutual Information metric.\\n\\n        This class implements the methods required by Optimizer to drive the\\n        registration process.\\n\\n        Parameters\\n        ----------\\n        nbins : int, optional\\n            the number of bins to be used for computing the intensity\\n            histograms. The default is 32.\\n        sampling_proportion : None or float in interval (0, 1], optional\\n            There are two types of sampling: dense and sparse. Dense sampling\\n            uses all voxels for estimating the (joint and marginal) intensity\\n            histograms, while sparse sampling uses a subset of them. If\\n            `sampling_proportion` is None, then dense sampling is\\n            used. If `sampling_proportion` is a floating point value in (0,1]\\n            then sparse sampling is used, where `sampling_proportion`\\n            specifies the proportion of voxels to be used. The default is\\n            None.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='Notes\\n        -----\\n        Since we use linear interpolation, images are not, in general,\\n        differentiable at exact voxel coordinates, but they are differentiable\\n        between voxel coordinates. When using sparse sampling, selected voxels\\n        are slightly moved by adding a small random displacement within one\\n        voxel to prevent sampling points from being located exactly at voxel\\n        coordinates. When using dense sampling, this random displacement is\\n        not applied.\\n\\n        \"\"\"\\n        self.histogram = ParzenJointHistogram(nbins)\\n        self.sampling_proportion = sampling_proportion\\n        self.metric_val = None\\n        self.metric_grad = None\\n\\n    def setup(self, transform, static, moving, static_grid2world=None,\\n              moving_grid2world=None, starting_affine=None,\\n              static_mask=None, moving_mask=None):\\n        r\"\"\"Prepare the metric to compute intensity densities and gradients.\\n\\n        The histograms will be setup to compute probability densities of\\n        intensities within the minimum and maximum values of `static` and\\n        `moving`', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content=\"Parameters\\n        ----------\\n        transform: instance of Transform\\n            the transformation with respect to whose parameters the gradient\\n            must be computed\\n        static : array, shape (S, R, C) or (R, C)\\n            static image\\n        moving : array, shape (S', R', C') or (R', C')\\n            moving image. The dimensions of the static (S, R, C) and moving\\n            (S', R', C') images do not need to be the same.\\n        static_grid2world : array (dim+1, dim+1), optional\\n            the grid-to-space transform of the static image. The default is\\n            None, implying the transform is the identity.\\n        moving_grid2world : array (dim+1, dim+1)\\n            the grid-to-space transform of the moving image. The default is\\n            None, implying the spacing along all axes is 1.\\n        starting_affine : array, shape (dim+1, dim+1), optional\\n            the pre-aligning matrix (an affine transform) that roughly aligns\\n            the moving image towards the static image. If None, no\\n            pre-alignment is performed. If a pre-alignment matrix is available,\\n            it is recommended to provide this matrix as `starting_affine`\\n            instead of manually transforming the moving image to reduce\\n            interpolation artifacts. The default is None, implying no\\n            pre-alignment is performed.\\n        static_mask : array, shape (S, R, C) or (R, C), optional\\n            static image mask that defines which pixels in the static image\\n            are used to calculate the mutual information.\\n        moving_mask : array, shape (S', R', C') or (R', C'), optional\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content=\"instead of manually transforming the moving image to reduce\\n            interpolation artifacts. The default is None, implying no\\n            pre-alignment is performed.\\n        static_mask : array, shape (S, R, C) or (R, C), optional\\n            static image mask that defines which pixels in the static image\\n            are used to calculate the mutual information.\\n        moving_mask : array, shape (S', R', C') or (R', C'), optional\\n            moving image mask that defines which pixels in the moving image\\n            are used to calculate the mutual information.\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='\"\"\"\\n        n = transform.get_number_of_parameters()\\n        self.metric_grad = np.zeros(n, dtype=np.float64)\\n        self.dim = len(static.shape)\\n        if moving_grid2world is None:\\n            moving_grid2world = np.eye(self.dim + 1)\\n        if static_grid2world is None:\\n            static_grid2world = np.eye(self.dim + 1)\\n        self.transform = transform\\n        self.static = np.array(static).astype(np.float64)\\n        self.moving = np.array(moving).astype(np.float64)\\n        self.static_grid2world = static_grid2world\\n        self.static_world2grid = npl.inv(static_grid2world)\\n        self.moving_grid2world = moving_grid2world\\n        self.moving_world2grid = npl.inv(moving_grid2world)\\n        self.static_direction, self.static_spacing = \\\\\\n            get_direction_and_spacings(static_grid2world, self.dim)\\n        self.moving_direction, self.moving_spacing = \\\\\\n            get_direction_and_spacings(moving_grid2world, self.dim)\\n        self.starting_affine = starting_affine\\n\\n        P = np.eye(self.dim + 1)\\n        if self.starting_affine is not None:\\n            P = self.starting_affine\\n\\n        self.affine_map = AffineMap(P, static.shape, static_grid2world,\\n                                    moving.shape, moving_grid2world)\\n\\n        # Masks can only be used with dense sampling\\n        if self.sampling_proportion in [None, 1.0]:\\n\\n            if static_mask is not None:\\n                self.static_mask = static_mask.astype(np.int32)\\n            else:\\n                self.static_mask = None\\n\\n            if moving_mask is not None:\\n                self.moving_mask = moving_mask.astype(np.int32)\\n            else:\\n                self.moving_mask = None\\n\\n        else:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='# Masks can only be used with dense sampling\\n        if self.sampling_proportion in [None, 1.0]:\\n\\n            if static_mask is not None:\\n                self.static_mask = static_mask.astype(np.int32)\\n            else:\\n                self.static_mask = None\\n\\n            if moving_mask is not None:\\n                self.moving_mask = moving_mask.astype(np.int32)\\n            else:\\n                self.moving_mask = None\\n\\n        else:\\n\\n            if (static_mask is not None) or (moving_mask is not None):\\n                wm = \"Masking is not implemented for sampling_proportion < 1, \"\\n                wm = wm + \"setting static_mask = None and moving_mask = None\"\\n                warn(wm, UserWarning)\\n\\n            self.static_mask, self.moving_mask = None, None\\n\\n        if self.dim == 2:\\n            self.interp_method = interpolate_scalar_2d\\n        else:\\n            self.interp_method = interpolate_scalar_3d', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='else:\\n\\n            if (static_mask is not None) or (moving_mask is not None):\\n                wm = \"Masking is not implemented for sampling_proportion < 1, \"\\n                wm = wm + \"setting static_mask = None and moving_mask = None\"\\n                warn(wm, UserWarning)\\n\\n            self.static_mask, self.moving_mask = None, None\\n\\n        if self.dim == 2:\\n            self.interp_method = interpolate_scalar_2d\\n        else:\\n            self.interp_method = interpolate_scalar_3d\\n\\n        if self.sampling_proportion is None:\\n            self.samples = None\\n            self.ns = 0\\n        else:\\n            k = int(np.ceil(1.0 / self.sampling_proportion))\\n            shape = np.array(static.shape, dtype=np.int32)\\n            self.samples = sample_domain_regular(k, shape, static_grid2world)\\n            self.samples = np.array(self.samples)\\n            self.ns = self.samples.shape[0]\\n            # Add a column of ones (homogeneous coordinates)\\n            self.samples = np.hstack((self.samples, np.ones(self.ns)[:, None]))\\n            if self.starting_affine is None:\\n                self.samples_prealigned = self.samples\\n            else:\\n                self.samples_prealigned = \\\\\\n                    self.starting_affine.dot(self.samples.T).T\\n            # Sample the static image\\n            static_p = self.static_world2grid.dot(self.samples.T).T\\n            static_p = static_p[..., :self.dim]\\n            self.static_vals, inside = self.interp_method(static, static_p)\\n            self.static_vals = np.array(self.static_vals, dtype=np.float64)\\n        self.histogram.setup(self.static, self.moving,\\n                             self.static_mask, self.moving_mask)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='def _update_histogram(self):\\n        r\"\"\"Update the histogram according to the current affine transform.\\n\\n        The current affine transform is given by `self.affine_map`, which\\n        must be set before calling this method.\\n\\n        Returns\\n        -------\\n        static_values: array, shape(n,) if sparse sampling is being used,\\n                       array, shape(S, R, C) or (R, C) if dense sampling\\n            the intensity values corresponding to the static image used to\\n            update the histogram. If sparse sampling is being used, then\\n            it is simply a sequence of scalars, obtained by sampling the static\\n            image at the `n` sampling points. If dense sampling is being used,\\n            then the intensities are given directly by the static image,\\n            whose shape is (S, R, C) in the 3D case or (R, C) in the 2D case.\\n        moving_values: array, shape(n,) if sparse sampling is being used,\\n                       array, shape(S, R, C) or (R, C) if dense sampling\\n            the intensity values corresponding to the moving image used to\\n            update the histogram. If sparse sampling is being used, then\\n            it is simply a sequence of scalars, obtained by sampling the moving\\n            image at the `n` sampling points (mapped to the moving space by the\\n            current affine transform). If dense sampling is being used,\\n            then the intensities are given by the moving imaged linearly\\n            transformed towards the static image by the current affine, which\\n            results in an image of the same shape as the static image.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='\"\"\"\\n        static_mask_values, moving_mask_values = None, None\\n        if self.sampling_proportion is None:  # Dense case\\n            static_values = self.static\\n            moving_values = self.affine_map.transform(self.moving)\\n\\n            if self.static_mask is not None:\\n                static_mask_values = self.static_mask\\n            if self.moving_mask is not None:\\n                moving_mask_values =\\\\\\n                 self.affine_map.transform(\\n                    self.moving_mask, interpolation=\\'nearest\\').astype(np.int32)\\n\\n            self.histogram.update_pdfs_dense(\\n                static_values, moving_values,\\n                self.static_mask, moving_mask_values)\\n        else:  # Sparse case\\n            sp_to_moving = self.moving_world2grid.dot(self.affine_map.affine)\\n            pts = sp_to_moving.dot(self.samples.T).T  # Points on moving grid\\n            pts = pts[..., :self.dim]\\n            self.moving_vals, inside = self.interp_method(self.moving, pts)\\n            self.moving_vals = np.array(self.moving_vals)\\n            static_values = self.static_vals\\n            moving_values = self.moving_vals\\n            self.histogram.update_pdfs_sparse(static_values, moving_values)\\n        return static_values, moving_values,\\\\\\n            static_mask_values, moving_mask_values\\n\\n    def _update_mutual_information(self, params, update_gradient=True):\\n        r\"\"\"Update marginal and joint distributions and the joint gradient.\\n\\n        The distributions are updated according to the static and transformed\\n        images. The transformed image is precisely the moving image after\\n        transforming it by the transform defined by the `params` parameters.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='def _update_mutual_information(self, params, update_gradient=True):\\n        r\"\"\"Update marginal and joint distributions and the joint gradient.\\n\\n        The distributions are updated according to the static and transformed\\n        images. The transformed image is precisely the moving image after\\n        transforming it by the transform defined by the `params` parameters.\\n\\n        The gradient of the joint PDF is computed only if update_gradient\\n        is True.\\n\\n        Parameters\\n        ----------\\n        params : array, shape (n,)\\n            the parameter vector of the transform currently used by the metric\\n            (the transform name is provided when self.setup is called), n is\\n            the number of parameters of the transform\\n        update_gradient : Boolean, optional\\n            if True, the gradient of the joint PDF will also be computed,\\n            otherwise, only the marginal and joint PDFs will be computed.\\n            The default is True.\\n\\n        \"\"\"\\n        # Get the matrix associated with the `params` parameter vector\\n        current_affine = self.transform.param_to_matrix(params)\\n        # Get the static-to-prealigned matrix (only needed for the MI gradient)\\n        static2prealigned = self.static_grid2world\\n        if self.starting_affine is not None:\\n            current_affine = current_affine.dot(self.starting_affine)\\n            static2prealigned = self.starting_affine.dot(static2prealigned)\\n        self.affine_map.set_affine(current_affine)\\n\\n        # Update the histogram with the current joint intensities\\n        static_values, moving_values, static_mask_values, moving_mask_values =\\\\\\n            self._update_histogram()', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content=\"H = self.histogram  # Shortcut to `self.histogram`\\n        grad = None  # Buffer to write the MI gradient into (if needed)\\n        if update_gradient:\\n            grad = self.metric_grad\\n            # Compute the gradient of the joint PDF w.r.t. parameters\\n            if self.sampling_proportion is None:  # Dense case\\n                # Compute the gradient of moving img. at physical points\\n                # associated with the >>static image's grid<< cells\\n                # The image gradient must be eval. at current moved points\\n                grid_to_world = current_affine.dot(self.static_grid2world)\\n                mgrad, inside = vf.gradient(self.moving,\\n                                            self.moving_world2grid,\\n                                            self.moving_spacing,\\n                                            self.static.shape,\\n                                            grid_to_world)\\n                # The Jacobian must be evaluated at the pre-aligned points\\n                H.update_gradient_dense(\\n                    params,\\n                    self.transform,\\n                    static_values,\\n                    moving_values,\\n                    static2prealigned,\\n                    mgrad,\\n                    static_mask_values,\\n                    moving_mask_values)\\n            else:  # Sparse case\\n                # Compute the gradient of moving at the sampling points\\n                # which are already given in physical space coordinates\\n                pts = current_affine.dot(self.samples.T).T  # Moved points\\n                mgrad, inside = vf.sparse_gradient(self.moving,\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='moving_values,\\n                    static2prealigned,\\n                    mgrad,\\n                    static_mask_values,\\n                    moving_mask_values)\\n            else:  # Sparse case\\n                # Compute the gradient of moving at the sampling points\\n                # which are already given in physical space coordinates\\n                pts = current_affine.dot(self.samples.T).T  # Moved points\\n                mgrad, inside = vf.sparse_gradient(self.moving,\\n                                                   self.moving_world2grid,\\n                                                   self.moving_spacing,\\n                                                   pts)\\n                # The Jacobian must be evaluated at the pre-aligned points\\n                pts = self.samples_prealigned[..., :self.dim]\\n                H.update_gradient_sparse(params, self.transform, static_values,\\n                                         moving_values, pts, mgrad)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='# Call the cythonized MI computation with self.histogram fields\\n        self.metric_val = compute_parzen_mi(H.joint, H.joint_grad,\\n                                            H.smarginal, H.mmarginal,\\n                                            grad)\\n\\n    def distance(self, params):\\n        r\"\"\"Numeric value of the negative Mutual Information.\\n\\n        We need to change the sign so we can use standard minimization\\n        algorithms.\\n\\n        Parameters\\n        ----------\\n        params : array, shape (n,)\\n            the parameter vector of the transform currently used by the metric\\n            (the transform name is provided when self.setup is called), n is\\n            the number of parameters of the transform\\n\\n        Returns\\n        -------\\n        neg_mi : float\\n            the negative mutual information of the input images after\\n            transforming the moving image by the currently set transform\\n            with `params` parameters\\n\\n        \"\"\"\\n        try:\\n            self._update_mutual_information(params, False)\\n        except (AffineInversionError, AffineInvalidValuesError):\\n            return np.inf\\n        return -1 * self.metric_val\\n\\n    def gradient(self, params):\\n        r\"\"\"Numeric value of the metric\\'s gradient at the given parameters.\\n\\n        Parameters\\n        ----------\\n        params : array, shape (n,)\\n            the parameter vector of the transform currently used by the metric\\n            (the transform name is provided when self.setup is called), n is\\n            the number of parameters of the transform\\n\\n        Returns\\n        -------\\n        grad : array, shape (n,)\\n            the gradient of the negative Mutual Information', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='Parameters\\n        ----------\\n        params : array, shape (n,)\\n            the parameter vector of the transform currently used by the metric\\n            (the transform name is provided when self.setup is called), n is\\n            the number of parameters of the transform\\n\\n        Returns\\n        -------\\n        grad : array, shape (n,)\\n            the gradient of the negative Mutual Information\\n\\n        \"\"\"\\n        try:\\n            self._update_mutual_information(params, True)\\n        except (AffineInversionError, AffineInvalidValuesError):\\n            return 0 * self.metric_grad\\n        return -1 * self.metric_grad\\n\\n    def distance_and_gradient(self, params):\\n        r\"\"\"Numeric value of the metric and its gradient at given parameters.\\n\\n        Parameters\\n        ----------\\n        params : array, shape (n,)\\n            the parameter vector of the transform currently used by the metric\\n            (the transform name is provided when self.setup is called), n is\\n            the number of parameters of the transform\\n\\n        Returns\\n        -------\\n        neg_mi : float\\n            the negative mutual information of the input images after\\n            transforming the moving image by the currently set transform\\n            with `params` parameters\\n        neg_mi_grad : array, shape (n,)\\n            the gradient of the negative Mutual Information\\n\\n        \"\"\"\\n        try:\\n            self._update_mutual_information(params, True)\\n        except (AffineInversionError, AffineInvalidValuesError):\\n            return np.inf, 0 * self.metric_grad\\n        return -1 * self.metric_val, -1 * self.metric_grad\\n\\n\\nclass AffineRegistration:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='\"\"\"\\n        try:\\n            self._update_mutual_information(params, True)\\n        except (AffineInversionError, AffineInvalidValuesError):\\n            return np.inf, 0 * self.metric_grad\\n        return -1 * self.metric_val, -1 * self.metric_grad\\n\\n\\nclass AffineRegistration:\\n\\n    def __init__(self,\\n                 metric=None,\\n                 level_iters=None,\\n                 sigmas=None,\\n                 factors=None,\\n                 method=\\'L-BFGS-B\\',\\n                 ss_sigma_factor=None,\\n                 options=None,\\n                 verbosity=VerbosityLevels.STATUS):\\n        \"\"\"Initialize an instance of the AffineRegistration class.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content=\"Parameters\\n        ----------\\n        metric : None or object, optional\\n            an instance of a metric. The default is None, implying\\n            the Mutual Information metric with default settings.\\n        level_iters : sequence, optional\\n            the number of iterations at each scale of the scale space.\\n            `level_iters[0]` corresponds to the coarsest scale,\\n            `level_iters[-1]` the finest, where n is the length of the\\n            sequence. By default, a 3-level scale space with iterations\\n            sequence equal to [10000, 1000, 100] will be used.\\n        sigmas : sequence of floats, optional\\n            custom smoothing parameter to build the scale space (one parameter\\n            for each scale). By default, the sequence of sigmas will be\\n            [3, 1, 0].\\n        factors : sequence of floats, optional\\n            custom scale factors to build the scale space (one factor for each\\n            scale). By default, the sequence of factors will be [4, 2, 1].\\n        method : string, optional\\n            optimization method to be used. If Scipy version < 0.12, then\\n            only L-BFGS-B is available. Otherwise, `method` can be any\\n            gradient-based method available in `dipy.core.Optimize`: CG, BFGS,\\n            Newton-CG, dogleg or trust-ncg.\\n            The default is 'L-BFGS-B'.\\n        ss_sigma_factor : float, optional\\n            If None, this parameter is not used and an isotropic scale\\n            space with the given `factors` and `sigmas` will be built.\\n            If not None, an anisotropic scale space will be used by\\n            automatically selecting the smoothing sigmas along each axis\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content=\"gradient-based method available in `dipy.core.Optimize`: CG, BFGS,\\n            Newton-CG, dogleg or trust-ncg.\\n            The default is 'L-BFGS-B'.\\n        ss_sigma_factor : float, optional\\n            If None, this parameter is not used and an isotropic scale\\n            space with the given `factors` and `sigmas` will be built.\\n            If not None, an anisotropic scale space will be used by\\n            automatically selecting the smoothing sigmas along each axis\\n            according to the voxel dimensions of the given image.\\n            The `ss_sigma_factor` is used to scale the automatically computed\\n            sigmas. For example, in the isotropic case, the sigma of the\\n            kernel will be $factor * (2 ^ i)$ where\\n            $i = 1, 2, ..., n_scales - 1$ is the scale (the finest resolution\\n            image $i=0$ is never smoothed). The default is None.\\n        options : dict, optional\\n            extra optimization options. The default is None, implying\\n            no extra options are passed to the optimizer.\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='\"\"\"\\n        self.metric = metric\\n\\n        if self.metric is None:\\n            self.metric = MutualInformationMetric()\\n\\n        if level_iters is None:\\n            level_iters = [10000, 1000, 100]\\n        self.level_iters = level_iters\\n        self.levels = len(level_iters)\\n        if self.levels == 0:\\n            raise ValueError(\\'The iterations sequence cannot be empty\\')\\n\\n        self.options = options\\n        self.method = method\\n        if ss_sigma_factor is not None:\\n            self.use_isotropic = False\\n            self.ss_sigma_factor = ss_sigma_factor\\n        else:\\n            self.use_isotropic = True\\n            if factors is None:\\n                factors = [4, 2, 1]\\n            if sigmas is None:\\n                sigmas = [3, 1, 0]\\n            self.factors = factors\\n            self.sigmas = sigmas\\n\\n        self.verbosity = verbosity\\n\\n    # Separately add a string that tells about the verbosity kwarg. This needs\\n    # to be separate, because it is set as a module-wide option in __init__:\\n    docstring_addendum = \\\\\\n        \"\"\"verbosity: int (one of {0, 1, 2, 3}), optional\\n            Set the verbosity level of the algorithm:\\n            0 : do not print anything\\n            1 : print information about the current status of the algorithm\\n            2 : print high level information of the components involved in\\n                the registration that can be used to detect a failing\\n                component.\\n            3 : print as much information as possible to isolate the cause\\n                of a bug.\\n            Default: % s\\n    \"\"\" % VerbosityLevels.STATUS\\n\\n    __init__.__doc__ = __init__.__doc__ + docstring_addendum', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='__init__.__doc__ = __init__.__doc__ + docstring_addendum\\n\\n    def _init_optimizer(self, static, moving, transform, params0,\\n                        static_grid2world, moving_grid2world,\\n                        starting_affine,\\n                        static_mask, moving_mask):\\n        r\"\"\"Initialize the registration optimizer.\\n\\n        Initializes the optimizer by computing the scale space of the input\\n        images', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='Parameters\\n        ----------\\n        static : array, shape (S, R, C) or (R, C)\\n            the image to be used as reference during optimization.\\n        moving : array, shape (S\\', R\\', C\\') or (R\\', C\\')\\n            the image to be used as \"moving\" during optimization. The\\n            dimensions of the static (S, R, C) and moving (S\\', R\\', C\\') images\\n            do not need to be the same.\\n        transform : instance of Transform\\n            the transformation with respect to whose parameters the gradient\\n            must be computed\\n        params0 : array, shape (n,)\\n            parameters from which to start the optimization. If None, the\\n            optimization will start at the identity transform. n is the\\n            number of parameters of the specified transformation.\\n        static_grid2world : array, shape (dim+1, dim+1)\\n            the voxel-to-space transformation associated with the static image\\n        moving_grid2world : array, shape (dim+1, dim+1)\\n            the voxel-to-space transformation associated with the moving image\\n        starting_affine : string, or matrix, or None\\n            If string:\\n                \\'mass\\': align centers of gravity\\n                \\'voxel-origin\\': align physical coordinates of voxel (0,0,0)\\n                \\'centers\\': align physical coordinates of central voxels\\n            If matrix:\\n                array, shape (dim+1, dim+1)\\n            If None:\\n                Start from identity\\n        static_mask : array, shape (S, R, C) or (R, C), optional\\n            static image mask that defines which pixels in the static image\\n            are used to calculate the mutual information.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content=\"'voxel-origin': align physical coordinates of voxel (0,0,0)\\n                'centers': align physical coordinates of central voxels\\n            If matrix:\\n                array, shape (dim+1, dim+1)\\n            If None:\\n                Start from identity\\n        static_mask : array, shape (S, R, C) or (R, C), optional\\n            static image mask that defines which pixels in the static image\\n            are used to calculate the mutual information.\\n        moving_mask : array, shape (S', R', C') or (R', C'), optional\\n            moving image mask that defines which pixels in the moving image\\n            are used to calculate the mutual information.\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='\"\"\"\\n        self.dim = len(static.shape)\\n        self.transform = transform\\n        n = transform.get_number_of_parameters()\\n        self.nparams = n\\n\\n        # ensure that masks are not all zeros\\n        if np.all(static_mask == 0):\\n            warn(\"static_mask is all zeros, setting to None (which means \\\\\\n                  the entire volume will be used)\", UserWarning)\\n            static_mask = None\\n        if np.all(moving_mask == 0):\\n            warn(\"moving_mask is all zeros, setting to None\", UserWarning)\\n            moving_mask = None\\n\\n        # save masks for use elsewhere\\n        self.static_mask, self.moving_mask = static_mask, moving_mask\\n\\n        # multiply images by masks for transform_centers_of_mass\\n        static_masked, moving_masked = static, moving\\n        if static_mask is not None:\\n            static_masked = static*static_mask\\n        if moving_mask is not None:\\n            moving_masked = moving*moving_mask', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='if params0 is None:\\n            params0 = self.transform.get_identity_parameters()\\n        self.params0 = params0\\n        if starting_affine is None:\\n            self.starting_affine = np.eye(self.dim + 1)\\n        elif isinstance(starting_affine, str):\\n            if starting_affine == \\'mass\\':\\n                affine_map = transform_centers_of_mass(static_masked,\\n                                                       static_grid2world,\\n                                                       moving_masked,\\n                                                       moving_grid2world)\\n                self.starting_affine = affine_map.affine\\n                print(\"starting_affine in imaffine:\", self.starting_affine)\\n            elif starting_affine == \\'voxel-origin\\':\\n                affine_map = transform_origins(static, static_grid2world,\\n                                               moving, moving_grid2world)\\n                self.starting_affine = affine_map.affine\\n            elif starting_affine == \\'centers\\':\\n                affine_map = transform_geometric_centers(static,\\n                                                         static_grid2world,\\n                                                         moving,\\n                                                         moving_grid2world)\\n                self.starting_affine = affine_map.affine\\n            else:\\n                raise ValueError(\\'Invalid starting_affine strategy\\')\\n        elif (isinstance(starting_affine, np.ndarray) and\\n              starting_affine.shape >= (self.dim, self.dim + 1)):\\n            self.starting_affine = starting_affine\\n        else:\\n            raise ValueError(\\'Invalid starting_affine matrix\\')', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='# Extract information from affine matrices to create the scale space\\n        static_direction, static_spacing = \\\\\\n            get_direction_and_spacings(static_grid2world, self.dim)\\n        moving_direction, moving_spacing = \\\\\\n            get_direction_and_spacings(moving_grid2world, self.dim)\\n\\n        # Scale the images by min and max values (where mask == 1)\\n        if static_mask is not None:\\n            smin = np.min(static[static_mask == 1])\\n            smax = np.max(static[static_mask == 1])\\n        else:\\n            smin, smax = np.min(static), np.max(static)\\n        static = (static.astype(np.float64) - smin) / (smax - smin)\\n        if moving_mask is not None:\\n            mmin = np.min(moving[moving_mask == 1])\\n            mmax = np.max(moving[moving_mask == 1])\\n        else:\\n            mmin, mmax = np.min(moving), np.max(moving)\\n        moving = (moving.astype(np.float64) - mmin) / (mmax - mmin)\\n\\n        # Build the scale space of the input images\\n        if self.use_isotropic:\\n            self.moving_ss = IsotropicScaleSpace(moving, self.factors,\\n                                                 self.sigmas,\\n                                                 moving_grid2world,\\n                                                 moving_spacing, False)\\n\\n            self.static_ss = IsotropicScaleSpace(static, self.factors,\\n                                                 self.sigmas,\\n                                                 static_grid2world,\\n                                                 static_spacing, False)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='self.static_ss = IsotropicScaleSpace(static, self.factors,\\n                                                 self.sigmas,\\n                                                 static_grid2world,\\n                                                 static_spacing, False)\\n\\n        else:\\n            self.moving_ss = ScaleSpace(moving, self.levels, moving_grid2world,\\n                                        moving_spacing, self.ss_sigma_factor,\\n                                        False)\\n\\n            self.static_ss = ScaleSpace(static, self.levels, static_grid2world,\\n                                        static_spacing, self.ss_sigma_factor,\\n                                        False)\\n\\n    def optimize(self, static, moving, transform, params0,\\n                 static_grid2world=None, moving_grid2world=None,\\n                 starting_affine=None, ret_metric=False,\\n                 static_mask=None, moving_mask=None):\\n        r\"\"\" Start the optimization process.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='Parameters\\n        ----------\\n        static : 2D or 3D array\\n            the image to be used as reference during optimization.\\n        moving : 2D or 3D array\\n            the image to be used as \"moving\" during optimization. It is\\n            necessary to pre-align the moving image to ensure its domain\\n            lies inside the domain of the deformation fields. This is assumed\\n            to be accomplished by \"pre-aligning\" the moving image towards the\\n            static using an affine transformation given by the\\n            \\'starting_affine\\' matrix\\n        transform : instance of Transform\\n            the transformation with respect to whose parameters the gradient\\n            must be computed\\n        params0 : array, shape (n,)\\n            parameters from which to start the optimization. If None, the\\n            optimization will start at the identity transform. n is the\\n            number of parameters of the specified transformation.\\n        static_grid2world : array, shape (dim+1, dim+1), optional\\n            the voxel-to-space transformation associated with the static\\n            image. The default is None, implying the transform is the\\n            identity.\\n        moving_grid2world : array, shape (dim+1, dim+1), optional\\n            the voxel-to-space transformation associated with the moving\\n            image. The default is None, implying the transform is the\\n            identity.\\n        starting_affine : string, or matrix, or None, optional\\n            If string:\\n                \\'mass\\': align centers of gravity\\n                \\'voxel-origin\\': align physical coordinates of voxel (0,0,0)\\n                \\'centers\\': align physical coordinates of central voxels', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content=\"the voxel-to-space transformation associated with the moving\\n            image. The default is None, implying the transform is the\\n            identity.\\n        starting_affine : string, or matrix, or None, optional\\n            If string:\\n                'mass': align centers of gravity\\n                'voxel-origin': align physical coordinates of voxel (0,0,0)\\n                'centers': align physical coordinates of central voxels\\n            If matrix:\\n                array, shape (dim+1, dim+1).\\n            If None:\\n                Start from identity.\\n            The default is None.\\n        ret_metric : boolean, optional\\n            if True, it returns the parameters for measuring the\\n            similarity between the images (default 'False').\\n            The metric containing optimal parameters and\\n            the distance between the images.\\n        static_mask : array, shape (S, R, C) or (R, C), optional\\n            static image mask that defines which pixels in the static image\\n            are used to calculate the mutual information.\\n        moving_mask : array, shape (S', R', C') or (R', C'), optional\\n            moving image mask that defines which pixels in the moving image\\n            are used to calculate the mutual information.\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='Returns\\n        -------\\n        affine_map : instance of AffineMap\\n            the affine resulting affine transformation\\n        xopt : optimal parameters\\n            the optimal parameters (translation, rotation shear etc.)\\n        fopt : Similarity metric\\n            the value of the function at the optimal parameters.\\n\\n        \"\"\"\\n        self._init_optimizer(static, moving, transform, params0,\\n                             static_grid2world, moving_grid2world,\\n                             starting_affine,\\n                             static_mask, moving_mask)\\n        del starting_affine  # Now we must refer to self.starting_affine\\n        del static_mask  # Now we must refer to self.static_mask\\n        del moving_mask  # Now we must refer to self.moving_mask\\n\\n        # Multi-resolution iterations\\n        original_static_shape = self.static_ss.get_image(0).shape\\n        original_static_grid2world = self.static_ss.get_affine(0)\\n        original_moving_shape = self.moving_ss.get_image(0).shape\\n        original_moving_grid2world = self.moving_ss.get_affine(0)\\n        affine_map = AffineMap(None,\\n                               original_static_shape,\\n                               original_static_grid2world,\\n                               original_moving_shape,\\n                               original_moving_grid2world)\\n\\n        for level in range(self.levels - 1, -1, -1):\\n            self.current_level = level\\n            max_iter = self.level_iters[-1 - level]\\n            if self.verbosity >= VerbosityLevels.STATUS:\\n                print(\\'Optimizing level %d [max iter: %d]\\' % (level, max_iter))', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='for level in range(self.levels - 1, -1, -1):\\n            self.current_level = level\\n            max_iter = self.level_iters[-1 - level]\\n            if self.verbosity >= VerbosityLevels.STATUS:\\n                print(\\'Optimizing level %d [max iter: %d]\\' % (level, max_iter))\\n\\n            # Resample the smooth static image to the shape of this level\\n            smooth_static = self.static_ss.get_image(level)\\n            current_static_shape = self.static_ss.get_domain_shape(level)\\n            current_static_grid2world = self.static_ss.get_affine(level)\\n            current_affine_map = AffineMap(None,\\n                                           current_static_shape,\\n                                           current_static_grid2world,\\n                                           original_static_shape,\\n                                           original_static_grid2world)\\n            current_static = current_affine_map.transform(smooth_static)\\n            current_static_mask = None\\n            if self.static_mask is not None:\\n                current_static_mask = current_affine_map.transform(\\n                    self.static_mask, interpolation=\"nearest\").astype(np.int32)\\n\\n            # The moving image is full resolution\\n            current_moving_grid2world = original_moving_grid2world\\n\\n            current_moving = self.moving_ss.get_image(level)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content=\"# The moving image is full resolution\\n            current_moving_grid2world = original_moving_grid2world\\n\\n            current_moving = self.moving_ss.get_image(level)\\n\\n            # Prepare the metric for iterations at this resolution\\n            self.metric.setup(transform, current_static, current_moving,\\n                              current_static_grid2world,\\n                              current_moving_grid2world, self.starting_affine,\\n                              current_static_mask, self.moving_mask)\\n\\n            # Optimize this level\\n            if self.options is None:\\n                self.options = {'gtol': 1e-4,\\n                                'disp': False}\\n\\n            if self.method == 'L-BFGS-B':\\n                self.options['maxfun'] = max_iter\\n            else:\\n                self.options['maxiter'] = max_iter\\n\\n            opt = Optimizer(self.metric.distance_and_gradient,\\n                            self.params0,\\n                            method=self.method, jac=True,\\n                            options=self.options)\\n            params = opt.xopt\\n\\n            # Update starting_affine matrix with optimal parameters\\n            T = self.transform.param_to_matrix(params)\\n            self.starting_affine = T.dot(self.starting_affine)\\n\\n            # Start next iteration at identity\\n            self.params0 = self.transform.get_identity_parameters()\\n\\n        affine_map.set_affine(self.starting_affine)\\n        if ret_metric:\\n            return affine_map, opt.xopt, opt.fopt\\n        return affine_map\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='# Update starting_affine matrix with optimal parameters\\n            T = self.transform.param_to_matrix(params)\\n            self.starting_affine = T.dot(self.starting_affine)\\n\\n            # Start next iteration at identity\\n            self.params0 = self.transform.get_identity_parameters()\\n\\n        affine_map.set_affine(self.starting_affine)\\n        if ret_metric:\\n            return affine_map, opt.xopt, opt.fopt\\n        return affine_map\\n\\n\\ndef transform_centers_of_mass(static, static_grid2world,\\n                              moving, moving_grid2world):\\n    r\"\"\" Transformation to align the center of mass of the input images.\\n\\n    Parameters\\n    ----------\\n    static : array, shape (S, R, C)\\n        static image\\n    static_grid2world : array, shape (dim+1, dim+1)\\n        the voxel-to-space transformation of the static image\\n    moving : array, shape (S, R, C)\\n        moving image\\n    moving_grid2world : array, shape (dim+1, dim+1)\\n        the voxel-to-space transformation of the moving image\\n\\n    Returns\\n    -------\\n    affine_map : instance of AffineMap\\n        the affine transformation (translation only, in this case) aligning\\n        the center of mass of the moving image towards the one of the static\\n        image', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    affine_map : instance of AffineMap\\n        the affine transformation (translation only, in this case) aligning\\n        the center of mass of the moving image towards the one of the static\\n        image\\n\\n    \"\"\"\\n    dim = len(static.shape)\\n    if static_grid2world is None:\\n        static_grid2world = np.eye(dim + 1)\\n    if moving_grid2world is None:\\n        moving_grid2world = np.eye(dim + 1)\\n    c_static = ndimage.center_of_mass(np.array(static))\\n    c_static = static_grid2world.dot(c_static + (1,))\\n    c_moving = ndimage.center_of_mass(np.array(moving))\\n    c_moving = moving_grid2world.dot(c_moving + (1,))\\n    transform = np.eye(dim + 1)\\n    transform[:dim, dim] = (c_moving - c_static)[:dim]\\n    affine_map = AffineMap(transform,\\n                           static.shape, static_grid2world,\\n                           moving.shape, moving_grid2world)\\n    return affine_map\\n\\n\\ndef transform_geometric_centers(static, static_grid2world,\\n                                moving, moving_grid2world):\\n    r\"\"\" Transformation to align the geometric center of the input images.\\n\\n    With \"geometric center\" of a volume we mean the physical coordinates of\\n    its central voxel\\n\\n    Parameters\\n    ----------\\n    static : array, shape (S, R, C)\\n        static image\\n    static_grid2world : array, shape (dim+1, dim+1)\\n        the voxel-to-space transformation of the static image\\n    moving : array, shape (S, R, C)\\n        moving image\\n    moving_grid2world : array, shape (dim+1, dim+1)\\n        the voxel-to-space transformation of the moving image', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='With \"geometric center\" of a volume we mean the physical coordinates of\\n    its central voxel\\n\\n    Parameters\\n    ----------\\n    static : array, shape (S, R, C)\\n        static image\\n    static_grid2world : array, shape (dim+1, dim+1)\\n        the voxel-to-space transformation of the static image\\n    moving : array, shape (S, R, C)\\n        moving image\\n    moving_grid2world : array, shape (dim+1, dim+1)\\n        the voxel-to-space transformation of the moving image\\n\\n    Returns\\n    -------\\n    affine_map : instance of AffineMap\\n        the affine transformation (translation only, in this case) aligning\\n        the geometric center of the moving image towards the one of the static\\n        image\\n\\n    \"\"\"\\n    dim = len(static.shape)\\n    if static_grid2world is None:\\n        static_grid2world = np.eye(dim + 1)\\n    if moving_grid2world is None:\\n        moving_grid2world = np.eye(dim + 1)\\n    c_static = tuple((np.array(static.shape, dtype=np.float64)) * 0.5)\\n    c_static = static_grid2world.dot(c_static + (1,))\\n    c_moving = tuple((np.array(moving.shape, dtype=np.float64)) * 0.5)\\n    c_moving = moving_grid2world.dot(c_moving + (1,))\\n    transform = np.eye(dim + 1)\\n    transform[:dim, dim] = (c_moving - c_static)[:dim]\\n    affine_map = AffineMap(transform,\\n                           static.shape, static_grid2world,\\n                           moving.shape, moving_grid2world)\\n    return affine_map\\n\\n\\ndef transform_origins(static, static_grid2world,\\n                      moving, moving_grid2world):\\n    r\"\"\" Transformation to align the origins of the input images.\\n\\n    With \"origin\" of a volume we mean the physical coordinates of\\n    voxel (0,0,0)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='def transform_origins(static, static_grid2world,\\n                      moving, moving_grid2world):\\n    r\"\"\" Transformation to align the origins of the input images.\\n\\n    With \"origin\" of a volume we mean the physical coordinates of\\n    voxel (0,0,0)\\n\\n    Parameters\\n    ----------\\n    static : array, shape (S, R, C)\\n        static image\\n    static_grid2world : array, shape (dim+1, dim+1)\\n        the voxel-to-space transformation of the static image\\n    moving : array, shape (S, R, C)\\n        moving image\\n    moving_grid2world : array, shape (dim+1, dim+1)\\n        the voxel-to-space transformation of the moving image\\n\\n    Returns\\n    -------\\n    affine_map : instance of AffineMap\\n        the affine transformation (translation only, in this case) aligning\\n        the origin of the moving image towards the one of the static\\n        image\\n\\n    \"\"\"\\n    dim = len(static.shape)\\n    if static_grid2world is None:\\n        static_grid2world = np.eye(dim + 1)\\n    if moving_grid2world is None:\\n        moving_grid2world = np.eye(dim + 1)\\n    c_static = static_grid2world[:dim, dim]\\n    c_moving = moving_grid2world[:dim, dim]\\n    transform = np.eye(dim + 1)\\n    transform[:dim, dim] = (c_moving - c_static)[:dim]\\n    affine_map = AffineMap(transform,\\n                           static.shape, static_grid2world,\\n                           moving.shape, moving_grid2world)\\n    return affine_map', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imaffine.py.txt'}),\n",
       " Document(page_content='\"\"\"  Classes and functions for Symmetric Diffeomorphic Registration \"\"\"\\n\\nimport logging\\nimport abc\\n\\nimport numpy as np\\nimport numpy.linalg as npl\\nimport nibabel as nib\\nfrom nibabel.streamlines import ArraySequence as Streamlines\\n\\nfrom dipy.align import vector_fields as vfu\\nfrom dipy.align import floating\\nfrom dipy.align import VerbosityLevels\\nfrom dipy.align import Bunch\\nfrom dipy.align.scalespace import ScaleSpace\\n\\nRegistrationStages = Bunch(INIT_START=0,\\n                           INIT_END=1,\\n                           OPT_START=2,\\n                           OPT_END=3,\\n                           SCALE_START=4,\\n                           SCALE_END=5,\\n                           ITER_START=6,\\n                           ITER_END=7)\\n\"\"\"Registration Stages\\n\\nThis enum defines the different stages which the Volumetric Registration\\nmay be in. The value of the stage is passed as a parameter to the call-back\\nfunction so that it can react accordingly.\\n\\nINIT_START: optimizer initialization starts\\nINIT_END: optimizer initialization ends\\nOPT_START: optimization starts\\nOPT_END: optimization ends\\nSCALE_START: optimization at a new scale space resolution starts\\nSCALE_END: optimization at the current scale space resolution ends\\nITER_START: a new iteration starts\\nITER_END: the current iteration ends\\n\"\"\"\\n\\nlogger = logging.getLogger(__name__)\\n\\ndef mult_aff(A, B):\\n    \"\"\"Returns the matrix product A.dot(B) considering None as the identity\\n\\n    Parameters\\n    ----------\\n    A : array, shape (n,k)\\n    B : array, shape (k,m)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='logger = logging.getLogger(__name__)\\n\\ndef mult_aff(A, B):\\n    \"\"\"Returns the matrix product A.dot(B) considering None as the identity\\n\\n    Parameters\\n    ----------\\n    A : array, shape (n,k)\\n    B : array, shape (k,m)\\n\\n    Returns\\n    -------\\n    The matrix product A.dot(B). If any of the input matrices is None, it is\\n    treated as the identity matrix. If both matrices are None, None is returned\\n    \"\"\"\\n    if A is None:\\n        return B\\n    elif B is None:\\n        return A\\n    return A.dot(B)\\n\\n\\ndef get_direction_and_spacings(affine, dim):\\n    \"\"\"Extracts the rotational and spacing components from a matrix\\n\\n    Extracts the rotational and spacing (voxel dimensions) components from a\\n    matrix. An image gradient represents the local variation of the image\\'s\\n    gray values per voxel. Since we are iterating on the physical space, we\\n    need to compute the gradients as variation per millimeter, so we need to\\n    divide each gradient\\'s component by the voxel size along the corresponding\\n    axis, that\\'s what the spacings are used for. Since the image\\'s gradients\\n    are oriented along the grid axes, we also need to re-orient the gradients\\n    to be given in physical space coordinates.\\n\\n    Parameters\\n    ----------\\n    affine : array, shape (k, k), k = 3, 4\\n        the matrix transforming grid coordinates to physical space.\\n\\n    Returns\\n    -------\\n    direction : array, shape (k-1, k-1)\\n        the rotational component of the input matrix\\n    spacings : array, shape (k-1,)\\n        the scaling component (voxel size) of the matrix', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='Parameters\\n    ----------\\n    affine : array, shape (k, k), k = 3, 4\\n        the matrix transforming grid coordinates to physical space.\\n\\n    Returns\\n    -------\\n    direction : array, shape (k-1, k-1)\\n        the rotational component of the input matrix\\n    spacings : array, shape (k-1,)\\n        the scaling component (voxel size) of the matrix\\n\\n    \"\"\"\\n    if affine is None:\\n        return np.eye(dim), np.ones(dim)\\n    dim = affine.shape[1]-1\\n    # Temporary hack: get the zooms by building a nifti image\\n    affine4x4 = np.eye(4)\\n    empty_volume = np.zeros((0, 0, 0))\\n    affine4x4[:dim, :dim] = affine[:dim, :dim]\\n    affine4x4[:dim, 3] = affine[:dim, dim-1]\\n    nib_nifti = nib.Nifti1Image(empty_volume, affine4x4)\\n    scalings = np.asarray(nib_nifti.header.get_zooms())\\n    scalings = np.asarray(scalings[:dim], dtype=np.float64)\\n    A = affine[:dim, :dim]\\n    return A.dot(np.diag(1.0/scalings)), scalings\\n\\n\\nclass DiffeomorphicMap:\\n    def __init__(self,\\n                 dim,\\n                 disp_shape,\\n                 disp_grid2world=None,\\n                 domain_shape=None,\\n                 domain_grid2world=None,\\n                 codomain_shape=None,\\n                 codomain_grid2world=None,\\n                 prealign=None):\\n        \"\"\" DiffeomorphicMap', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='class DiffeomorphicMap:\\n    def __init__(self,\\n                 dim,\\n                 disp_shape,\\n                 disp_grid2world=None,\\n                 domain_shape=None,\\n                 domain_grid2world=None,\\n                 codomain_shape=None,\\n                 codomain_grid2world=None,\\n                 prealign=None):\\n        \"\"\" DiffeomorphicMap\\n\\n        Implements a diffeomorphic transformation on the physical space. The\\n        deformation fields encoding the direct and inverse transformations\\n        share the same domain discretization (both the discretization grid\\n        shape and voxel-to-space matrix). The input coordinates (physical\\n        coordinates) are first aligned using prealign, and then displaced\\n        using the corresponding vector field interpolated at the aligned\\n        coordinates.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='Parameters\\n        ----------\\n        dim : int, 2 or 3\\n            the transformation\\'s dimension\\n        disp_shape : array, shape (dim,)\\n            the number of slices (if 3D), rows and columns of the deformation\\n            field\\'s discretization\\n        disp_grid2world : the voxel-to-space transform between the def. fields\\n            grid and space\\n        domain_shape : array, shape (dim,)\\n            the number of slices (if 3D), rows and columns of the default\\n            discretization of this map\\'s domain\\n        domain_grid2world : array, shape (dim+1, dim+1)\\n            the default voxel-to-space transformation between this map\\'s\\n            discretization and physical space\\n        codomain_shape : array, shape (dim,)\\n            the number of slices (if 3D), rows and columns of the images that\\n            are \\'normally\\' warped using this transformation in the forward\\n            direction (this will provide default transformation parameters to\\n            warp images under this transformation). By default, we assume that\\n            the inverse transformation is \\'normally\\' used to warp images with\\n            the same discretization and voxel-to-space transformation as the\\n            deformation field grid.\\n        codomain_grid2world : array, shape (dim+1, dim+1)\\n            the voxel-to-space transformation of images that are \\'normally\\'\\n            warped using this transformation (in the forward direction).\\n        prealign : array, shape (dim+1, dim+1)\\n            the linear transformation to be applied to align input images to\\n            the reference space before warping under the deformation field.\\n\\n        \"\"\"\\n\\n        self.dim = dim', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='\"\"\"\\n\\n        self.dim = dim\\n\\n        if disp_shape is None:\\n            raise ValueError(\"Invalid displacement field discretization\")\\n\\n        self.disp_shape = np.asarray(disp_shape, dtype=np.int32)\\n\\n        # If the discretization affine is None, we assume it\\'s the identity\\n        self.disp_grid2world = disp_grid2world\\n        if self.disp_grid2world is None:\\n            self.disp_world2grid = None\\n        else:\\n            self.disp_world2grid = npl.inv(self.disp_grid2world)\\n\\n        # If domain_shape isn\\'t provided, we use the map\\'s discretization shape\\n        if domain_shape is None:\\n            self.domain_shape = self.disp_shape\\n        else:\\n            self.domain_shape = np.asarray(domain_shape, dtype=np.int32)\\n        self.domain_grid2world = domain_grid2world\\n        if domain_grid2world is None:\\n            self.domain_world2grid = None\\n        else:\\n            self.domain_world2grid = npl.inv(domain_grid2world)\\n\\n        # If codomain shape was not provided, we assume it is an endomorphism:\\n        # use the same domain_shape and codomain_grid2world as the field domain\\n        if codomain_shape is None:\\n            self.codomain_shape = self.domain_shape\\n        else:\\n            self.codomain_shape = np.asarray(codomain_shape, dtype=np.int32)\\n        self.codomain_grid2world = codomain_grid2world\\n        if codomain_grid2world is None:\\n            self.codomain_world2grid = None\\n        else:\\n            self.codomain_world2grid = npl.inv(codomain_grid2world)\\n\\n        self.prealign = prealign\\n        if prealign is None:\\n            self.prealign_inv = None\\n        else:\\n            self.prealign_inv = npl.inv(prealign)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='self.prealign = prealign\\n        if prealign is None:\\n            self.prealign_inv = None\\n        else:\\n            self.prealign_inv = npl.inv(prealign)\\n\\n        self.is_inverse = False\\n        self.forward = None\\n        self.backward = None\\n\\n    def interpret_matrix(self, obj):\\n        \"\"\" Try to interpret `obj` as a matrix\\n\\n        Some operations are performed faster if we know in advance if a matrix\\n        is the identity (so we can skip the actual matrix-vector\\n        multiplication). This function returns None if the given object\\n        is None or the \\'identity\\' string. It returns the same object if it is\\n        a numpy array. It raises an exception otherwise.\\n\\n        Parameters\\n        ----------\\n        obj : object\\n            any object\\n\\n        Returns\\n        -------\\n        obj : object\\n            the same object given as argument if `obj` is None or a numpy\\n            array. None if `obj` is the \\'identity\\' string.\\n        \"\"\"\\n        if (obj is None) or isinstance(obj, np.ndarray):\\n            return obj\\n        if isinstance(obj, str) and (obj == \\'identity\\'):\\n            return None\\n        raise ValueError(\\'Invalid matrix\\')\\n\\n    def get_forward_field(self):\\n        \"\"\"Deformation field to transform an image in the forward direction\\n\\n        Returns the deformation field that must be used to warp an image under\\n        this transformation in the forward direction (note the \\'is_inverse\\'\\n        flag).\\n        \"\"\"\\n        if self.is_inverse:\\n            return self.backward\\n        else:\\n            return self.forward\\n\\n    def get_backward_field(self):\\n        \"\"\"Deformation field to transform an image in the backward direction', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='Returns the deformation field that must be used to warp an image under\\n        this transformation in the forward direction (note the \\'is_inverse\\'\\n        flag).\\n        \"\"\"\\n        if self.is_inverse:\\n            return self.backward\\n        else:\\n            return self.forward\\n\\n    def get_backward_field(self):\\n        \"\"\"Deformation field to transform an image in the backward direction\\n\\n        Returns the deformation field that must be used to warp an image under\\n        this transformation in the backward direction (note the \\'is_inverse\\'\\n        flag).\\n        \"\"\"\\n        if self.is_inverse:\\n            return self.forward\\n        else:\\n            return self.backward\\n\\n    def allocate(self):\\n        \"\"\"Creates a zero displacement field\\n\\n        Creates a zero displacement field (the identity transformation).\\n        \"\"\"\\n        self.forward = np.zeros(tuple(self.disp_shape) + (self.dim,),\\n                                dtype=floating)\\n        self.backward = np.zeros(tuple(self.disp_shape) + (self.dim,),\\n                                 dtype=floating)\\n\\n    def _get_warping_function(self, interpolation, warp_coordinates=False):\\n        r\"\"\"Appropriate warping function for the given interpolation type\\n\\n        Returns the right warping function from vector_fields that must be\\n        called for the specified data dimension and interpolation type', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='def _get_warping_function(self, interpolation, warp_coordinates=False):\\n        r\"\"\"Appropriate warping function for the given interpolation type\\n\\n        Returns the right warping function from vector_fields that must be\\n        called for the specified data dimension and interpolation type\\n\\n        Parameters\\n        ----------\\n        interpolation : string, either \\'linear\\' or \\'nearest\\'\\n            specifies the type of interpolation used for image warping. It\\n            does not have any effect if `warp_coordinates` is True, in which\\n            case no interpolation is intended to be performed.\\n        warp_coordinates : Boolean,\\n            if False, then returns the right image warping function for this\\n            DiffeomorphicMap dimension and the specified `interpolation`. If\\n            True, then returns the right coordinate warping function.\\n        \"\"\"\\n        if self.dim == 2:\\n            if warp_coordinates:\\n                return vfu.warp_coordinates_2d\\n            if interpolation == \\'linear\\':\\n                return vfu.warp_2d\\n            else:\\n                return vfu.warp_2d_nn\\n        else:\\n            if warp_coordinates:\\n                return vfu.warp_coordinates_3d\\n            if interpolation == \\'linear\\':\\n                return vfu.warp_3d\\n            else:\\n                return vfu.warp_3d_nn\\n\\n    def _warp_coordinates_forward(self, points, coord2world=None,\\n                                  world2coord=None):\\n        r\"\"\"Warps the list of points in the forward direction', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='def _warp_coordinates_forward(self, points, coord2world=None,\\n                                  world2coord=None):\\n        r\"\"\"Warps the list of points in the forward direction\\n\\n        Applies this diffeomorphic map to the list of points given by `points`.\\n        We assume that the points\\' coordinates are mapped to world coordinates\\n        by applying the `coord2world` affine transform. The warped coordinates\\n        are given in world coordinates unless `world2coord` affine transform\\n        is specified, in which case the warped points will be transformed\\n        to the corresponding coordinate system.\\n\\n        Parameters\\n        ----------\\n        points :\\n        coord2world :\\n        world2coord :\\n        \"\"\"\\n        warp_f = self._get_warping_function(None, warp_coordinates=True)\\n        coord2prealigned = mult_aff(self.prealign, coord2world)\\n        out = warp_f(points, self.forward, coord2prealigned, world2coord,\\n                     self.disp_world2grid)\\n        return out\\n\\n    def _warp_coordinates_backward(self, points, coord2world=None,\\n                                   world2coord=None):\\n        \"\"\"Warps the list of points in the backward direction\\n\\n        Applies this diffeomorphic map to the list of points given by `points`.\\n        We assume that the points\\' coordinates are mapped to world coordinates\\n        by applying the `coord2world` affine transform. The warped coordinates\\n        are given in world coordinates unless `world2coord` affine transform\\n        is specified, in which case the warped points will be transformed\\n        to the corresponding coordinate system.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='Applies this diffeomorphic map to the list of points given by `points`.\\n        We assume that the points\\' coordinates are mapped to world coordinates\\n        by applying the `coord2world` affine transform. The warped coordinates\\n        are given in world coordinates unless `world2coord` affine transform\\n        is specified, in which case the warped points will be transformed\\n        to the corresponding coordinate system.\\n\\n        Parameters\\n        ----------\\n        points :\\n        coord2world :\\n        world2coord :\\n        \"\"\"\\n        warp_f = self._get_warping_function(None, warp_coordinates=True)\\n        world2invprealigned = mult_aff(world2coord, self.prealign_inv)\\n        out = warp_f(points, self.backward, coord2world, world2invprealigned,\\n                     self.disp_world2grid)\\n        return out\\n\\n    def _warp_forward(self, image, interpolation=\\'linear\\',\\n                      image_world2grid=None, out_shape=None,\\n                      out_grid2world=None):\\n        \"\"\"Warps an image in the forward direction\\n\\n        Deforms the input image under this diffeomorphic map in the forward\\n        direction. Since the mapping is defined in the physical space, the user\\n        must specify the sampling grid shape and its space-to-voxel mapping.\\n        By default, the transformation will use the discretization information\\n        given at initialization.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content=\"Deforms the input image under this diffeomorphic map in the forward\\n        direction. Since the mapping is defined in the physical space, the user\\n        must specify the sampling grid shape and its space-to-voxel mapping.\\n        By default, the transformation will use the discretization information\\n        given at initialization.\\n\\n        Parameters\\n        ----------\\n        image : array, shape (s, r, c) if dim = 3 or (r, c) if dim = 2\\n            the image to be warped under this transformation in the forward\\n            direction\\n        interpolation : string, either 'linear' or 'nearest'\\n            the type of interpolation to be used for warping, either 'linear'\\n            (for k-linear interpolation) or 'nearest' for nearest neighbor\\n        image_world2grid : array, shape (dim+1, dim+1)\\n            the transformation bringing world (space) coordinates to voxel\\n            coordinates of the image given as input\\n        out_shape : array, shape (dim,)\\n            the number of slices, rows, and columns of the desired warped image\\n        out_grid2world : the transformation bringing voxel coordinates of the\\n            warped image to physical space\\n\\n        Returns\\n        -------\\n        warped : array, shape = out_shape or self.codomain_shape if None\\n            the warped image under this transformation in the forward direction\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='Returns\\n        -------\\n        warped : array, shape = out_shape or self.codomain_shape if None\\n            the warped image under this transformation in the forward direction\\n\\n        Notes\\n        -----\\n        A diffeomorphic map must be thought as a mapping between points\\n        in space. Warping an image J towards an image I means transforming\\n        each voxel with (discrete) coordinates i in I to (floating-point) voxel\\n        coordinates j in J. The transformation we consider \\'forward\\' is\\n        precisely mapping coordinates i from the input image to coordinates j\\n        from reference image, which has the effect of warping an image with\\n        reference discretization (typically, the \"static image\") \"towards\" an\\n        image with input discretization (typically, the \"moving image\"). More\\n        precisely, the warped image is produced by the following interpolation:\\n\\n        warped[i] = image[W * forward[Dinv * P * S * i] + W * P * S * i )]\\n\\n        where i denotes the coordinates of a voxel in the input grid, W is\\n        the world-to-grid transformation of the image given as input, Dinv\\n        is the world-to-grid transformation of the deformation field\\n        discretization, P is the pre-aligning matrix (transforming input\\n        points to reference points), S is the voxel-to-space transformation of\\n        the sampling grid (see comment below) and forward is the forward\\n        deformation field.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='where i denotes the coordinates of a voxel in the input grid, W is\\n        the world-to-grid transformation of the image given as input, Dinv\\n        is the world-to-grid transformation of the deformation field\\n        discretization, P is the pre-aligning matrix (transforming input\\n        points to reference points), S is the voxel-to-space transformation of\\n        the sampling grid (see comment below) and forward is the forward\\n        deformation field.\\n\\n        If we want to warp an image, we also must specify on what grid we\\n        want to sample the resulting warped image (the images are considered as\\n        points in space and its representation on a grid depends on its\\n        grid-to-space transform telling us for each grid voxel what point in\\n        space we need to bring via interpolation). So, S is the matrix that\\n        converts the sampling grid (whose shape is given as parameter\\n        \\'out_shape\\' ) to space coordinates.\\n        \"\"\"\\n        # if no world-to-image transform is provided, we use the codomain info\\n        if image_world2grid is None:\\n            image_world2grid = self.codomain_world2grid\\n        # if no sampling info is provided, we use the domain info\\n        if out_shape is None:\\n            if self.domain_shape is None:\\n                raise ValueError(\\'Unable to infer sampling info. \\'\\n                                 \\'Provide a valid out_shape.\\')\\n            out_shape = self.domain_shape\\n        else:\\n            out_shape = np.asarray(out_shape, dtype=np.int32)\\n        if out_grid2world is None:\\n            out_grid2world = self.domain_grid2world', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='W = self.interpret_matrix(image_world2grid)\\n        Dinv = self.disp_world2grid\\n        P = self.prealign\\n        S = self.interpret_matrix(out_grid2world)\\n\\n        # this is the matrix which we need to multiply the voxel coordinates\\n        # to interpolate on the forward displacement field (\"in\"side the\\n        # \\'forward\\' brackets in the expression above)\\n        affine_idx_in = mult_aff(Dinv, mult_aff(P, S))\\n\\n        # this is the matrix which we need to multiply the voxel coordinates\\n        # to add to the displacement (\"out\"side the \\'forward\\' brackets in the\\n        # expression above)\\n        affine_idx_out = mult_aff(W, mult_aff(P, S))\\n\\n        # this is the matrix which we need to multiply the displacement vector\\n        # prior to adding to the transformed input point\\n        affine_disp = W\\n\\n        # Convert the data to required types to use the cythonized functions\\n        if interpolation == \\'nearest\\':\\n            if image.dtype is np.dtype(\\'float64\\') and floating is np.float32:\\n                image = image.astype(floating)\\n            elif image.dtype is np.dtype(\\'int64\\'):\\n                image = image.astype(np.int32)\\n        else:\\n            image = np.asarray(image, dtype=floating)\\n\\n        warp_f = self._get_warping_function(interpolation)\\n\\n        warped = warp_f(image, self.forward, affine_idx_in, affine_idx_out,\\n                        affine_disp, out_shape)\\n        return warped\\n\\n    def _warp_backward(self, image, interpolation=\\'linear\\',\\n                       image_world2grid=None, out_shape=None,\\n                       out_grid2world=None):\\n        \"\"\"Warps an image in the backward direction', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='warp_f = self._get_warping_function(interpolation)\\n\\n        warped = warp_f(image, self.forward, affine_idx_in, affine_idx_out,\\n                        affine_disp, out_shape)\\n        return warped\\n\\n    def _warp_backward(self, image, interpolation=\\'linear\\',\\n                       image_world2grid=None, out_shape=None,\\n                       out_grid2world=None):\\n        \"\"\"Warps an image in the backward direction\\n\\n        Deforms the input image under this diffeomorphic map in the backward\\n        direction. Since the mapping is defined in the physical space, the user\\n        must specify the sampling grid shape and its space-to-voxel mapping.\\n        By default, the transformation will use the discretization information\\n        given at initialization.\\n\\n        Parameters\\n        ----------\\n        image : array, shape (s, r, c) if dim = 3 or (r, c) if dim = 2\\n            the image to be warped under this transformation in the backward\\n            direction\\n        interpolation : string, either \\'linear\\' or \\'nearest\\'\\n            the type of interpolation to be used for warping, either \\'linear\\'\\n            (for k-linear interpolation) or \\'nearest\\' for nearest neighbor\\n        image_world2grid : array, shape (dim+1, dim+1)\\n            the transformation bringing world (space) coordinates to voxel\\n            coordinates of the image given as input\\n        out_shape : array, shape (dim,)\\n            the number of slices, rows and columns of the desired warped image\\n        out_grid2world : the transformation bringing voxel coordinates of the\\n            warped image to physical space', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='Returns\\n        -------\\n        warped : array, shape = out_shape or self.domain_shape if None\\n            the warped image under this transformation in the backward\\n            direction\\n\\n        Notes\\n        -----\\n        A diffeomorphic map must be thought as a mapping between points\\n        in space. Warping an image J towards an image I means transforming\\n        each voxel with (discrete) coordinates i in I to (floating-point) voxel\\n        coordinates j in J. The transformation we consider \\'backward\\' is\\n        precisely mapping coordinates i from the reference grid to coordinates\\n        j from the input image (that\\'s why it\\'s \"backward\"), which has the\\n        effect of warping the input image (moving) \"towards\" the reference.\\n        More precisely, the warped image is produced by the following\\n        interpolation:\\n\\n        warped[i]=image[W * Pinv * backward[Dinv * S * i] + W * Pinv * S * i )]\\n\\n        where i denotes the coordinates of a voxel in the input grid, W is\\n        the world-to-grid transformation of the image given as input, Dinv\\n        is the world-to-grid transformation of the deformation field\\n        discretization, Pinv is the pre-aligning matrix\\'s inverse (transforming\\n        reference points to input points), S is the grid-to-space\\n        transformation of the sampling grid (see comment below) and backward is\\n        the backward deformation field.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='where i denotes the coordinates of a voxel in the input grid, W is\\n        the world-to-grid transformation of the image given as input, Dinv\\n        is the world-to-grid transformation of the deformation field\\n        discretization, Pinv is the pre-aligning matrix\\'s inverse (transforming\\n        reference points to input points), S is the grid-to-space\\n        transformation of the sampling grid (see comment below) and backward is\\n        the backward deformation field.\\n\\n        If we want to warp an image, we also must specify on what grid we\\n        want to sample the resulting warped image (the images are considered as\\n        points in space and its representation on a grid depends on its\\n        grid-to-space transform telling us for each grid voxel what point in\\n        space we need to bring via interpolation). So, S is the matrix that\\n        converts the sampling grid (whose shape is given as parameter\\n        \\'out_shape\\' ) to space coordinates.\\n\\n        \"\"\"\\n        # if no world-to-image transform is provided, we use the domain info\\n        if image_world2grid is None:\\n            image_world2grid = self.domain_world2grid\\n\\n        # if no sampling info is provided, we use the codomain info\\n        if out_shape is None:\\n            if self.codomain_shape is None:\\n                msg = \\'Unknown sampling info. Provide a valid out_shape.\\'\\n                raise ValueError(msg)\\n            out_shape = self.codomain_shape\\n        if out_grid2world is None:\\n            out_grid2world = self.codomain_grid2world', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='# if no sampling info is provided, we use the codomain info\\n        if out_shape is None:\\n            if self.codomain_shape is None:\\n                msg = \\'Unknown sampling info. Provide a valid out_shape.\\'\\n                raise ValueError(msg)\\n            out_shape = self.codomain_shape\\n        if out_grid2world is None:\\n            out_grid2world = self.codomain_grid2world\\n\\n        W = self.interpret_matrix(image_world2grid)\\n        Dinv = self.disp_world2grid\\n        Pinv = self.prealign_inv\\n        S = self.interpret_matrix(out_grid2world)\\n\\n        # this is the matrix which we need to multiply the voxel coordinates\\n        # to interpolate on the backward displacement field (\"in\"side the\\n        # \\'backward\\' brackets in the expression above)\\n        affine_idx_in = mult_aff(Dinv, S)\\n\\n        # this is the matrix which we need to multiply the voxel coordinates\\n        # to add to the displacement (\"out\"side the \\'backward\\' brackets in the\\n        # expression above)\\n        affine_idx_out = mult_aff(W, mult_aff(Pinv, S))\\n\\n        # this is the matrix which we need to multiply the displacement vector\\n        # prior to adding to the transformed input point\\n        affine_disp = mult_aff(W, Pinv)\\n\\n        if interpolation == \\'nearest\\':\\n            if image.dtype is np.dtype(\\'float64\\') and floating is np.float32:\\n                image = image.astype(floating)\\n            elif image.dtype is np.dtype(\\'int64\\'):\\n                image = image.astype(np.int32)\\n        else:\\n            image = np.asarray(image, dtype=floating)\\n\\n        warp_f = self._get_warping_function(interpolation)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='if interpolation == \\'nearest\\':\\n            if image.dtype is np.dtype(\\'float64\\') and floating is np.float32:\\n                image = image.astype(floating)\\n            elif image.dtype is np.dtype(\\'int64\\'):\\n                image = image.astype(np.int32)\\n        else:\\n            image = np.asarray(image, dtype=floating)\\n\\n        warp_f = self._get_warping_function(interpolation)\\n\\n        warped = warp_f(image, self.backward, affine_idx_in, affine_idx_out,\\n                        affine_disp, out_shape)\\n\\n        return warped\\n\\n    def transform(self, image, interpolation=\\'linear\\', image_world2grid=None,\\n                  out_shape=None, out_grid2world=None):\\n        \"\"\"Warps an image in the forward direction\\n\\n        Transforms the input image under this transformation in the forward\\n        direction. It uses the \"is_inverse\" flag to switch between \"forward\"\\n        and \"backward\" (if is_inverse is False, then transform(...) warps the\\n        image forwards, else it warps the image backwards).', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='def transform(self, image, interpolation=\\'linear\\', image_world2grid=None,\\n                  out_shape=None, out_grid2world=None):\\n        \"\"\"Warps an image in the forward direction\\n\\n        Transforms the input image under this transformation in the forward\\n        direction. It uses the \"is_inverse\" flag to switch between \"forward\"\\n        and \"backward\" (if is_inverse is False, then transform(...) warps the\\n        image forwards, else it warps the image backwards).\\n\\n        Parameters\\n        ----------\\n        image : array, shape (s, r, c) if dim = 3 or (r, c) if dim = 2\\n            the image to be warped under this transformation in the forward\\n            direction\\n        interpolation : string, either \\'linear\\' or \\'nearest\\'\\n            the type of interpolation to be used for warping, either \\'linear\\'\\n            (for k-linear interpolation) or \\'nearest\\' for nearest neighbor\\n        image_world2grid : array, shape (dim+1, dim+1)\\n            the transformation bringing world (space) coordinates to voxel\\n            coordinates of the image given as input\\n        out_shape : array, shape (dim,)\\n            the number of slices, rows and columns of the desired warped image\\n        out_grid2world : the transformation bringing voxel coordinates of the\\n            warped image to physical space\\n\\n        Returns\\n        -------\\n        warped : array, shape = out_shape or self.codomain_shape if None\\n            the warped image under this transformation in the forward direction', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='Returns\\n        -------\\n        warped : array, shape = out_shape or self.codomain_shape if None\\n            the warped image under this transformation in the forward direction\\n\\n        Notes\\n        -----\\n        See _warp_forward and _warp_backward documentation for further\\n        information.\\n        \"\"\"\\n        if out_shape is not None:\\n            out_shape = np.asarray(out_shape, dtype=np.int32)\\n        if self.is_inverse:\\n            warped = self._warp_backward(image, interpolation,\\n                                         image_world2grid, out_shape,\\n                                         out_grid2world)\\n        else:\\n            warped = self._warp_forward(image, interpolation, image_world2grid,\\n                                        out_shape, out_grid2world)\\n        return np.asarray(warped)\\n\\n    def transform_inverse(self, image, interpolation=\\'linear\\',\\n                          image_world2grid=None, out_shape=None,\\n                          out_grid2world=None):\\n        \"\"\"Warps an image in the backward direction\\n\\n        Transforms the input image under this transformation in the backward\\n        direction. It uses the \"is_inverse\" flag to switch between \"forward\"\\n        and \"backward\" (if is_inverse is False, then transform_inverse(...)\\n        warps the image backwards, else it warps the image forwards)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='Transforms the input image under this transformation in the backward\\n        direction. It uses the \"is_inverse\" flag to switch between \"forward\"\\n        and \"backward\" (if is_inverse is False, then transform_inverse(...)\\n        warps the image backwards, else it warps the image forwards)\\n\\n        Parameters\\n        ----------\\n        image : array, shape (s, r, c) if dim = 3 or (r, c) if dim = 2\\n            the image to be warped under this transformation in the forward\\n            direction\\n        interpolation : string, either \\'linear\\' or \\'nearest\\'\\n            the type of interpolation to be used for warping, either \\'linear\\'\\n            (for k-linear interpolation) or \\'nearest\\' for nearest neighbor\\n        image_world2grid : array, shape (dim+1, dim+1)\\n            the transformation bringing world (space) coordinates to voxel\\n            coordinates of the image given as input\\n        out_shape : array, shape (dim,)\\n            the number of slices, rows, and columns of the desired warped image\\n        out_grid2world : the transformation bringing voxel coordinates of the\\n            warped image to physical space\\n\\n        Returns\\n        -------\\n        warped : array, shape = out_shape or self.codomain_shape if None\\n            warped image under this transformation in the backward direction', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='Returns\\n        -------\\n        warped : array, shape = out_shape or self.codomain_shape if None\\n            warped image under this transformation in the backward direction\\n\\n        Notes\\n        -----\\n        See _warp_forward and _warp_backward documentation for further\\n        information.\\n        \"\"\"\\n        if self.is_inverse:\\n            warped = self._warp_forward(image, interpolation, image_world2grid,\\n                                        out_shape, out_grid2world)\\n        else:\\n            warped = self._warp_backward(image, interpolation,\\n                                         image_world2grid, out_shape,\\n                                         out_grid2world)\\n        return np.asarray(warped)\\n\\n    def transform_points(self, points, coord2world=None, world2coord=None):\\n        \"\"\"Warp the list of points in the forward direction.\\n\\n        Applies this diffeomorphic map to the list of points (or streamlines)\\n        given by `points`. We assume that the points\\' coordinates are mapped\\n        to world coordinates by applying the `coord2world` affine transform.\\n        The warped coordinates are given in world coordinates unless\\n        `world2coord` affine transform is specified, in which case the warped\\n        points will be transformed to the corresponding coordinate system.\\n\\n        Parameters\\n        ----------\\n        points : array, shape (N, dim) or Streamlines object\\n\\n        coord2world : array, shape (dim+1, dim+1), optional\\n            affine matrix mapping points to world coordinates\\n\\n        world2coord : array, shape (dim+1, dim+1), optional\\n            affine matrix mapping world coordinates to points', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='Parameters\\n        ----------\\n        points : array, shape (N, dim) or Streamlines object\\n\\n        coord2world : array, shape (dim+1, dim+1), optional\\n            affine matrix mapping points to world coordinates\\n\\n        world2coord : array, shape (dim+1, dim+1), optional\\n            affine matrix mapping world coordinates to points\\n\\n        \"\"\"\\n        return self._transform_coordinates(points, coord2world, world2coord,\\n                                           inverse=self.is_inverse)\\n\\n    def transform_points_inverse(self, points, coord2world=None,\\n                                 world2coord=None):\\n        \"\"\"Warp the list of points in the backward direction.\\n\\n        Applies this diffeomorphic map to the list of points (or streamlines)\\n        given by `points`. We assume that the points\\' coordinates are mapped\\n        to world coordinates by applying the `coord2world` affine transform.\\n        The warped coordinates are given in world coordinates unless\\n        `world2coord` affine transform is specified, in which case the warped\\n        points will be transformed to the corresponding coordinate system.\\n\\n        Parameters\\n        ----------\\n        points : array, shape (N, dim) or Streamlines object\\n\\n        coord2world : array, shape (dim+1, dim+1), optional\\n            affine matrix mapping points to world coordinates\\n\\n        world2coord : array, shape (dim+1, dim+1), optional\\n            affine matrix mapping world coordinates to points\\n\\n        \"\"\"\\n        return self._transform_coordinates(points, coord2world, world2coord,\\n                                           inverse=not self.is_inverse)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='coord2world : array, shape (dim+1, dim+1), optional\\n            affine matrix mapping points to world coordinates\\n\\n        world2coord : array, shape (dim+1, dim+1), optional\\n            affine matrix mapping world coordinates to points\\n\\n        \"\"\"\\n        return self._transform_coordinates(points, coord2world, world2coord,\\n                                           inverse=not self.is_inverse)\\n\\n    def _transform_coordinates(self, points, coord2world, world2coord,\\n                               inverse=False):\\n\\n        is_streamline_obj = isinstance(points, Streamlines)\\n        data = points.get_data() if is_streamline_obj else points\\n\\n        if inverse:\\n            out = self._warp_coordinates_backward(data, coord2world,\\n                                                  world2coord)\\n        else:\\n            out = self._warp_coordinates_forward(data, coord2world,\\n                                                 world2coord)\\n\\n        if is_streamline_obj:\\n            old_data_dtype = points._data.dtype\\n            old_offsets_dtype = points._offsets.dtype\\n            streamlines = points.copy()\\n            streamlines._offsets = points._offsets.astype(old_offsets_dtype)\\n            streamlines._data = out.astype(old_data_dtype)\\n            return streamlines\\n\\n        return out\\n\\n    def inverse(self):\\n        \"\"\"Inverse of this DiffeomorphicMap instance\\n\\n        Returns a diffeomorphic map object representing the inverse of this\\n        transformation. The internal arrays are not copied but just referenced.\\n\\n        Returns\\n        -------\\n        inv : DiffeomorphicMap object\\n            the inverse of this diffeomorphic map.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='return out\\n\\n    def inverse(self):\\n        \"\"\"Inverse of this DiffeomorphicMap instance\\n\\n        Returns a diffeomorphic map object representing the inverse of this\\n        transformation. The internal arrays are not copied but just referenced.\\n\\n        Returns\\n        -------\\n        inv : DiffeomorphicMap object\\n            the inverse of this diffeomorphic map.\\n\\n        \"\"\"\\n        inv = DiffeomorphicMap(self.dim,\\n                               self.disp_shape,\\n                               self.disp_grid2world,\\n                               self.domain_shape,\\n                               self.domain_grid2world,\\n                               self.codomain_shape,\\n                               self.codomain_grid2world,\\n                               self.prealign)\\n        inv.forward = self.forward\\n        inv.backward = self.backward\\n        inv.is_inverse = True\\n        return inv\\n\\n    def expand_fields(self, expand_factors, new_shape):\\n        \"\"\"Expands the displacement fields from current shape to new_shape\\n\\n        Up-samples the discretization of the displacement fields to be of\\n        new_shape shape.\\n\\n        Parameters\\n        ----------\\n        expand_factors : array, shape (dim,)\\n            the factors scaling current spacings (voxel sizes) to spacings in\\n            the expanded discretization.\\n        new_shape : array, shape (dim,)\\n            the shape of the arrays holding the up-sampled discretization\\n\\n        \"\"\"\\n        if self.dim == 2:\\n            expand_f = vfu.resample_displacement_field_2d\\n        else:\\n            expand_f = vfu.resample_displacement_field_3d', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='Parameters\\n        ----------\\n        expand_factors : array, shape (dim,)\\n            the factors scaling current spacings (voxel sizes) to spacings in\\n            the expanded discretization.\\n        new_shape : array, shape (dim,)\\n            the shape of the arrays holding the up-sampled discretization\\n\\n        \"\"\"\\n        if self.dim == 2:\\n            expand_f = vfu.resample_displacement_field_2d\\n        else:\\n            expand_f = vfu.resample_displacement_field_3d\\n\\n        expanded_forward = expand_f(self.forward, expand_factors, new_shape)\\n        expanded_backward = expand_f(self.backward, expand_factors, new_shape)\\n\\n        expand_factors = np.append(expand_factors, [1])\\n        expanded_grid2world = mult_aff(self.disp_grid2world,\\n                                       np.diag(expand_factors))\\n        expanded_world2grid = npl.inv(expanded_grid2world)\\n        self.forward = expanded_forward\\n        self.backward = expanded_backward\\n        self.disp_shape = new_shape\\n        self.disp_grid2world = expanded_grid2world\\n        self.disp_world2grid = expanded_world2grid\\n\\n    def compute_inversion_error(self):\\n        \"\"\"Inversion error of the displacement fields\\n\\n        Estimates the inversion error of the displacement fields by computing\\n        statistics of the residual vectors obtained after composing the forward\\n        and backward displacement fields.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='def compute_inversion_error(self):\\n        \"\"\"Inversion error of the displacement fields\\n\\n        Estimates the inversion error of the displacement fields by computing\\n        statistics of the residual vectors obtained after composing the forward\\n        and backward displacement fields.\\n\\n        Returns\\n        -------\\n        residual : array, shape (R, C) or (S, R, C)\\n            the displacement field resulting from composing the forward and\\n            backward displacement fields of this transformation (the residual\\n            should be zero for a perfect diffeomorphism)\\n        stats : array, shape (3,)\\n            statistics from the norms of the vectors of the residual\\n            displacement field: maximum, mean and standard deviation\\n\\n        Notes\\n        -----\\n        Since the forward and backward displacement fields have the same\\n        discretization, the final composition is given by\\n\\n        comp[i] = forward[ i + Dinv * backward[i]]\\n\\n        where Dinv is the space-to-grid transformation of the displacement\\n        fields\\n\\n        \"\"\"\\n        Dinv = self.disp_world2grid\\n        if self.dim == 2:\\n            compose_f = vfu.compose_vector_fields_2d\\n        else:\\n            compose_f = vfu.compose_vector_fields_3d\\n\\n        residual, stats = compose_f(self.backward, self.forward,\\n                                    None, Dinv, 1.0, None)\\n\\n        return np.asarray(residual), np.asarray(stats)\\n\\n    def shallow_copy(self):\\n        \"\"\"Shallow copy of this DiffeomorphicMap instance\\n\\n        Creates a shallow copy of this diffeomorphic map (the arrays are not\\n        copied but just referenced)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='residual, stats = compose_f(self.backward, self.forward,\\n                                    None, Dinv, 1.0, None)\\n\\n        return np.asarray(residual), np.asarray(stats)\\n\\n    def shallow_copy(self):\\n        \"\"\"Shallow copy of this DiffeomorphicMap instance\\n\\n        Creates a shallow copy of this diffeomorphic map (the arrays are not\\n        copied but just referenced)\\n\\n        Returns\\n        -------\\n        new_map : DiffeomorphicMap object\\n            the shallow copy of this diffeomorphic map\\n\\n        \"\"\"\\n        new_map = DiffeomorphicMap(self.dim,\\n                                   self.disp_shape,\\n                                   self.disp_grid2world,\\n                                   self.domain_shape,\\n                                   self.domain_grid2world,\\n                                   self.codomain_shape,\\n                                   self.codomain_grid2world,\\n                                   self.prealign)\\n        new_map.forward = self.forward\\n        new_map.backward = self.backward\\n        new_map.is_inverse = self.is_inverse\\n        return new_map\\n\\n    def warp_endomorphism(self, phi):\\n        \"\"\"Composition of this DiffeomorphicMap with a given endomorphism', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='def warp_endomorphism(self, phi):\\n        \"\"\"Composition of this DiffeomorphicMap with a given endomorphism\\n\\n        Creates a new DiffeomorphicMap C with the same properties as self and\\n        composes its displacement fields with phi\\'s corresponding fields.\\n        The resulting diffeomorphism is of the form C(x) = phi(self(x)) with\\n        inverse C^{-1}(y) = self^{-1}(phi^{-1}(y)). We assume that phi is an\\n        endomorphism with the same discretization and domain affine as self\\n        to ensure that the composition inherits self\\'s properties (we also\\n        assume that the pre-aligning matrix of phi is None or identity).\\n\\n        Parameters\\n        ----------\\n        phi : DiffeomorphicMap object\\n            the endomorphism to be warped by this diffeomorphic map\\n\\n        Returns\\n        -------\\n        composition : the composition of this diffeomorphic map with the\\n            endomorphism given as input\\n\\n        Notes\\n        -----\\n        The problem with our current representation of a DiffeomorphicMap is\\n        that the set of Diffeomorphism that can be represented this way (a\\n        pre-aligning matrix followed by a non-linear endomorphism given as a\\n        displacement field) is not closed under the composition operation.\\n\\n        Supporting a general DiffeomorphicMap class, closed under composition,\\n        may be extremely costly computationally, and the kind of\\n        transformations we actually need for Avants\\' mid-point algorithm (SyN)\\n        are much simpler.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='Supporting a general DiffeomorphicMap class, closed under composition,\\n        may be extremely costly computationally, and the kind of\\n        transformations we actually need for Avants\\' mid-point algorithm (SyN)\\n        are much simpler.\\n\\n        \"\"\"\\n        # Compose the forward deformation fields\\n        d1 = self.get_forward_field()\\n        d2 = phi.get_forward_field()\\n        d1_inv = self.get_backward_field()\\n        d2_inv = phi.get_backward_field()\\n\\n        premult_disp = self.disp_world2grid\\n\\n        if self.dim == 2:\\n            compose_f = vfu.compose_vector_fields_2d\\n        else:\\n            compose_f = vfu.compose_vector_fields_3d\\n\\n        forward, stats = compose_f(d1, d2, None, premult_disp, 1.0, None)\\n        backward, stats, = compose_f(d2_inv, d1_inv, None, premult_disp, 1.0,\\n                                     None)\\n\\n        composition = self.shallow_copy()\\n        composition.forward = forward\\n        composition.backward = backward\\n        return composition\\n\\n    def get_simplified_transform(self):\\n        \"\"\" Constructs a simplified version of this Diffeomorhic Map', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='forward, stats = compose_f(d1, d2, None, premult_disp, 1.0, None)\\n        backward, stats, = compose_f(d2_inv, d1_inv, None, premult_disp, 1.0,\\n                                     None)\\n\\n        composition = self.shallow_copy()\\n        composition.forward = forward\\n        composition.backward = backward\\n        return composition\\n\\n    def get_simplified_transform(self):\\n        \"\"\" Constructs a simplified version of this Diffeomorhic Map\\n\\n        The simplified version incorporates the pre-align transform, as well as\\n        the domain and codomain affine transforms into the displacement field.\\n        The resulting transformation may be regarded as operating on the\\n        image spaces given by the domain and codomain discretization. As a\\n        result, self.prealign, self.disp_grid2world, self.domain_grid2world and\\n        self.codomain affine will be None (denoting Identity) in the resulting\\n        diffeomorphic map.\\n        \"\"\"\\n        if self.dim == 2:\\n            simplify_f = vfu.simplify_warp_function_2d\\n        else:\\n            simplify_f = vfu.simplify_warp_function_3d\\n        # Simplify the forward transform\\n        D = self.domain_grid2world\\n        P = self.prealign\\n        Rinv = self.disp_world2grid\\n        Cinv = self.codomain_world2grid\\n\\n        # this is the matrix which we need to multiply the voxel coordinates\\n        # to interpolate on the forward displacement field (\"in\"side the\\n        # \\'forward\\' brackets in the expression above)\\n        affine_idx_in = mult_aff(Rinv, mult_aff(P, D))', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='# this is the matrix which we need to multiply the voxel coordinates\\n        # to interpolate on the forward displacement field (\"in\"side the\\n        # \\'forward\\' brackets in the expression above)\\n        affine_idx_in = mult_aff(Rinv, mult_aff(P, D))\\n\\n        # this is the matrix which we need to multiply the voxel coordinates\\n        # to add to the displacement (\"out\"side the \\'forward\\' brackets in the\\n        # expression above)\\n        affine_idx_out = mult_aff(Cinv, mult_aff(P, D))\\n\\n        # this is the matrix which we need to multiply the displacement vector\\n        # prior to adding to the transformed input point\\n        affine_disp = Cinv\\n\\n        new_forward = simplify_f(self.forward, affine_idx_in,\\n                                 affine_idx_out, affine_disp,\\n                                 self.domain_shape)\\n\\n        # Simplify the backward transform\\n        C = self.codomain_world2grid\\n        Pinv = self.prealign_inv\\n        Dinv = self.domain_world2grid', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='# this is the matrix which we need to multiply the displacement vector\\n        # prior to adding to the transformed input point\\n        affine_disp = Cinv\\n\\n        new_forward = simplify_f(self.forward, affine_idx_in,\\n                                 affine_idx_out, affine_disp,\\n                                 self.domain_shape)\\n\\n        # Simplify the backward transform\\n        C = self.codomain_world2grid\\n        Pinv = self.prealign_inv\\n        Dinv = self.domain_world2grid\\n\\n        affine_idx_in = mult_aff(Rinv, C)\\n        affine_idx_out = mult_aff(Dinv, mult_aff(Pinv, C))\\n        affine_disp = mult_aff(Dinv, Pinv)\\n        new_backward = simplify_f(self.backward, affine_idx_in,\\n                                  affine_idx_out, affine_disp,\\n                                  self.codomain_shape)\\n        simplified = DiffeomorphicMap(self.dim,\\n                                      self.disp_shape,\\n                                      None,\\n                                      self.domain_shape,\\n                                      None,\\n                                      self.codomain_shape,\\n                                      None,\\n                                      None)\\n        simplified.forward = new_forward\\n        simplified.backward = new_backward\\n        return simplified\\n\\n\\nclass DiffeomorphicRegistration(metaclass=abc.ABCMeta):\\n    def __init__(self, metric=None):\\n        \"\"\" Diffeomorphic Registration\\n\\n        This abstract class defines the interface to be implemented by any\\n        optimization algorithm for diffeomorphic registration.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='class DiffeomorphicRegistration(metaclass=abc.ABCMeta):\\n    def __init__(self, metric=None):\\n        \"\"\" Diffeomorphic Registration\\n\\n        This abstract class defines the interface to be implemented by any\\n        optimization algorithm for diffeomorphic registration.\\n\\n        Parameters\\n        ----------\\n        metric : SimilarityMetric object\\n            the object measuring the similarity of the two images. The\\n            registration algorithm will minimize (or maximize) the provided\\n            similarity.\\n        \"\"\"\\n        if metric is None:\\n            raise ValueError(\\'The metric cannot be None\\')\\n        self.metric = metric\\n        self.dim = metric.dim\\n\\n    def set_level_iters(self, level_iters):\\n        \"\"\"Sets the number of iterations at each pyramid level\\n\\n        Establishes the maximum number of iterations to be performed at each\\n        level of the Gaussian pyramid, similar to ANTS.\\n\\n        Parameters\\n        ----------\\n        level_iters : list\\n            the number of iterations at each level of the Gaussian pyramid.\\n            level_iters[0] corresponds to the finest level, level_iters[n-1]\\n            the coarsest, where n is the length of the list\\n        \"\"\"\\n        self.levels = len(level_iters) if level_iters else 0\\n        self.level_iters = level_iters\\n\\n    @abc.abstractmethod\\n    def optimize(self):\\n        \"\"\"Starts the metric optimization\\n\\n        This is the main function each specialized class derived from this must\\n        implement. Upon completion, the deformation field must be available\\n        from the forward transformation model.\\n        \"\"\"', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='@abc.abstractmethod\\n    def optimize(self):\\n        \"\"\"Starts the metric optimization\\n\\n        This is the main function each specialized class derived from this must\\n        implement. Upon completion, the deformation field must be available\\n        from the forward transformation model.\\n        \"\"\"\\n\\n    @abc.abstractmethod\\n    def get_map(self):\\n        \"\"\"\\n        Returns the resulting diffeomorphic map after optimization\\n        \"\"\"\\n\\n\\nclass SymmetricDiffeomorphicRegistration(DiffeomorphicRegistration):\\n    def __init__(self,\\n                 metric,\\n                 level_iters=None,\\n                 step_length=0.25,\\n                 ss_sigma_factor=0.2,\\n                 opt_tol=1e-5,\\n                 inv_iter=20,\\n                 inv_tol=1e-3,\\n                 callback=None):\\n        \"\"\" Symmetric Diffeomorphic Registration (SyN) Algorithm\\n\\n        Performs the multi-resolution optimization algorithm for non-linear\\n        registration using a given similarity metric.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='Parameters\\n        ----------\\n        metric : SimilarityMetric object\\n            the metric to be optimized\\n        level_iters : list of int\\n            the number of iterations at each level of the Gaussian Pyramid (the\\n            length of the list defines the number of pyramid levels to be\\n            used)\\n        opt_tol : float\\n            the optimization will stop when the estimated derivative of the\\n            energy profile w.r.t. time falls below this threshold\\n        inv_iter : int\\n            the number of iterations to be performed by the displacement field\\n            inversion algorithm\\n        step_length : float\\n            the length of the maximum displacement vector of the update\\n            displacement field at each iteration\\n        ss_sigma_factor : float\\n            parameter of the scale-space smoothing kernel. For example, the\\n            std. dev. of the kernel will be factor*(2^i) in the isotropic case\\n            where i = 0, 1, ..., n_scales is the scale\\n        inv_tol : float\\n            the displacement field inversion algorithm will stop iterating\\n            when the inversion error falls below this threshold\\n        callback : function(SymmetricDiffeomorphicRegistration)\\n            a function receiving a SymmetricDiffeomorphicRegistration object\\n            to be called after each iteration (this optimizer will call this\\n            function passing self as parameter)\\n        \"\"\"\\n        super(SymmetricDiffeomorphicRegistration, self).__init__(metric)\\n        if level_iters is None:\\n            level_iters = [100, 100, 25]\\n\\n        if len(level_iters) == 0:\\n            raise ValueError(\\'The iterations list cannot be empty\\')', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='if len(level_iters) == 0:\\n            raise ValueError(\\'The iterations list cannot be empty\\')\\n\\n        self.set_level_iters(level_iters)\\n        self.step_length = step_length\\n        self.ss_sigma_factor = ss_sigma_factor\\n        self.opt_tol = opt_tol\\n        self.inv_tol = inv_tol\\n        self.inv_iter = inv_iter\\n        self.energy_window = 12\\n        self.energy_list = []\\n        self.full_energy_profile = []\\n        self.verbosity = VerbosityLevels.STATUS\\n        self.callback = callback\\n        self.moving_ss = None\\n        self.static_ss = None\\n        self.static_direction = None\\n        self.moving_direction = None\\n        self.mask0 = metric.mask0\\n\\n    def update(self, current_displacement, new_displacement,\\n               disp_world2grid, time_scaling):\\n        \"\"\"Composition of the current displacement field with the given field\\n\\n        Interpolates new displacement at the locations defined by\\n        current_displacement. Equivalently, computes the composition C of the\\n        given displacement fields as C(x) = B(A(x)), where A is\\n        current_displacement and B is new_displacement. This function is\\n        intended to be used with deformation fields of the same sampling\\n        (e.g. to be called by a registration algorithm).', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content=\"Interpolates new displacement at the locations defined by\\n        current_displacement. Equivalently, computes the composition C of the\\n        given displacement fields as C(x) = B(A(x)), where A is\\n        current_displacement and B is new_displacement. This function is\\n        intended to be used with deformation fields of the same sampling\\n        (e.g. to be called by a registration algorithm).\\n\\n        Parameters\\n        ----------\\n        current_displacement : array, shape (R', C', 2) or (S', R', C', 3)\\n            the displacement field defining where to interpolate\\n            new_displacement\\n        new_displacement : array, shape (R, C, 2) or (S, R, C, 3)\\n            the displacement field to be warped by current_displacement\\n        disp_world2grid : array, shape (dim+1, dim+1)\\n            the space-to-grid transform associated with the displacements'\\n            grid (we assume that both displacements are discretized over the\\n            same grid)\\n        time_scaling : float\\n            scaling factor applied to d2. The effect may be interpreted as\\n            moving d1 displacements along a factor (`time_scaling`) of d2.\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='Returns\\n        -------\\n        updated : array, shape (the same as new_displacement)\\n            the warped displacement field\\n        mean_norm : the mean norm of all vectors in current_displacement\\n        \"\"\"\\n        sq_field = np.sum((np.array(current_displacement) ** 2), -1)\\n        mean_norm = np.sqrt(sq_field).mean()\\n        # We assume that both displacement fields have the same\\n        # grid2world transform, which implies premult_index=Identity\\n        # and premult_disp is the world2grid transform associated with\\n        # the displacements\\' grid\\n        self.compose(current_displacement, new_displacement, None,\\n                     disp_world2grid, time_scaling, current_displacement)\\n\\n        return np.array(current_displacement), np.array(mean_norm)\\n\\n    def get_map(self):\\n        \"\"\"Return the resulting diffeomorphic map.\\n\\n        Returns the DiffeomorphicMap registering the moving image towards\\n        the static image.\\n\\n        \"\"\"\\n        if not hasattr(self, \\'static_to_ref\\'):\\n            msg = \\'Diffeormorphic map can not be obtained without running \\'\\n            msg += \\'the optimizer. Please call first \\'\\n            msg += \\'SymmetricDiffeomorphicRegistration.optimize()\\'\\n            raise ValueError(msg)\\n        return self.static_to_ref\\n\\n    def _connect_functions(self):\\n        \"\"\"Assign the methods to be called according to the image dimension', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='\"\"\"\\n        if not hasattr(self, \\'static_to_ref\\'):\\n            msg = \\'Diffeormorphic map can not be obtained without running \\'\\n            msg += \\'the optimizer. Please call first \\'\\n            msg += \\'SymmetricDiffeomorphicRegistration.optimize()\\'\\n            raise ValueError(msg)\\n        return self.static_to_ref\\n\\n    def _connect_functions(self):\\n        \"\"\"Assign the methods to be called according to the image dimension\\n\\n        Assigns the appropriate functions to be called for displacement field\\n        inversion, Gaussian pyramid, and affine / dense deformation composition\\n        according to the dimension of the input images e.g. 2D or 3D.\\n        \"\"\"\\n        if self.dim == 2:\\n            self.invert_vector_field = vfu.invert_vector_field_fixed_point_2d\\n            self.compose = vfu.compose_vector_fields_2d\\n        else:\\n            self.invert_vector_field = vfu.invert_vector_field_fixed_point_3d\\n            self.compose = vfu.compose_vector_fields_3d\\n\\n    def _init_optimizer(self, static, moving,\\n                        static_grid2world, moving_grid2world, prealign):\\n        \"\"\"Initializes the registration optimizer\\n\\n        Initializes the optimizer by computing the scale space of the input\\n        images and allocating the required memory for the transformation models\\n        at the coarsest scale.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='def _init_optimizer(self, static, moving,\\n                        static_grid2world, moving_grid2world, prealign):\\n        \"\"\"Initializes the registration optimizer\\n\\n        Initializes the optimizer by computing the scale space of the input\\n        images and allocating the required memory for the transformation models\\n        at the coarsest scale.\\n\\n        Parameters\\n        ----------\\n        static : array, shape (S, R, C) or (R, C)\\n            the image to be used as reference during optimization. The\\n            displacement fields will have the same discretization as the static\\n            image.\\n        moving : array, shape (S, R, C) or (R, C)\\n            the image to be used as \"moving\" during optimization. Since the\\n            deformation fields\\' discretization is the same as the static image,\\n            it is necessary to pre-align the moving image to ensure its domain\\n            lies inside the domain of the deformation fields. This is assumed\\n            to be accomplished by \"pre-aligning\" the moving image towards the\\n            static using an affine transformation given by the \\'prealign\\'\\n            matrix\\n        static_grid2world : array, shape (dim+1, dim+1)\\n            the voxel-to-space transformation associated to the static image\\n        moving_grid2world : array, shape (dim+1, dim+1)\\n            the voxel-to-space transformation associated to the moving image\\n        prealign : array, shape (dim+1, dim+1)\\n            the affine transformation (operating on the physical space)\\n            pre-aligning the moving image towards the static', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='\"\"\"\\n        self._connect_functions()\\n        # Extract information from affine matrices to create the scale space\\n        static_direction, static_spacing = \\\\\\n            get_direction_and_spacings(static_grid2world, self.dim)\\n        moving_direction, moving_spacing = \\\\\\n            get_direction_and_spacings(moving_grid2world, self.dim)\\n\\n        # the images\\' directions don\\'t change with scale\\n        self.static_direction = np.eye(self.dim + 1)\\n        self.moving_direction = np.eye(self.dim + 1)\\n        self.static_direction[:self.dim, :self.dim] = static_direction\\n        self.moving_direction[:self.dim, :self.dim] = moving_direction\\n\\n        # Build the scale space of the input images\\n        if self.verbosity >= VerbosityLevels.DIAGNOSE:\\n            logger.info(\\'Applying zero mask: \\' + str(self.mask0))\\n\\n        if self.verbosity >= VerbosityLevels.STATUS:\\n            logger.info(\\'Creating scale space from the moving image.\\' +\\n                        \\' Levels: %d. Sigma factor: %f.\\' %\\n                        (self.levels, self.ss_sigma_factor))\\n\\n        self.moving_ss = ScaleSpace(moving, self.levels, moving_grid2world,\\n                                    moving_spacing, self.ss_sigma_factor,\\n                                    self.mask0)\\n\\n        if self.verbosity >= VerbosityLevels.STATUS:\\n            logger.info(\\'Creating scale space from the static image.\\' +\\n                        \\' Levels: %d. Sigma factor: %f.\\' %\\n                        (self.levels, self.ss_sigma_factor))', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content=\"self.moving_ss = ScaleSpace(moving, self.levels, moving_grid2world,\\n                                    moving_spacing, self.ss_sigma_factor,\\n                                    self.mask0)\\n\\n        if self.verbosity >= VerbosityLevels.STATUS:\\n            logger.info('Creating scale space from the static image.' +\\n                        ' Levels: %d. Sigma factor: %f.' %\\n                        (self.levels, self.ss_sigma_factor))\\n\\n        self.static_ss = ScaleSpace(static, self.levels, static_grid2world,\\n                                    static_spacing, self.ss_sigma_factor,\\n                                    self.mask0)\\n\\n        if self.verbosity >= VerbosityLevels.DEBUG:\\n            logger.info('Moving scale space:')\\n            for level in range(self.levels):\\n                self.moving_ss.print_level(level)\\n\\n            logger.info('Static scale space:')\\n            for level in range(self.levels):\\n                self.static_ss.print_level(level)\\n\\n        # Get the properties of the coarsest level from the static image. These\\n        # properties will be taken as the reference discretization.\\n        disp_shape = self.static_ss.get_domain_shape(self.levels-1)\\n        disp_grid2world = self.static_ss.get_affine(self.levels-1)\\n\\n        # The codomain discretization of both diffeomorphic maps is\\n        # precisely the discretization of the static image\\n        codomain_shape = static.shape\\n        codomain_grid2world = static_grid2world\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content=\"# Get the properties of the coarsest level from the static image. These\\n        # properties will be taken as the reference discretization.\\n        disp_shape = self.static_ss.get_domain_shape(self.levels-1)\\n        disp_grid2world = self.static_ss.get_affine(self.levels-1)\\n\\n        # The codomain discretization of both diffeomorphic maps is\\n        # precisely the discretization of the static image\\n        codomain_shape = static.shape\\n        codomain_grid2world = static_grid2world\\n\\n        # The forward model transforms points from the static image\\n        # to points on the reference (which is the static as well). So the\\n        # domain properties are taken from the static image. Since its the same\\n        # as the reference, we don't need to pre-align.\\n        domain_shape = static.shape\\n        domain_grid2world = static_grid2world\\n        self.static_to_ref = DiffeomorphicMap(self.dim,\\n                                              disp_shape,\\n                                              disp_grid2world,\\n                                              domain_shape,\\n                                              domain_grid2world,\\n                                              codomain_shape,\\n                                              codomain_grid2world,\\n                                              None)\\n        self.static_to_ref.allocate()\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='# The backward model transforms points from the moving image\\n        # to points on the reference (which is the static). So the input\\n        # properties are taken from the moving image, and we need to pre-align\\n        # points on the moving physical space to the reference physical space\\n        # by applying the inverse of pre-align. This is done this way to make\\n        # it clear for the user: the pre-align matrix is usually obtained by\\n        # doing affine registration of the moving image towards the static\\n        # image, which results in a matrix transforming points in the static\\n        # physical space to points in the moving physical space\\n        prealign_inv = None if prealign is None else npl.inv(prealign)\\n        domain_shape = moving.shape\\n        domain_grid2world = moving_grid2world\\n        self.moving_to_ref = DiffeomorphicMap(self.dim,\\n                                              disp_shape,\\n                                              disp_grid2world,\\n                                              domain_shape,\\n                                              domain_grid2world,\\n                                              codomain_shape,\\n                                              codomain_grid2world,\\n                                              prealign_inv)\\n        self.moving_to_ref.allocate()\\n\\n    def _end_optimizer(self):\\n        \"\"\"Frees the resources allocated during initialization\\n        \"\"\"\\n        del self.moving_ss\\n        del self.static_ss\\n\\n    def _iterate(self):\\n        \"\"\"Performs one symmetric iteration', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='def _end_optimizer(self):\\n        \"\"\"Frees the resources allocated during initialization\\n        \"\"\"\\n        del self.moving_ss\\n        del self.static_ss\\n\\n    def _iterate(self):\\n        \"\"\"Performs one symmetric iteration\\n\\n        Performs one iteration of the SyN algorithm:\\n            1.Compute forward\\n            2.Compute backward\\n            3.Update forward\\n            4.Update backward\\n            5.Compute inverses\\n            6.Invert the inverses\\n\\n        Returns\\n        -------\\n        der : float\\n            the derivative of the energy profile, computed by fitting a\\n            quadratic function to the energy values at the latest T iterations,\\n            where T = self.energy_window. If the current iteration is less than\\n            T then np.inf is returned instead.\\n        \"\"\"\\n        # Acquire current resolution information from scale spaces\\n        current_moving = self.moving_ss.get_image(self.current_level)\\n        current_static = self.static_ss.get_image(self.current_level)\\n\\n        current_disp_shape = \\\\\\n            self.static_ss.get_domain_shape(self.current_level)\\n        current_disp_grid2world = \\\\\\n            self.static_ss.get_affine(self.current_level)\\n        current_disp_world2grid = \\\\\\n            self.static_ss.get_affine_inv(self.current_level)\\n        current_disp_spacing = \\\\\\n            self.static_ss.get_spacing(self.current_level)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content=\"current_disp_shape = \\\\\\n            self.static_ss.get_domain_shape(self.current_level)\\n        current_disp_grid2world = \\\\\\n            self.static_ss.get_affine(self.current_level)\\n        current_disp_world2grid = \\\\\\n            self.static_ss.get_affine_inv(self.current_level)\\n        current_disp_spacing = \\\\\\n            self.static_ss.get_spacing(self.current_level)\\n\\n        # Warp the input images (smoothed to the current scale) to the common\\n        # (reference) space at the current resolution\\n        wstatic = self.static_to_ref.transform_inverse(current_static,\\n                                                       'linear',\\n                                                       None,\\n                                                       current_disp_shape,\\n                                                       current_disp_grid2world)\\n        wmoving = self.moving_to_ref.transform_inverse(current_moving,\\n                                                       'linear',\\n                                                       None,\\n                                                       current_disp_shape,\\n                                                       current_disp_grid2world)\\n        # Pass both images to the metric. Now both images are sampled on the\\n        # reference grid (equal to the static image's grid) and the direction\\n        # doesn't change across scales\\n        self.metric.set_moving_image(wmoving, current_disp_grid2world,\\n                                     current_disp_spacing,\\n                                     self.static_direction)\\n        self.metric.use_moving_image_dynamics(\\n            current_moving, self.moving_to_ref.inverse())\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='self.metric.set_static_image(wstatic, current_disp_grid2world,\\n                                     current_disp_spacing,\\n                                     self.static_direction)\\n        self.metric.use_static_image_dynamics(\\n            current_static, self.static_to_ref.inverse())\\n\\n        # Initialize the metric for a new iteration\\n        self.metric.initialize_iteration()\\n        if self.callback is not None:\\n            self.callback(self, RegistrationStages.ITER_START)\\n\\n        # Compute the forward step (to be used to update the forward transform)\\n        fw_step = np.array(self.metric.compute_forward())\\n\\n        # set zero displacements at the boundary\\n        fw_step = self.__set_no_boundary_displacement(fw_step)\\n\\n        # Normalize the forward step\\n        nrm = np.sqrt(np.sum((fw_step/current_disp_spacing)**2, -1)).max()\\n        if nrm > 0:\\n            fw_step /= nrm\\n\\n        # Add to current total field\\n        self.static_to_ref.forward, md_forward = self.update(\\n            self.static_to_ref.forward, fw_step,\\n            current_disp_world2grid, self.step_length)\\n        del fw_step\\n\\n        # Keep track of the forward energy\\n        fw_energy = self.metric.get_energy()\\n\\n        # Compose backward step (to be used to update the backward transform)\\n        bw_step = np.array(self.metric.compute_backward())\\n\\n        # set zero displacements at the boundary\\n        bw_step = self.__set_no_boundary_displacement(bw_step)\\n\\n        # Normalize the backward step\\n        nrm = np.sqrt(np.sum((bw_step/current_disp_spacing) ** 2, -1)).max()\\n        if nrm > 0:\\n            bw_step /= nrm', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='# Compose backward step (to be used to update the backward transform)\\n        bw_step = np.array(self.metric.compute_backward())\\n\\n        # set zero displacements at the boundary\\n        bw_step = self.__set_no_boundary_displacement(bw_step)\\n\\n        # Normalize the backward step\\n        nrm = np.sqrt(np.sum((bw_step/current_disp_spacing) ** 2, -1)).max()\\n        if nrm > 0:\\n            bw_step /= nrm\\n\\n        # Add to current total field\\n        self.moving_to_ref.forward, md_backward = self.update(\\n            self.moving_to_ref.forward, bw_step,\\n            current_disp_world2grid, self.step_length)\\n        del bw_step\\n\\n        # Keep track of the energy\\n        bw_energy = self.metric.get_energy()\\n        der = np.inf\\n        n_iter = len(self.energy_list)\\n        if len(self.energy_list) >= self.energy_window:\\n            der = self._get_energy_derivative()\\n\\n        if self.verbosity >= VerbosityLevels.DIAGNOSE:\\n            ch = \\'-\\' if np.isnan(der) else der\\n            logger.info(\\'%d:\\\\t%0.6f\\\\t%0.6f\\\\t%0.6f\\\\t%s\\' %\\n                        (n_iter, fw_energy, bw_energy,\\n                         fw_energy + bw_energy, ch))\\n\\n        self.energy_list.append(fw_energy + bw_energy)\\n\\n        self.__invert_models(current_disp_world2grid, current_disp_spacing)\\n\\n        # Free resources no longer needed to compute the forward and backward\\n        # steps\\n        if self.callback is not None:\\n            self.callback(self, RegistrationStages.ITER_END)\\n        self.metric.free_iteration()\\n\\n        return der\\n\\n    def __set_no_boundary_displacement(self, step):\\n        \"\"\" set zero displacements at the boundary', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='self.energy_list.append(fw_energy + bw_energy)\\n\\n        self.__invert_models(current_disp_world2grid, current_disp_spacing)\\n\\n        # Free resources no longer needed to compute the forward and backward\\n        # steps\\n        if self.callback is not None:\\n            self.callback(self, RegistrationStages.ITER_END)\\n        self.metric.free_iteration()\\n\\n        return der\\n\\n    def __set_no_boundary_displacement(self, step):\\n        \"\"\" set zero displacements at the boundary\\n\\n        Parameters\\n        ----------\\n        step : array, ndim 2 or 3\\n            displacements field\\n\\n        Returns\\n        -------\\n        step : array, ndim 2 or 3\\n            displacements field\\n        \"\"\"\\n        step[0, ...] = 0\\n        step[:, 0, ...] = 0\\n        step[-1, ...] = 0\\n        step[:, -1, ...] = 0\\n        if self.dim == 3:\\n            step[:, :, 0, ...] = 0\\n            step[:, :, -1, ...] = 0\\n        return step\\n\\n    def __invert_models(self, current_disp_world2grid, current_disp_spacing):\\n        \"\"\"Converting static - moving models in both direction.\\n\\n        Parameters\\n        ----------\\n        current_disp_world2grid : array, shape (3, 3) or  (4, 4)\\n            the space-to-grid transformation associated to the displacement field\\n            d (transforming physical space coordinates to voxel coordinates of the\\n            displacement field grid)\\n        current_disp_spacing :array, shape (2,) or  (3,)\\n            the spacing between voxels (voxel size along each axis)\\n        \"\"\"', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='Parameters\\n        ----------\\n        current_disp_world2grid : array, shape (3, 3) or  (4, 4)\\n            the space-to-grid transformation associated to the displacement field\\n            d (transforming physical space coordinates to voxel coordinates of the\\n            displacement field grid)\\n        current_disp_spacing :array, shape (2,) or  (3,)\\n            the spacing between voxels (voxel size along each axis)\\n        \"\"\"\\n\\n        # Invert the forward model\\'s forward field\\n        self.static_to_ref.backward = np.array(\\n            self.invert_vector_field(self.static_to_ref.forward,\\n                                     current_disp_world2grid,\\n                                     current_disp_spacing,\\n                                     self.inv_iter, self.inv_tol,\\n                                     self.static_to_ref.backward))\\n\\n        # Invert the backward model\\'s forward field\\n        self.moving_to_ref.backward = np.array(\\n            self.invert_vector_field(self.moving_to_ref.forward,\\n                                     current_disp_world2grid,\\n                                     current_disp_spacing,\\n                                     self.inv_iter, self.inv_tol,\\n                                     self.moving_to_ref.backward))\\n\\n        # Invert the forward model\\'s backward field\\n        self.static_to_ref.forward = np.array(\\n            self.invert_vector_field(self.static_to_ref.backward,\\n                                     current_disp_world2grid,\\n                                     current_disp_spacing,\\n                                     self.inv_iter, self.inv_tol,\\n                                     self.static_to_ref.forward))', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='# Invert the forward model\\'s backward field\\n        self.static_to_ref.forward = np.array(\\n            self.invert_vector_field(self.static_to_ref.backward,\\n                                     current_disp_world2grid,\\n                                     current_disp_spacing,\\n                                     self.inv_iter, self.inv_tol,\\n                                     self.static_to_ref.forward))\\n\\n        # Invert the backward model\\'s backward field\\n        self.moving_to_ref.forward = np.array(\\n            self.invert_vector_field(self.moving_to_ref.backward,\\n                                     current_disp_world2grid,\\n                                     current_disp_spacing,\\n                                     self.inv_iter, self.inv_tol,\\n                                     self.moving_to_ref.forward))\\n\\n    def _approximate_derivative_direct(self, x, y):\\n        \"\"\"Derivative of the degree-2 polynomial fit of the given x, y pairs\\n\\n        Directly computes the derivative of the least-squares-fit quadratic\\n        function estimated from (x[...],y[...]) pairs.\\n\\n        Parameters\\n        ----------\\n        x : array, shape (n,)\\n            increasing array representing the x-coordinates of the points to\\n            be fit\\n        y : array, shape (n,)\\n            array representing the y-coordinates of the points to be fit', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='Directly computes the derivative of the least-squares-fit quadratic\\n        function estimated from (x[...],y[...]) pairs.\\n\\n        Parameters\\n        ----------\\n        x : array, shape (n,)\\n            increasing array representing the x-coordinates of the points to\\n            be fit\\n        y : array, shape (n,)\\n            array representing the y-coordinates of the points to be fit\\n\\n        Returns\\n        -------\\n        y0 : float\\n            the estimated derivative at x0 = 0.5*len(x)\\n        \"\"\"\\n        x = np.asarray(x)\\n        y = np.asarray(y)\\n        X = np.row_stack((x**2, x, np.ones_like(x)))\\n        XX = X.dot(X.T)\\n        b = X.dot(y)\\n        beta = npl.solve(XX, b)\\n        x0 = 0.5 * len(x)\\n        y0 = 2.0 * beta[0] * x0 + beta[1]\\n        return y0\\n\\n    def _get_energy_derivative(self):\\n        \"\"\"Approximate derivative of the energy profile\\n\\n        Returns the derivative of the estimated energy as a function of \"time\"\\n        (iterations) at the last iteration\\n        \"\"\"\\n        n_iter = len(self.energy_list)\\n        if n_iter < self.energy_window:\\n            raise ValueError(\\'Not enough data to fit the energy profile\\')\\n        x = range(self.energy_window)\\n        y = self.energy_list[(n_iter - self.energy_window):n_iter]\\n        ss = sum(y)\\n        if not ss == 0:  # avoid division by zero\\n            ss = - ss if ss > 0 else ss\\n            y = [v / ss for v in y]\\n        der = self._approximate_derivative_direct(x, y)\\n        return der\\n\\n    def _optimize(self):\\n        \"\"\"Starts the optimization', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='def _optimize(self):\\n        \"\"\"Starts the optimization\\n\\n        The main multi-scale symmetric optimization algorithm\\n        \"\"\"\\n        self.full_energy_profile = []\\n        if self.callback is not None:\\n            self.callback(self, RegistrationStages.OPT_START)\\n        for level in range(self.levels - 1, -1, -1):\\n            if self.verbosity >= VerbosityLevels.STATUS:\\n                logger.info(\\'Optimizing level %d\\' % level)\\n\\n            self.current_level = level\\n\\n            self.metric.set_levels_below(self.levels - level)\\n            self.metric.set_levels_above(level)\\n\\n            if level < self.levels - 1:\\n                expand_factors = \\\\\\n                    self.static_ss.get_expand_factors(level+1, level)\\n                new_shape = self.static_ss.get_domain_shape(level)\\n                self.static_to_ref.expand_fields(expand_factors, new_shape)\\n                self.moving_to_ref.expand_fields(expand_factors, new_shape)\\n\\n            self.niter = 0\\n            self.energy_list = []\\n            derivative = np.inf\\n\\n            if self.callback is not None:\\n                self.callback(self, RegistrationStages.SCALE_START)\\n\\n            while ((self.niter < self.level_iters[self.levels - 1 - level]) and\\n                   (self.opt_tol < derivative)):\\n                derivative = self._iterate()\\n                self.niter += 1\\n\\n            self.full_energy_profile.extend(self.energy_list)\\n\\n            if self.callback is not None:\\n                self.callback(self, RegistrationStages.SCALE_END)\\n\\n        # Reporting mean and std in stats[1] and stats[2]\\n        residual, stats = self.static_to_ref.compute_inversion_error()', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='self.full_energy_profile.extend(self.energy_list)\\n\\n            if self.callback is not None:\\n                self.callback(self, RegistrationStages.SCALE_END)\\n\\n        # Reporting mean and std in stats[1] and stats[2]\\n        residual, stats = self.static_to_ref.compute_inversion_error()\\n\\n        if self.verbosity >= VerbosityLevels.DIAGNOSE:\\n            logger.info(\\'Static-Reference Residual error: %0.6f (%0.6f)\\'\\n                        % (stats[1], stats[2]))\\n\\n        residual, stats = self.moving_to_ref.compute_inversion_error()\\n\\n        if self.verbosity >= VerbosityLevels.DIAGNOSE:\\n            logger.info(\\'Moving-Reference Residual error :%0.6f (%0.6f)\\'\\n                        % (stats[1], stats[2]))\\n\\n        # Compose the two partial transformations\\n        self.static_to_ref = self.moving_to_ref.warp_endomorphism(\\n            self.static_to_ref.inverse()).inverse()\\n\\n        # Report mean and std for the composed deformation field\\n        residual, stats = self.static_to_ref.compute_inversion_error()\\n        if self.verbosity >= VerbosityLevels.DIAGNOSE:\\n            logger.info(\\'Final residual error: %0.6f (%0.6f)\\' % (stats[1],\\n                        stats[2]))\\n        if self.callback is not None:\\n            self.callback(self, RegistrationStages.OPT_END)\\n\\n    def optimize(self, static, moving, static_grid2world=None,\\n                 moving_grid2world=None, prealign=None):\\n        \"\"\"\\n        Starts the optimization', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='def optimize(self, static, moving, static_grid2world=None,\\n                 moving_grid2world=None, prealign=None):\\n        \"\"\"\\n        Starts the optimization\\n\\n        Parameters\\n        ----------\\n        static : array, shape (S, R, C) or (R, C)\\n            the image to be used as reference during optimization. The\\n            displacement fields will have the same discretization as the static\\n            image.\\n        moving : array, shape (S, R, C) or (R, C)\\n            the image to be used as \"moving\" during optimization. Since the\\n            deformation fields\\' discretization is the same as the static image,\\n            it is necessary to pre-align the moving image to ensure its domain\\n            lies inside the domain of the deformation fields. This is assumed\\n            to be accomplished by \"pre-aligning\" the moving image towards the\\n            static using an affine transformation given by the \\'prealign\\'\\n            matrix\\n        static_grid2world : array, shape (dim+1, dim+1)\\n            the voxel-to-space transformation associated to the static image\\n        moving_grid2world : array, shape (dim+1, dim+1)\\n            the voxel-to-space transformation associated to the moving image\\n        prealign : array, shape (dim+1, dim+1)\\n            the affine transformation (operating on the physical space)\\n            pre-aligning the moving image towards the static', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content='Returns\\n        -------\\n        static_to_ref : DiffeomorphicMap object\\n            the diffeomorphic map that brings the moving image towards the\\n            static one in the forward direction (i.e. by calling\\n            static_to_ref.transform) and the static image towards the\\n            moving one in the backward direction (i.e. by calling\\n            static_to_ref.transform_inverse).\\n\\n        \"\"\"\\n        if self.verbosity >= VerbosityLevels.DEBUG:\\n            if prealign is not None:\\n                logger.info(\"Pre-align: \" + str(prealign))\\n\\n        self._init_optimizer(static.astype(floating), moving.astype(floating),\\n                             static_grid2world, moving_grid2world, prealign)\\n        self._optimize()\\n        self._end_optimizer()\\n        self.static_to_ref.forward = np.array(self.static_to_ref.forward)\\n        self.static_to_ref.backward = np.array(self.static_to_ref.backward)\\n        return self.static_to_ref', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\imwarp.py.txt'}),\n",
       " Document(page_content=\"cython_sources = ['bundlemin',\\n'crosscorr',\\n'expectmax',\\n'parzenhist',\\n'sumsqdiff',\\n'transforms',\\n'vector_fields',]\\n\\ncython_headers = [\\n  'fused_types.pxd',\\n  'transforms.pxd',\\n  'vector_fields.pxd',\\n]\\n\\nforeach ext: cython_sources\\n  extra_args = []\\n  # Todo: check why it is failing to compile with transforms.pxd\\n  # C attributes cannot be added in implementation part of extension type\\n  # defined in a pxd\\n  # if fs.exists(ext + '.pxd')\\n  #   extra_args += ['--depfile', meson.current_source_dir() +'/'+ ext + '.pxd', ]\\n  # endif\\n  py3.extension_module(ext,\\n    cython_gen.process(ext + '.pyx', extra_args: extra_args),\\n    c_args: cython_c_args,\\n    include_directories: [incdir_numpy, inc_local],\\n    dependencies: [omp],\\n    install: true,\\n    subdir: 'dipy/align'\\n  )\\nendforeach\\n\\n\\npython_sources = ['__init__.py',\\n  '_public.py',\\n  'cpd.py',\\n  'imaffine.py',\\n  'imwarp.py',\\n  'metrics.py',\\n  'reslice.py',\\n  'scalespace.py',\\n  'streamlinear.py',\\n  'streamwarp.py',\\n  ]\\n\\n\\npy3.install_sources(\\n  python_sources + cython_headers,\\n  pure: false,\\n  subdir: 'dipy/align'\\n)\\n\\n\\nsubdir('tests')\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\meson.build.txt'}),\n",
       " Document(page_content='\"\"\"  Metrics for Symmetric Diffeomorphic Registration \"\"\"\\n\\nimport abc\\nimport numpy as np\\nfrom numpy import gradient\\nfrom scipy import ndimage\\nfrom dipy.align import vector_fields as vfu\\nfrom dipy.align import sumsqdiff as ssd\\nfrom dipy.align import crosscorr as cc\\nfrom dipy.align import expectmax as em\\nfrom dipy.align import floating\\n\\n\\nclass SimilarityMetric(metaclass=abc.ABCMeta):\\n    def __init__(self, dim):\\n        r\"\"\" Similarity Metric abstract class\\n\\n        A similarity metric is in charge of keeping track of the numerical\\n        value of the similarity (or distance) between the two given images. It\\n        also computes the update field for the forward and inverse displacement\\n        fields to be used in a gradient-based optimization algorithm. Note that\\n        this metric does not depend on any transformation (affine or\\n        non-linear) so it assumes the static and moving images are already\\n        warped\\n\\n        Parameters\\n        ----------\\n        dim : int (either 2 or 3)\\n            the dimension of the image domain\\n        \"\"\"\\n        self.dim = dim\\n        self.levels_above = None\\n        self.levels_below = None\\n\\n        self.static_image = None\\n        self.static_affine = None\\n        self.static_spacing = None\\n        self.static_direction = None\\n\\n        self.moving_image = None\\n        self.moving_affine = None\\n        self.moving_spacing = None\\n        self.moving_direction = None\\n        self.mask0 = False\\n\\n    def set_levels_below(self, levels):\\n        r\"\"\"Informs the metric how many pyramid levels are below the current one', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='self.static_image = None\\n        self.static_affine = None\\n        self.static_spacing = None\\n        self.static_direction = None\\n\\n        self.moving_image = None\\n        self.moving_affine = None\\n        self.moving_spacing = None\\n        self.moving_direction = None\\n        self.mask0 = False\\n\\n    def set_levels_below(self, levels):\\n        r\"\"\"Informs the metric how many pyramid levels are below the current one\\n\\n        Informs this metric the number of pyramid levels below the current one.\\n        The metric may change its behavior (e.g. number of inner iterations)\\n        accordingly\\n\\n        Parameters\\n        ----------\\n        levels : int\\n            the number of levels below the current Gaussian Pyramid level\\n        \"\"\"\\n        self.levels_below = levels\\n\\n    def set_levels_above(self, levels):\\n        r\"\"\"Informs the metric how many pyramid levels are above the current one\\n\\n        Informs this metric the number of pyramid levels above the current one.\\n        The metric may change its behavior (e.g. number of inner iterations)\\n        accordingly\\n\\n        Parameters\\n        ----------\\n        levels : int\\n            the number of levels above the current Gaussian Pyramid level\\n        \"\"\"\\n        self.levels_above = levels\\n\\n    def set_static_image(self, static_image, static_affine, static_spacing,\\n                         static_direction):\\n        r\"\"\"Sets the static image being compared against the moving one.\\n\\n        Sets the static image. The default behavior (of this abstract class) is\\n        simply to assign the reference to an attribute, but\\n        generalizations of the metric may need to perform other operations', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='def set_static_image(self, static_image, static_affine, static_spacing,\\n                         static_direction):\\n        r\"\"\"Sets the static image being compared against the moving one.\\n\\n        Sets the static image. The default behavior (of this abstract class) is\\n        simply to assign the reference to an attribute, but\\n        generalizations of the metric may need to perform other operations\\n\\n        Parameters\\n        ----------\\n        static_image : array, shape (R, C) or (S, R, C)\\n            the static image\\n        \"\"\"\\n        self.static_image = static_image\\n        self.static_affine = static_affine\\n        self.static_spacing = static_spacing\\n        self.static_direction = static_direction\\n\\n    def use_static_image_dynamics(self, original_static_image, transformation):\\n        r\"\"\"This is called by the optimizer just after setting the static image.\\n\\n        This method allows the metric to compute any useful\\n        information from knowing how the current static image was generated\\n        (as the transformation of an original static image). This method is\\n        called by the optimizer just after it sets the static image.\\n        Transformation will be an instance of DiffeomorficMap or None\\n        if the original_static_image equals self.moving_image.\\n\\n        Parameters\\n        ----------\\n        original_static_image : array, shape (R, C) or (S, R, C)\\n            original image from which the current static image was generated\\n        transformation : DiffeomorphicMap object\\n            the transformation that was applied to original image to generate\\n            the current static image\\n        \"\"\"\\n        pass', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='Parameters\\n        ----------\\n        original_static_image : array, shape (R, C) or (S, R, C)\\n            original image from which the current static image was generated\\n        transformation : DiffeomorphicMap object\\n            the transformation that was applied to original image to generate\\n            the current static image\\n        \"\"\"\\n        pass\\n\\n    def set_moving_image(self, moving_image, moving_affine, moving_spacing,\\n                         moving_direction):\\n        r\"\"\"Sets the moving image being compared against the static one.\\n\\n        Sets the moving image. The default behavior (of this abstract class) is\\n        simply to assign the reference to an attribute, but\\n        generalizations of the metric may need to perform other operations\\n\\n        Parameters\\n        ----------\\n        moving_image : array, shape (R, C) or (S, R, C)\\n            the moving image\\n        \"\"\"\\n        self.moving_image = moving_image\\n        self.moving_affine = moving_affine\\n        self.moving_spacing = moving_spacing\\n        self.moving_direction = moving_direction\\n\\n    def use_moving_image_dynamics(self, original_moving_image, transformation):\\n        r\"\"\"This is called by the optimizer just after setting the moving image\\n\\n        This method allows the metric to compute any useful\\n        information from knowing how the current static image was generated\\n        (as the transformation of an original static image). This method is\\n        called by the optimizer just after it sets the static image.\\n        Transformation will be an instance of DiffeomorficMap or None if\\n        the original_moving_image equals self.moving_image.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='This method allows the metric to compute any useful\\n        information from knowing how the current static image was generated\\n        (as the transformation of an original static image). This method is\\n        called by the optimizer just after it sets the static image.\\n        Transformation will be an instance of DiffeomorficMap or None if\\n        the original_moving_image equals self.moving_image.\\n\\n        Parameters\\n        ----------\\n        original_moving_image : array, shape (R, C) or (S, R, C)\\n            original image from which the current moving image was generated\\n        transformation : DiffeomorphicMap object\\n            the transformation that was applied to the original image to generate\\n            the current moving image\\n        \"\"\"\\n        pass\\n\\n    @abc.abstractmethod\\n    def initialize_iteration(self):\\n        r\"\"\"Prepares the metric to compute one displacement field iteration.\\n\\n        This method will be called before any compute_forward or\\n        compute_backward call, this allows the Metric to pre-compute any useful\\n        information for speeding up the update computations. This\\n        initialization was needed in ANTS because the updates are called once\\n        per voxel. In Python this is unpractical, though.\\n        \"\"\"\\n\\n    @abc.abstractmethod\\n    def free_iteration(self):\\n        r\"\"\"Releases the resources no longer needed by the metric\\n\\n        This method is called by the RegistrationOptimizer after the required\\n        iterations have been computed (forward and / or backward) so that the\\n        SimilarityMetric can safely delete any data it computed as part of the\\n        initialization\\n        \"\"\"', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='@abc.abstractmethod\\n    def free_iteration(self):\\n        r\"\"\"Releases the resources no longer needed by the metric\\n\\n        This method is called by the RegistrationOptimizer after the required\\n        iterations have been computed (forward and / or backward) so that the\\n        SimilarityMetric can safely delete any data it computed as part of the\\n        initialization\\n        \"\"\"\\n\\n    @abc.abstractmethod\\n    def compute_forward(self):\\n        r\"\"\"Computes one step bringing the reference image towards the static.\\n\\n        Computes the forward update field to register the moving image towards\\n        the static image in a gradient-based optimization algorithm\\n        \"\"\"\\n\\n    @abc.abstractmethod\\n    def compute_backward(self):\\n        r\"\"\"Computes one step bringing the static image towards the moving.\\n\\n        Computes the backward update field to register the static image towards\\n        the moving image in a gradient-based optimization algorithm\\n        \"\"\"\\n\\n    @abc.abstractmethod\\n    def get_energy(self):\\n        r\"\"\"Numerical value assigned by this metric to the current image pair\\n\\n        Must return the numeric value of the similarity between the given\\n        static and moving images\\n        \"\"\"\\n\\n\\nclass CCMetric(SimilarityMetric):\\n\\n    def __init__(self, dim, sigma_diff=2.0, radius=4):\\n        r\"\"\"Normalized Cross-Correlation Similarity metric.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='@abc.abstractmethod\\n    def get_energy(self):\\n        r\"\"\"Numerical value assigned by this metric to the current image pair\\n\\n        Must return the numeric value of the similarity between the given\\n        static and moving images\\n        \"\"\"\\n\\n\\nclass CCMetric(SimilarityMetric):\\n\\n    def __init__(self, dim, sigma_diff=2.0, radius=4):\\n        r\"\"\"Normalized Cross-Correlation Similarity metric.\\n\\n        Parameters\\n        ----------\\n        dim : int (either 2 or 3)\\n            the dimension of the image domain\\n        sigma_diff : the standard deviation of the Gaussian smoothing kernel to\\n            be applied to the update field at each iteration\\n        radius : int\\n            the radius of the squared (cubic) neighborhood at each voxel to be\\n            considered to compute the cross correlation\\n        \"\"\"\\n        super(CCMetric, self).__init__(dim)\\n        self.sigma_diff = sigma_diff\\n        self.radius = radius\\n        self._connect_functions()\\n\\n    def _connect_functions(self):\\n        r\"\"\"Assign the methods to be called according to the image dimension', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='def _connect_functions(self):\\n        r\"\"\"Assign the methods to be called according to the image dimension\\n\\n        Assigns the appropriate functions to be called for precomputing the\\n        cross-correlation factors according to the dimension of the input\\n        images\\n        \"\"\"\\n        if self.dim == 2:\\n            self.precompute_factors = cc.precompute_cc_factors_2d\\n            self.compute_forward_step = cc.compute_cc_forward_step_2d\\n            self.compute_backward_step = cc.compute_cc_backward_step_2d\\n            self.reorient_vector_field = vfu.reorient_vector_field_2d\\n        elif self.dim == 3:\\n            self.precompute_factors = cc.precompute_cc_factors_3d\\n            self.compute_forward_step = cc.compute_cc_forward_step_3d\\n            self.compute_backward_step = cc.compute_cc_backward_step_3d\\n            self.reorient_vector_field = vfu.reorient_vector_field_3d\\n        else:\\n            raise ValueError(\\'CC Metric not defined for dim. %d\\' % self.dim)\\n\\n    def initialize_iteration(self):\\n        r\"\"\"Prepares the metric to compute one displacement field iteration.\\n\\n        Pre-computes the cross-correlation factors for efficient computation\\n        of the gradient of the Cross Correlation w.r.t. the displacement field.\\n        It also pre-computes the image gradients in the physical space by\\n        re-orienting the gradients in the voxel space using the corresponding\\n        affine transformations.\\n        \"\"\"\\n        min_size = self.radius * 2 + 1\\n\\n        def invalid_image_size(image):\\n            return any(size < min_size for size in image.shape)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='Pre-computes the cross-correlation factors for efficient computation\\n        of the gradient of the Cross Correlation w.r.t. the displacement field.\\n        It also pre-computes the image gradients in the physical space by\\n        re-orienting the gradients in the voxel space using the corresponding\\n        affine transformations.\\n        \"\"\"\\n        min_size = self.radius * 2 + 1\\n\\n        def invalid_image_size(image):\\n            return any(size < min_size for size in image.shape)\\n\\n        msg = (\"Each image dimension should be superior to 2 * radius + 1 \"\\n               f\"({min_size}). Decrease CCMetric radius ({self.radius}) or \"\\n               \"increase your image size (shape=%(shape)s).\")\\n\\n        if invalid_image_size(self.static_image):\\n            raise ValueError(\"Static image size is too small. \" +\\n                             msg % dict(shape=self.static_image.shape))\\n        if invalid_image_size(self.moving_image):\\n            raise ValueError(\"Moving image size is too small. \" +\\n                             msg % dict(shape=self.moving_image.shape))\\n\\n        self.factors = self.precompute_factors(self.static_image,\\n                                               self.moving_image,\\n                                               self.radius)\\n        self.factors = np.array(self.factors)\\n\\n        self.gradient_moving = np.empty(\\n            shape=self.moving_image.shape+(self.dim,), dtype=floating)\\n        for i, grad in enumerate(gradient(self.moving_image)):\\n            self.gradient_moving[..., i] = grad', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='self.factors = self.precompute_factors(self.static_image,\\n                                               self.moving_image,\\n                                               self.radius)\\n        self.factors = np.array(self.factors)\\n\\n        self.gradient_moving = np.empty(\\n            shape=self.moving_image.shape+(self.dim,), dtype=floating)\\n        for i, grad in enumerate(gradient(self.moving_image)):\\n            self.gradient_moving[..., i] = grad\\n\\n        # Convert moving image\\'s gradient field from voxel to physical space\\n        if self.moving_spacing is not None:\\n            self.gradient_moving /= self.moving_spacing\\n        if self.moving_direction is not None:\\n            self.reorient_vector_field(self.gradient_moving,\\n                                       self.moving_direction)\\n\\n        self.gradient_static = np.empty(\\n            shape=self.static_image.shape+(self.dim,), dtype=floating)\\n        for i, grad in enumerate(gradient(self.static_image)):\\n            self.gradient_static[..., i] = grad\\n\\n        # Convert moving image\\'s gradient field from voxel to physical space\\n        if self.static_spacing is not None:\\n            self.gradient_static /= self.static_spacing\\n        if self.static_direction is not None:\\n            self.reorient_vector_field(self.gradient_static,\\n                                       self.static_direction)\\n\\n    def free_iteration(self):\\n        r\"\"\"Frees the resources allocated during initialization\\n        \"\"\"\\n        del self.factors\\n        del self.gradient_moving\\n        del self.gradient_static\\n\\n    def compute_forward(self):\\n        r\"\"\"Computes one step bringing the moving image towards the static.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='def free_iteration(self):\\n        r\"\"\"Frees the resources allocated during initialization\\n        \"\"\"\\n        del self.factors\\n        del self.gradient_moving\\n        del self.gradient_static\\n\\n    def compute_forward(self):\\n        r\"\"\"Computes one step bringing the moving image towards the static.\\n\\n        Computes the update displacement field to be used for registration of\\n        the moving image towards the static image\\n        \"\"\"\\n        displacement, self.energy = self.compute_forward_step(\\n            self.gradient_static, self.factors, self.radius)\\n        displacement = np.array(displacement)\\n        for i in range(self.dim):\\n            displacement[..., i] = ndimage.gaussian_filter(\\n                displacement[..., i], self.sigma_diff)\\n        return displacement\\n\\n    def compute_backward(self):\\n        r\"\"\"Computes one step bringing the static image towards the moving.\\n\\n        Computes the update displacement field to be used for registration of\\n        the static image towards the moving image\\n        \"\"\"\\n        displacement, energy = self.compute_backward_step(self.gradient_moving,\\n                                                          self.factors,\\n                                                          self.radius)\\n        displacement = np.array(displacement)\\n        for i in range(self.dim):\\n            displacement[..., i] = ndimage.gaussian_filter(\\n                displacement[..., i], self.sigma_diff)\\n        return displacement\\n\\n    def get_energy(self):\\n        r\"\"\"Numerical value assigned by this metric to the current image pair', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='def get_energy(self):\\n        r\"\"\"Numerical value assigned by this metric to the current image pair\\n\\n        Returns the Cross Correlation (data term) energy computed at the\\n        largest iteration\\n        \"\"\"\\n        return self.energy\\n\\n\\nclass EMMetric(SimilarityMetric):\\n    def __init__(self,\\n                 dim,\\n                 smooth=1.0,\\n                 inner_iter=5,\\n                 q_levels=256,\\n                 double_gradient=True,\\n                 step_type=\\'gauss_newton\\'):\\n        r\"\"\"Expectation-Maximization Metric\\n\\n        Similarity metric based on the Expectation-Maximization algorithm to\\n        handle multi-modal images. The transfer function is modeled as a set of\\n        hidden random variables that are estimated at each iteration of the\\n        algorithm.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='Parameters\\n        ----------\\n        dim : int (either 2 or 3)\\n            the dimension of the image domain\\n        smooth : float\\n            smoothness parameter, the larger the value the smoother the\\n            deformation field\\n        inner_iter : int\\n            number of iterations to be performed at each level of the multi-\\n            resolution Gauss-Seidel optimization algorithm (this is not the\\n            number of steps per Gaussian Pyramid level, that parameter must\\n            be set for the optimizer, not the metric)\\n        q_levels : number of quantization levels (equal to the number of hidden\\n            variables in the EM algorithm)\\n        double_gradient : boolean\\n            if True, the gradient of the expected static image under the moving\\n            modality will be added to the gradient of the moving image,\\n            similarly, the gradient of the expected moving image under the\\n            static modality will be added to the gradient of the static image.\\n        step_type : string (\\'gauss_newton\\', \\'demons\\')\\n            the optimization schedule to be used in the multi-resolution\\n            Gauss-Seidel optimization algorithm (not used if Demons Step is\\n            selected)\\n        \"\"\"\\n        super(EMMetric, self).__init__(dim)\\n        self.smooth = smooth\\n        self.inner_iter = inner_iter\\n        self.q_levels = q_levels\\n        self.use_double_gradient = double_gradient\\n        self.step_type = step_type\\n        self.static_image_mask = None\\n        self.moving_image_mask = None\\n        self.staticq_means_field = None\\n        self.movingq_means_field = None\\n        self.movingq_levels = None\\n        self.staticq_levels = None', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='selected)\\n        \"\"\"\\n        super(EMMetric, self).__init__(dim)\\n        self.smooth = smooth\\n        self.inner_iter = inner_iter\\n        self.q_levels = q_levels\\n        self.use_double_gradient = double_gradient\\n        self.step_type = step_type\\n        self.static_image_mask = None\\n        self.moving_image_mask = None\\n        self.staticq_means_field = None\\n        self.movingq_means_field = None\\n        self.movingq_levels = None\\n        self.staticq_levels = None\\n        self._connect_functions()', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='def _connect_functions(self):\\n        r\"\"\"Assign the methods to be called according to the image dimension\\n\\n        Assigns the appropriate functions to be called for image quantization,\\n        statistics computation and multi-resolution iterations according to the\\n        dimension of the input images\\n        \"\"\"\\n        if self.dim == 2:\\n            self.quantize = em.quantize_positive_2d\\n            self.compute_stats = em.compute_masked_class_stats_2d\\n            self.reorient_vector_field = vfu.reorient_vector_field_2d\\n        elif self.dim == 3:\\n            self.quantize = em.quantize_positive_3d\\n            self.compute_stats = em.compute_masked_class_stats_3d\\n            self.reorient_vector_field = vfu.reorient_vector_field_3d\\n        else:\\n            raise ValueError(\\'EM Metric not defined for dim. %d\\' % self.dim)\\n\\n        if self.step_type == \\'demons\\':\\n            self.compute_step = self.compute_demons_step\\n        elif self.step_type == \\'gauss_newton\\':\\n            self.compute_step = self.compute_gauss_newton_step\\n        else:\\n            raise ValueError(\\'Opt. step %s not defined\\' % self.step_type)\\n\\n    def initialize_iteration(self):\\n        r\"\"\"Prepares the metric to compute one displacement field iteration.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='def initialize_iteration(self):\\n        r\"\"\"Prepares the metric to compute one displacement field iteration.\\n\\n        Pre-computes the transfer functions (hidden random variables) and\\n        variances of the estimators. Also pre-computes the gradient of both\\n        input images. Note that once the images are transformed to the opposite\\n        modality, the gradient of the transformed images can be used with the\\n        gradient of the corresponding modality in the same fashion as\\n        diff-demons does for mono-modality images. If the flag\\n        self.use_double_gradient is True these gradients are averaged.\\n        \"\"\"\\n        sampling_mask = self.static_image_mask*self.moving_image_mask\\n        self.sampling_mask = sampling_mask\\n        staticq, self.staticq_levels, hist = self.quantize(self.static_image,\\n                                                           self.q_levels)\\n        staticq = np.array(staticq, dtype=np.int32)\\n        self.staticq_levels = np.array(self.staticq_levels)\\n        staticq_means, staticq_vars = self.compute_stats(sampling_mask,\\n                                                         self.moving_image,\\n                                                         self.q_levels,\\n                                                         staticq)\\n        staticq_means[0] = 0\\n        self.staticq_means = np.array(staticq_means)\\n        self.staticq_variances = np.array(staticq_vars)\\n        self.staticq_sigma_sq_field = self.staticq_variances[staticq]\\n        self.staticq_means_field = self.staticq_means[staticq]\\n\\n        self.gradient_moving = np.empty(\\n            shape=self.moving_image.shape+(self.dim,), dtype=floating)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content=\"self.gradient_moving = np.empty(\\n            shape=self.moving_image.shape+(self.dim,), dtype=floating)\\n\\n        for i, grad in enumerate(gradient(self.moving_image)):\\n            self.gradient_moving[..., i] = grad\\n\\n        # Convert moving image's gradient field from voxel to physical space\\n        if self.moving_spacing is not None:\\n            self.gradient_moving /= self.moving_spacing\\n        if self.moving_direction is not None:\\n            self.reorient_vector_field(self.gradient_moving,\\n                                       self.moving_direction)\\n\\n        self.gradient_static = np.empty(\\n            shape=self.static_image.shape+(self.dim,), dtype=floating)\\n\\n        for i, grad in enumerate(gradient(self.static_image)):\\n            self.gradient_static[..., i] = grad\\n\\n        # Convert moving image's gradient field from voxel to physical space\\n        if self.static_spacing is not None:\\n            self.gradient_static /= self.static_spacing\\n        if self.static_direction is not None:\\n            self.reorient_vector_field(self.gradient_static,\\n                                       self.static_direction)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content=\"for i, grad in enumerate(gradient(self.static_image)):\\n            self.gradient_static[..., i] = grad\\n\\n        # Convert moving image's gradient field from voxel to physical space\\n        if self.static_spacing is not None:\\n            self.gradient_static /= self.static_spacing\\n        if self.static_direction is not None:\\n            self.reorient_vector_field(self.gradient_static,\\n                                       self.static_direction)\\n\\n        movingq, self.movingq_levels, hist = self.quantize(self.moving_image,\\n                                                           self.q_levels)\\n        movingq = np.array(movingq, dtype=np.int32)\\n        self.movingq_levels = np.array(self.movingq_levels)\\n        movingq_means, movingq_variances = self.compute_stats(\\n            sampling_mask, self.static_image, self.q_levels, movingq)\\n        movingq_means[0] = 0\\n        self.movingq_means = np.array(movingq_means)\\n        self.movingq_variances = np.array(movingq_variances)\\n        self.movingq_sigma_sq_field = self.movingq_variances[movingq]\\n        self.movingq_means_field = self.movingq_means[movingq]\\n        if self.use_double_gradient:\\n            for i, grad in enumerate(gradient(self.staticq_means_field)):\\n                self.gradient_moving[..., i] += grad\\n\\n            for i, grad in enumerate(gradient(self.movingq_means_field)):\\n                self.gradient_static[..., i] += grad\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='for i, grad in enumerate(gradient(self.movingq_means_field)):\\n                self.gradient_static[..., i] += grad\\n\\n    def free_iteration(self):\\n        r\"\"\"\\n        Frees the resources allocated during initialization\\n        \"\"\"\\n        del self.sampling_mask\\n        del self.staticq_levels\\n        del self.movingq_levels\\n        del self.staticq_sigma_sq_field\\n        del self.staticq_means_field\\n        del self.movingq_sigma_sq_field\\n        del self.movingq_means_field\\n        del self.gradient_moving\\n        del self.gradient_static\\n\\n    def compute_forward(self):\\n        \"\"\"Computes one step bringing the reference image towards the static.\\n\\n        Computes the forward update field to register the moving image towards\\n        the static image in a gradient-based optimization algorithm\\n        \"\"\"\\n        return self.compute_step(True)\\n\\n    def compute_backward(self):\\n        r\"\"\"Computes one step bringing the static image towards the moving.\\n\\n        Computes the update displacement field to be used for registration of\\n        the static image towards the moving image\\n        \"\"\"\\n        return self.compute_step(False)\\n\\n    def compute_gauss_newton_step(self, forward_step=True):\\n        r\"\"\"Computes the Gauss-Newton energy minimization step\\n\\n        Computes the Newton step to minimize this energy, i.e., minimizes the\\n        linearized energy function with respect to the\\n        regularized displacement field (this step does not require\\n        post-smoothing, as opposed to the demons step, which does not include\\n        regularization). To accelerate convergence we use the multi-grid\\n        Gauss-Seidel algorithm proposed by Bruhn and Weickert et al [Bruhn05]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='Computes the Newton step to minimize this energy, i.e., minimizes the\\n        linearized energy function with respect to the\\n        regularized displacement field (this step does not require\\n        post-smoothing, as opposed to the demons step, which does not include\\n        regularization). To accelerate convergence we use the multi-grid\\n        Gauss-Seidel algorithm proposed by Bruhn and Weickert et al [Bruhn05]\\n\\n        Parameters\\n        ----------\\n        forward_step : boolean\\n            if True, computes the Newton step in the forward direction\\n            (warping the moving towards the static image). If False,\\n            computes the backward step (warping the static image to the\\n            moving image)\\n\\n        Returns\\n        -------\\n        displacement : array, shape (R, C, 2) or (S, R, C, 3)\\n            the Newton step\\n\\n        References\\n        ----------\\n        [Bruhn05] Andres Bruhn and Joachim Weickert, \"Towards ultimate motion\\n                  estimation: combining highest accuracy with real-time\\n                  performance\", 10th IEEE International Conference on Computer\\n                  Vision, 2005. ICCV 2005.\\n        \"\"\"\\n        reference_shape = self.static_image.shape\\n\\n        if forward_step:\\n            gradient = self.gradient_static\\n            delta = self.staticq_means_field - self.moving_image\\n            sigma_sq_field = self.staticq_sigma_sq_field\\n        else:\\n            gradient = self.gradient_moving\\n            delta = self.movingq_means_field - self.static_image\\n            sigma_sq_field = self.movingq_sigma_sq_field', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='if forward_step:\\n            gradient = self.gradient_static\\n            delta = self.staticq_means_field - self.moving_image\\n            sigma_sq_field = self.staticq_sigma_sq_field\\n        else:\\n            gradient = self.gradient_moving\\n            delta = self.movingq_means_field - self.static_image\\n            sigma_sq_field = self.movingq_sigma_sq_field\\n\\n        displacement = np.zeros(shape=reference_shape+(self.dim,),\\n                                dtype=floating)\\n\\n        if self.dim == 2:\\n            self.energy = v_cycle_2d(self.levels_below,\\n                                     self.inner_iter, delta,\\n                                     sigma_sq_field,\\n                                     gradient,\\n                                     None,\\n                                     self.smooth,\\n                                     displacement)\\n        else:\\n            self.energy = v_cycle_3d(self.levels_below,\\n                                     self.inner_iter, delta,\\n                                     sigma_sq_field,\\n                                     gradient,\\n                                     None,\\n                                     self.smooth,\\n                                     displacement)\\n        return displacement\\n\\n    def compute_demons_step(self, forward_step=True):\\n        r\"\"\"Demons step for EM metric\\n\\n        Parameters\\n        ----------\\n        forward_step : boolean\\n            if True, computes the Demons step in the forward direction\\n            (warping the moving towards the static image). If False,\\n            computes the backward step (warping the static image to the\\n            moving image)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='def compute_demons_step(self, forward_step=True):\\n        r\"\"\"Demons step for EM metric\\n\\n        Parameters\\n        ----------\\n        forward_step : boolean\\n            if True, computes the Demons step in the forward direction\\n            (warping the moving towards the static image). If False,\\n            computes the backward step (warping the static image to the\\n            moving image)\\n\\n        Returns\\n        -------\\n        displacement : array, shape (R, C, 2) or (S, R, C, 3)\\n            the Demons step\\n        \"\"\"\\n        sigma_reg_2 = np.sum(self.static_spacing**2)/self.dim\\n\\n        if forward_step:\\n            gradient = self.gradient_static\\n            delta_field = self.static_image - self.movingq_means_field\\n            sigma_sq_field = self.movingq_sigma_sq_field\\n        else:\\n            gradient = self.gradient_moving\\n            delta_field = self.moving_image - self.staticq_means_field\\n            sigma_sq_field = self.staticq_sigma_sq_field', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='if forward_step:\\n            gradient = self.gradient_static\\n            delta_field = self.static_image - self.movingq_means_field\\n            sigma_sq_field = self.movingq_sigma_sq_field\\n        else:\\n            gradient = self.gradient_moving\\n            delta_field = self.moving_image - self.staticq_means_field\\n            sigma_sq_field = self.staticq_sigma_sq_field\\n\\n        if self.dim == 2:\\n            step, self.energy = em.compute_em_demons_step_2d(delta_field,\\n                                                             sigma_sq_field,\\n                                                             gradient,\\n                                                             sigma_reg_2,\\n                                                             None)\\n        else:\\n            step, self.energy = em.compute_em_demons_step_3d(delta_field,\\n                                                             sigma_sq_field,\\n                                                             gradient,\\n                                                             sigma_reg_2,\\n                                                             None)\\n        for i in range(self.dim):\\n            step[..., i] = ndimage.gaussian_filter(step[..., i], self.smooth)\\n        return step\\n\\n    def get_energy(self):\\n        r\"\"\"The numerical value assigned by this metric to the current image pair\\n\\n        Returns the EM (data term) energy computed at the largest\\n        iteration\\n        \"\"\"\\n        return self.energy\\n\\n    def use_static_image_dynamics(self, original_static_image, transformation):\\n        r\"\"\"This is called by the optimizer just after setting the static image.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='def get_energy(self):\\n        r\"\"\"The numerical value assigned by this metric to the current image pair\\n\\n        Returns the EM (data term) energy computed at the largest\\n        iteration\\n        \"\"\"\\n        return self.energy\\n\\n    def use_static_image_dynamics(self, original_static_image, transformation):\\n        r\"\"\"This is called by the optimizer just after setting the static image.\\n\\n        EMMetric takes advantage of the image dynamics by computing the\\n        current static image mask from the originalstaticImage mask (warped\\n        by nearest neighbor interpolation)\\n\\n        Parameters\\n        ----------\\n        original_static_image : array, shape (R, C) or (S, R, C)\\n            the original static image from which the current static image was\\n            generated, the current static image is the one that was provided\\n            via \\'set_static_image(...)\\', which may not be the same as the\\n            original static image but a warped version of it (even the static\\n            image changes during Symmetric Normalization, not only the moving\\n            one).\\n        transformation : DiffeomorphicMap object\\n            the transformation that was applied to the original_static_image\\n            to generate the current static image\\n        \"\"\"\\n        self.static_image_mask = (original_static_image > 0).astype(np.int32)\\n        if transformation is None:\\n            return\\n        shape = np.array(self.static_image.shape, dtype=np.int32)\\n        affine = self.static_affine\\n        self.static_image_mask = transformation.transform(\\n            self.static_image_mask, \\'nearest\\', None, shape, affine)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='def use_moving_image_dynamics(self, original_moving_image, transformation):\\n        r\"\"\"This is called by the optimizer just after setting the moving image.\\n\\n        EMMetric takes advantage of the image dynamics by computing the\\n        current moving image mask from the original_moving_image mask (warped\\n        by nearest neighbor interpolation)\\n\\n        Parameters\\n        ----------\\n        original_moving_image : array, shape (R, C) or (S, R, C)\\n            the original moving image from which the current moving image was\\n            generated, the current moving image is the one that was provided\\n            via \\'set_moving_image(...)\\', which may not be the same as the\\n            original moving image but a warped version of it.\\n        transformation : DiffeomorphicMap object\\n            the transformation that was applied to the original_moving_image\\n            to generate the current moving image\\n        \"\"\"\\n        self.moving_image_mask = (original_moving_image > 0).astype(np.int32)\\n        if transformation is None:\\n            return\\n        shape = np.array(self.moving_image.shape, dtype=np.int32)\\n        affine = self.moving_affine\\n        self.moving_image_mask = transformation.transform(\\n            self.moving_image_mask, \\'nearest\\', None, shape, affine)\\n\\n\\nclass SSDMetric(SimilarityMetric):\\n\\n    def __init__(self, dim, smooth=4, inner_iter=10, step_type=\\'demons\\'):\\n        r\"\"\"Sum of Squared Differences (SSD) Metric\\n\\n        Similarity metric for (mono-modal) nonlinear image registration defined\\n        by the sum of squared differences (SSD)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='class SSDMetric(SimilarityMetric):\\n\\n    def __init__(self, dim, smooth=4, inner_iter=10, step_type=\\'demons\\'):\\n        r\"\"\"Sum of Squared Differences (SSD) Metric\\n\\n        Similarity metric for (mono-modal) nonlinear image registration defined\\n        by the sum of squared differences (SSD)\\n\\n        Parameters\\n        ----------\\n        dim : int (either 2 or 3)\\n            the dimension of the image domain\\n        smooth : float\\n            smoothness parameter, the larger the value the smoother the\\n            deformation field\\n        inner_iter : int\\n            number of iterations to be performed at each level of the multi-\\n            resolution Gauss-Seidel optimization algorithm (this is not the\\n            number of steps per Gaussian Pyramid level, that parameter must\\n            be set for the optimizer, not the metric)\\n        step_type : string\\n            the displacement field step to be computed when \\'compute_forward\\'\\n            and \\'compute_backward\\' are called. Either \\'demons\\' or\\n            \\'gauss_newton\\'\\n        \"\"\"\\n        super(SSDMetric, self).__init__(dim)\\n        self.smooth = smooth\\n        self.inner_iter = inner_iter\\n        self.step_type = step_type\\n        self.levels_below = 0\\n        self._connect_functions()\\n\\n    def _connect_functions(self):\\n        r\"\"\"Assign the methods to be called according to the image dimension', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='def _connect_functions(self):\\n        r\"\"\"Assign the methods to be called according to the image dimension\\n\\n        Assigns the appropriate functions to be called for vector field\\n        reorientation and displacement field steps according to the\\n        dimension of the input images and the select type of step (either\\n        Demons or Gauss Newton)\\n        \"\"\"\\n        if self.dim == 2:\\n            self.reorient_vector_field = vfu.reorient_vector_field_2d\\n        elif self.dim == 3:\\n            self.reorient_vector_field = vfu.reorient_vector_field_3d\\n        else:\\n            raise ValueError(\\'SSD Metric not defined for dim. %d\\' % self.dim)\\n\\n        if self.step_type == \\'gauss_newton\\':\\n            self.compute_step = self.compute_gauss_newton_step\\n        elif self.step_type == \\'demons\\':\\n            self.compute_step = self.compute_demons_step\\n        else:\\n            raise ValueError(\\'Opt. step %s not defined\\' % self.step_type)\\n\\n    def initialize_iteration(self):\\n        r\"\"\"Prepares the metric to compute one displacement field iteration.\\n\\n        Pre-computes the gradient of the input images to be used in the\\n        computation of the forward and backward steps.\\n        \"\"\"\\n        self.gradient_moving = np.empty(\\n            shape=self.moving_image.shape+(self.dim,), dtype=floating)\\n        for i, grad in enumerate(gradient(self.moving_image)):\\n            self.gradient_moving[..., i] = grad', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='def initialize_iteration(self):\\n        r\"\"\"Prepares the metric to compute one displacement field iteration.\\n\\n        Pre-computes the gradient of the input images to be used in the\\n        computation of the forward and backward steps.\\n        \"\"\"\\n        self.gradient_moving = np.empty(\\n            shape=self.moving_image.shape+(self.dim,), dtype=floating)\\n        for i, grad in enumerate(gradient(self.moving_image)):\\n            self.gradient_moving[..., i] = grad\\n\\n        # Convert static image\\'s gradient field from voxel to physical space\\n        if self.moving_spacing is not None:\\n            self.gradient_moving /= self.moving_spacing\\n        if self.moving_direction is not None:\\n            self.reorient_vector_field(self.gradient_moving,\\n                                       self.moving_direction)\\n\\n        self.gradient_static = np.empty(\\n            shape=self.static_image.shape+(self.dim,), dtype=floating)\\n        for i, grad in enumerate(gradient(self.static_image)):\\n            self.gradient_static[..., i] = grad\\n\\n        # Convert static image\\'s gradient field from voxel to physical space\\n        if self.static_spacing is not None:\\n            self.gradient_static /= self.static_spacing\\n        if self.static_direction is not None:\\n            self.reorient_vector_field(self.gradient_static,\\n                                       self.static_direction)\\n\\n    def compute_forward(self):\\n        r\"\"\"Computes one step bringing the reference image towards the static.\\n\\n        Computes the update displacement field to be used for registration of\\n        the moving image towards the static image\\n        \"\"\"\\n        return self.compute_step(True)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='def compute_forward(self):\\n        r\"\"\"Computes one step bringing the reference image towards the static.\\n\\n        Computes the update displacement field to be used for registration of\\n        the moving image towards the static image\\n        \"\"\"\\n        return self.compute_step(True)\\n\\n    def compute_backward(self):\\n        r\"\"\"Computes one step bringing the static image towards the moving.\\n\\n        Computes the updated displacement field to be used for registration of\\n        the static image towards the moving image\\n        \"\"\"\\n        return self.compute_step(False)\\n\\n    def compute_gauss_newton_step(self, forward_step=True):\\n        r\"\"\"Computes the Gauss-Newton energy minimization step\\n\\n        Minimizes the linearized energy function (Newton step) defined by the\\n        sum of squared differences of corresponding pixels of the input images\\n        with respect to the displacement field.\\n\\n        Parameters\\n        ----------\\n        forward_step : boolean\\n            if True, computes the Newton step in the forward direction\\n            (warping the moving towards the static image). If False,\\n            computes the backward step (warping the static image to the\\n            moving image)\\n\\n        Returns\\n        -------\\n        displacement : array, shape = static_image.shape + (3,)\\n            if forward_step==True, the forward SSD Gauss-Newton step,\\n            else, the backward step\\n        \"\"\"\\n        reference_shape = self.static_image.shape', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='Returns\\n        -------\\n        displacement : array, shape = static_image.shape + (3,)\\n            if forward_step==True, the forward SSD Gauss-Newton step,\\n            else, the backward step\\n        \"\"\"\\n        reference_shape = self.static_image.shape\\n\\n        if forward_step:\\n            gradient = self.gradient_static\\n            delta_field = self.static_image-self.moving_image\\n        else:\\n            gradient = self.gradient_moving\\n            delta_field = self.moving_image - self.static_image\\n\\n        displacement = np.zeros(shape=reference_shape+(self.dim,),\\n                                dtype=floating)\\n\\n        if self.dim == 2:\\n            self.energy = v_cycle_2d(self.levels_below, self.inner_iter,\\n                                     delta_field, None, gradient, None,\\n                                     self.smooth, displacement)\\n        else:\\n            self.energy = v_cycle_3d(self.levels_below, self.inner_iter,\\n                                     delta_field, None, gradient, None,\\n                                     self.smooth, displacement)\\n        return displacement\\n\\n    def compute_demons_step(self, forward_step=True):\\n        r\"\"\"Demons step for SSD metric\\n\\n        Computes the demons step proposed by Vercauteren et al.[Vercauteren09]\\n        for the SSD metric.\\n\\n        Parameters\\n        ----------\\n        forward_step : boolean\\n            if True, computes the Demons step in the forward direction\\n            (warping the moving towards the static image). If False,\\n            computes the backward step (warping the static image to the\\n            moving image)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='Computes the demons step proposed by Vercauteren et al.[Vercauteren09]\\n        for the SSD metric.\\n\\n        Parameters\\n        ----------\\n        forward_step : boolean\\n            if True, computes the Demons step in the forward direction\\n            (warping the moving towards the static image). If False,\\n            computes the backward step (warping the static image to the\\n            moving image)\\n\\n        Returns\\n        -------\\n        displacement : array, shape (R, C, 2) or (S, R, C, 3)\\n            the Demons step\\n\\n        References\\n        ----------\\n        [Vercauteren09] Tom Vercauteren, Xavier Pennec, Aymeric Perchant,\\n                        Nicholas Ayache, \"Diffeomorphic Demons: Efficient\\n                        Non-parametric Image Registration\", Neuroimage 2009\\n        \"\"\"\\n        sigma_reg_2 = np.sum(self.static_spacing**2)/self.dim\\n\\n        if forward_step:\\n            gradient = self.gradient_static\\n            delta_field = self.static_image - self.moving_image\\n        else:\\n            gradient = self.gradient_moving\\n            delta_field = self.moving_image - self.static_image', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='if forward_step:\\n            gradient = self.gradient_static\\n            delta_field = self.static_image - self.moving_image\\n        else:\\n            gradient = self.gradient_moving\\n            delta_field = self.moving_image - self.static_image\\n\\n        if self.dim == 2:\\n            step, self.energy = ssd.compute_ssd_demons_step_2d(delta_field,\\n                                                               gradient,\\n                                                               sigma_reg_2,\\n                                                               None)\\n        else:\\n            step, self.energy = ssd.compute_ssd_demons_step_3d(delta_field,\\n                                                               gradient,\\n                                                               sigma_reg_2,\\n                                                               None)\\n        for i in range(self.dim):\\n            step[..., i] = ndimage.gaussian_filter(step[..., i], self.smooth)\\n        return step\\n\\n    def get_energy(self):\\n        r\"\"\"The numerical value assigned by this metric to the current image pair\\n\\n        Returns the Sum of Squared Differences (data term) energy computed at\\n        the largest iteration\\n        \"\"\"\\n        return self.energy\\n\\n    def free_iteration(self):\\n        r\"\"\"\\n        Nothing to free for the SSD metric\\n        \"\"\"\\n        pass\\n\\n\\ndef v_cycle_2d(n, k, delta_field, sigma_sq_field, gradient_field, target,\\n               lambda_param, displacement, depth=0):\\n    r\"\"\"Multi-resolution Gauss-Seidel solver using V-type cycles', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='Returns the Sum of Squared Differences (data term) energy computed at\\n        the largest iteration\\n        \"\"\"\\n        return self.energy\\n\\n    def free_iteration(self):\\n        r\"\"\"\\n        Nothing to free for the SSD metric\\n        \"\"\"\\n        pass\\n\\n\\ndef v_cycle_2d(n, k, delta_field, sigma_sq_field, gradient_field, target,\\n               lambda_param, displacement, depth=0):\\n    r\"\"\"Multi-resolution Gauss-Seidel solver using V-type cycles\\n\\n    Multi-resolution Gauss-Seidel solver: solves the Gauss-Newton linear system\\n    by first filtering (GS-iterate) the current level, then solves for the\\n    residual at a coarser resolution and finally refines the solution at the\\n    current resolution. This scheme corresponds to the V-cycle proposed by\\n    Bruhn and Weickert[Bruhn05].', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content=\"Multi-resolution Gauss-Seidel solver: solves the Gauss-Newton linear system\\n    by first filtering (GS-iterate) the current level, then solves for the\\n    residual at a coarser resolution and finally refines the solution at the\\n    current resolution. This scheme corresponds to the V-cycle proposed by\\n    Bruhn and Weickert[Bruhn05].\\n\\n    Parameters\\n    ----------\\n    n : int\\n        number of levels of the multi-resolution algorithm (it will be called\\n        recursively until level n == 0)\\n    k : int\\n        the number of iterations at each multi-resolution level\\n    delta_field : array, shape (R, C)\\n        the difference between the static and moving image (the 'derivative\\n        w.r.t. time' in the optical flow model)\\n    sigma_sq_field : array, shape (R, C)\\n        the variance of the gray level value at each voxel, according to the\\n        EM model (for SSD, it is 1 for all voxels). Inf and 0 values\\n        are processed specially to support infinite and zero variance.\\n    gradient_field : array, shape (R, C, 2)\\n        the gradient of the moving image\\n    target : array, shape (R, C, 2)\\n        right-hand side of the linear system to be solved in the Weickert's\\n        multi-resolution algorithm\\n    lambda_param : float\\n        smoothness parameter, the larger its value the smoother the\\n        displacement field\\n    displacement : array, shape (R, C, 2)\\n        the displacement field to start the optimization from\\n\\n    Returns\\n    -------\\n    energy : the energy of the EM (or SSD if sigmafield[...]==1) metric at this\\n        iteration\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    energy : the energy of the EM (or SSD if sigmafield[...]==1) metric at this\\n        iteration\\n\\n    References\\n    ----------\\n    [Bruhn05] Andres Bruhn and Joachim Weickert, \"Towards ultimate motion\\n              estimation: combining the highest accuracy with real-time\\n              performance\", 10th IEEE International Conference on Computer\\n              Vision, 2005. ICCV 2005.\\n    \"\"\"\\n    # pre-smoothing\\n    for i in range(k):\\n        ssd.iterate_residual_displacement_field_ssd_2d(delta_field,\\n                                                       sigma_sq_field,\\n                                                       gradient_field,\\n                                                       target,\\n                                                       lambda_param,\\n                                                       displacement)\\n    if n == 0:\\n        energy = ssd.compute_energy_ssd_2d(delta_field)\\n        return energy', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='# solve at coarser grid\\n    residual = None\\n    residual = ssd.compute_residual_displacement_field_ssd_2d(delta_field,\\n                                                              sigma_sq_field,\\n                                                              gradient_field,\\n                                                              target,\\n                                                              lambda_param,\\n                                                              displacement,\\n                                                              residual)\\n    sub_residual = np.array(vfu.downsample_displacement_field_2d(residual))\\n    del residual\\n    subsigma_sq_field = None\\n    if sigma_sq_field is not None:\\n        subsigma_sq_field = vfu.downsample_scalar_field_2d(sigma_sq_field)\\n    subdelta_field = vfu.downsample_scalar_field_2d(delta_field)\\n\\n    subgradient_field = np.array(\\n        vfu.downsample_displacement_field_2d(gradient_field))\\n\\n    shape = np.array(displacement.shape).astype(np.int32)\\n    half_shape = ((shape[0] + 1) // 2, (shape[1] + 1) // 2, 2)\\n    sub_displacement = np.zeros(shape=half_shape,\\n                                dtype=floating)\\n    sublambda_param = lambda_param*0.25\\n    v_cycle_2d(n-1, k, subdelta_field, subsigma_sq_field, subgradient_field,\\n               sub_residual, sublambda_param, sub_displacement, depth+1)\\n    # displacement += np.array(\\n    #    vfu.upsample_displacement_field(sub_displacement, shape))\\n    displacement += vfu.resample_displacement_field_2d(sub_displacement,\\n                                                       np.array([0.5, 0.5]),\\n                                                       shape)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='# post-smoothing\\n    for i in range(k):\\n        ssd.iterate_residual_displacement_field_ssd_2d(delta_field,\\n                                                       sigma_sq_field,\\n                                                       gradient_field,\\n                                                       target,\\n                                                       lambda_param,\\n                                                       displacement)\\n    energy = ssd.compute_energy_ssd_2d(delta_field)\\n    return energy\\n\\n\\ndef v_cycle_3d(n, k, delta_field, sigma_sq_field, gradient_field, target,\\n               lambda_param, displacement, depth=0):\\n    r\"\"\"Multi-resolution Gauss-Seidel solver using V-type cycles\\n\\n    Multi-resolution Gauss-Seidel solver: solves the linear system by first\\n    filtering (GS-iterate) the current level, then solves for the residual\\n    at a coarser resolution and finally refines the solution at the current\\n    resolution. This scheme corresponds to the V-cycle proposed by Bruhn and\\n    Weickert[1].\\n    [1] Andres Bruhn and Joachim Weickert, \"Towards ultimate motion estimation:\\n        combining highest accuracy with real-time performance\",\\n        10th IEEE International Conference on Computer Vision, 2005.\\n        ICCV 2005.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content=\"Parameters\\n    ----------\\n    n : int\\n        number of levels of the multi-resolution algorithm (it will be called\\n        recursively until level n == 0)\\n    k : int\\n        the number of iterations at each multi-resolution level\\n    delta_field : array, shape (S, R, C)\\n        the difference between the static and moving image (the 'derivative\\n        w.r.t. time' in the optical flow model)\\n    sigma_sq_field : array, shape (S, R, C)\\n        the variance of the gray level value at each voxel, according to the\\n        EM model (for SSD, it is 1 for all voxels). Inf and 0 values\\n        are processed specially to support infinite and zero variance.\\n    gradient_field : array, shape (S, R, C, 3)\\n        the gradient of the moving image\\n    target : array, shape (S, R, C, 3)\\n        right-hand side of the linear system to be solved in the Weickert's\\n        multi-resolution algorithm\\n    lambda_param : float\\n        smoothness parameter, the larger its value the smoother the\\n        displacement field\\n    displacement : array, shape (S, R, C, 3)\\n        the displacement field to start the optimization from\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    energy : the energy of the EM (or SSD if sigmafield[...]==1) metric at this\\n        iteration\\n    \"\"\"\\n    # pre-smoothing\\n    for i in range(k):\\n        ssd.iterate_residual_displacement_field_ssd_3d(delta_field,\\n                                                       sigma_sq_field,\\n                                                       gradient_field,\\n                                                       target,\\n                                                       lambda_param,\\n                                                       displacement)\\n    if n == 0:\\n        energy = ssd.compute_energy_ssd_3d(delta_field)\\n        return energy\\n    # solve at coarser grid\\n    residual = ssd.compute_residual_displacement_field_ssd_3d(delta_field,\\n                                                              sigma_sq_field,\\n                                                              gradient_field,\\n                                                              target,\\n                                                              lambda_param,\\n                                                              displacement,\\n                                                              None)\\n    sub_residual = np.array(vfu.downsample_displacement_field_3d(residual))\\n    del residual\\n    subsigma_sq_field = None\\n    if sigma_sq_field is not None:\\n        subsigma_sq_field = vfu.downsample_scalar_field_3d(sigma_sq_field)\\n    subdelta_field = vfu.downsample_scalar_field_3d(delta_field)\\n    subgradient_field = np.array(\\n        vfu.downsample_displacement_field_3d(gradient_field))\\n    shape = np.array(displacement.shape).astype(np.int32)\\n    sub_displacement = np.zeros(', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='sub_residual = np.array(vfu.downsample_displacement_field_3d(residual))\\n    del residual\\n    subsigma_sq_field = None\\n    if sigma_sq_field is not None:\\n        subsigma_sq_field = vfu.downsample_scalar_field_3d(sigma_sq_field)\\n    subdelta_field = vfu.downsample_scalar_field_3d(delta_field)\\n    subgradient_field = np.array(\\n        vfu.downsample_displacement_field_3d(gradient_field))\\n    shape = np.array(displacement.shape).astype(np.int32)\\n    sub_displacement = np.zeros(\\n        shape=((shape[0]+1)//2, (shape[1]+1)//2, (shape[2]+1)//2, 3),\\n        dtype=floating)\\n    sublambda_param = lambda_param*0.25\\n    v_cycle_3d(n-1, k, subdelta_field, subsigma_sq_field, subgradient_field,\\n               sub_residual, sublambda_param, sub_displacement, depth+1)\\n    del subdelta_field\\n    del subsigma_sq_field\\n    del subgradient_field\\n    del sub_residual\\n    displacement += vfu.resample_displacement_field_3d(sub_displacement,\\n                                                       0.5 * np.ones(3),\\n                                                       shape)\\n    del sub_displacement\\n    # post-smoothing\\n    for i in range(k):\\n        ssd.iterate_residual_displacement_field_ssd_3d(delta_field,\\n                                                       sigma_sq_field,\\n                                                       gradient_field,\\n                                                       target,\\n                                                       lambda_param,\\n                                                       displacement)\\n    energy = ssd.compute_energy_ssd_3d(delta_field)\\n    return energy', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\metrics.py.txt'}),\n",
       " Document(page_content='#!python\\n#cython: boundscheck=False\\n#cython: wraparound=False\\n#cython: cdivision=True\\n\\nimport numpy as np\\ncimport numpy as cnp\\ncimport cython\\nfrom dipy.align.fused_types cimport floating\\nfrom dipy.align import vector_fields as vf\\n\\nfrom dipy.align.vector_fields cimport(_apply_affine_3d_x0,\\n                                      _apply_affine_3d_x1,\\n                                      _apply_affine_3d_x2,\\n                                      _apply_affine_2d_x0,\\n                                      _apply_affine_2d_x1)\\n\\nfrom dipy.align.transforms cimport (Transform)\\n\\ncdef extern from \"dpy_math.h\" nogil:\\n    double cos(double)\\n    double sin(double)\\n    double log(double)\\n\\nclass ParzenJointHistogram:\\n    def __init__(self, nbins):\\n        r\"\"\" Computes joint histogram and derivatives with Parzen windows\\n\\n        Base class to compute joint and marginal probability density\\n        functions and their derivatives with respect to a transform\\'s\\n        parameters. The smooth histograms are computed by using Parzen\\n        windows [Parzen62] with a cubic spline kernel, as proposed by\\n        Mattes et al. [Mattes03]. This implementation is not tied to any\\n        optimization (registration) method, the idea is that\\n        information-theoretic matching functionals (such as Mutual\\n        Information) can inherit from this class to perform the low-level\\n        computations of the joint intensity distributions and its gradient\\n        w.r.t. the transform parameters. The derived class can then compute\\n        the similarity/dissimilarity measure and gradient, and finally\\n        communicate the results to the appropriate optimizer.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='Parameters\\n        ----------\\n        nbins : int\\n            the number of bins of the joint and marginal probability density\\n            functions (the actual number of bins of the joint PDF is nbins**2)\\n\\n        References\\n        ----------\\n        [Parzen62] E. Parzen. On the estimation of a probability density\\n                   function and the mode. Annals of Mathematical Statistics,\\n                   33(3), 1065-1076, 1962.\\n        [Mattes03] Mattes, D., Haynor, D. R., Vesselle, H., Lewellen, T. K.,\\n                   & Eubank, W. PET-CT image registration in the chest using\\n                   free-form deformations. IEEE Transactions on Medical\\n                   Imaging, 22(1), 120-8, 2003.\\n\\n        Notes\\n        -----\\n        We need this class in cython to allow _joint_pdf_gradient_dense_2d and\\n        _joint_pdf_gradient_dense_3d to use a nogil Jacobian function (obtained\\n        from an instance of the Transform class), which allows us to evaluate\\n        Jacobians at all the sampling points (maybe the full grid) inside a\\n        nogil loop.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='Notes\\n        -----\\n        We need this class in cython to allow _joint_pdf_gradient_dense_2d and\\n        _joint_pdf_gradient_dense_3d to use a nogil Jacobian function (obtained\\n        from an instance of the Transform class), which allows us to evaluate\\n        Jacobians at all the sampling points (maybe the full grid) inside a\\n        nogil loop.\\n\\n        The reason we need a class is to encapsulate all the parameters related\\n        to the joint and marginal distributions.\\n        \"\"\"\\n        self.nbins = nbins\\n        # Since the kernel used to compute the Parzen histogram covers more\\n        # than one bin, we need to add extra bins to both sides of the\\n        # histogram to account for the contributions of the minimum and maximum\\n        # intensities. Padding is the number of extra bins used at each side\\n        # of the histogram (a total of [2 * padding] extra bins). Since the\\n        # support of the cubic spline is 5 bins (the center plus 2 bins at each\\n        # side) we need a padding of 2, in the case of cubic splines.\\n        self.padding = 2\\n        self.setup_called = False\\n\\n    def setup(self, static, moving, smask=None, mmask=None):\\n        r\"\"\" Compute histogram settings to store the PDF of input images', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='def setup(self, static, moving, smask=None, mmask=None):\\n        r\"\"\" Compute histogram settings to store the PDF of input images\\n\\n        Parameters\\n        ----------\\n        static : array\\n            static image\\n        moving : array\\n            moving image\\n        smask : array\\n            mask of static object being registered (a binary array with 1\\'s\\n            inside the object of interest and 0\\'s along the background).\\n            If None, the behaviour is equivalent to smask=ones_like(static)\\n        mmask : array\\n            mask of moving object being registered (a binary array with 1\\'s\\n            inside the object of interest and 0\\'s along the background).\\n            If None, the behaviour is equivalent to mmask=ones_like(static)\\n        \"\"\"\\n\\n        if smask is None:\\n            smask = np.ones_like(static)\\n        if mmask is None:\\n            mmask = np.ones_like(moving)\\n\\n        self.smin = np.min(static[smask != 0])\\n        self.smax = np.max(static[smask != 0])\\n        self.mmin = np.min(moving[mmask != 0])\\n        self.mmax = np.max(moving[mmask != 0])\\n\\n        numerator = self.smax - self.smin\\n        denominator = self.nbins - 2 * self.padding\\n        self.sdelta = np.divide(numerator, denominator,\\n                                out=np.zeros_like(numerator, dtype=np.float64),\\n                                where=denominator!=0)\\n        numerator = self.mmax - self.mmin\\n        self.mdelta = np.divide(numerator, denominator,\\n                                out=np.zeros_like(numerator, dtype=np.float64),\\n                                where=denominator!=0)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='self.smin = np.divide(self.smin, self.sdelta,\\n                              out=np.zeros_like(self.smin, dtype=np.float64),\\n                              where=self.sdelta!=0) - self.padding\\n        self.mmin = np.divide(self.mmin, self.mdelta,\\n                              out=np.zeros_like(self.mmin, dtype=np.float64),\\n                              where=self.mdelta!=0) - self.padding\\n\\n        self.joint_grad = None\\n        self.metric_grad = None\\n        self.metric_val = 0\\n        self.joint = np.zeros(shape=(self.nbins, self.nbins))\\n        self.smarginal = np.zeros(shape=(self.nbins,), dtype=np.float64)\\n        self.mmarginal = np.zeros(shape=(self.nbins,), dtype=np.float64)\\n\\n        self.setup_called = True\\n\\n    def bin_normalize_static(self, x):\\n        r\"\"\" Maps intensity x to the range covered by the static histogram\\n\\n        If the input intensity is in [self.smin, self.smax] then the normalized\\n        intensity will be in [self.padding, self.nbins - self.padding]\\n\\n        Parameters\\n        ----------\\n        x : float\\n            the intensity to be normalized\\n\\n        Returns\\n        -------\\n        xnorm : float\\n            normalized intensity to the range covered by the static histogram\\n        \"\"\"\\n        return _bin_normalize(x, self.smin, self.sdelta)\\n\\n    def bin_normalize_moving(self, x):\\n        r\"\"\" Maps intensity x to the range covered by the moving histogram\\n\\n        If the input intensity is in [self.mmin, self.mmax] then the normalized\\n        intensity will be in [self.padding, self.nbins - self.padding]\\n\\n        Parameters\\n        ----------\\n        x : float\\n            the intensity to be normalized', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='def bin_normalize_moving(self, x):\\n        r\"\"\" Maps intensity x to the range covered by the moving histogram\\n\\n        If the input intensity is in [self.mmin, self.mmax] then the normalized\\n        intensity will be in [self.padding, self.nbins - self.padding]\\n\\n        Parameters\\n        ----------\\n        x : float\\n            the intensity to be normalized\\n\\n        Returns\\n        -------\\n        xnorm : float\\n            normalized intensity to the range covered by the moving histogram\\n        \"\"\"\\n        return _bin_normalize(x, self.mmin, self.mdelta)\\n\\n    def bin_index(self, xnorm):\\n        r\"\"\" Bin index associated with the given normalized intensity\\n\\n        The return value is an integer in [padding, nbins - 1 - padding]\\n\\n        Parameters\\n        ----------\\n        xnorm : float\\n            intensity value normalized to the range covered by the histogram\\n\\n        Returns\\n        -------\\n        bin : int\\n            the bin index associated with the given normalized intensity\\n        \"\"\"\\n        return _bin_index(xnorm, self.nbins, self.padding)\\n\\n    def update_pdfs_dense(self, static, moving, smask=None, mmask=None):\\n        r\"\"\" Computes the Probability Density Functions of two images\\n\\n        The joint PDF is stored in self.joint. The marginal distributions\\n        corresponding to the static and moving images are computed and\\n        stored in self.smarginal and self.mmarginal, respectively.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='def update_pdfs_dense(self, static, moving, smask=None, mmask=None):\\n        r\"\"\" Computes the Probability Density Functions of two images\\n\\n        The joint PDF is stored in self.joint. The marginal distributions\\n        corresponding to the static and moving images are computed and\\n        stored in self.smarginal and self.mmarginal, respectively.\\n\\n        Parameters\\n        ----------\\n        static : array, shape (S, R, C)\\n            static image\\n        moving : array, shape (S, R, C)\\n            moving image\\n        smask : array, shape (S, R, C)\\n            mask of static object being registered (a binary array with 1\\'s\\n            inside the object of interest and 0\\'s along the background).\\n            If None, ones_like(static) is used as mask.\\n        mmask : array, shape (S, R, C)\\n            mask of moving object being registered (a binary array with 1\\'s\\n            inside the object of interest and 0\\'s along the background).\\n            If None, ones_like(moving) is used as mask.\\n        \"\"\"\\n        if static.shape != moving.shape:\\n            raise ValueError(\"Images must have the same shape\")\\n        dim = len(static.shape)\\n        if not dim in [2, 3]:\\n            msg = \\'Only dimensions 2 and 3 are supported. \\' +\\\\\\n                    str(dim) + \\' received\\'\\n            raise ValueError(msg)\\n        if not self.setup_called:\\n            self.setup(static, moving, smask=None, mmask=None)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='if dim == 2:\\n            _compute_pdfs_dense_2d(static, moving, smask, mmask, self.smin,\\n                                   self.sdelta, self.mmin, self.mdelta,\\n                                   self.nbins, self.padding, self.joint,\\n                                   self.smarginal, self.mmarginal)\\n        elif dim == 3:\\n            _compute_pdfs_dense_3d(static, moving, smask, mmask, self.smin,\\n                                   self.sdelta, self.mmin, self.mdelta,\\n                                   self.nbins, self.padding, self.joint,\\n                                   self.smarginal, self.mmarginal)\\n\\n    def update_pdfs_sparse(self, sval, mval):\\n        r\"\"\" Computes the Probability Density Functions from a set of samples\\n\\n        The list of intensities `sval` and `mval` are assumed to be sampled\\n        from the static and moving images, respectively, at the same\\n        physical points. Of course, the images may not be perfectly aligned\\n        at the moment the sampling was performed. The resulting  distributions\\n        corresponds to the paired intensities according to the alignment at the\\n        moment the images were sampled.\\n\\n        The joint PDF is stored in self.joint. The marginal distributions\\n        corresponding to the static and moving images are computed and\\n        stored in self.smarginal and self.mmarginal, respectively.\\n\\n        Parameters\\n        ----------\\n        sval : array, shape (n,)\\n            sampled intensities from the static image at sampled_points\\n        mval : array, shape (n,)\\n            sampled intensities from the moving image at sampled_points\\n        \"\"\"\\n        if not self.setup_called:\\n            self.setup(sval, mval)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='Parameters\\n        ----------\\n        sval : array, shape (n,)\\n            sampled intensities from the static image at sampled_points\\n        mval : array, shape (n,)\\n            sampled intensities from the moving image at sampled_points\\n        \"\"\"\\n        if not self.setup_called:\\n            self.setup(sval, mval)\\n\\n        energy = _compute_pdfs_sparse(sval, mval, self.smin, self.sdelta,\\n                                      self.mmin, self.mdelta, self.nbins,\\n                                      self.padding, self.joint,\\n                                      self.smarginal, self.mmarginal)\\n\\n    def update_gradient_dense(self, theta, transform, static, moving,\\n                              grid2world, mgradient, smask=None, mmask=None):\\n        r\"\"\" Computes the Gradient of the joint PDF w.r.t. transform parameters\\n\\n        Computes the vector of partial derivatives of the joint histogram\\n        w.r.t. each transformation parameter.\\n\\n        The gradient is stored in self.joint_grad.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='def update_gradient_dense(self, theta, transform, static, moving,\\n                              grid2world, mgradient, smask=None, mmask=None):\\n        r\"\"\" Computes the Gradient of the joint PDF w.r.t. transform parameters\\n\\n        Computes the vector of partial derivatives of the joint histogram\\n        w.r.t. each transformation parameter.\\n\\n        The gradient is stored in self.joint_grad.\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (n,)\\n            parameters of the transformation to compute the gradient from\\n        transform : instance of Transform\\n            the transformation with respect to whose parameters the gradient\\n            must be computed\\n        static : array, shape (S, R, C)\\n            static image\\n        moving : array, shape (S, R, C)\\n            moving image\\n        grid2world : array, shape (4, 4)\\n            we assume that both images have already been sampled at a common\\n            grid. This transform must map voxel coordinates of this common grid\\n            to physical coordinates of its corresponding voxel in the moving\\n            image. For example, if the moving image was sampled on the static\\n            image\\'s grid (this is the typical setting) using an aligning\\n            matrix A, then\\n\\n            (1) grid2world = A.dot(static_affine)\\n\\n            where static_affine is the transformation mapping static image\\'s\\n            grid coordinates to physical space.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='(1) grid2world = A.dot(static_affine)\\n\\n            where static_affine is the transformation mapping static image\\'s\\n            grid coordinates to physical space.\\n\\n        mgradient : array, shape (S, R, C, 3)\\n            the gradient of the moving image\\n        smask : array, shape (S, R, C), optional\\n            mask of static object being registered (a binary array with 1\\'s\\n            inside the object of interest and 0\\'s along the background).\\n            The default is None, indicating all voxels are considered.\\n        mmask : array, shape (S, R, C), optional\\n            mask of moving object being registered (a binary array with 1\\'s\\n            inside the object of interest and 0\\'s along the background).\\n            The default is None, indicating all voxels are considered.\\n        \"\"\"\\n        if static.shape != moving.shape:\\n            raise ValueError(\"Images must have the same shape\")\\n        dim = len(static.shape)\\n        if not dim in [2, 3]:\\n            msg = \\'Only dimensions 2 and 3 are supported. \\' +\\\\\\n                str(dim) + \\' received\\'\\n            raise ValueError(msg)\\n\\n        if mgradient.shape != moving.shape + (dim,):\\n            raise ValueError(\\'Invalid gradient field dimensions.\\')\\n\\n        if not self.setup_called:\\n            self.setup(static, moving, smask, mmask)\\n\\n        n = theta.shape[0]\\n        nbins = self.nbins', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content=\"if mgradient.shape != moving.shape + (dim,):\\n            raise ValueError('Invalid gradient field dimensions.')\\n\\n        if not self.setup_called:\\n            self.setup(static, moving, smask, mmask)\\n\\n        n = theta.shape[0]\\n        nbins = self.nbins\\n\\n        if (self.joint_grad is None) or (self.joint_grad.shape[2] != n):\\n            self.joint_grad = np.zeros((nbins, nbins, n))\\n        if dim == 2:\\n            if mgradient.dtype == np.float64:\\n                _joint_pdf_gradient_dense_2d[cython.double](theta, transform,\\n                    static, moving, grid2world, mgradient, smask, mmask,\\n                    self.smin, self.sdelta, self.mmin, self.mdelta,\\n                    self.nbins, self.padding, self.joint_grad)\\n            elif mgradient.dtype == np.float32:\\n                _joint_pdf_gradient_dense_2d[cython.float](theta, transform,\\n                    static, moving, grid2world, mgradient, smask, mmask,\\n                    self.smin, self.sdelta, self.mmin, self.mdelta,\\n                    self.nbins, self.padding, self.joint_grad)\\n            else:\\n                raise ValueError('Grad. field dtype must be floating point')\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='elif dim == 3:\\n            if mgradient.dtype == np.float64:\\n                _joint_pdf_gradient_dense_3d[cython.double](theta, transform,\\n                    static, moving, grid2world, mgradient, smask, mmask,\\n                    self.smin, self.sdelta, self.mmin, self.mdelta,\\n                    self.nbins, self.padding, self.joint_grad)\\n            elif mgradient.dtype == np.float32:\\n                _joint_pdf_gradient_dense_3d[cython.float](theta, transform,\\n                    static, moving, grid2world, mgradient, smask, mmask,\\n                    self.smin, self.sdelta, self.mmin, self.mdelta,\\n                    self.nbins, self.padding, self.joint_grad)\\n            else:\\n                raise ValueError(\\'Grad. field dtype must be floating point\\')\\n\\n    def update_gradient_sparse(self, theta, transform, sval, mval,\\n                               sample_points, mgradient):\\n        r\"\"\" Computes the Gradient of the joint PDF w.r.t. transform parameters\\n\\n        Computes the vector of partial derivatives of the joint histogram\\n        w.r.t. each transformation parameter.\\n\\n        The list of intensities `sval` and `mval` are assumed to be sampled\\n        from the static and moving images, respectively, at the same\\n        physical points. Of course, the images may not be perfectly aligned\\n        at the moment the sampling was performed. The resulting  gradient\\n        corresponds to the paired intensities according to the alignment at the\\n        moment the images were sampled.\\n\\n        The gradient is stored in self.joint_grad.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='The list of intensities `sval` and `mval` are assumed to be sampled\\n        from the static and moving images, respectively, at the same\\n        physical points. Of course, the images may not be perfectly aligned\\n        at the moment the sampling was performed. The resulting  gradient\\n        corresponds to the paired intensities according to the alignment at the\\n        moment the images were sampled.\\n\\n        The gradient is stored in self.joint_grad.\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (n,)\\n            parameters to compute the gradient at\\n        transform : instance of Transform\\n            the transformation with respect to whose parameters the gradient\\n            must be computed\\n        sval : array, shape (m,)\\n            sampled intensities from the static image at sampled_points\\n        mval : array, shape (m,)\\n            sampled intensities from the moving image at sampled_points\\n        sample_points : array, shape (m, 3)\\n            coordinates (in physical space) of the points the images were\\n            sampled at\\n        mgradient : array, shape (m, 3)\\n            the gradient of the moving image at the sample points\\n        \"\"\"\\n        dim = sample_points.shape[1]\\n        if mgradient.shape[1] != dim:\\n            raise ValueError(\\'Dimensions of gradients and points are different\\')\\n\\n        nsamples = sval.shape[0]\\n        if ((mgradient.shape[0] != nsamples) or (mval.shape[0] != nsamples)\\n            or sample_points.shape[0] != nsamples):\\n            raise ValueError(\\'Number of points and gradients are different.\\')', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content=\"nsamples = sval.shape[0]\\n        if ((mgradient.shape[0] != nsamples) or (mval.shape[0] != nsamples)\\n            or sample_points.shape[0] != nsamples):\\n            raise ValueError('Number of points and gradients are different.')\\n\\n        if not mgradient.dtype in [np.float32, np.float64]:\\n            raise ValueError('Gradients dtype must be floating point')\\n\\n        n = theta.shape[0]\\n        nbins = self.nbins\\n\\n        if (self.joint_grad is None) or (self.joint_grad.shape[2] != n):\\n            self.joint_grad = np.zeros(shape=(nbins, nbins, n))\\n\\n        if dim == 2:\\n            if mgradient.dtype == np.float64:\\n                _joint_pdf_gradient_sparse_2d[cython.double](theta, transform,\\n                    sval, mval, sample_points, mgradient, self.smin,\\n                    self.sdelta, self.mmin, self.mdelta, self.nbins,\\n                    self.padding, self.joint_grad)\\n            elif mgradient.dtype == np.float32:\\n                _joint_pdf_gradient_sparse_2d[cython.float](theta, transform,\\n                    sval, mval, sample_points, mgradient, self.smin,\\n                    self.sdelta, self.mmin, self.mdelta, self.nbins,\\n                    self.padding, self.joint_grad)\\n            else:\\n                raise ValueError('Gradients dtype must be floating point')\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='elif dim == 3:\\n            if mgradient.dtype == np.float64:\\n                _joint_pdf_gradient_sparse_3d[cython.double](theta, transform,\\n                    sval, mval, sample_points, mgradient, self.smin,\\n                    self.sdelta, self.mmin, self.mdelta, self.nbins,\\n                    self.padding, self.joint_grad)\\n            elif mgradient.dtype == np.float32:\\n                _joint_pdf_gradient_sparse_3d[cython.float](theta, transform,\\n                    sval, mval, sample_points, mgradient, self.smin,\\n                    self.sdelta, self.mmin, self.mdelta, self.nbins,\\n                    self.padding, self.joint_grad)\\n            else:\\n                raise ValueError(\\'Gradients dtype must be floating point\\')\\n        else:\\n            msg = \\'Only dimensions 2 and 3 are supported. \\' + str(dim) +\\\\\\n                \\' received\\'\\n            raise ValueError(msg)\\n\\n\\ncdef inline double _bin_normalize(double x, double mval, double delta) nogil:\\n    r\"\"\" Normalizes intensity x to the range covered by the Parzen histogram\\n    We assume that mval was computed as:\\n\\n    (1) mval = xmin / delta - padding\\n\\n    where xmin is the minimum observed image intensity and delta is the\\n    bin size, computed as:\\n\\n    (2) delta = (xmax - xmin)/(nbins - 2 * padding)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='cdef inline double _bin_normalize(double x, double mval, double delta) nogil:\\n    r\"\"\" Normalizes intensity x to the range covered by the Parzen histogram\\n    We assume that mval was computed as:\\n\\n    (1) mval = xmin / delta - padding\\n\\n    where xmin is the minimum observed image intensity and delta is the\\n    bin size, computed as:\\n\\n    (2) delta = (xmax - xmin)/(nbins - 2 * padding)\\n\\n    If the minimum and maximum intensities were assigned to the first and last\\n    bins (with no padding), it could be possible that samples at the first and\\n    last bins contribute to \"non-existing\" bins beyond the boundary (because\\n    the support of the Parzen window may be larger than one bin). The padding\\n    bins are used to collect such contributions (i.e. the probability of\\n    observing a value beyond the minimum and maximum observed intensities may\\n    correctly be assigned a positive value).\\n\\n    The normalized intensity is (from eq(1) ):\\n\\n    (3) nx = (x - xmin) / delta + padding = x / delta - mval\\n\\n    This means that normalized intensity nx must lie in the closed interval\\n    [padding, nbins-padding], which contains bins with indices\\n    padding, padding+1, ..., nbins - 1 - padding (i.e., nbins - 2*padding bins)\\n\\n    \"\"\"\\n    if delta == 0:\\n        return 0\\n    return x / delta - mval\\n\\n\\ncdef inline cnp.npy_intp _bin_index(double normalized, int nbins,\\n                                    int padding) nogil:\\n    r\"\"\" Index of the bin in which the normalized intensity `normalized` lies.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='This means that normalized intensity nx must lie in the closed interval\\n    [padding, nbins-padding], which contains bins with indices\\n    padding, padding+1, ..., nbins - 1 - padding (i.e., nbins - 2*padding bins)\\n\\n    \"\"\"\\n    if delta == 0:\\n        return 0\\n    return x / delta - mval\\n\\n\\ncdef inline cnp.npy_intp _bin_index(double normalized, int nbins,\\n                                    int padding) nogil:\\n    r\"\"\" Index of the bin in which the normalized intensity `normalized` lies.\\n\\n    The intensity is assumed to have been normalized to the range of\\n    intensities covered by the histogram: the bin index is the integer part of\\n    `normalized`, which must be within the interval\\n    [padding, nbins - 1 - padding].\\n\\n    Parameters\\n    ----------\\n    normalized : float\\n        normalized intensity\\n    nbins : int\\n        number of histogram bins\\n    padding : int\\n        number of bins used as padding (the total bins used for padding at\\n        both sides of the histogram is actually 2*padding)\\n\\n    Returns\\n    -------\\n    bin_id : int\\n        index of the bin in which the normalized intensity \\'normalized\\' lies\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp bin_id\\n\\n    bin_id = <cnp.npy_intp>normalized\\n    if bin_id < padding:\\n        return padding\\n    if bin_id > nbins - 1 - padding:\\n        return nbins - 1 - padding\\n    return bin_id\\n\\n\\ndef cubic_spline(double[:] x):\\n    r\"\"\" Evaluates the cubic spline at a set of values', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    bin_id : int\\n        index of the bin in which the normalized intensity \\'normalized\\' lies\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp bin_id\\n\\n    bin_id = <cnp.npy_intp>normalized\\n    if bin_id < padding:\\n        return padding\\n    if bin_id > nbins - 1 - padding:\\n        return nbins - 1 - padding\\n    return bin_id\\n\\n\\ndef cubic_spline(double[:] x):\\n    r\"\"\" Evaluates the cubic spline at a set of values\\n\\n    Parameters\\n    ----------\\n    x : array, shape (n)\\n        input values\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp i\\n        cnp.npy_intp n = x.shape[0]\\n        double[:] sx = np.zeros(n, dtype=np.float64)\\n    with nogil:\\n        for i in range(n):\\n            sx[i] = _cubic_spline(x[i])\\n    return np.asarray(sx)\\n\\n\\ncdef inline double _cubic_spline(double x) nogil:\\n    r\"\"\" Cubic B-Spline evaluated at x\\n    See eq. (3) of [Matttes03].\\n\\n    References\\n    ----------\\n    [Mattes03] Mattes, D., Haynor, D. R., Vesselle, H., Lewellen, T. K.,\\n               & Eubank, W. PET-CT image registration in the chest using\\n               free-form deformations. IEEE Transactions on Medical Imaging,\\n               22(1), 120-8, 2003.\\n    \"\"\"\\n    cdef:\\n        double absx = -x if x < 0.0 else x\\n        double sqrx = x * x\\n\\n    if absx < 1.0:\\n        return (4.0 - 6.0 * sqrx + 3.0 * sqrx * absx) / 6.0\\n    elif absx < 2.0:\\n        return (8.0 - 12 * absx + 6.0 * sqrx - sqrx * absx) / 6.0\\n    return 0.0\\n\\n\\ndef cubic_spline_derivative(double[:] x):\\n    r\"\"\" Evaluates the cubic spline derivative at a set of values', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='if absx < 1.0:\\n        return (4.0 - 6.0 * sqrx + 3.0 * sqrx * absx) / 6.0\\n    elif absx < 2.0:\\n        return (8.0 - 12 * absx + 6.0 * sqrx - sqrx * absx) / 6.0\\n    return 0.0\\n\\n\\ndef cubic_spline_derivative(double[:] x):\\n    r\"\"\" Evaluates the cubic spline derivative at a set of values\\n\\n    Parameters\\n    ----------\\n    x : array, shape (n)\\n        input values\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp i\\n        cnp.npy_intp n = x.shape[0]\\n        double[:] sx = np.zeros(n, dtype=np.float64)\\n    with nogil:\\n        for i in range(n):\\n            sx[i] = _cubic_spline_derivative(x[i])\\n    return np.asarray(sx)\\n\\n\\ncdef inline double _cubic_spline_derivative(double x) nogil:\\n    r\"\"\" Derivative of cubic B-Spline evaluated at x\\n    See eq. (3) of [Mattes03].\\n\\n    References\\n    ----------\\n    [Mattes03] Mattes, D., Haynor, D. R., Vesselle, H., Lewellen, T. K.,\\n               & Eubank, W. PET-CT image registration in the chest using\\n               free-form deformations. IEEE Transactions on Medical Imaging,\\n               22(1), 120-8, 2003.\\n    \"\"\"\\n    cdef:\\n        double absx = -x if x < 0.0 else x\\n    if absx < 1.0:\\n        if x >= 0.0:\\n            return -2.0 * x + 1.5 * x * x\\n        else:\\n            return -2.0 * x - 1.5 * x * x\\n    elif absx < 2.0:\\n        if x >= 0:\\n            return -2.0 + 2.0 * x - 0.5 * x * x\\n        else:\\n            return 2.0 + 2.0 * x + 0.5 * x * x\\n    return 0.0', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='cdef _compute_pdfs_dense_2d(double[:, :] static, double[:, :] moving,\\n                            int[:, :] smask, int[:, :] mmask,\\n                            double smin, double sdelta,\\n                            double mmin, double mdelta,\\n                            int nbins, int padding, double[:, :] joint,\\n                            double[:] smarginal, double[:] mmarginal):\\n    r\"\"\" Joint Probability Density Function of intensities of two 2D images', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='Parameters\\n    ----------\\n    static : array, shape (R, C)\\n        static image\\n    moving : array, shape (R, C)\\n        moving image\\n    smask : array, shape (R, C)\\n        mask of static object being registered (a binary array with 1\\'s inside\\n        the object of interest and 0\\'s along the background)\\n    mmask : array, shape (R, C)\\n        mask of moving object being registered (a binary array with 1\\'s inside\\n        the object of interest and 0\\'s along the background)\\n    smin : float\\n        the minimum observed intensity associated with the static image, which\\n        was used to define the joint PDF\\n    sdelta : float\\n        bin size associated with the intensities of the static image\\n    mmin : float\\n        the minimum observed intensity associated with the moving image, which\\n        was used to define the joint PDF\\n    mdelta : float\\n        bin size associated with the intensities of the moving image\\n    nbins : int\\n        number of histogram bins\\n    padding : int\\n        number of bins used as padding (the total bins used for padding at both\\n        sides of the histogram is actually 2*padding)\\n    joint : array, shape (nbins, nbins)\\n        the array to write the joint PDF\\n    smarginal : array, shape (nbins,)\\n        the array to write the marginal PDF associated with the static image\\n    mmarginal : array, shape (nbins,)\\n        the array to write the marginal PDF associated with the moving image\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp nrows = static.shape[0]\\n        cnp.npy_intp ncols = static.shape[1]\\n        cnp.npy_intp offset, valid_points\\n        cnp.npy_intp i, j, r, c\\n        double rn, cn\\n        double val, spline_arg, total_sum', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='joint[...] = 0\\n    total_sum = 0\\n    valid_points = 0\\n    with nogil:\\n        smarginal[:] = 0\\n        for i in range(nrows):\\n            for j in range(ncols):\\n                if smask is not None and smask[i, j] == 0:\\n                    continue\\n                if mmask is not None and mmask[i, j] == 0:\\n                    continue\\n                valid_points += 1\\n                rn = _bin_normalize(static[i, j], smin, sdelta)\\n                r = _bin_index(rn, nbins, padding)\\n                cn = _bin_normalize(moving[i, j], mmin, mdelta)\\n                c = _bin_index(cn, nbins, padding)\\n                spline_arg = (c - 2) - cn\\n\\n                smarginal[r] += 1\\n                for offset in range(-2, 3):\\n                    val = _cubic_spline(spline_arg)\\n                    joint[r, c + offset] += val\\n                    total_sum += val\\n                    spline_arg += 1.0\\n        if total_sum > 0:\\n            for i in range(nbins):\\n                for j in range(nbins):\\n                    joint[i, j] /= valid_points\\n\\n            for i in range(nbins):\\n                smarginal[i] /= valid_points\\n\\n            for j in range(nbins):\\n                mmarginal[j] = 0\\n                for i in range(nbins):\\n                    mmarginal[j] += joint[i, j]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='for i in range(nbins):\\n                smarginal[i] /= valid_points\\n\\n            for j in range(nbins):\\n                mmarginal[j] = 0\\n                for i in range(nbins):\\n                    mmarginal[j] += joint[i, j]\\n\\n\\ncdef _compute_pdfs_dense_3d(double[:, :, :] static, double[:, :, :] moving,\\n                            int[:, :, :] smask, int[:, :, :] mmask,\\n                            double smin, double sdelta,\\n                            double mmin, double mdelta,\\n                            int nbins, int padding, double[:, :] joint,\\n                            double[:] smarginal, double[:] mmarginal):\\n    r\"\"\" Joint Probability Density Function of intensities of two 3D images', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='Parameters\\n    ----------\\n    static : array, shape (S, R, C)\\n        static image\\n    moving : array, shape (S, R, C)\\n        moving image\\n    smask : array, shape (S, R, C)\\n        mask of static object being registered (a binary array with 1\\'s inside\\n        the object of interest and 0\\'s along the background)\\n    mmask : array, shape (S, R, C)\\n        mask of moving object being registered (a binary array with 1\\'s inside\\n        the object of interest and 0\\'s along the background)\\n    smin : float\\n        the minimum observed intensity associated with the static image, which\\n        was used to define the joint PDF\\n    sdelta : float\\n        bin size associated with the intensities of the static image\\n    mmin : float\\n        the minimum observed intensity associated with the moving image, which\\n        was used to define the joint PDF\\n    mdelta : float\\n        bin size associated with the intensities of the moving image\\n    nbins : int\\n        number of histogram bins\\n    padding : int\\n        number of bins used as padding (the total bins used for padding at both\\n        sides of the histogram is actually 2*padding)\\n    joint : array, shape (nbins, nbins)\\n        the array to write the joint PDF to\\n    smarginal : array, shape (nbins,)\\n        the array to write the marginal PDF associated with the static image\\n    mmarginal : array, shape (nbins,)\\n        the array to write the marginal PDF associated with the moving image\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp nslices = static.shape[0]\\n        cnp.npy_intp nrows = static.shape[1]\\n        cnp.npy_intp ncols = static.shape[2]\\n        cnp.npy_intp offset, valid_points\\n        cnp.npy_intp k, i, j, r, c', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='smarginal : array, shape (nbins,)\\n        the array to write the marginal PDF associated with the static image\\n    mmarginal : array, shape (nbins,)\\n        the array to write the marginal PDF associated with the moving image\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp nslices = static.shape[0]\\n        cnp.npy_intp nrows = static.shape[1]\\n        cnp.npy_intp ncols = static.shape[2]\\n        cnp.npy_intp offset, valid_points\\n        cnp.npy_intp k, i, j, r, c\\n        double rn, cn\\n        double val, spline_arg, total_sum', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='joint[...] = 0\\n    total_sum = 0\\n    with nogil:\\n        valid_points = 0\\n        smarginal[:] = 0\\n        for k in range(nslices):\\n            for i in range(nrows):\\n                for j in range(ncols):\\n                    if smask is not None and smask[k, i, j] == 0:\\n                        continue\\n                    if mmask is not None and mmask[k, i, j] == 0:\\n                        continue\\n                    valid_points += 1\\n                    rn = _bin_normalize(static[k, i, j], smin, sdelta)\\n                    r = _bin_index(rn, nbins, padding)\\n                    cn = _bin_normalize(moving[k, i, j], mmin, mdelta)\\n                    c = _bin_index(cn, nbins, padding)\\n                    spline_arg = (c - 2) - cn\\n\\n                    smarginal[r] += 1\\n                    for offset in range(-2, 3):\\n                        val = _cubic_spline(spline_arg)\\n                        joint[r, c + offset] += val\\n                        total_sum += val\\n                        spline_arg += 1.0\\n\\n        if total_sum > 0:\\n            for i in range(nbins):\\n                for j in range(nbins):\\n                    joint[i, j] /= total_sum\\n\\n            for i in range(nbins):\\n                smarginal[i] /= valid_points\\n\\n            for j in range(nbins):\\n                mmarginal[j] = 0\\n                for i in range(nbins):\\n                    mmarginal[j] += joint[i, j]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='if total_sum > 0:\\n            for i in range(nbins):\\n                for j in range(nbins):\\n                    joint[i, j] /= total_sum\\n\\n            for i in range(nbins):\\n                smarginal[i] /= valid_points\\n\\n            for j in range(nbins):\\n                mmarginal[j] = 0\\n                for i in range(nbins):\\n                    mmarginal[j] += joint[i, j]\\n\\n\\ncdef _compute_pdfs_sparse(double[:] sval, double[:] mval, double smin,\\n                          double sdelta, double mmin, double mdelta,\\n                          int nbins, int padding, double[:, :] joint,\\n                          double[:] smarginal, double[:] mmarginal):\\n    r\"\"\" Probability Density Functions of paired intensities', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='Parameters\\n    ----------\\n    sval : array, shape (n,)\\n        sampled intensities from the static image at sampled_points\\n    mval : array, shape (n,)\\n        sampled intensities from the moving image at sampled_points\\n    smin : float\\n        the minimum observed intensity associated with the static image, which\\n        was used to define the joint PDF\\n    sdelta : float\\n        bin size associated with the intensities of the static image\\n    mmin : float\\n        the minimum observed intensity associated with the moving image, which\\n        was used to define the joint PDF\\n    mdelta : float\\n        bin size associated with the intensities of the moving image\\n    nbins : int\\n        number of histogram bins\\n    padding : int\\n        number of bins used as padding (the total bins used for padding at both\\n        sides of the histogram is actually 2*padding)\\n    joint : array, shape (nbins, nbins)\\n        the array to write the joint PDF to\\n    smarginal : array, shape (nbins,)\\n        the array to write the marginal PDF associated with the static image\\n    mmarginal : array, shape (nbins,)\\n        the array to write the marginal PDF associated with the moving image\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp n = sval.shape[0]\\n        cnp.npy_intp offset, valid_points\\n        cnp.npy_intp i, r, c\\n        double rn, cn\\n        double val, spline_arg, total_sum\\n\\n    joint[...] = 0\\n    total_sum = 0', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='joint[...] = 0\\n    total_sum = 0\\n\\n    with nogil:\\n        valid_points = 0\\n        smarginal[:] = 0\\n        for i in range(n):\\n            valid_points += 1\\n            rn = _bin_normalize(sval[i], smin, sdelta)\\n            r = _bin_index(rn, nbins, padding)\\n            cn = _bin_normalize(mval[i], mmin, mdelta)\\n            c = _bin_index(cn, nbins, padding)\\n            spline_arg = (c - 2) - cn\\n\\n            smarginal[r] += 1\\n            for offset in range(-2, 3):\\n                val = _cubic_spline(spline_arg)\\n                joint[r, c + offset] += val\\n                total_sum += val\\n                spline_arg += 1.0\\n\\n        if total_sum > 0:\\n            for i in range(nbins):\\n                for j in range(nbins):\\n                    joint[i, j] /= total_sum\\n\\n            for i in range(nbins):\\n                smarginal[i] /= valid_points\\n\\n            for j in range(nbins):\\n                mmarginal[j] = 0\\n                for i in range(nbins):\\n                    mmarginal[j] += joint[i, j]\\n\\n\\ncdef _joint_pdf_gradient_dense_2d(double[:] theta, Transform transform,\\n                                  double[:, :] static, double[:, :] moving,\\n                                  double[:, :] grid2world,\\n                                  floating[:, :, :] mgradient, int[:, :] smask,\\n                                  int[:, :] mmask, double smin, double sdelta,\\n                                  double mmin, double mdelta, int nbins,\\n                                  int padding, double[:, :, :] grad_pdf):\\n    r\"\"\" Gradient of the joint PDF w.r.t. transform parameters theta', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='Computes the vector of partial derivatives of the joint histogram w.r.t.\\n    each transformation parameter. The transformation itself is not necessary\\n    to compute the gradient, but only its Jacobian.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content=\"Parameters\\n    ----------\\n    theta : array, shape (n,)\\n        parameters of the transformation to compute the gradient from\\n    transform : instance of Transform\\n        the transformation with respect to whose parameters the gradient\\n        must be computed\\n    static : array, shape (R, C)\\n        static image\\n    moving : array, shape (R, C)\\n        moving image\\n    grid2world : array, shape (3, 3)\\n        the grid-to-space transform associated with images static and moving\\n        (we assume that both images have already been sampled at a common grid)\\n    mgradient : array, shape (R, C, 2)\\n        the gradient of the moving image\\n    smask : array, shape (R, C)\\n        mask of static object being registered (a binary array with 1's inside\\n        the object of interest and 0's along the background)\\n    mmask : array, shape (R, C)\\n        mask of moving object being registered (a binary array with 1's inside\\n        the object of interest and 0's along the background)\\n    smin : float\\n        the minimum observed intensity associated with the static image, which\\n        was used to define the joint PDF\\n    sdelta : float\\n        bin size associated with the intensities of the static image\\n    mmin : float\\n        the minimum observed intensity associated with the moving image, which\\n        was used to define the joint PDF\\n    mdelta : float\\n        bin size associated with the intensities of the moving image\\n    nbins : int\\n        number of histogram bins\\n    padding : int\\n        number of bins used as padding (the total bins used for padding at both\\n        sides of the histogram is actually 2*padding)\\n    grad_pdf : array, shape (nbins, nbins, len(theta))\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='mmin : float\\n        the minimum observed intensity associated with the moving image, which\\n        was used to define the joint PDF\\n    mdelta : float\\n        bin size associated with the intensities of the moving image\\n    nbins : int\\n        number of histogram bins\\n    padding : int\\n        number of bins used as padding (the total bins used for padding at both\\n        sides of the histogram is actually 2*padding)\\n    grad_pdf : array, shape (nbins, nbins, len(theta))\\n        the array to write the gradient to\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp nrows = static.shape[0]\\n        cnp.npy_intp ncols = static.shape[1]\\n        cnp.npy_intp n = theta.shape[0]\\n        cnp.npy_intp offset, valid_points\\n        int constant_jacobian = 0\\n        cnp.npy_intp k, i, j, r, c\\n        double rn, cn\\n        double val, spline_arg, norm_factor\\n        double[:, :] J = np.empty(shape=(2, n), dtype=np.float64)\\n        double[:] prod = np.empty(shape=(n,), dtype=np.float64)\\n        double[:] x = np.empty(shape=(2,), dtype=np.float64)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='grad_pdf[...] = 0\\n    with nogil:\\n        valid_points = 0\\n        for i in range(nrows):\\n            for j in range(ncols):\\n                if smask is not None and smask[i, j] == 0:\\n                    continue\\n                if mmask is not None and mmask[i, j] == 0:\\n                    continue\\n\\n                valid_points += 1\\n                x[0] = _apply_affine_2d_x0(i, j, 1, grid2world)\\n                x[1] = _apply_affine_2d_x1(i, j, 1, grid2world)\\n\\n                if constant_jacobian == 0:\\n                    constant_jacobian = transform._jacobian(theta, x, J)\\n\\n                for k in range(n):\\n                    prod[k] = (J[0, k] * mgradient[i, j, 0] +\\n                               J[1, k] * mgradient[i, j, 1])\\n\\n                rn = _bin_normalize(static[i, j], smin, sdelta)\\n                r = _bin_index(rn, nbins, padding)\\n                cn = _bin_normalize(moving[i, j], mmin, mdelta)\\n                c = _bin_index(cn, nbins, padding)\\n                spline_arg = (c - 2) - cn\\n\\n                for offset in range(-2, 3):\\n                    val = _cubic_spline_derivative(spline_arg)\\n                    for k in range(n):\\n                        grad_pdf[r, c + offset, k] -= val * prod[k]\\n                    spline_arg += 1.0\\n\\n        norm_factor = valid_points * mdelta\\n        if norm_factor > 0:\\n            for i in range(nbins):\\n                for j in range(nbins):\\n                    for k in range(n):\\n                        grad_pdf[i, j, k] /= norm_factor', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='for offset in range(-2, 3):\\n                    val = _cubic_spline_derivative(spline_arg)\\n                    for k in range(n):\\n                        grad_pdf[r, c + offset, k] -= val * prod[k]\\n                    spline_arg += 1.0\\n\\n        norm_factor = valid_points * mdelta\\n        if norm_factor > 0:\\n            for i in range(nbins):\\n                for j in range(nbins):\\n                    for k in range(n):\\n                        grad_pdf[i, j, k] /= norm_factor\\n\\n\\ncdef _joint_pdf_gradient_dense_3d(double[:] theta, Transform transform,\\n                                  double[:, :, :] static,\\n                                  double[:, :, :] moving,\\n                                  double[:, :] grid2world,\\n                                  floating[:, :, :, :] mgradient,\\n                                  int[:, :, :] smask,\\n                                  int[:, :, :] mmask, double smin,\\n                                  double sdelta, double mmin, double mdelta,\\n                                  int nbins, int padding,\\n                                  double[:, :, :] grad_pdf):\\n    r\"\"\" Gradient of the joint PDF w.r.t. transform parameters theta\\n\\n    Computes the vector of partial derivatives of the joint histogram w.r.t.\\n    each transformation parameter. The transformation itself is not necessary\\n    to compute the gradient, but only its Jacobian.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content=\"Parameters\\n    ----------\\n    theta : array, shape (n,)\\n        parameters of the transformation to compute the gradient from\\n    transform : instance of Transform\\n        the transformation with respect to whose parameters the gradient\\n        must be computed\\n    static : array, shape (S, R, C)\\n        static image\\n    moving : array, shape (S, R, C)\\n        moving image\\n    grid2world : array, shape (4, 4)\\n        the grid-to-space transform associated with images static and moving\\n        (we assume that both images have already been sampled at a common grid)\\n    mgradient : array, shape (S, R, C, 3)\\n        the gradient of the moving image\\n    smask : array, shape (S, R, C)\\n        mask of static object being registered (a binary array with 1's inside\\n        the object of interest and 0's along the background)\\n    mmask : array, shape (S, R, C)\\n        mask of moving object being registered (a binary array with 1's inside\\n        the object of interest and 0's along the background)\\n    smin : float\\n        the minimum observed intensity associated with the static image, which\\n        was used to define the joint PDF\\n    sdelta : float\\n        bin size associated with the intensities of the static image\\n    mmin : float\\n        the minimum observed intensity associated with the moving image, which\\n        was used to define the joint PDF\\n    mdelta : float\\n        bin size associated with the intensities of the moving image\\n    nbins : int\\n        number of histogram bins\\n    padding : int\\n        number of bins used as padding (the total bins used for padding at both\\n        sides of the histogram is actually 2*padding)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='bin size associated with the intensities of the static image\\n    mmin : float\\n        the minimum observed intensity associated with the moving image, which\\n        was used to define the joint PDF\\n    mdelta : float\\n        bin size associated with the intensities of the moving image\\n    nbins : int\\n        number of histogram bins\\n    padding : int\\n        number of bins used as padding (the total bins used for padding at both\\n        sides of the histogram is actually 2*padding)\\n    grad_pdf : array, shape (nbins, nbins, len(theta))\\n        the array to write the gradient to\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp nslices = static.shape[0]\\n        cnp.npy_intp nrows = static.shape[1]\\n        cnp.npy_intp ncols = static.shape[2]\\n        cnp.npy_intp n = theta.shape[0]\\n        cnp.npy_intp offset, valid_points\\n        int constant_jacobian = 0\\n        cnp.npy_intp l, k, i, j, r, c\\n        double rn, cn\\n        double val, spline_arg, norm_factor\\n        double[:, :] J = np.empty(shape=(3, n), dtype=np.float64)\\n        double[:] prod = np.empty(shape=(n,), dtype=np.float64)\\n        double[:] x = np.empty(shape=(3,), dtype=np.float64)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='grad_pdf[...] = 0\\n    with nogil:\\n        valid_points = 0\\n        for k in range(nslices):\\n            for i in range(nrows):\\n                for j in range(ncols):\\n                    if smask is not None and smask[k, i, j] == 0:\\n                        continue\\n                    if mmask is not None and mmask[k, i, j] == 0:\\n                        continue\\n                    valid_points += 1\\n                    x[0] = _apply_affine_3d_x0(k, i, j, 1, grid2world)\\n                    x[1] = _apply_affine_3d_x1(k, i, j, 1, grid2world)\\n                    x[2] = _apply_affine_3d_x2(k, i, j, 1, grid2world)\\n\\n                    if constant_jacobian == 0:\\n                        constant_jacobian = transform._jacobian(theta, x, J)\\n\\n                    for l in range(n):\\n                        prod[l] = (J[0, l] * mgradient[k, i, j, 0] +\\n                                   J[1, l] * mgradient[k, i, j, 1] +\\n                                   J[2, l] * mgradient[k, i, j, 2])\\n\\n                    rn = _bin_normalize(static[k, i, j], smin, sdelta)\\n                    r = _bin_index(rn, nbins, padding)\\n                    cn = _bin_normalize(moving[k, i, j], mmin, mdelta)\\n                    c = _bin_index(cn, nbins, padding)\\n                    spline_arg = (c - 2) - cn\\n\\n                    for offset in range(-2, 3):\\n                        val = _cubic_spline_derivative(spline_arg)\\n                        for l in range(n):\\n                            grad_pdf[r, c + offset, l] -= val * prod[l]\\n                        spline_arg += 1.0', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='for offset in range(-2, 3):\\n                        val = _cubic_spline_derivative(spline_arg)\\n                        for l in range(n):\\n                            grad_pdf[r, c + offset, l] -= val * prod[l]\\n                        spline_arg += 1.0\\n\\n        norm_factor = valid_points * mdelta\\n        if norm_factor > 0:\\n            for i in range(nbins):\\n                for j in range(nbins):\\n                    for k in range(n):\\n                        grad_pdf[i, j, k] /= norm_factor\\n\\n\\ncdef _joint_pdf_gradient_sparse_2d(double[:] theta, Transform transform,\\n                                   double[:] sval, double[:] mval,\\n                                   double[:, :] sample_points,\\n                                   floating[:, :] mgradient, double smin,\\n                                   double sdelta, double mmin,\\n                                   double mdelta, int nbins, int padding,\\n                                   double[:, :, :] grad_pdf):\\n    r\"\"\" Gradient of the joint PDF w.r.t. transform parameters theta\\n\\n    Computes the vector of partial derivatives of the joint histogram w.r.t.\\n    each transformation parameter. The transformation itself is not necessary\\n    to compute the gradient, but only its Jacobian.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='Parameters\\n    ----------\\n    theta : array, shape (n,)\\n        parameters to compute the gradient at\\n    transform : instance of Transform\\n        the transformation with respect to whose parameters the gradient\\n        must be computed\\n    sval : array, shape (m,)\\n        sampled intensities from the static image at sampled_points\\n    mval : array, shape (m,)\\n        sampled intensities from the moving image at sampled_points\\n    sample_points : array, shape (m, 2)\\n        positions (in physical space) of the points the images were sampled at\\n    mgradient : array, shape (m, 2)\\n        the gradient of the moving image at the sample points\\n    smin : float\\n        the minimum observed intensity associated with the static image, which\\n        was used to define the joint PDF\\n    sdelta : float\\n        bin size associated with the intensities of the static image\\n    mmin : float\\n        the minimum observed intensity associated with the moving image, which\\n        was used to define the joint PDF\\n    mdelta : float\\n        bin size associated with the intensities of the moving image\\n    nbins : int\\n        number of histogram bins\\n    padding : int\\n        number of bins used as padding (the total bins used for padding at both\\n        sides of the histogram is actually 2*padding)\\n    grad_pdf : array, shape (nbins, nbins, len(theta))\\n        the array to write the gradient to\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp n = theta.shape[0]\\n        cnp.npy_intp m = sval.shape[0]\\n        cnp.npy_intp offset\\n        int constant_jacobian = 0\\n        cnp.npy_intp i, j, r, c, valid_points\\n        double rn, cn\\n        double val, spline_arg, norm_factor', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='sides of the histogram is actually 2*padding)\\n    grad_pdf : array, shape (nbins, nbins, len(theta))\\n        the array to write the gradient to\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp n = theta.shape[0]\\n        cnp.npy_intp m = sval.shape[0]\\n        cnp.npy_intp offset\\n        int constant_jacobian = 0\\n        cnp.npy_intp i, j, r, c, valid_points\\n        double rn, cn\\n        double val, spline_arg, norm_factor\\n        double[:, :] J = np.empty(shape=(2, n), dtype=np.float64)\\n        double[:] prod = np.empty(shape=(n,), dtype=np.float64)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='grad_pdf[...] = 0\\n    with nogil:\\n        valid_points = 0\\n        for i in range(m):\\n            valid_points += 1\\n            if constant_jacobian == 0:\\n                constant_jacobian = transform._jacobian(theta,\\n                                                        sample_points[i], J)\\n\\n            for j in range(n):\\n                prod[j] = (J[0, j] * mgradient[i, 0] +\\n                           J[1, j] * mgradient[i, 1])\\n\\n            rn = _bin_normalize(sval[i], smin, sdelta)\\n            r = _bin_index(rn, nbins, padding)\\n            cn = _bin_normalize(mval[i], mmin, mdelta)\\n            c = _bin_index(cn, nbins, padding)\\n            spline_arg = (c - 2) - cn\\n\\n            for offset in range(-2, 3):\\n                val = _cubic_spline_derivative(spline_arg)\\n                for j in range(n):\\n                    grad_pdf[r, c + offset, j] -= val * prod[j]\\n                spline_arg += 1.0\\n\\n        norm_factor = valid_points * mdelta\\n        if norm_factor > 0:\\n            for i in range(nbins):\\n                for j in range(nbins):\\n                    for k in range(n):\\n                        grad_pdf[i, j, k] /= norm_factor\\n\\n\\ncdef _joint_pdf_gradient_sparse_3d(double[:] theta, Transform transform,\\n                                   double[:] sval, double[:] mval,\\n                                   double[:, :] sample_points,\\n                                   floating[:, :] mgradient, double smin,\\n                                   double sdelta, double mmin,\\n                                   double mdelta, int nbins, int padding,\\n                                   double[:, :, :] grad_pdf):\\n    r\"\"\" Gradient of the joint PDF w.r.t. transform parameters theta', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='Computes the vector of partial derivatives of the joint histogram w.r.t.\\n    each transformation parameter. The transformation itself is not necessary\\n    to compute the gradient, but only its Jacobian.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='Parameters\\n    ----------\\n    theta : array, shape (n,)\\n        parameters to compute the gradient at\\n    transform : instance of Transform\\n        the transformation with respect to whose parameters the gradient\\n        must be computed\\n    sval : array, shape (m,)\\n        sampled intensities from the static image at sampled_points\\n    mval : array, shape (m,)\\n        sampled intensities from the moving image at sampled_points\\n    sample_points : array, shape (m, 3)\\n        positions (in physical space) of the points the images were sampled at\\n    mgradient : array, shape (m, 3)\\n        the gradient of the moving image at the sample points\\n    smin : float\\n        the minimum observed intensity associated with the static image, which\\n        was used to define the joint PDF\\n    sdelta : float\\n        bin size associated with the intensities of the static image\\n    mmin : float\\n        the minimum observed intensity associated with the moving image, which\\n        was used to define the joint PDF\\n    mdelta : float\\n        bin size associated with the intensities of the moving image\\n    nbins : int\\n        number of histogram bins\\n    padding : int\\n        number of bins used as padding (the total bins used for padding at both\\n        sides of the histogram is actually 2*padding)\\n    grad_pdf : array, shape (nbins, nbins, len(theta))\\n        the array to write the gradient to\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp n = theta.shape[0]\\n        cnp.npy_intp m = sval.shape[0]\\n        cnp.npy_intp offset, valid_points\\n        int constant_jacobian = 0\\n        cnp.npy_intp i, j, r, c\\n        double rn, cn\\n        double val, spline_arg, norm_factor', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='sides of the histogram is actually 2*padding)\\n    grad_pdf : array, shape (nbins, nbins, len(theta))\\n        the array to write the gradient to\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp n = theta.shape[0]\\n        cnp.npy_intp m = sval.shape[0]\\n        cnp.npy_intp offset, valid_points\\n        int constant_jacobian = 0\\n        cnp.npy_intp i, j, r, c\\n        double rn, cn\\n        double val, spline_arg, norm_factor\\n        double[:, :] J = np.empty(shape=(3, n), dtype=np.float64)\\n        double[:] prod = np.empty(shape=(n,), dtype=np.float64)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='grad_pdf[...] = 0\\n    with nogil:\\n        valid_points = 0\\n        for i in range(m):\\n            valid_points += 1\\n\\n            if constant_jacobian == 0:\\n                constant_jacobian = transform._jacobian(theta,\\n                                                        sample_points[i], J)\\n\\n            for j in range(n):\\n                prod[j] = (J[0, j] * mgradient[i, 0] +\\n                           J[1, j] * mgradient[i, 1] +\\n                           J[2, j] * mgradient[i, 2])\\n\\n            rn = _bin_normalize(sval[i], smin, sdelta)\\n            r = _bin_index(rn, nbins, padding)\\n            cn = _bin_normalize(mval[i], mmin, mdelta)\\n            c = _bin_index(cn, nbins, padding)\\n            spline_arg = (c - 2) - cn\\n\\n            for offset in range(-2, 3):\\n                val = _cubic_spline_derivative(spline_arg)\\n                for j in range(n):\\n                    grad_pdf[r, c + offset, j] -= val * prod[j]\\n                spline_arg += 1.0\\n\\n        norm_factor = valid_points * mdelta\\n        if norm_factor > 0:\\n            for i in range(nbins):\\n                for j in range(nbins):\\n                    for k in range(n):\\n                        grad_pdf[i, j, k] /= norm_factor\\n\\n\\ndef compute_parzen_mi(double[:, :] joint,\\n                      double[:, :, :] joint_gradient,\\n                      double[:] smarginal, double[:] mmarginal,\\n                      double[:] mi_gradient):\\n    r\"\"\" Computes the mutual information and its gradient (if requested)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='def compute_parzen_mi(double[:, :] joint,\\n                      double[:, :, :] joint_gradient,\\n                      double[:] smarginal, double[:] mmarginal,\\n                      double[:] mi_gradient):\\n    r\"\"\" Computes the mutual information and its gradient (if requested)\\n\\n    Parameters\\n    ----------\\n    joint : array, shape (nbins, nbins)\\n        the joint intensity distribution\\n    joint_gradient : array, shape (nbins, nbins, n)\\n        the gradient of the joint distribution w.r.t. the transformation\\n        parameters\\n    smarginal : array, shape (nbins,)\\n        the marginal intensity distribution of the static image\\n    mmarginal : array, shape (nbins,)\\n        the marginal intensity distribution of the moving image\\n    mi_gradient : array, shape (n,)\\n        the buffer in which to write the gradient of the mutual information.\\n        If None, the gradient is not computed\\n    \"\"\"\\n    cdef:\\n        double epsilon = 2.2204460492503131e-016\\n        double metric_value\\n        cnp.npy_intp nrows = joint.shape[0]\\n        cnp.npy_intp ncols = joint.shape[1]\\n        cnp.npy_intp n = joint_gradient.shape[2]\\n    with nogil:\\n        mi_gradient[:] = 0\\n        metric_value = 0\\n        for i in range(nrows):\\n            for j in range(ncols):\\n                if joint[i, j] < epsilon or mmarginal[j] < epsilon:\\n                    continue\\n\\n                factor = log(joint[i, j] / mmarginal[j])\\n\\n                if mi_gradient is not None:\\n                    for k in range(n):\\n                        mi_gradient[k] += joint_gradient[i, j, k] * factor\\n\\n                if smarginal[i] > epsilon:\\n                    metric_value += joint[i, j] * (factor - log(smarginal[i]))', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='factor = log(joint[i, j] / mmarginal[j])\\n\\n                if mi_gradient is not None:\\n                    for k in range(n):\\n                        mi_gradient[k] += joint_gradient[i, j, k] * factor\\n\\n                if smarginal[i] > epsilon:\\n                    metric_value += joint[i, j] * (factor - log(smarginal[i]))\\n\\n    return metric_value\\n\\n\\ndef sample_domain_regular(int k, int[:] shape, double[:, :] grid2world,\\n                          double sigma=0.25, object rng=None):\\n    r\"\"\" Take floor(total_voxels/k) samples from a (2D or 3D) grid\\n\\n    The sampling is made by taking all pixels whose index (in lexicographical\\n    order) is a multiple of k. Each selected point is slightly perturbed by\\n    adding a realization of a normally distributed random variable and then\\n    mapped to physical space by the given grid-to-space transform.\\n\\n    The lexicographical order of a pixels in a grid of shape (a, b, c) is\\n    defined by assigning to each voxel position (i, j, k) the integer index\\n\\n    F((i, j, k)) = i * (b * c) + j * (c) + k\\n\\n    and sorting increasingly by this index.\\n\\n    Parameters\\n    ----------\\n    k : int\\n        the sampling rate, as described before\\n    shape : array, shape (dim,)\\n        the shape of the grid to be sampled\\n    grid2world : array, shape (dim+1, dim+1)\\n        the grid-to-space transform\\n    sigma : float\\n        the standard deviation of the Normal random distortion to be applied\\n        to the sampled points\\n\\n    Returns\\n    -------\\n    samples : array, shape (total_pixels//k, dim)\\n        the matrix whose rows are the sampled points', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='Parameters\\n    ----------\\n    k : int\\n        the sampling rate, as described before\\n    shape : array, shape (dim,)\\n        the shape of the grid to be sampled\\n    grid2world : array, shape (dim+1, dim+1)\\n        the grid-to-space transform\\n    sigma : float\\n        the standard deviation of the Normal random distortion to be applied\\n        to the sampled points\\n\\n    Returns\\n    -------\\n    samples : array, shape (total_pixels//k, dim)\\n        the matrix whose rows are the sampled points\\n\\n    Examples\\n    --------\\n    >>> from dipy.align.parzenhist import sample_domain_regular\\n    >>> import dipy.align.vector_fields as vf\\n    >>> shape = np.array((10, 10), dtype=np.int32)\\n    >>> sigma = 0\\n    >>> dim = len(shape)\\n    >>> grid2world = np.eye(dim+1)\\n    >>> n = shape[0]*shape[1]\\n    >>> k = 2\\n    >>> samples = sample_domain_regular(k, shape, grid2world, sigma)\\n    >>> (samples.shape[0], samples.shape[1]) == (n//k, dim)\\n    True\\n    >>> isamples = np.array(samples, dtype=np.int32)\\n    >>> indices = (isamples[:, 0] * shape[1] + isamples[:, 1])\\n    >>> len(set(indices)) == len(indices)\\n    True\\n    >>> (indices%k).sum()\\n    0\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp i, dim, n, m, slice_size\\n        double s, r, c\\n        double[:, :] samples\\n    dim = len(shape)\\n    if not vf.is_valid_affine(grid2world, dim):\\n        raise ValueError(\"Invalid grid-to-space matrix\")', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='if rng is None:\\n        rng = np.random.default_rng(1234)\\n    if dim == 2:\\n        n = shape[0] * shape[1]\\n        m = n // k\\n        samples = rng.standard_normal((m, dim)) * sigma\\n        with nogil:\\n            for i in range(m):\\n                r = ((i * k) // shape[1]) + samples[i, 0]\\n                c = ((i * k) % shape[1]) + samples[i, 1]\\n                samples[i, 0] = _apply_affine_2d_x0(r, c, 1, grid2world)\\n                samples[i, 1] = _apply_affine_2d_x1(r, c, 1, grid2world)\\n    else:\\n        slice_size = shape[1] * shape[2]\\n        n = shape[0] * slice_size\\n        m = n // k\\n        samples = rng.standard_normal((m, dim)) * sigma\\n        with nogil:\\n            for i in range(m):\\n                s = ((i * k) // slice_size) + samples[i, 0]\\n                r = (((i * k) % slice_size) // shape[2]) + samples[i, 1]\\n                c = (((i * k) % slice_size) % shape[2]) + samples[i, 2]\\n                samples[i, 0] = _apply_affine_3d_x0(s, r, c, 1, grid2world)\\n                samples[i, 1] = _apply_affine_3d_x1(s, r, c, 1, grid2world)\\n                samples[i, 2] = _apply_affine_3d_x2(s, r, c, 1, grid2world)\\n    return np.asarray(samples)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\parzenhist.pyx.txt'}),\n",
       " Document(page_content='from multiprocessing import Pool\\nimport warnings\\n\\nimport numpy as np\\nfrom scipy.ndimage import affine_transform\\n\\nfrom dipy.utils.multiproc import determine_num_processes\\n\\ndef _affine_transform(kwargs):\\n    with warnings.catch_warnings():\\n        warnings.filterwarnings(\"ignore\", message=\".*scipy.*18.*\",\\n                                category=UserWarning)\\n        return affine_transform(**kwargs)\\n\\n\\ndef reslice(data, affine, zooms, new_zooms, order=1, mode=\\'constant\\', cval=0,\\n            num_processes=1):\\n    \"\"\" Reslice data with new voxel resolution defined by ``new_zooms``.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\reslice.py.txt'}),\n",
       " Document(page_content='from dipy.utils.multiproc import determine_num_processes\\n\\ndef _affine_transform(kwargs):\\n    with warnings.catch_warnings():\\n        warnings.filterwarnings(\"ignore\", message=\".*scipy.*18.*\",\\n                                category=UserWarning)\\n        return affine_transform(**kwargs)\\n\\n\\ndef reslice(data, affine, zooms, new_zooms, order=1, mode=\\'constant\\', cval=0,\\n            num_processes=1):\\n    \"\"\" Reslice data with new voxel resolution defined by ``new_zooms``.\\n\\n    Parameters\\n    ----------\\n    data : array, shape (I,J,K) or (I,J,K,N)\\n        3d volume or 4d volume with datasets\\n    affine : array, shape (4,4)\\n        mapping from voxel coordinates to world coordinates\\n    zooms : tuple, shape (3,)\\n        voxel size for (i,j,k) dimensions\\n    new_zooms : tuple, shape (3,)\\n        new voxel size for (i,j,k) after resampling\\n    order : int, from 0 to 5\\n        order of interpolation for resampling/reslicing,\\n        0 nearest interpolation, 1 trilinear etc..\\n        if you don\\'t want any smoothing 0 is the option you need.\\n    mode : string (\\'constant\\', \\'nearest\\', \\'reflect\\' or \\'wrap\\')\\n        Points outside the boundaries of the input are filled according\\n        to the given mode.\\n    cval : float\\n        Value used for points outside the boundaries of the input if\\n        mode=\\'constant\\'.\\n    num_processes : int, optional\\n        Split the calculation to a pool of children processes. This only\\n        applies to 4D `data` arrays. Default is 1. If < 0 the maximal number\\n        of cores minus ``num_processes + 1`` is used (enter -1 to use as many\\n        cores as possible). 0 raises an error.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\reslice.py.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    data2 : array, shape (I,J,K) or (I,J,K,N)\\n        datasets resampled into isotropic voxel size\\n    affine2 : array, shape (4,4)\\n        new affine for the resampled image\\n\\n    Examples\\n    --------\\n    >>> from dipy.io.image import load_nifti\\n    >>> from dipy.align.reslice import reslice\\n    >>> from dipy.data import get_fnames\\n    >>> f_name = get_fnames(\\'aniso_vox\\')\\n    >>> data, affine, zooms = load_nifti(f_name, return_voxsize=True)\\n    >>> data.shape == (58, 58, 24)\\n    True\\n    >>> zooms\\n    (4.0, 4.0, 5.0)\\n    >>> new_zooms = (3.,3.,3.)\\n    >>> new_zooms\\n    (3.0, 3.0, 3.0)\\n    >>> data2, affine2 = reslice(data, affine, zooms, new_zooms)\\n    >>> data2.shape == (77, 77, 40)\\n    True\\n\\n    \"\"\"\\n    num_processes = determine_num_processes(num_processes)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\reslice.py.txt'}),\n",
       " Document(page_content='\"\"\"\\n    num_processes = determine_num_processes(num_processes)\\n\\n    # We are suppressing warnings emitted by scipy >= 0.18,\\n    # described in https://github.com/dipy/dipy/issues/1107.\\n    # These warnings are not relevant to us, as long as our offset\\n    # input to scipy\\'s affine_transform is [0, 0, 0]\\n    with warnings.catch_warnings():\\n        warnings.filterwarnings(\"ignore\", message=\".*scipy.*18.*\",\\n                                category=UserWarning)\\n        new_zooms = np.array(new_zooms, dtype=\\'f8\\')\\n        zooms = np.array(zooms, dtype=\\'f8\\')\\n        R = new_zooms / zooms\\n        new_shape = zooms / new_zooms * np.array(data.shape[:3])\\n        new_shape = tuple(np.round(new_shape).astype(\\'i8\\'))\\n        kwargs = {\\'matrix\\': R, \\'output_shape\\': new_shape, \\'order\\': order,\\n                  \\'mode\\': mode, \\'cval\\': cval}\\n        if data.ndim == 3:\\n            data2 = affine_transform(input=data, **kwargs)\\n        elif data.ndim == 4:\\n            data2 = np.zeros(new_shape+(data.shape[-1],), data.dtype)\\n\\n            if num_processes == 1:\\n                for i in range(data.shape[-1]):\\n                    affine_transform(input=data[..., i], output=data2[..., i],\\n                                     **kwargs)\\n            else:\\n                params = []\\n                for i in range(data.shape[-1]):\\n                    _kwargs = {\\'input\\': data[..., i]}\\n                    _kwargs.update(kwargs)\\n                    params.append(_kwargs)\\n\\n                pool = Pool(num_processes)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\reslice.py.txt'}),\n",
       " Document(page_content='if num_processes == 1:\\n                for i in range(data.shape[-1]):\\n                    affine_transform(input=data[..., i], output=data2[..., i],\\n                                     **kwargs)\\n            else:\\n                params = []\\n                for i in range(data.shape[-1]):\\n                    _kwargs = {\\'input\\': data[..., i]}\\n                    _kwargs.update(kwargs)\\n                    params.append(_kwargs)\\n\\n                pool = Pool(num_processes)\\n\\n                for i, res in enumerate(pool.imap(_affine_transform, params)):\\n                    data2[..., i] = res\\n                pool.close()\\n        else:\\n            raise ValueError(\"dimension of data should be 3 or 4 but you\"\\n                             \" provided %d\" % data.ndim)\\n\\n        Rx = np.eye(4)\\n        Rx[:3, :3] = np.diag(R)\\n        affine2 = np.dot(affine, Rx)\\n    return data2, affine2', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\reslice.py.txt'}),\n",
       " Document(page_content='import logging\\nfrom dipy.align import floating\\nimport numpy as np\\nimport numpy.linalg as npl\\nfrom scipy.ndimage import gaussian_filter\\n\\nlogger = logging.getLogger(__name__)\\n\\nclass ScaleSpace:\\n    def __init__(self, image, num_levels,\\n                 image_grid2world=None,\\n                 input_spacing=None,\\n                 sigma_factor=0.2,\\n                 mask0=False):\\n        \"\"\" ScaleSpace.\\n\\n        Computes the Scale Space representation of an image. The scale space is\\n        simply a list of images produced by smoothing the input image with a\\n        Gaussian kernel with increasing smoothing parameter. If the image\\'s\\n        voxels are isotropic, the smoothing will be the same along all\\n        directions: at level L = 0, 1, ..., the sigma is given by\\n        $s * ( 2^L - 1 )$.\\n        If the voxel dimensions are not isotropic, then the smoothing is\\n        weaker along low resolution directions.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\scalespace.py.txt'}),\n",
       " Document(page_content='Parameters\\n        ----------\\n        image : array, shape (r,c) or (s, r, c) where s is the number of\\n            slices, r is the number of rows and c is the number of columns of\\n            the input image.\\n        num_levels : int\\n            the desired number of levels (resolutions) of the scale space\\n        image_grid2world : array, shape (dim + 1, dim + 1), optional\\n            the grid-to-space transform of the image grid. The default is\\n            the identity matrix\\n        input_spacing : array, shape (dim,), optional\\n            the spacing (voxel size) between voxels in physical space. The\\n            default is 1.0 along all axes\\n        sigma_factor : float, optional\\n            the smoothing factor to be used in the construction of the scale\\n            space. The default is 0.2\\n        mask0 : Boolean, optional\\n            if True, all smoothed images will be zero at all voxels that are\\n            zero in the input image. The default is False.\\n\\n        \"\"\"\\n        self.dim = len(image.shape)\\n        self.num_levels = num_levels\\n        input_size = np.array(image.shape)\\n        if mask0:\\n            mask = np.asarray(image > 0, dtype=np.int32)\\n\\n        # Normalize input image to [0,1]\\n        img = (image - np.min(image))/(np.max(image) - np.min(image))\\n        if mask0:\\n            img *= mask', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\scalespace.py.txt'}),\n",
       " Document(page_content='\"\"\"\\n        self.dim = len(image.shape)\\n        self.num_levels = num_levels\\n        input_size = np.array(image.shape)\\n        if mask0:\\n            mask = np.asarray(image > 0, dtype=np.int32)\\n\\n        # Normalize input image to [0,1]\\n        img = (image - np.min(image))/(np.max(image) - np.min(image))\\n        if mask0:\\n            img *= mask\\n\\n        # The properties are saved in separate lists. Insert input image\\n        # properties at the first level of the scale space\\n        self.images = [img.astype(floating)]\\n        self.domain_shapes = [input_size.astype(np.int32)]\\n        if input_spacing is None:\\n            input_spacing = np.ones((self.dim,), dtype=np.int32)\\n        self.spacings = [input_spacing]\\n        self.scalings = [np.ones(self.dim)]\\n        self.affines = [image_grid2world]\\n        self.sigmas = [np.zeros(self.dim)]\\n\\n        if image_grid2world is not None:\\n            self.affine_invs = [npl.inv(image_grid2world)]\\n        else:\\n            self.affine_invs = [None]\\n\\n        # Compute the rest of the levels\\n        min_spacing = np.min(input_spacing)\\n        for i in range(1, num_levels):\\n            scaling_factor = 2 ** i\\n            # Note: the minimum below is present in ANTS to prevent the scaling\\n            # from being too large (making the sub-sampled image to be too\\n            # small) this makes the sub-sampled image at least 32 voxels at\\n            # each direction it is risky to make this decision based on image\\n            # size, though (we need to investigate more the effect of this)\\n\\n            # scaling = np.minimum(scaling_factor * min_spacing /input_spacing,\\n            #                     input_size / 32)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\scalespace.py.txt'}),\n",
       " Document(page_content='# scaling = np.minimum(scaling_factor * min_spacing /input_spacing,\\n            #                     input_size / 32)\\n\\n            scaling = scaling_factor * min_spacing / input_spacing\\n            output_spacing = input_spacing * scaling\\n            extended = np.append(scaling, [1])\\n            if image_grid2world is not None:\\n                affine = image_grid2world.dot(np.diag(extended))\\n            else:\\n                affine = np.diag(extended)\\n            output_size = input_size * (input_spacing / output_spacing) + 0.5\\n            output_size = output_size.astype(np.int32)\\n            sigmas = sigma_factor * (output_spacing / input_spacing - 1.0)\\n\\n            # Filter along each direction with the appropriate sigma\\n            filtered = gaussian_filter(image, sigmas)\\n            filtered = ((filtered - np.min(filtered)) /\\n                        (np.max(filtered) - np.min(filtered)))\\n            if mask0:\\n                filtered *= mask\\n\\n            # Add current level to the scale space\\n            self.images.append(filtered.astype(floating))\\n            self.domain_shapes.append(output_size)\\n            self.spacings.append(output_spacing)\\n            self.scalings.append(scaling)\\n            self.affines.append(affine)\\n            self.affine_invs.append(npl.inv(affine))\\n            self.sigmas.append(sigmas)\\n\\n    def get_expand_factors(self, from_level, to_level):\\n        \"\"\"Ratio of voxel size from pyramid level from_level to to_level.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\scalespace.py.txt'}),\n",
       " Document(page_content='def get_expand_factors(self, from_level, to_level):\\n        \"\"\"Ratio of voxel size from pyramid level from_level to to_level.\\n\\n        Given two scale space resolutions a = from_level, b = to_level,\\n        returns the ratio of voxels size at level b to voxel size at level a\\n        (the factor that must be used to multiply voxels at level a to\\n        \\'expand\\' them to level b).\\n\\n        Parameters\\n        ----------\\n        from_level : int, 0 <= from_level < L, (L = number of resolutions)\\n            the resolution to expand voxels from\\n        to_level : int, 0 <= to_level < from_level\\n            the resolution to expand voxels to\\n\\n        Returns\\n        -------\\n        factors : array, shape (k,), k = 2, 3\\n            the expand factors (a scalar for each voxel dimension)\\n\\n        \"\"\"\\n        factors = (np.array(self.spacings[to_level]) /\\n                   np.array(self.spacings[from_level]))\\n        return factors\\n\\n    def print_level(self, level):\\n        \"\"\"Prints properties of a pyramid level.\\n\\n        Prints the properties of a level of this scale space to standard output\\n\\n        Parameters\\n        ----------\\n        level : int, 0 <= from_level < L, (L = number of resolutions)\\n            the scale space level to be printed\\n\\n        \"\"\"\\n        logger.info(\\'Domain shape: \\' + str(self.get_domain_shape(level)))\\n        logger.info(\\'Spacing: \\' + str(self.get_spacing(level)))\\n        logger.info(\\'Scaling: \\' + str(self.get_scaling(level)))\\n        logger.info(\\'Affine: \\' + str(self.get_affine(level)))\\n        logger.info(\\'Sigmas: \\' + str(self.get_sigmas(level)))', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\scalespace.py.txt'}),\n",
       " Document(page_content='Parameters\\n        ----------\\n        level : int, 0 <= from_level < L, (L = number of resolutions)\\n            the scale space level to be printed\\n\\n        \"\"\"\\n        logger.info(\\'Domain shape: \\' + str(self.get_domain_shape(level)))\\n        logger.info(\\'Spacing: \\' + str(self.get_spacing(level)))\\n        logger.info(\\'Scaling: \\' + str(self.get_scaling(level)))\\n        logger.info(\\'Affine: \\' + str(self.get_affine(level)))\\n        logger.info(\\'Sigmas: \\' + str(self.get_sigmas(level)))\\n\\n    def _get_attribute(self, attribute, level):\\n        \"\"\"Return an attribute from the Scale Space at a given level.\\n\\n        Returns the level-th element of attribute if level is a valid level\\n        of this scale space. Otherwise, returns None.\\n\\n        Parameters\\n        ----------\\n        attribute : list\\n            the attribute to retrieve the level-th element from\\n        level : int,\\n            the index of the required element from attribute.\\n\\n        Returns\\n        -------\\n        attribute[level] : object\\n            the requested attribute if level is valid, else it raises\\n            a ValueError\\n\\n        \"\"\"\\n        if 0 <= level < self.num_levels:\\n            return attribute[level]\\n        raise ValueError(\\'Invalid pyramid level: \\'+str(level))\\n\\n    def get_image(self, level):\\n        \"\"\"Smoothed image at a given level.\\n\\n        Returns the smoothed image at the requested level in the Scale Space.\\n\\n        Parameters\\n        ----------\\n        level : int, 0 <= from_level < L, (L = number of resolutions)\\n            the scale space level to get the smooth image from', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\scalespace.py.txt'}),\n",
       " Document(page_content='\"\"\"\\n        if 0 <= level < self.num_levels:\\n            return attribute[level]\\n        raise ValueError(\\'Invalid pyramid level: \\'+str(level))\\n\\n    def get_image(self, level):\\n        \"\"\"Smoothed image at a given level.\\n\\n        Returns the smoothed image at the requested level in the Scale Space.\\n\\n        Parameters\\n        ----------\\n        level : int, 0 <= from_level < L, (L = number of resolutions)\\n            the scale space level to get the smooth image from\\n\\n        Returns\\n        -------\\n            the smooth image at the requested resolution or None if an invalid\\n            level was requested\\n\\n        \"\"\"\\n        return self._get_attribute(self.images, level)\\n\\n    def get_domain_shape(self, level):\\n        \"\"\"Shape the sub-sampled image must have at a particular level.\\n\\n        Returns the shape the sub-sampled image must have at a particular\\n        resolution of the scale space (note that this object does not\\n        explicitly subsample the smoothed images, but only provides the\\n        properties the sub-sampled images must have).\\n\\n        Parameters\\n        ----------\\n        level : int, 0 <= from_level < L, (L = number of resolutions)\\n            the scale space level to get the sub-sampled shape from\\n\\n        Returns\\n        -------\\n            the sub-sampled shape at the requested resolution or None if an\\n            invalid level was requested\\n\\n        \"\"\"\\n        return self._get_attribute(self.domain_shapes, level)\\n\\n    def get_spacing(self, level):\\n        \"\"\"Spacings the sub-sampled image must have at a particular level.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\scalespace.py.txt'}),\n",
       " Document(page_content='Returns\\n        -------\\n            the sub-sampled shape at the requested resolution or None if an\\n            invalid level was requested\\n\\n        \"\"\"\\n        return self._get_attribute(self.domain_shapes, level)\\n\\n    def get_spacing(self, level):\\n        \"\"\"Spacings the sub-sampled image must have at a particular level.\\n\\n        Returns the spacings (voxel sizes) the sub-sampled image must have at a\\n        particular resolution of the scale space (note that this object does\\n        not explicitly subsample the smoothed images, but only provides the\\n        properties the sub-sampled images must have).\\n\\n        Parameters\\n        ----------\\n        level : int, 0 <= from_level < L, (L = number of resolutions)\\n            the scale space level to get the sub-sampled shape from\\n\\n        Returns\\n        -------\\n        the spacings (voxel sizes) at the requested resolution or None if an\\n        invalid level was requested\\n\\n        \"\"\"\\n        return self._get_attribute(self.spacings, level)\\n\\n    def get_scaling(self, level):\\n        \"\"\"Adjustment factor for input-spacing to reflect voxel sizes at level.\\n\\n        Returns the scaling factor that needs to be applied to the input\\n        spacing (the voxel sizes of the image at level 0 of the scale space) to\\n        transform them to voxel sizes at the requested level.\\n\\n        Parameters\\n        ----------\\n        level : int, 0 <= from_level < L, (L = number of resolutions)\\n            the scale space level to get the scalings from\\n\\n        Returns\\n        -------\\n        the scaling factors from the original spacing to the spacings at the\\n        requested level', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\scalespace.py.txt'}),\n",
       " Document(page_content='Parameters\\n        ----------\\n        level : int, 0 <= from_level < L, (L = number of resolutions)\\n            the scale space level to get the scalings from\\n\\n        Returns\\n        -------\\n        the scaling factors from the original spacing to the spacings at the\\n        requested level\\n\\n        \"\"\"\\n        return self._get_attribute(self.scalings, level)\\n\\n    def get_affine(self, level):\\n        \"\"\"Voxel-to-space transformation at a given level.\\n\\n        Returns the voxel-to-space transformation associated with the\\n        sub-sampled image at a particular resolution of the scale space (note\\n        that this object does not explicitly subsample the smoothed images, but\\n        only provides the properties the sub-sampled images must have).\\n\\n        Parameters\\n        ----------\\n        level : int, 0 <= from_level < L, (L = number of resolutions)\\n            the scale space level to get affine transform from\\n\\n        Returns\\n        -------\\n            the affine (voxel-to-space) transform at the requested resolution\\n            or None if an invalid level was requested\\n        \"\"\"\\n        return self._get_attribute(self.affines, level)\\n\\n    def get_affine_inv(self, level):\\n        \"\"\"Space-to-voxel transformation at a given level.\\n\\n        Returns the space-to-voxel transformation associated with the\\n        sub-sampled image at a particular resolution of the scale space (note\\n        that this object does not explicitly subsample the smoothed images, but\\n        only provides the properties the sub-sampled images must have).', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\scalespace.py.txt'}),\n",
       " Document(page_content='def get_affine_inv(self, level):\\n        \"\"\"Space-to-voxel transformation at a given level.\\n\\n        Returns the space-to-voxel transformation associated with the\\n        sub-sampled image at a particular resolution of the scale space (note\\n        that this object does not explicitly subsample the smoothed images, but\\n        only provides the properties the sub-sampled images must have).\\n\\n        Parameters\\n        ----------\\n        level : int, 0 <= from_level < L, (L = number of resolutions)\\n            the scale space level to get the inverse transform from\\n\\n        Returns\\n        -------\\n        the inverse (space-to-voxel) transform at the requested resolution or\\n        None if an invalid level was requested\\n\\n        \"\"\"\\n        return self._get_attribute(self.affine_invs, level)\\n\\n    def get_sigmas(self, level):\\n        \"\"\"Smoothing parameters used at a given level.\\n\\n        Returns the smoothing parameters (a scalar for each axis) used at the\\n        requested level of the scale space\\n\\n        Parameters\\n        ----------\\n        level : int, 0 <= from_level < L, (L = number of resolutions)\\n            the scale space level to get the smoothing parameters from\\n\\n        Returns\\n        -------\\n        the smoothing parameters at the requested level\\n\\n        \"\"\"\\n        return self._get_attribute(self.sigmas, level)\\n\\n\\nclass IsotropicScaleSpace(ScaleSpace):\\n    def __init__(self, image, factors, sigmas,\\n                 image_grid2world=None,\\n                 input_spacing=None,\\n                 mask0=False):\\n        \"\"\" IsotropicScaleSpace.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\scalespace.py.txt'}),\n",
       " Document(page_content='Returns\\n        -------\\n        the smoothing parameters at the requested level\\n\\n        \"\"\"\\n        return self._get_attribute(self.sigmas, level)\\n\\n\\nclass IsotropicScaleSpace(ScaleSpace):\\n    def __init__(self, image, factors, sigmas,\\n                 image_grid2world=None,\\n                 input_spacing=None,\\n                 mask0=False):\\n        \"\"\" IsotropicScaleSpace.\\n\\n        Computes the Scale Space representation of an image using isotropic\\n        smoothing kernels for all scales. The scale space is simply a list\\n        of images produced by smoothing the input image with a Gaussian\\n        kernel with different smoothing parameters.\\n\\n        This specialization of ScaleSpace allows the user to provide custom\\n        scale and smoothing factors for all scales.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\scalespace.py.txt'}),\n",
       " Document(page_content='Computes the Scale Space representation of an image using isotropic\\n        smoothing kernels for all scales. The scale space is simply a list\\n        of images produced by smoothing the input image with a Gaussian\\n        kernel with different smoothing parameters.\\n\\n        This specialization of ScaleSpace allows the user to provide custom\\n        scale and smoothing factors for all scales.\\n\\n        Parameters\\n        ----------\\n        image : array, shape (r,c) or (s, r, c) where s is the number of\\n            slices, r is the number of rows and c is the number of columns of\\n            the input image.\\n        factors : list of floats\\n            custom scale factors to build the scale space (one factor for each\\n            scale).\\n        sigmas : list of floats\\n            custom smoothing parameter to build the scale space (one parameter\\n            for each scale).\\n        image_grid2world : array, shape (dim + 1, dim + 1), optional\\n            the grid-to-space transform of the image grid. The default is\\n            the identity matrix.\\n        input_spacing : array, shape (dim,), optional\\n            the spacing (voxel size) between voxels in physical space. The\\n            default if 1.0 along all axes.\\n        mask0 : Boolean, optional\\n            if True, all smoothed images will be zero at all voxels that are\\n            zero in the input image. The default is False.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\scalespace.py.txt'}),\n",
       " Document(page_content='\"\"\"\\n        self.dim = len(image.shape)\\n        self.num_levels = len(factors)\\n        if len(sigmas) != self.num_levels:\\n            raise ValueError(\"sigmas and factors must have the same length\")\\n        input_size = np.array(image.shape)\\n        if mask0:\\n            mask = np.asarray(image > 0, dtype=np.int32)\\n\\n        # Normalize input image to [0,1]\\n        img = ((image.astype(np.float64) - np.min(image)) /\\n               (np.max(image) - np.min(image)))\\n        if mask0:\\n            img *= mask\\n\\n        # The properties are saved in separate lists. Insert input image\\n        # properties at the first level of the scale space\\n        self.images = [img.astype(floating)]\\n        self.domain_shapes = [input_size.astype(np.int32)]\\n        if input_spacing is None:\\n            input_spacing = np.ones((self.dim,), dtype=np.int32)\\n        self.spacings = [input_spacing]\\n        self.scalings = [np.ones(self.dim)]\\n        self.affines = [image_grid2world]\\n        self.sigmas = [np.ones(self.dim) * sigmas[self.num_levels - 1]]\\n\\n        if image_grid2world is not None:\\n            self.affine_invs = [npl.inv(image_grid2world)]\\n        else:\\n            self.affine_invs = [None]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\scalespace.py.txt'}),\n",
       " Document(page_content='if image_grid2world is not None:\\n            self.affine_invs = [npl.inv(image_grid2world)]\\n        else:\\n            self.affine_invs = [None]\\n\\n        # Compute the rest of the levels\\n        min_index = np.argmin(input_spacing)\\n        for i in range(1, self.num_levels):\\n            factor = factors[self.num_levels - 1 - i]\\n            shrink_factors = np.zeros(self.dim)\\n            new_spacing = np.zeros(self.dim)\\n            shrink_factors[min_index] = factor\\n            new_spacing[min_index] = input_spacing[min_index] * factor\\n            for j in range(self.dim):\\n                if j != min_index:\\n                    # Select the factor that maximizes isotropy\\n                    shrink_factors[j] = factor\\n                    new_spacing[j] = input_spacing[j] * factor\\n                    min_diff = np.abs(new_spacing[j] - new_spacing[min_index])\\n                    for f in range(1, factor):\\n                        diff = input_spacing[j] * f - new_spacing[min_index]\\n                        diff = np.abs(diff)\\n                        if diff < min_diff:\\n                            shrink_factors[j] = f\\n                            new_spacing[j] = input_spacing[j] * f\\n                            min_diff = diff\\n\\n            extended = np.append(shrink_factors, [1])\\n            if image_grid2world is not None:\\n                affine = image_grid2world.dot(np.diag(extended))\\n            else:\\n                affine = np.diag(extended)\\n            output_size = (input_size / shrink_factors).astype(np.int32)\\n            new_sigmas = np.ones(self.dim) * sigmas[self.num_levels - i - 1]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\scalespace.py.txt'}),\n",
       " Document(page_content='extended = np.append(shrink_factors, [1])\\n            if image_grid2world is not None:\\n                affine = image_grid2world.dot(np.diag(extended))\\n            else:\\n                affine = np.diag(extended)\\n            output_size = (input_size / shrink_factors).astype(np.int32)\\n            new_sigmas = np.ones(self.dim) * sigmas[self.num_levels - i - 1]\\n\\n            # Filter along each direction with the appropriate sigma\\n            filtered = gaussian_filter(image.astype(np.float64), new_sigmas)\\n            filtered = ((filtered.astype(np.float64) - np.min(filtered)) /\\n                        (np.max(filtered) - np.min(filtered)))\\n            if mask0:\\n                filtered *= mask\\n\\n            # Add current level to the scale space\\n            self.images.append(filtered.astype(floating))\\n            self.domain_shapes.append(output_size)\\n            self.spacings.append(new_spacing)\\n            self.scalings.append(shrink_factors)\\n            self.affines.append(affine)\\n            self.affine_invs.append(npl.inv(affine))\\n            self.sigmas.append(new_sigmas)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\scalespace.py.txt'}),\n",
       " Document(page_content='import logging\\nimport abc\\nfrom itertools import combinations\\nimport numpy as np\\nfrom dipy.core.optimize import Optimizer\\nfrom dipy.align.bundlemin import (_bundle_minimum_distance,\\n                                  _bundle_minimum_distance_asymmetric,\\n                                  distance_matrix_mdf)\\nfrom dipy.tracking.streamline import (transform_streamlines,\\n                                      unlist_streamlines,\\n                                      center_streamlines,\\n                                      set_number_of_points,\\n                                      select_random_set_of_streamlines,\\n                                      length,\\n                                      Streamlines)\\nfrom dipy.segment.clustering import qbx_and_merge\\nfrom dipy.core.geometry import (compose_transformations,\\n                                compose_matrix,\\n                                decompose_matrix)\\nfrom time import time\\n\\nDEFAULT_BOUNDS = [(-35, 35), (-35, 35), (-35, 35),\\n                  (-45, 45), (-45, 45), (-45, 45),\\n                  (0.6, 1.4), (0.6, 1.4), (0.6, 1.4),\\n                  (-10, 10), (-10, 10), (-10, 10)]\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass StreamlineDistanceMetric(metaclass=abc.ABCMeta):\\n\\n    def __init__(self, num_threads=None):\\n        \"\"\" An abstract class for the metric used for streamline registration.\\n\\n        If the two sets of streamlines match exactly then method ``distance``\\n        of this object should be minimum.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='logger = logging.getLogger(__name__)\\n\\n\\nclass StreamlineDistanceMetric(metaclass=abc.ABCMeta):\\n\\n    def __init__(self, num_threads=None):\\n        \"\"\" An abstract class for the metric used for streamline registration.\\n\\n        If the two sets of streamlines match exactly then method ``distance``\\n        of this object should be minimum.\\n\\n        Parameters\\n        ----------\\n        num_threads : int, optional\\n            Number of threads to be used for OpenMP parallelization. If None\\n            (default) the value of OMP_NUM_THREADS environment variable is used\\n            if it is set, otherwise all available threads are used. If < 0 the\\n            maximal number of threads minus |num_threads + 1| is used (enter -1\\n            to use as many threads as possible). 0 raises an error. Only\\n            metrics using OpenMP will use this variable.\\n\\n        \"\"\"\\n        self.static = None\\n        self.moving = None\\n        self.num_threads = num_threads\\n\\n    @abc.abstractmethod\\n    def setup(self, static, moving):\\n        pass\\n\\n    @abc.abstractmethod\\n    def distance(self, xopt):\\n        \"\"\" calculate distance for current set of parameters.\\n        \"\"\"\\n        pass\\n\\n\\nclass BundleMinDistanceMetric(StreamlineDistanceMetric):\\n    \"\"\" Bundle-based Minimum Distance aka BMD\\n\\n    This is the cost function used by the StreamlineLinearRegistration.\\n\\n    Methods\\n    -------\\n    setup(static, moving)\\n    distance(xopt)\\n\\n    References\\n    ----------\\n    .. [Garyfallidis14] Garyfallidis et al., \"Direct native-space fiber\\n                        bundle alignment for group comparisons\", ISMRM,\\n                        2014.\\n    \"\"\"', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='class BundleMinDistanceMetric(StreamlineDistanceMetric):\\n    \"\"\" Bundle-based Minimum Distance aka BMD\\n\\n    This is the cost function used by the StreamlineLinearRegistration.\\n\\n    Methods\\n    -------\\n    setup(static, moving)\\n    distance(xopt)\\n\\n    References\\n    ----------\\n    .. [Garyfallidis14] Garyfallidis et al., \"Direct native-space fiber\\n                        bundle alignment for group comparisons\", ISMRM,\\n                        2014.\\n    \"\"\"\\n\\n    def setup(self, static, moving):\\n        \"\"\" Setup static and moving sets of streamlines.\\n\\n        Parameters\\n        ----------\\n        static : streamlines\\n            Fixed or reference set of streamlines.\\n        moving : streamlines\\n            Moving streamlines.\\n\\n        Notes\\n        -----\\n        Call this after the object is initiated and before distance.\\n        \"\"\"\\n\\n        self._set_static(static)\\n        self._set_moving(moving)\\n\\n    def _set_static(self, static):\\n        static_centered_pts, st_idx = unlist_streamlines(static)\\n        self.static_centered_pts = np.ascontiguousarray(static_centered_pts,\\n                                                        dtype=np.float64)\\n        self.block_size = st_idx[0]\\n\\n    def _set_moving(self, moving):\\n        self.moving_centered_pts, _ = unlist_streamlines(moving)\\n\\n    def distance(self, xopt):\\n        \"\"\" Distance calculated from this Metric.\\n\\n        Parameters\\n        ----------\\n        xopt : sequence\\n            List of affine parameters as an 1D vector,', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='def _set_moving(self, moving):\\n        self.moving_centered_pts, _ = unlist_streamlines(moving)\\n\\n    def distance(self, xopt):\\n        \"\"\" Distance calculated from this Metric.\\n\\n        Parameters\\n        ----------\\n        xopt : sequence\\n            List of affine parameters as an 1D vector,\\n\\n        \"\"\"\\n        return bundle_min_distance_fast(xopt,\\n                                        self.static_centered_pts,\\n                                        self.moving_centered_pts,\\n                                        self.block_size,\\n                                        self.num_threads)\\n\\n\\nclass BundleMinDistanceMatrixMetric(StreamlineDistanceMetric):\\n    \"\"\" Bundle-based Minimum Distance aka BMD\\n\\n    This is the cost function used by the StreamlineLinearRegistration\\n\\n    Methods\\n    -------\\n    setup(static, moving)\\n    distance(xopt)\\n\\n    Notes\\n    -----\\n    The difference with BundleMinDistanceMetric is that this creates\\n    the entire distance matrix and therefore requires more memory.\\n\\n    \"\"\"\\n\\n    def setup(self, static, moving):\\n        \"\"\" Setup static and moving sets of streamlines.\\n\\n        Parameters\\n        ----------\\n        static : streamlines\\n            Fixed or reference set of streamlines.\\n        moving : streamlines\\n            Moving streamlines.\\n\\n        Notes\\n        -----\\n        Call this after the object is initiated and before distance.\\n\\n        Num_threads is not used in this class. Use ``BundleMinDistanceMetric``\\n        for a faster, threaded and less memory hungry metric\\n\\n        \"\"\"\\n        self.static = static\\n        self.moving = moving\\n\\n    def distance(self, xopt):\\n        \"\"\" Distance calculated from this Metric.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='Notes\\n        -----\\n        Call this after the object is initiated and before distance.\\n\\n        Num_threads is not used in this class. Use ``BundleMinDistanceMetric``\\n        for a faster, threaded and less memory hungry metric\\n\\n        \"\"\"\\n        self.static = static\\n        self.moving = moving\\n\\n    def distance(self, xopt):\\n        \"\"\" Distance calculated from this Metric.\\n\\n        Parameters\\n        ----------\\n        xopt : sequence\\n            List of affine parameters as an 1D vector\\n        \"\"\"\\n        return bundle_min_distance(xopt, self.static, self.moving)\\n\\n\\nclass BundleMinDistanceAsymmetricMetric(BundleMinDistanceMetric):\\n    \"\"\" Asymmetric Bundle-based Minimum distance.\\n\\n    This is a cost function that can be used by the\\n    StreamlineLinearRegistration class.\\n\\n    \"\"\"\\n\\n    def distance(self, xopt):\\n        \"\"\" Distance calculated from this Metric.\\n\\n        Parameters\\n        ----------\\n        xopt : sequence\\n            List of affine parameters as an 1D vector\\n\\n        \"\"\"\\n        return bundle_min_distance_asymmetric_fast(xopt,\\n                                                   self.static_centered_pts,\\n                                                   self.moving_centered_pts,\\n                                                   self.block_size)\\n\\n\\nclass BundleSumDistanceMatrixMetric(BundleMinDistanceMatrixMetric):\\n    \"\"\" Bundle-based Sum Distance aka BMD\\n\\n    This is a cost function that can be used by the\\n    StreamlineLinearRegistration class.\\n\\n    Methods\\n    -------\\n    setup(static, moving)\\n    distance(xopt)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='class BundleSumDistanceMatrixMetric(BundleMinDistanceMatrixMetric):\\n    \"\"\" Bundle-based Sum Distance aka BMD\\n\\n    This is a cost function that can be used by the\\n    StreamlineLinearRegistration class.\\n\\n    Methods\\n    -------\\n    setup(static, moving)\\n    distance(xopt)\\n\\n    Notes\\n    -----\\n    The difference with BundleMinDistanceMatrixMetric is that it uses\\n    uses the sum of the distance matrix and not the sum of mins.\\n    \"\"\"\\n\\n    def distance(self, xopt):\\n        \"\"\" Distance calculated from this Metric\\n\\n        Parameters\\n        ----------\\n        xopt : sequence\\n            List of affine parameters as an 1D vector\\n        \"\"\"\\n        return bundle_sum_distance(xopt, self.static, self.moving)\\n\\n\\nclass JointBundleMinDistanceMetric(StreamlineDistanceMetric):\\n    \"\"\" Bundle-based Minimum Distance for joint optimization.\\n\\n    This cost function is used by the StreamlineLinearRegistration class when\\n    running halfway streamline linear registration for unbiased groupwise\\n    bundle registration and atlasing.\\n\\n    It computes the BMD distance after moving both static and moving bundles to\\n    a halfway space in between both.\\n\\n    Methods\\n    -------\\n    setup(static, moving)\\n    distance(xopt)\\n\\n    Notes\\n    -----\\n    In this metric both static and moving bundles are treated equally (i.e.,\\n    there is no static reference bundle as both are intended to move). The\\n    naming convention is kept for consistency.\\n    \"\"\"\\n\\n    def setup(self, static, moving):\\n        \"\"\" Setup static and moving sets of streamlines.\\n\\n        Parameters\\n        ----------\\n        static : streamlines\\n            Set of streamlines\\n        moving : streamlines\\n            Set of streamlines', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='Notes\\n    -----\\n    In this metric both static and moving bundles are treated equally (i.e.,\\n    there is no static reference bundle as both are intended to move). The\\n    naming convention is kept for consistency.\\n    \"\"\"\\n\\n    def setup(self, static, moving):\\n        \"\"\" Setup static and moving sets of streamlines.\\n\\n        Parameters\\n        ----------\\n        static : streamlines\\n            Set of streamlines\\n        moving : streamlines\\n            Set of streamlines\\n\\n        Notes\\n        -----\\n        Call this after the object is initiated and before distance.\\n        Num_threads is not used in this class.\\n        \"\"\"\\n        self.static = static\\n        self.moving = moving\\n\\n    def distance(self, xopt):\\n        \"\"\" Distance calculated from this Metric.\\n\\n        Parameters\\n        ----------\\n        xopt : sequence\\n            List of affine parameters as an 1D vector. These affine parameters\\n            are used to derive the corresponding halfway transformation\\n            parameters for each bundle.\\n        \"\"\"\\n        # Define halfway space transformations\\n        x_static = np.concatenate((xopt[0:6]/2, (1+xopt[6:9])/2, xopt[9:12]/2))\\n        x_moving = np.concatenate((-xopt[0:6]/2, 2/(1+xopt[6:9]),\\n                                   -xopt[9:12]/2))\\n\\n        # Move static bundle to the halfway space\\n        aff_static = compose_matrix44(x_static)\\n        static = transform_streamlines(self.static, aff_static)\\n\\n        # Move moving bundle to halfway space and compute distance\\n        return bundle_min_distance(x_moving, static, self.moving)\\n\\n\\nclass StreamlineLinearRegistration:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='# Move static bundle to the halfway space\\n        aff_static = compose_matrix44(x_static)\\n        static = transform_streamlines(self.static, aff_static)\\n\\n        # Move moving bundle to halfway space and compute distance\\n        return bundle_min_distance(x_moving, static, self.moving)\\n\\n\\nclass StreamlineLinearRegistration:\\n\\n    def __init__(self, metric=None, x0=\"rigid\", method=\\'L-BFGS-B\\',\\n                 bounds=None, verbose=False, options=None, evolution=False,\\n                 num_threads=None):\\n        r\"\"\" Linear registration of 2 sets of streamlines [Garyfallidis15]_.\\n\\n        Parameters\\n        ----------\\n        metric : StreamlineDistanceMetric,\\n            If None and fast is False then the BMD distance is used. If fast\\n            is True then a faster implementation of BMD is used. Otherwise,\\n            use the given distance metric.\\n\\n        x0 : array or int or str\\n            Initial parametrization for the optimization.\\n\\n            If 1D array with:\\n                a) 6 elements then only rigid registration is performed with\\n                the 3 first elements for translation and 3 for rotation.\\n                b) 7 elements also isotropic scaling is performed (similarity).\\n                c) 12 elements then translation, rotation (in degrees),\\n                scaling and shearing is performed (affine).\\n\\n                Here is an example of x0 with 12 elements:\\n                ``x0=np.array([0, 10, 0, 40, 0, 0, 2., 1.5, 1, 0.1, -0.5, 0])``\\n\\n                This has translation (0, 10, 0), rotation (40, 0, 0) in\\n                degrees, scaling (2., 1.5, 1) and shearing (0.1, -0.5, 0).', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='Here is an example of x0 with 12 elements:\\n                ``x0=np.array([0, 10, 0, 40, 0, 0, 2., 1.5, 1, 0.1, -0.5, 0])``\\n\\n                This has translation (0, 10, 0), rotation (40, 0, 0) in\\n                degrees, scaling (2., 1.5, 1) and shearing (0.1, -0.5, 0).\\n\\n            If int:\\n                a) 6\\n                    ``x0 = np.array([0, 0, 0, 0, 0, 0])``\\n                b) 7\\n                    ``x0 = np.array([0, 0, 0, 0, 0, 0, 1.])``\\n                c) 12\\n                    ``x0 = np.array([0, 0, 0, 0, 0, 0, 1., 1., 1, 0, 0, 0])``\\n\\n            If str:\\n                a) \"rigid\"\\n                    ``x0 = np.array([0, 0, 0, 0, 0, 0])``\\n                b) \"similarity\"\\n                    ``x0 = np.array([0, 0, 0, 0, 0, 0, 1.])``\\n                c) \"affine\"\\n                    ``x0 = np.array([0, 0, 0, 0, 0, 0, 1., 1., 1, 0, 0, 0])``\\n\\n        method : str,\\n            \\'L_BFGS_B\\' or \\'Powell\\' optimizers can be used. Default is\\n            \\'L_BFGS_B\\'.\\n\\n        bounds : list of tuples or None,\\n            If method == \\'L_BFGS_B\\' then we can use bounded optimization.\\n            For example for the six parameters of rigid rotation we can set\\n            the bounds = [(-30, 30), (-30, 30), (-30, 30),\\n                          (-45, 45), (-45, 45), (-45, 45)]\\n            That means that we have set the bounds for the three translations\\n            and three rotation axes (in degrees).\\n\\n        verbose : bool, optional.\\n            If True, if True then information about the optimization is shown.\\n            Default: False.\\n\\n        options : None or dict,\\n            Extra options to be used with the selected method.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='verbose : bool, optional.\\n            If True, if True then information about the optimization is shown.\\n            Default: False.\\n\\n        options : None or dict,\\n            Extra options to be used with the selected method.\\n\\n        evolution : boolean\\n            If True save the transformation for each iteration of the\\n            optimizer. Default is False. Supported only with Scipy >= 0.11.\\n\\n        num_threads : int, optional\\n            Number of threads to be used for OpenMP parallelization. If None\\n            (default) the value of OMP_NUM_THREADS environment variable is used\\n            if it is set, otherwise all available threads are used. If < 0 the\\n            maximal number of threads minus |num_threads + 1| is used (enter -1\\n            to use as many threads as possible). 0 raises an error. Only\\n            metrics using OpenMP will use this variable.\\n\\n        References\\n        ----------\\n        .. [Garyfallidis15] Garyfallidis et al. \"Robust and efficient linear\\n           registration of white-matter fascicles in the space of streamlines\",\\n           NeuroImage, 117, 124--140, 2015\\n\\n        .. [Garyfallidis14] Garyfallidis et al., \"Direct native-space fiber\\n           bundle alignment for group comparisons\", ISMRM, 2014.\\n\\n        .. [Garyfallidis17] Garyfallidis et al. Recognition of white matter\\n           bundles using local and global streamline-based\\n           registration and clustering, Neuroimage, 2017.\\n\\n        \"\"\"\\n        self.x0 = self._set_x0(x0)\\n        self.metric = metric\\n\\n        if self.metric is None:\\n            self.metric = BundleMinDistanceMetric(num_threads=num_threads)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='.. [Garyfallidis17] Garyfallidis et al. Recognition of white matter\\n           bundles using local and global streamline-based\\n           registration and clustering, Neuroimage, 2017.\\n\\n        \"\"\"\\n        self.x0 = self._set_x0(x0)\\n        self.metric = metric\\n\\n        if self.metric is None:\\n            self.metric = BundleMinDistanceMetric(num_threads=num_threads)\\n\\n        self.verbose = verbose\\n        self.method = method\\n        if self.method not in [\\'Powell\\', \\'L-BFGS-B\\']:\\n            raise ValueError(\\'Only Powell and L-BFGS-B can be used\\')\\n        self.bounds = bounds\\n        self.options = options\\n        self.evolution = evolution\\n\\n    def optimize(self, static, moving, mat=None):\\n        \"\"\" Find the minimum of the provided metric.\\n\\n        Parameters\\n        ----------\\n        static : streamlines\\n            Reference or fixed set of streamlines.\\n        moving : streamlines\\n            Moving set of streamlines.\\n        mat : array\\n            Transformation (4, 4) matrix to start the registration. ``mat``\\n            is applied to moving. Default value None which means that initial\\n            transformation will be generated by shifting the centers of moving\\n            and static sets of streamlines to the origin.\\n\\n        Returns\\n        -------\\n        map : StreamlineRegistrationMap\\n\\n        \"\"\"\\n        msg = \\'need to have the same number of points. Use \\'\\n        msg += \\'set_number_of_points from dipy.tracking.streamline\\'\\n\\n        if not np.all(np.array(list(map(len, static))) == static[0].shape[0]):\\n            raise ValueError(\\'Static streamlines \\' + msg)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='Returns\\n        -------\\n        map : StreamlineRegistrationMap\\n\\n        \"\"\"\\n        msg = \\'need to have the same number of points. Use \\'\\n        msg += \\'set_number_of_points from dipy.tracking.streamline\\'\\n\\n        if not np.all(np.array(list(map(len, static))) == static[0].shape[0]):\\n            raise ValueError(\\'Static streamlines \\' + msg)\\n\\n        if not np.all(np.array(list(map(len, moving))) == moving[0].shape[0]):\\n            raise ValueError(\\'Moving streamlines \\' + msg)\\n\\n        if not np.all(np.array(list(map(len, moving))) == static[0].shape[0]):\\n            raise ValueError(\\'Static and moving streamlines \\' + msg)\\n\\n        if mat is None:\\n            static_centered, static_shift = center_streamlines(static)\\n            moving_centered, moving_shift = center_streamlines(moving)\\n            static_mat = compose_matrix44([static_shift[0], static_shift[1],\\n                                           static_shift[2], 0, 0, 0])\\n\\n            moving_mat = compose_matrix44([-moving_shift[0], -moving_shift[1],\\n                                           -moving_shift[2], 0, 0, 0])\\n        else:\\n            static_centered = static\\n            moving_centered = transform_streamlines(moving, mat)\\n            static_mat = np.eye(4)\\n            moving_mat = mat\\n\\n        self.metric.setup(static_centered, moving_centered)\\n\\n        distance = self.metric.distance\\n\\n        if self.method == \\'Powell\\':\\n\\n            if self.options is None:\\n                self.options = {\\'xtol\\': 1e-6, \\'ftol\\': 1e-6, \\'maxiter\\': 1e6}', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content=\"self.metric.setup(static_centered, moving_centered)\\n\\n        distance = self.metric.distance\\n\\n        if self.method == 'Powell':\\n\\n            if self.options is None:\\n                self.options = {'xtol': 1e-6, 'ftol': 1e-6, 'maxiter': 1e6}\\n\\n            opt = Optimizer(distance, self.x0.tolist(),\\n                            method=self.method, options=self.options,\\n                            evolution=self.evolution)\\n\\n        if self.method == 'L-BFGS-B':\\n\\n            if self.options is None:\\n                self.options = {'maxcor': 10, 'ftol': 1e-7,\\n                                'gtol': 1e-5, 'eps': 1e-8,\\n                                'maxiter': 100}\\n\\n            opt = Optimizer(distance, self.x0.tolist(),\\n                            method=self.method,\\n                            bounds=self.bounds, options=self.options,\\n                            evolution=self.evolution)\\n        if self.verbose:\\n            opt.print_summary()\\n\\n        opt_mat = compose_matrix44(opt.xopt)\\n\\n        mat = compose_transformations(moving_mat, opt_mat, static_mat)\\n\\n        mat_history = []\\n\\n        if opt.evolution is not None:\\n            for vecs in opt.evolution:\\n                mat_history.append(\\n                    compose_transformations(moving_mat,\\n                                            compose_matrix44(vecs),\\n                                            static_mat))\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='opt_mat = compose_matrix44(opt.xopt)\\n\\n        mat = compose_transformations(moving_mat, opt_mat, static_mat)\\n\\n        mat_history = []\\n\\n        if opt.evolution is not None:\\n            for vecs in opt.evolution:\\n                mat_history.append(\\n                    compose_transformations(moving_mat,\\n                                            compose_matrix44(vecs),\\n                                            static_mat))\\n\\n        # If we are running halfway streamline linear registration (for\\n        # groupwise registration or atlasing) the registration map is different\\n        if isinstance(self.metric, JointBundleMinDistanceMetric):\\n            srm = JointStreamlineRegistrationMap(opt.xopt, opt.fopt,\\n                                                 mat_history, opt.nfev,\\n                                                 opt.nit)\\n        else:\\n            srm = StreamlineRegistrationMap(mat, opt.xopt, opt.fopt,\\n                                            mat_history, opt.nfev, opt.nit)\\n\\n        del opt\\n        return srm\\n\\n    def _set_x0(self, x0):\\n        \"\"\" check if input is of correct type.\"\"\"\\n\\n        if hasattr(x0, \\'ndim\\'):\\n\\n            if len(x0) not in [3, 6, 7, 9, 12]:\\n                m_ = \\'Only 1D arrays of 3, 6, 7, 9 and 12 elements are allowed\\'\\n                raise ValueError(m_)\\n            if x0.ndim != 1:\\n                raise ValueError(\"Array should have only one dimension\")\\n            return x0\\n\\n        if isinstance(x0, str):\\n\\n            if x0.lower() == \\'translation\\':\\n                return np.zeros(3)\\n\\n            if x0.lower() == \\'rigid\\':\\n                return np.zeros(6)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='if len(x0) not in [3, 6, 7, 9, 12]:\\n                m_ = \\'Only 1D arrays of 3, 6, 7, 9 and 12 elements are allowed\\'\\n                raise ValueError(m_)\\n            if x0.ndim != 1:\\n                raise ValueError(\"Array should have only one dimension\")\\n            return x0\\n\\n        if isinstance(x0, str):\\n\\n            if x0.lower() == \\'translation\\':\\n                return np.zeros(3)\\n\\n            if x0.lower() == \\'rigid\\':\\n                return np.zeros(6)\\n\\n            if x0.lower() == \\'similarity\\':\\n                return np.array([0, 0, 0, 0, 0, 0, 1.])\\n\\n            if x0.lower() == \\'scaling\\':\\n                return np.array([0, 0, 0, 0, 0, 0, 1., 1., 1.])\\n\\n            if x0.lower() == \\'affine\\':\\n                return np.array([0, 0, 0, 0, 0, 0, 1., 1., 1., 0, 0, 0])\\n\\n        if isinstance(x0, int):\\n            if x0 not in [3, 6, 7, 9, 12]:\\n                msg = \\'Only 3, 6, 7, 9 and 12 are accepted as integers\\'\\n                raise ValueError(msg)\\n            else:\\n                if x0 == 3:\\n                    return np.zeros(3)\\n                if x0 == 6:\\n                    return np.zeros(6)\\n                if x0 == 7:\\n                    return np.array([0, 0, 0, 0, 0, 0, 1.])\\n                if x0 == 9:\\n                    return np.array([0, 0, 0, 0, 0, 0, 1., 1., 1.])\\n                if x0 == 12:\\n                    return np.array([0, 0, 0, 0, 0, 0, 1., 1., 1., 0, 0, 0])\\n\\n        raise ValueError(\\'Wrong input\\')\\n\\n\\nclass StreamlineRegistrationMap:\\n\\n    def __init__(self, matopt, xopt, fopt, matopt_history, funcs, iterations):\\n        r\"\"\" A map holding the optimum affine matrix and some other parameters\\n        of the optimization', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='raise ValueError(\\'Wrong input\\')\\n\\n\\nclass StreamlineRegistrationMap:\\n\\n    def __init__(self, matopt, xopt, fopt, matopt_history, funcs, iterations):\\n        r\"\"\" A map holding the optimum affine matrix and some other parameters\\n        of the optimization\\n\\n        Parameters\\n        ----------\\n        matrix : array,\\n            4x4 affine matrix which transforms the moving to the static\\n            streamlines\\n\\n        xopt : array,\\n            1d array with the parameters of the transformation after centering\\n\\n        fopt : float,\\n            final value of the metric\\n\\n        matrix_history : array\\n            All transformation matrices created during the optimization\\n\\n        funcs : int,\\n            Number of function evaluations of the optimizer\\n\\n        iterations : int\\n            Number of iterations of the optimizer\\n\\n        \"\"\"\\n        self.matrix = matopt\\n        self.xopt = xopt\\n        self.fopt = fopt\\n        self.matrix_history = matopt_history\\n        self.funcs = funcs\\n        self.iterations = iterations\\n\\n    def transform(self, moving):\\n        \"\"\" Transform moving streamlines to the static.\\n\\n        Parameters\\n        ----------\\n        moving : streamlines\\n\\n        Returns\\n        -------\\n        moved : streamlines\\n\\n        Notes\\n        -----\\n        All this does is apply ``self.matrix`` to the input streamlines.\\n\\n        \"\"\"\\n        return transform_streamlines(moving, self.matrix)\\n\\n\\nclass JointStreamlineRegistrationMap():\\n\\n    def __init__(self, xopt, fopt, matopt_history, funcs, iterations):\\n        \"\"\" A map holding the optimum affine matrices for halfway streamline\\n        linear registration and some other parameters of the optimization.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='Returns\\n        -------\\n        moved : streamlines\\n\\n        Notes\\n        -----\\n        All this does is apply ``self.matrix`` to the input streamlines.\\n\\n        \"\"\"\\n        return transform_streamlines(moving, self.matrix)\\n\\n\\nclass JointStreamlineRegistrationMap():\\n\\n    def __init__(self, xopt, fopt, matopt_history, funcs, iterations):\\n        \"\"\" A map holding the optimum affine matrices for halfway streamline\\n        linear registration and some other parameters of the optimization.\\n\\n        xopt is optimized by StreamlineLinearRegistration using the\\n        JointBundleMinDistanceMetric. In that case the mat argument of the\\n        optimize method needs to be np.eye(4) to avoid streamline centering.\\n\\n        This constructor derives and stores the transformations to move both\\n        static and moving bundles to the halfway space.\\n\\n        Parameters\\n        ----------\\n        xopt : array\\n            1d array with the parameters of the transformation.\\n\\n        fopt : float\\n            Final value of the metric.\\n\\n        matopt_history : array\\n            All transformation matrices created during the optimization.\\n\\n        funcs : int\\n            Number of function evaluations of the optimizer.\\n\\n        iterations : int\\n            Number of iterations of the optimizer.\\n\\n        \"\"\"\\n\\n        trans, angles, scale, shear = xopt[:3], xopt[3:6], xopt[6:9], xopt[9:]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='fopt : float\\n            Final value of the metric.\\n\\n        matopt_history : array\\n            All transformation matrices created during the optimization.\\n\\n        funcs : int\\n            Number of function evaluations of the optimizer.\\n\\n        iterations : int\\n            Number of iterations of the optimizer.\\n\\n        \"\"\"\\n\\n        trans, angles, scale, shear = xopt[:3], xopt[3:6], xopt[6:9], xopt[9:]\\n\\n        self.x1 = np.concatenate((trans/2, angles/2, (1+scale)/2, shear/2))\\n        self.x2 = np.concatenate((-trans/2, -angles/2, 2/(1+scale), -shear/2))\\n        self.matrix1 = compose_matrix44(self.x1)\\n        self.matrix2 = compose_matrix44(self.x2)\\n        self.fopt = fopt\\n        self.matrix_history = matopt_history\\n        self.funcs = funcs\\n        self.iterations = iterations\\n\\n    def transform(self, static, moving):\\n        \"\"\" Transform both static and moving bundles to the halfway space.\\n\\n        All this does is apply ``self.matrix1`` and `self.matrix2`` to the\\n        static and moving bundles, respectively.\\n\\n        Parameters\\n        ----------\\n        static : streamlines\\n\\n        moving : streamlines\\n\\n        Returns\\n        -------\\n        static : streamlines\\n\\n        moving : streamlines\\n\\n        \"\"\"\\n\\n        static = transform_streamlines(static, self.matrix1)\\n        moving = transform_streamlines(moving, self.matrix2)\\n\\n        return static, moving\\n\\n\\ndef bundle_sum_distance(t, static, moving, num_threads=None):\\n    \"\"\" MDF distance optimization function (SUM).\\n\\n    We minimize the distance between moving streamlines as they align\\n    with the static streamlines.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='Returns\\n        -------\\n        static : streamlines\\n\\n        moving : streamlines\\n\\n        \"\"\"\\n\\n        static = transform_streamlines(static, self.matrix1)\\n        moving = transform_streamlines(moving, self.matrix2)\\n\\n        return static, moving\\n\\n\\ndef bundle_sum_distance(t, static, moving, num_threads=None):\\n    \"\"\" MDF distance optimization function (SUM).\\n\\n    We minimize the distance between moving streamlines as they align\\n    with the static streamlines.\\n\\n    Parameters\\n    ----------\\n    t : ndarray\\n        t is a vector of affine transformation parameters with\\n        size at least 6.\\n        If the size is 6, t is interpreted as translation + rotation.\\n        If the size is 7, t is interpreted as translation + rotation +\\n        isotropic scaling.\\n        If size is 12, t is interpreted as translation + rotation +\\n        scaling + shearing.\\n\\n    static : list\\n        Static streamlines\\n\\n    moving : list\\n        Moving streamlines. These will be transformed to align with\\n        the static streamlines\\n\\n    num_threads : int, optional\\n        Number of threads. If -1 then all available threads will be used.\\n\\n    Returns\\n    -------\\n    cost: float\\n\\n    \"\"\"\\n\\n    aff = compose_matrix44(t)\\n    moving = transform_streamlines(moving, aff)\\n    d01 = distance_matrix_mdf(static, moving)\\n    return np.sum(d01)\\n\\n\\ndef bundle_min_distance(t, static, moving):\\n    \"\"\" MDF-based pairwise distance optimization function (MIN).\\n\\n    We minimize the distance between moving streamlines as they align\\n    with the static streamlines.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    cost: float\\n\\n    \"\"\"\\n\\n    aff = compose_matrix44(t)\\n    moving = transform_streamlines(moving, aff)\\n    d01 = distance_matrix_mdf(static, moving)\\n    return np.sum(d01)\\n\\n\\ndef bundle_min_distance(t, static, moving):\\n    \"\"\" MDF-based pairwise distance optimization function (MIN).\\n\\n    We minimize the distance between moving streamlines as they align\\n    with the static streamlines.\\n\\n    Parameters\\n    ----------\\n    t : ndarray\\n        t is a vector of affine transformation parameters with\\n        size at least 6.\\n        If size is 6, t is interpreted as translation + rotation.\\n        If size is 7, t is interpreted as translation + rotation +\\n        isotropic scaling.\\n        If size is 12, t is interpreted as translation + rotation +\\n        scaling + shearing.\\n\\n    static : list\\n        Static streamlines\\n\\n    moving : list\\n        Moving streamlines.\\n\\n    Returns\\n    -------\\n    cost: float\\n\\n    \"\"\"\\n    aff = compose_matrix44(t)\\n    moving = transform_streamlines(moving, aff)\\n    d01 = distance_matrix_mdf(static, moving)\\n\\n    rows, cols = d01.shape\\n    return 0.25 * (np.sum(np.min(d01, axis=0)) / float(cols) +\\n                   np.sum(np.min(d01, axis=1)) / float(rows)) ** 2\\n\\n\\ndef bundle_min_distance_fast(t, static, moving, block_size, num_threads=None):\\n    \"\"\" MDF-based pairwise distance optimization function (MIN).\\n\\n    We minimize the distance between moving streamlines as they align\\n    with the static streamlines.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='rows, cols = d01.shape\\n    return 0.25 * (np.sum(np.min(d01, axis=0)) / float(cols) +\\n                   np.sum(np.min(d01, axis=1)) / float(rows)) ** 2\\n\\n\\ndef bundle_min_distance_fast(t, static, moving, block_size, num_threads=None):\\n    \"\"\" MDF-based pairwise distance optimization function (MIN).\\n\\n    We minimize the distance between moving streamlines as they align\\n    with the static streamlines.\\n\\n    Parameters\\n    ----------\\n    t : array\\n        1D array. t is a vector of affine transformation parameters with\\n        size at least 6.\\n        If the size is 6, t is interpreted as translation + rotation.\\n        If the size is 7, t is interpreted as translation + rotation +\\n        isotropic scaling.\\n        If size is 12, t is interpreted as translation + rotation +\\n        scaling + shearing.\\n\\n    static : array\\n        N*M x 3 array. All the points of the static streamlines. With order of\\n        streamlines intact. Where N is the number of streamlines and M\\n        is the number of points per streamline.\\n\\n    moving : array\\n        K*M x 3 array. All the points of the moving streamlines. With order of\\n        streamlines intact. Where K is the number of streamlines and M\\n        is the number of points per streamline.\\n\\n    block_size : int\\n        Number of points per streamline. All streamlines in static and moving\\n        should have the same number of points M.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='moving : array\\n        K*M x 3 array. All the points of the moving streamlines. With order of\\n        streamlines intact. Where K is the number of streamlines and M\\n        is the number of points per streamline.\\n\\n    block_size : int\\n        Number of points per streamline. All streamlines in static and moving\\n        should have the same number of points M.\\n\\n    num_threads : int, optional\\n        Number of threads to be used for OpenMP parallelization. If None\\n        (default) the value of OMP_NUM_THREADS environment variable is used\\n        if it is set, otherwise all available threads are used. If < 0 the\\n        maximal number of threads minus |num_threads + 1| is used (enter -1 to\\n        use as many threads as possible). 0 raises an error.\\n\\n    Returns\\n    -------\\n    cost: float\\n\\n    Notes\\n    -----\\n    This is a faster implementation of ``bundle_min_distance``, which requires\\n    that all the points of each streamline are allocated into an ndarray\\n    (of shape N*M by 3, with N the number of points per streamline and M the\\n    number of streamlines). This can be done by calling\\n    `dipy.tracking.streamlines.unlist_streamlines`.\\n\\n    \"\"\"\\n\\n    aff = compose_matrix44(t)\\n    moving = np.dot(aff[:3, :3], moving.T).T + aff[:3, 3]\\n    moving = np.ascontiguousarray(moving, dtype=np.float64)\\n\\n    rows = static.shape[0] // block_size\\n    cols = moving.shape[0] // block_size\\n\\n    return _bundle_minimum_distance(static, moving,\\n                                    rows,\\n                                    cols,\\n                                    block_size,\\n                                    num_threads)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='\"\"\"\\n\\n    aff = compose_matrix44(t)\\n    moving = np.dot(aff[:3, :3], moving.T).T + aff[:3, 3]\\n    moving = np.ascontiguousarray(moving, dtype=np.float64)\\n\\n    rows = static.shape[0] // block_size\\n    cols = moving.shape[0] // block_size\\n\\n    return _bundle_minimum_distance(static, moving,\\n                                    rows,\\n                                    cols,\\n                                    block_size,\\n                                    num_threads)\\n\\n\\ndef bundle_min_distance_asymmetric_fast(t, static, moving, block_size):\\n    \"\"\" MDF-based pairwise distance optimization function (MIN).\\n\\n    We minimize the distance between moving streamlines as they align\\n    with the static streamlines.\\n\\n    Parameters\\n    ----------\\n    t : array\\n        1D array. t is a vector of affine transformation parameters with\\n        size at least 6.\\n        If the size is 6, t is interpreted as translation + rotation.\\n        If the size is 7, t is interpreted as translation + rotation +\\n        isotropic scaling.\\n        If size is 12, t is interpreted as translation + rotation +\\n        scaling + shearing.\\n\\n    static : array\\n        N*M x 3 array. All the points of the static streamlines. With order of\\n        streamlines intact. Where N is the number of streamlines and M\\n        is the number of points per streamline.\\n\\n    moving : array\\n        K*M x 3 array. All the points of the moving streamlines. With order of\\n        streamlines intact. Where K is the number of streamlines and M\\n        is the number of points per streamline.\\n\\n    block_size : int\\n        Number of points per streamline. All streamlines in static and moving\\n        should have the same number of points M.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='moving : array\\n        K*M x 3 array. All the points of the moving streamlines. With order of\\n        streamlines intact. Where K is the number of streamlines and M\\n        is the number of points per streamline.\\n\\n    block_size : int\\n        Number of points per streamline. All streamlines in static and moving\\n        should have the same number of points M.\\n\\n    Returns\\n    -------\\n    cost: float\\n\\n    \"\"\"\\n    aff = compose_matrix44(t)\\n    moving = np.dot(aff[:3, :3], moving.T).T + aff[:3, 3]\\n    moving = np.ascontiguousarray(moving, dtype=np.float64)\\n\\n    rows = static.shape[0] // block_size\\n    cols = moving.shape[0] // block_size\\n\\n    return _bundle_minimum_distance_asymmetric(static, moving,\\n                                               rows,\\n                                               cols,\\n                                               block_size)\\n\\n\\ndef remove_clusters_by_size(clusters, min_size=0):\\n    ob = filter(lambda c: len(c) >= min_size, clusters)\\n\\n    centroids = Streamlines()\\n    for cluster in ob:\\n        centroids.append(cluster.centroid)\\n\\n    return centroids\\n\\n\\ndef progressive_slr(static, moving, metric, x0, bounds, method=\\'L-BFGS-B\\',\\n                    verbose=False, num_threads=None):\\n    \"\"\" Progressive SLR.\\n\\n    This is an utility function that allows for example to do affine\\n    registration using Streamline-based Linear Registration (SLR)\\n    [Garyfallidis15]_ by starting with translation first, then rigid,\\n    then similarity, scaling and finally affine.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='return centroids\\n\\n\\ndef progressive_slr(static, moving, metric, x0, bounds, method=\\'L-BFGS-B\\',\\n                    verbose=False, num_threads=None):\\n    \"\"\" Progressive SLR.\\n\\n    This is an utility function that allows for example to do affine\\n    registration using Streamline-based Linear Registration (SLR)\\n    [Garyfallidis15]_ by starting with translation first, then rigid,\\n    then similarity, scaling and finally affine.\\n\\n    Similarly, if for example, you want to perform rigid then you start with\\n    translation first. This progressive strategy can helps with finding the\\n    optimal parameters of the final transformation.\\n\\n    Parameters\\n    ----------\\n    static : Streamlines\\n    moving : Streamlines\\n    metric : StreamlineDistanceMetric\\n    x0 : string\\n        Could be any of \\'translation\\', \\'rigid\\', \\'similarity\\', \\'scaling\\',\\n        \\'affine\\'\\n    bounds : array\\n        Boundaries of registration parameters. See variable `DEFAULT_BOUNDS`\\n        for example.\\n    method : string\\n        L_BFGS_B\\' or \\'Powell\\' optimizers can be used. Default is \\'L_BFGS_B\\'.\\n    verbose :  bool, optional.\\n        If True, log messages. Default:\\n    num_threads : int, optional\\n        Number of threads to be used for OpenMP parallelization. If None\\n        (default) the value of OMP_NUM_THREADS environment variable is used\\n        if it is set, otherwise all available threads are used. If < 0 the\\n        maximal number of threads minus |num_threads + 1| is used (enter -1 to\\n        use as many threads as possible). 0 raises an error. Only metrics\\n        using OpenMP will use this variable.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='References\\n    ----------\\n    .. [Garyfallidis15] Garyfallidis et al. \"Robust and efficient linear\\n        registration of white-matter fascicles in the space of streamlines\",\\n        NeuroImage, 117, 124--140, 2015\\n\\n    \"\"\"\\n    if verbose:\\n        logger.info(\\'Progressive Registration is Enabled\\')\\n\\n    if x0 in (\\'translation\\', \\'rigid\\', \\'similarity\\', \\'scaling\\', \\'affine\\'):\\n        if verbose:\\n            logger.info(\\' Translation  (3 parameters)...\\')\\n        slr_t = StreamlineLinearRegistration(metric=metric,\\n                                             x0=\\'translation\\',\\n                                             bounds=bounds[:3],\\n                                             method=method)\\n\\n        slm_t = slr_t.optimize(static, moving)\\n\\n    if x0 in (\\'rigid\\', \\'similarity\\', \\'scaling\\', \\'affine\\'):\\n\\n        x_translation = slm_t.xopt\\n        x = np.zeros(6)\\n        x[:3] = x_translation\\n        if verbose:\\n            logger.info(\\' Rigid  (6 parameters) ...\\')\\n        slr_r = StreamlineLinearRegistration(metric=metric,\\n                                             x0=x,\\n                                             bounds=bounds[:6],\\n                                             method=method)\\n        slm_r = slr_r.optimize(static, moving)\\n\\n    if x0 in (\\'similarity\\', \\'scaling\\', \\'affine\\'):', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content=\"x_translation = slm_t.xopt\\n        x = np.zeros(6)\\n        x[:3] = x_translation\\n        if verbose:\\n            logger.info(' Rigid  (6 parameters) ...')\\n        slr_r = StreamlineLinearRegistration(metric=metric,\\n                                             x0=x,\\n                                             bounds=bounds[:6],\\n                                             method=method)\\n        slm_r = slr_r.optimize(static, moving)\\n\\n    if x0 in ('similarity', 'scaling', 'affine'):\\n\\n        x_rigid = slm_r.xopt\\n        x = np.zeros(7)\\n        x[:6] = x_rigid\\n        x[6] = 1.\\n        if verbose:\\n            logger.info(' Similarity (7 parameters) ...')\\n        slr_s = StreamlineLinearRegistration(metric=metric,\\n                                             x0=x,\\n                                             bounds=bounds[:7],\\n                                             method=method)\\n        slm_s = slr_s.optimize(static, moving)\\n\\n    if x0 in ('scaling', 'affine'):\\n\\n        x_similarity = slm_s.xopt\\n        x = np.zeros(9)\\n        x[:6] = x_similarity[:6]\\n        x[6:] = np.array((x_similarity[6],) * 3)\\n        if verbose:\\n            logger.info(' Scaling (9 parameters) ...')\\n\\n        slr_c = StreamlineLinearRegistration(metric=metric,\\n                                             x0=x,\\n                                             bounds=bounds[:9],\\n                                             method=method)\\n        slm_c = slr_c.optimize(static, moving)\\n\\n    if x0 == 'affine':\\n\\n        x_scaling = slm_c.xopt\\n        x = np.zeros(12)\\n        x[:9] = x_scaling[:9]\\n        x[9:] = np.zeros(3)\\n        if verbose:\\n            logger.info(' Affine (12 parameters) ...')\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='slr_c = StreamlineLinearRegistration(metric=metric,\\n                                             x0=x,\\n                                             bounds=bounds[:9],\\n                                             method=method)\\n        slm_c = slr_c.optimize(static, moving)\\n\\n    if x0 == \\'affine\\':\\n\\n        x_scaling = slm_c.xopt\\n        x = np.zeros(12)\\n        x[:9] = x_scaling[:9]\\n        x[9:] = np.zeros(3)\\n        if verbose:\\n            logger.info(\\' Affine (12 parameters) ...\\')\\n\\n        slr_a = StreamlineLinearRegistration(metric=metric,\\n                                             x0=x,\\n                                             bounds=bounds[:12],\\n                                             method=method)\\n        slm_a = slr_a.optimize(static, moving)\\n\\n    if x0 == \\'translation\\':\\n        slm = slm_t\\n    elif x0 == \\'rigid\\':\\n        slm = slm_r\\n    elif x0 == \\'similarity\\':\\n        slm = slm_s\\n    elif x0 == \\'scaling\\':\\n        slm = slm_c\\n    elif x0 == \\'affine\\':\\n        slm = slm_a\\n    else:\\n        raise ValueError(\\'Incorrect SLR transform\\')\\n\\n    return slm\\n\\n\\ndef slr_with_qbx(static, moving,\\n                 x0=\\'affine\\',\\n                 rm_small_clusters=50,\\n                 maxiter=100,\\n                 select_random=None,\\n                 verbose=False,\\n                 greater_than=50,\\n                 less_than=250,\\n                 qbx_thr=(40, 30, 20, 15),\\n                 nb_pts=20,\\n                 progressive=True, rng=None, num_threads=None):\\n    \"\"\" Utility function for registering large tractograms.\\n\\n    For efficiency, we apply the registration on cluster centroids and remove\\n    small clusters.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='For efficiency, we apply the registration on cluster centroids and remove\\n    small clusters.\\n\\n    Parameters\\n    ----------\\n    static : Streamlines\\n    moving : Streamlines\\n\\n    x0 : str, optional.\\n        rigid, similarity or affine transformation model (default affine)\\n\\n    rm_small_clusters : int, optional\\n        Remove clusters that have less than `rm_small_clusters`\\n\\n    maxiter : int, optional\\n        Maximum number of iterations to perform.\\n\\n    select_random : int, optional.\\n        If not, None selects a random number of streamlines to apply clustering\\n        Default None.\\n\\n    verbose : bool, optional\\n        If True, logs information about optimization. Default: False\\n\\n    greater_than : int, optional\\n            Keep streamlines that have length greater than\\n            this value (default 50)\\n\\n    less_than : int, optional\\n            Keep streamlines have length less than this value (default 250)\\n\\n    qbx_thr : variable int\\n            Thresholds for QuickBundlesX (default [40, 30, 20, 15])\\n\\n    nb_pts : int, optional\\n            Number of points for discretizing each streamline (default 20)\\n\\n    progressive : boolean, optional\\n            (default True)\\n\\n    rng : np.random.Generator\\n        If None creates random generator in function.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='less_than : int, optional\\n            Keep streamlines have length less than this value (default 250)\\n\\n    qbx_thr : variable int\\n            Thresholds for QuickBundlesX (default [40, 30, 20, 15])\\n\\n    nb_pts : int, optional\\n            Number of points for discretizing each streamline (default 20)\\n\\n    progressive : boolean, optional\\n            (default True)\\n\\n    rng : np.random.Generator\\n        If None creates random generator in function.\\n\\n    num_threads : int, optional\\n        Number of threads to be used for OpenMP parallelization. If None\\n        (default) the value of OMP_NUM_THREADS environment variable is used\\n        if it is set, otherwise all available threads are used. If < 0 the\\n        maximal number of threads minus |num_threads + 1| is used (enter -1 to\\n        use as many threads as possible). 0 raises an error. Only metrics\\n        using OpenMP will use this variable.\\n\\n    Notes\\n    -----\\n    The order of operations is the following. First short or long streamlines\\n    are removed. Second, the tractogram or a random selection of the tractogram\\n    is clustered with QuickBundles. Then SLR [Garyfallidis15]_ is applied.\\n\\n    References\\n    ----------\\n    .. [Garyfallidis15] Garyfallidis et al. \"Robust and efficient linear\\n    registration of white-matter fascicles in the space of streamlines\",\\n    NeuroImage, 117, 124--140, 2015\\n    .. [Garyfallidis14] Garyfallidis et al., \"Direct native-space fiber\\n            bundle alignment for group comparisons\", ISMRM, 2014.\\n    .. [Garyfallidis17] Garyfallidis et al. Recognition of white matter\\n    bundles using local and global streamline-based registration and\\n    clustering, Neuroimage, 2017.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='\"\"\"\\n    if rng is None:\\n        rng = np.random.default_rng()\\n\\n    if verbose:\\n        logger.info(\\'Static streamlines size {}\\'.format(len(static)))\\n        logger.info(\\'Moving streamlines size {}\\'.format(len(moving)))\\n\\n    def check_range(streamline, gt=greater_than, lt=less_than):\\n\\n        if (length(streamline) > gt) & (length(streamline) < lt):\\n            return True\\n        else:\\n            return False\\n\\n    streamlines1 = Streamlines(static[np.array([check_range(s)\\n                                                for s in static])])\\n    streamlines2 = Streamlines(moving[np.array([check_range(s)\\n                                                for s in moving])])\\n    if verbose:\\n        logger.info(\\'Static streamlines after length reduction {}\\'\\n                    .format(len(streamlines1)))\\n        logger.info(\\'Moving streamlines after length reduction {}\\'\\n                    .format(len(streamlines2)))\\n\\n    if select_random is not None:\\n        rstreamlines1 = select_random_set_of_streamlines(streamlines1,\\n                                                         select_random,\\n                                                         rng=rng)\\n    else:\\n        rstreamlines1 = streamlines1\\n\\n    rstreamlines1 = set_number_of_points(rstreamlines1, nb_pts)\\n\\n    rstreamlines1._data.astype(\\'f4\\')\\n\\n    cluster_map1 = qbx_and_merge(rstreamlines1, thresholds=qbx_thr, rng=rng)\\n    qb_centroids1 = remove_clusters_by_size(cluster_map1, rm_small_clusters)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='rstreamlines1 = set_number_of_points(rstreamlines1, nb_pts)\\n\\n    rstreamlines1._data.astype(\\'f4\\')\\n\\n    cluster_map1 = qbx_and_merge(rstreamlines1, thresholds=qbx_thr, rng=rng)\\n    qb_centroids1 = remove_clusters_by_size(cluster_map1, rm_small_clusters)\\n\\n    if select_random is not None:\\n        rstreamlines2 = select_random_set_of_streamlines(streamlines2,\\n                                                         select_random,\\n                                                         rng=rng)\\n    else:\\n        rstreamlines2 = streamlines2\\n\\n    rstreamlines2 = set_number_of_points(rstreamlines2, nb_pts)\\n    rstreamlines2._data.astype(\\'f4\\')\\n\\n    cluster_map2 = qbx_and_merge(rstreamlines2, thresholds=qbx_thr, rng=rng)\\n\\n    qb_centroids2 = remove_clusters_by_size(cluster_map2, rm_small_clusters)\\n\\n    if verbose:\\n        t = time()\\n\\n    if not len(qb_centroids1):\\n        msg = \"No cluster centroids found in Static Streamlines. Please \"\\n        msg += \"decrease  the value of rm_small_clusters.\"\\n        raise ValueError(msg)\\n    if not len(qb_centroids2):\\n        msg = \"No cluster centroids found in Moving Streamlines. Please \"\\n        msg += \"decrease the value of rm_small_clusters.\"\\n        raise ValueError(msg)\\n\\n    if not progressive:\\n        slr = StreamlineLinearRegistration(x0=x0,\\n                                           options={\\'maxiter\\': maxiter},\\n                                           num_threads=num_threads)\\n        slm = slr.optimize(qb_centroids1, qb_centroids2)\\n    else:\\n        bounds = DEFAULT_BOUNDS', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='if not progressive:\\n        slr = StreamlineLinearRegistration(x0=x0,\\n                                           options={\\'maxiter\\': maxiter},\\n                                           num_threads=num_threads)\\n        slm = slr.optimize(qb_centroids1, qb_centroids2)\\n    else:\\n        bounds = DEFAULT_BOUNDS\\n\\n        slm = progressive_slr(qb_centroids1, qb_centroids2,\\n                              x0=x0, metric=None,\\n                              bounds=bounds, num_threads=num_threads)\\n\\n    if verbose:\\n        logger.info(\\'QB static centroids size %d\\' % len(qb_centroids1,))\\n        logger.info(\\'QB moving centroids size %d\\' % len(qb_centroids2,))\\n        duration = time() - t\\n        logger.info(\\'SLR finished in  %0.3f seconds.\\' % (duration,))\\n        if slm.iterations is not None:\\n            logger.info(\\'SLR iterations: %d \\' % (slm.iterations,))\\n\\n    moved = slm.transform(moving)\\n\\n    return moved, slm.matrix, qb_centroids1, qb_centroids2\\n\\n\\n# In essence whole_brain_slr can be thought as a combination of\\n# SLR on QuickBundles centroids and some thresholding see\\n# Garyfallidis et al. Recognition of white matter\\n# bundles using local and global streamline-based registration and\\n# clustering, NeuroImage, 2017.\\nwhole_brain_slr = slr_with_qbx\\n\\n\\ndef groupwise_slr(bundles, x0=\\'affine\\', tol=0, max_iter=20, qbx_thr=[4],\\n                  nb_pts=20, select_random=10000, verbose=False, rng=None):\\n    \"\"\" Function to perform unbiased groupwise bundle registration.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='def groupwise_slr(bundles, x0=\\'affine\\', tol=0, max_iter=20, qbx_thr=[4],\\n                  nb_pts=20, select_random=10000, verbose=False, rng=None):\\n    \"\"\" Function to perform unbiased groupwise bundle registration.\\n\\n    All bundles are moved to the same space by iteratively applying halfway\\n    streamline linear registration in pairs. With each iteration, bundles get\\n    closer to each other until the procedure converges and there is no more\\n    improvement.\\n\\n    Parameters\\n    ----------\\n    bundles : list\\n        List with streamlines of the bundles to be registered.\\n\\n    x0 : str, optional\\n        rigid, similarity or affine transformation model. Default: affine.\\n\\n    tol : float, optional\\n        Tolerance value to be used to assume convergence. Default: 0.\\n\\n    max_iter : int, optional\\n        Maximum number of iterations. Depending on the number of bundles to be\\n        registered this may need to be larger. Default: 20.\\n\\n    qbx_thr : variable int, optional\\n        Thresholds for Quickbundles used for clustering streamlines and reduce\\n        computational time. If None, no clustering is performed. Higher values\\n        cluster streamlines into a smaller number of centroids. Default: [4].\\n\\n    nb_pts : int, optional\\n        Number of points for discretizing each streamline. Default: 20.\\n\\n    select_random : int, optional\\n        Maximum number of streamlines for each bundle. If None, all the\\n        streamlines are used. Default: 10000.\\n\\n    verbose : bool, optional\\n        If True, logs information. Default: False.\\n\\n    rng : np.random.Generator\\n        If None, creates random generator in function. Default: None.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='nb_pts : int, optional\\n        Number of points for discretizing each streamline. Default: 20.\\n\\n    select_random : int, optional\\n        Maximum number of streamlines for each bundle. If None, all the\\n        streamlines are used. Default: 10000.\\n\\n    verbose : bool, optional\\n        If True, logs information. Default: False.\\n\\n    rng : np.random.Generator\\n        If None, creates random generator in function. Default: None.\\n\\n    References\\n    ----------\\n    .. [Garyfallidis15] Garyfallidis et al. \"Robust and efficient linear\\n    registration of white-matter fascicles in the space of streamlines\",\\n    NeuroImage, 117, 124--140, 2015\\n    .. [Garyfallidis14] Garyfallidis et al., \"Direct native-space fiber\\n            bundle alignment for group comparisons\", ISMRM, 2014.\\n    .. [Garyfallidis17] Garyfallidis et al. Recognition of white matter\\n    bundles using local and global streamline-based registration and\\n    clustering, Neuroimage, 2017.\\n\\n    \"\"\"\\n    def group_distance(bundles, n_bundle):\\n        all_pairs = list(combinations(np.arange(n_bundle), 2))\\n        d = np.zeros(len(all_pairs))\\n        for i, ind in enumerate(all_pairs):\\n            mdf = distance_matrix_mdf(bundles[ind[0]], bundles[ind[1]])\\n            rows, cols = mdf.shape\\n            d[i] = 0.25 * (np.sum(np.min(mdf, axis=0)) / float(cols) +\\n                           np.sum(np.min(mdf, axis=1)) / float(rows)) ** 2\\n        return d\\n\\n    if rng is None:\\n        rng = np.random.default_rng()\\n\\n    metric = JointBundleMinDistanceMetric()\\n\\n    bundles = bundles.copy()\\n    n_bundle = len(bundles)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='if rng is None:\\n        rng = np.random.default_rng()\\n\\n    metric = JointBundleMinDistanceMetric()\\n\\n    bundles = bundles.copy()\\n    n_bundle = len(bundles)\\n\\n    if verbose:\\n        logging.info(\"Groupwise bundle registration running.\")\\n        logging.info(f\"Number of bundles found: {n_bundle}.\")\\n\\n    # Preprocess bundles: streamline selection, centering and clustering\\n    centroids = []\\n    aff_list = []\\n    for i in range(n_bundle):\\n        if verbose:\\n            logging.info(f\"Preprocessing: bundle {i}/{n_bundle}: \" +\\n                         f\"{len(bundles[i])} streamlines found.\")\\n\\n        if select_random is not None:\\n            bundles[i] = select_random_set_of_streamlines(bundles[i],\\n                                                          select_random, rng)\\n\\n        bundles[i] = set_number_of_points(bundles[i], nb_pts)\\n\\n        bundle, shift = center_streamlines(bundles[i])\\n        aff_list.append(compose_matrix44(-shift))\\n\\n        if qbx_thr is not None:\\n            cluster_map = qbx_and_merge(bundle, thresholds=qbx_thr, rng=rng)\\n            bundle = remove_clusters_by_size(cluster_map, 1)\\n\\n        centroids.append(bundle)\\n\\n    # Compute initial group distance (mean distance between all bundle pairs)\\n    d = group_distance(centroids, n_bundle)\\n\\n    if verbose:\\n        logging.info(f\"Initial group distance: {np.mean(d)}.\")\\n\\n    # Make pairs and start iterating\\n    pairs, excluded = get_unique_pairs(n_bundle)\\n    n_pair = n_bundle//2\\n\\n    for i_iter in range(1, max_iter+1):\\n        for i_pair, pair in enumerate(pairs):\\n            ind1 = pair[0]\\n            ind2 = pair[1]\\n\\n            centroids1 = centroids[ind1]\\n            centroids2 = centroids[ind2]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='if verbose:\\n        logging.info(f\"Initial group distance: {np.mean(d)}.\")\\n\\n    # Make pairs and start iterating\\n    pairs, excluded = get_unique_pairs(n_bundle)\\n    n_pair = n_bundle//2\\n\\n    for i_iter in range(1, max_iter+1):\\n        for i_pair, pair in enumerate(pairs):\\n            ind1 = pair[0]\\n            ind2 = pair[1]\\n\\n            centroids1 = centroids[ind1]\\n            centroids2 = centroids[ind2]\\n\\n            hslr = StreamlineLinearRegistration(x0=x0, metric=metric)\\n            hsrm = hslr.optimize(static=centroids1, moving=centroids2,\\n                                 mat=np.eye(4))\\n\\n            # Update transformation matrices\\n            aff_list[ind1] = np.dot(hsrm.matrix1, aff_list[ind1])\\n            aff_list[ind2] = np.dot(hsrm.matrix2, aff_list[ind2])\\n\\n            centroids1, centroids2 = hsrm.transform(centroids1, centroids2)\\n\\n            centroids[ind1] = centroids1\\n            centroids[ind2] = centroids2\\n\\n            if verbose:\\n                logging.info(f\"Iteration: {i_iter} pair: {i_pair+1}/{n_pair}.\")\\n\\n        d = np.vstack((d, group_distance(centroids, n_bundle)))\\n\\n        # Use as reference the distance 3 iterations ago\\n        prev_iter = np.max([0, i_iter-3])\\n        d_improve = np.mean(d[prev_iter, :]) - np.mean(d[i_iter, :])\\n\\n        if verbose:\\n            logging.info(f\"Iteration {i_iter} group distance: \" +\\n                         f\"{np.mean(d[i_iter, :])}\")\\n            logging.info(f\"Iteration {i_iter} improvement previous 3: \" +\\n                         f\"{d_improve}\")', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='d = np.vstack((d, group_distance(centroids, n_bundle)))\\n\\n        # Use as reference the distance 3 iterations ago\\n        prev_iter = np.max([0, i_iter-3])\\n        d_improve = np.mean(d[prev_iter, :]) - np.mean(d[i_iter, :])\\n\\n        if verbose:\\n            logging.info(f\"Iteration {i_iter} group distance: \" +\\n                         f\"{np.mean(d[i_iter, :])}\")\\n            logging.info(f\"Iteration {i_iter} improvement previous 3: \" +\\n                         f\"{d_improve}\")\\n\\n        if d_improve < tol:\\n            if verbose:\\n                logging.info(\"Registration converged \" +\\n                             f\"{d_improve} < {tol}\")\\n            break\\n\\n        pairs, excluded = get_unique_pairs(n_bundle, pairs)\\n\\n    # Move bundles just once at the end\\n    for i, aff in enumerate(aff_list):\\n        bundles[i] = transform_streamlines(bundles[i], aff)\\n\\n    return bundles, aff_list, d\\n\\n\\ndef get_unique_pairs(n_bundle, pairs=None):\\n    \"\"\" Make unique pairs from n_bundle bundles.\\n\\n    The function allows to input a previous pairs assignment so that the new\\n    pairs are different.\\n\\n    Parameters\\n    ----------\\n    n_bundle : int\\n        Number of bundles to be matched in pairs.\\n\\n    pairs : array, optional\\n        array containing the indexes of previous pairs.\\n    \"\"\"\\n    if not isinstance(n_bundle, int):\\n        raise TypeError(f\"n_bundle must be an int but is a {type(n_bundle)}\")\\n\\n    if n_bundle <= 1:\\n        raise ValueError(f\"n_bundle must be > 1 but is {n_bundle}\")\\n\\n    # Generate indexes\\n    index = np.arange(n_bundle)\\n    n_pair = n_bundle // 2', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='Parameters\\n    ----------\\n    n_bundle : int\\n        Number of bundles to be matched in pairs.\\n\\n    pairs : array, optional\\n        array containing the indexes of previous pairs.\\n    \"\"\"\\n    if not isinstance(n_bundle, int):\\n        raise TypeError(f\"n_bundle must be an int but is a {type(n_bundle)}\")\\n\\n    if n_bundle <= 1:\\n        raise ValueError(f\"n_bundle must be > 1 but is {n_bundle}\")\\n\\n    # Generate indexes\\n    index = np.arange(n_bundle)\\n    n_pair = n_bundle // 2\\n\\n    # If n_bundle is odd, we exclude one ensuring it wasn\\'t previously excluded\\n    excluded = None\\n    if np.mod(n_bundle, 2) == 1:\\n        if pairs is None:\\n            excluded = np.random.choice(index)\\n        else:\\n            excluded = np.random.choice(np.unique(pairs))\\n\\n        index = index[index != excluded]\\n\\n    # Shuffle indexes\\n    index = np.random.permutation(index)\\n    new_pairs = index.reshape((n_pair, 2))\\n\\n    if pairs is None or n_bundle <= 3:\\n        return new_pairs, excluded\\n\\n    # Repeat the shuffle process until we find new unique pairs\\n    all_pairs = np.vstack((new_pairs, new_pairs[:, ::-1],\\n                           pairs, pairs[:, ::-1]))\\n\\n    while len(np.unique(all_pairs, axis=0)) < 4*n_pair:\\n        index = np.random.permutation(index)\\n        new_pairs = index.reshape((n_pair, 2))\\n        all_pairs = np.vstack((new_pairs, new_pairs[:, ::-1],\\n                               pairs, pairs[:, ::-1]))\\n\\n    return new_pairs, excluded\\n\\n\\ndef _threshold(x, th):\\n    return np.maximum(np.minimum(x, th), -th)\\n\\n\\ndef compose_matrix44(t, dtype=np.double):\\n    \"\"\" Compose a 4x4 transformation matrix.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='while len(np.unique(all_pairs, axis=0)) < 4*n_pair:\\n        index = np.random.permutation(index)\\n        new_pairs = index.reshape((n_pair, 2))\\n        all_pairs = np.vstack((new_pairs, new_pairs[:, ::-1],\\n                               pairs, pairs[:, ::-1]))\\n\\n    return new_pairs, excluded\\n\\n\\ndef _threshold(x, th):\\n    return np.maximum(np.minimum(x, th), -th)\\n\\n\\ndef compose_matrix44(t, dtype=np.double):\\n    \"\"\" Compose a 4x4 transformation matrix.\\n\\n    Parameters\\n    ----------\\n    t : ndarray\\n        This is a 1D vector of affine transformation parameters with\\n        size at least 3.\\n        If the size is 3, t is interpreted as translation.\\n        If the size is 6, t is interpreted as translation + rotation.\\n        If the size is 7, t is interpreted as translation + rotation +\\n        isotropic scaling.\\n        If the size is 9, t is interpreted as translation + rotation +\\n        anisotropic scaling.\\n        If size is 12, t is interpreted as translation + rotation +\\n        scaling + shearing.\\n\\n    Returns\\n    -------\\n    T : ndarray\\n        Homogeneous transformation matrix of size 4x4.\\n\\n    \"\"\"\\n    if isinstance(t, list):\\n        t = np.array(t)\\n    size = t.size\\n\\n    if size not in [3, 6, 7, 9, 12]:\\n        raise ValueError(\\'Accepted number of parameters is 3, 6, 7, 9 and 12\\')', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    T : ndarray\\n        Homogeneous transformation matrix of size 4x4.\\n\\n    \"\"\"\\n    if isinstance(t, list):\\n        t = np.array(t)\\n    size = t.size\\n\\n    if size not in [3, 6, 7, 9, 12]:\\n        raise ValueError(\\'Accepted number of parameters is 3, 6, 7, 9 and 12\\')\\n\\n    MAX_DIST = 1e10\\n    scale, shear, angles, translate = (None, ) * 4\\n    translate = _threshold(t[0:3], MAX_DIST)\\n    if size in [6, 7, 9, 12]:\\n        angles = np.deg2rad(t[3:6])\\n    if size == 7:\\n        scale = np.array((t[6],) * 3)\\n    if size in [9, 12]:\\n        scale = t[6: 9]\\n    if size == 12:\\n        shear = t[9: 12]\\n    return compose_matrix(scale=scale, shear=shear,\\n                          angles=angles,\\n                          translate=translate)\\n\\n\\ndef decompose_matrix44(mat, size=12):\\n    \"\"\" Given a 4x4 homogeneous matrix return the parameter vector.\\n\\n    Parameters\\n    ----------\\n    mat : array\\n        Homogeneous 4x4 transformation matrix\\n    size : int\\n        Size of the output vector. 3, for translation, 6 for rigid,\\n        7 for similarity, 9 for scaling and 12 for affine. Default is 12.\\n\\n    Returns\\n    -------\\n    t : ndarray\\n        One dimensional ndarray of 3, 6, 7, 9 or 12 affine parameters.\\n\\n    \"\"\"\\n    scale, shear, angles, translate, _ = decompose_matrix(mat)\\n\\n    t = np.zeros(12)\\n    t[:3] = translate\\n    if size == 3:\\n        return t[:3]\\n    t[3: 6] = np.rad2deg(angles)\\n    if size == 6:\\n        return t[:6]\\n    if size == 7:\\n        t[6] = np.mean(scale)\\n        return t[:7]\\n    if size == 9:\\n        t[6:9] = scale\\n        return t[:9]\\n    if size == 12:\\n        t[6: 9] = scale\\n        t[9: 12] = shear\\n        return t', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='\"\"\"\\n    scale, shear, angles, translate, _ = decompose_matrix(mat)\\n\\n    t = np.zeros(12)\\n    t[:3] = translate\\n    if size == 3:\\n        return t[:3]\\n    t[3: 6] = np.rad2deg(angles)\\n    if size == 6:\\n        return t[:6]\\n    if size == 7:\\n        t[6] = np.mean(scale)\\n        return t[:7]\\n    if size == 9:\\n        t[6:9] = scale\\n        return t[:9]\\n    if size == 12:\\n        t[6: 9] = scale\\n        t[9: 12] = shear\\n        return t\\n\\n    raise ValueError(\\'Size can be 3, 6, 7, 9 or 12\\')', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamlinear.py.txt'}),\n",
       " Document(page_content='import warnings\\n\\nimport numpy as np\\nfrom scipy.optimize import linear_sum_assignment\\n\\nfrom dipy.align.streamlinear import slr_with_qbx\\nfrom dipy.align.cpd import DeformableRegistration\\nfrom dipy.tracking.streamline import (unlist_streamlines,\\n                                      Streamlines)\\nfrom dipy.stats.analysis import assignment_map\\nfrom dipy.tracking.streamline import length\\nfrom dipy.align.bundlemin import distance_matrix_mdf\\nfrom dipy.viz.plotting import bundle_shape_profile\\nfrom dipy.segment.clustering import QuickBundles\\nfrom dipy.segment.metricspeed import AveragePointwiseEuclideanMetric\\n\\n\\ndef average_bundle_length(bundle):\\n    \"\"\"Find average Euclidean length of the bundle in mm.\\n\\n    Parameters\\n    ----------\\n    bundle : Streamlines\\n        Bundle who\\'s average length is to be calculated.\\n\\n    Returns\\n    -------\\n    int\\n        Average Euclidean length of bundle in mm.\\n\\n    \"\"\"\\n    metric = AveragePointwiseEuclideanMetric()\\n    qb = QuickBundles(threshold=85., metric=metric)\\n    clusters = qb.cluster(bundle)\\n    centroids = Streamlines(clusters.centroids)\\n\\n    return length(centroids)[0]\\n\\n\\ndef find_missing(lst, cb):\\n    \"\"\"Find unmatched streamline indices in moving bundle.\\n\\n    Parameters\\n    ----------\\n    lst : List\\n        List of integers containing all the streamlines indices in moving\\n        bundle.\\n    cb : List\\n        List of integers containing streamline indices of the moving bundle\\n        that were not matched to any streamline in static bundle.\\n\\n    Returns\\n    -------\\n    list\\n        List containing unmatched streamlines from moving bundle\\n\\n    \"\"\"\\n    return [x for x in range(0, len(cb)) if x not in lst]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamwarp.py.txt'}),\n",
       " Document(page_content='Parameters\\n    ----------\\n    lst : List\\n        List of integers containing all the streamlines indices in moving\\n        bundle.\\n    cb : List\\n        List of integers containing streamline indices of the moving bundle\\n        that were not matched to any streamline in static bundle.\\n\\n    Returns\\n    -------\\n    list\\n        List containing unmatched streamlines from moving bundle\\n\\n    \"\"\"\\n    return [x for x in range(0, len(cb)) if x not in lst]\\n\\n\\ndef bundlewarp(static, moving, dist=None, alpha=0.3, beta=20, max_iter=15,\\n               affine=True):\\n    \"\"\"Register two bundles using nonlinear method.\\n\\n    Parameters\\n    ----------\\n    static : Streamlines\\n        Reference/fixed bundle\\n\\n    moving : Streamlines\\n        Target bundle that will be moved/registered to match the static bundle\\n\\n    dist : float, optional.\\n        Precomputed distance matrix (default None)\\n\\n    alpha : float, optional\\n        Represents the trade-off between regularizing the deformation and\\n        having points match very closely. Lower value of alpha means high\\n        deformations (default 0.3)\\n\\n    beta : int, optional\\n        Represents the strength of the interaction between points\\n        Gaussian kernel size (default 20)\\n\\n    max_iter : int, optional\\n        Maximum number of iterations for deformation process in ml-CPD method\\n        (default 15)\\n\\n    affine : boolean, optional\\n        If False, use rigid registration as starting point (default True)\\n\\n    Returns\\n    -------\\n    deformed_bundle : Streamlines\\n        Nonlinearly moved bundle (warped bundle)\\n\\n    moving_aligned : Streamlines\\n        Linearly moved bundle (affinely moved)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamwarp.py.txt'}),\n",
       " Document(page_content='max_iter : int, optional\\n        Maximum number of iterations for deformation process in ml-CPD method\\n        (default 15)\\n\\n    affine : boolean, optional\\n        If False, use rigid registration as starting point (default True)\\n\\n    Returns\\n    -------\\n    deformed_bundle : Streamlines\\n        Nonlinearly moved bundle (warped bundle)\\n\\n    moving_aligned : Streamlines\\n        Linearly moved bundle (affinely moved)\\n\\n    dist : np.ndarray\\n        Float array containing distance between moving and static bundle\\n\\n    matched_pairs : np.ndarray\\n        Int array containing streamline correspondences between two bundles\\n\\n    warp : np.ndarray\\n        Nonlinear warp map generated by BundleWarp\\n\\n    References\\n    ----------\\n    .. [Chandio2023] Chandio et al. \"BundleWarp, streamline-based nonlinear\\n            registration of white matter tracts.\" bioRxiv (2023): 2023-01.\\n\\n    \"\"\"\\n    if alpha <= 0.01:\\n        warnings.warn(\"Using alpha<=0.01 will result in extreme deformations\")\\n\\n    if average_bundle_length(static) <= 50:\\n        beta = 10\\n\\n    x0 = \\'affine\\' if affine else \\'rigid\\'\\n    moving_aligned, _, _, _ = slr_with_qbx(static, moving, x0=x0,\\n                                           rm_small_clusters=0)\\n\\n    if dist is not None:\\n        print(\"using pre-computed distances\")\\n    else:\\n        dist = distance_matrix_mdf(static, moving_aligned).T\\n\\n    matched_pairs = np.zeros((len(moving), 2))\\n    matched_pairs1 = np.asarray(linear_sum_assignment(dist)).T\\n\\n    for mt in matched_pairs1:\\n        matched_pairs[mt[0]] = mt\\n\\n    num = len(matched_pairs1)\\n\\n    all_pairs = list(matched_pairs1[:, 0])\\n    all_matched = False\\n\\n    while all_matched is False:\\n\\n        num = len(all_pairs)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamwarp.py.txt'}),\n",
       " Document(page_content='if dist is not None:\\n        print(\"using pre-computed distances\")\\n    else:\\n        dist = distance_matrix_mdf(static, moving_aligned).T\\n\\n    matched_pairs = np.zeros((len(moving), 2))\\n    matched_pairs1 = np.asarray(linear_sum_assignment(dist)).T\\n\\n    for mt in matched_pairs1:\\n        matched_pairs[mt[0]] = mt\\n\\n    num = len(matched_pairs1)\\n\\n    all_pairs = list(matched_pairs1[:, 0])\\n    all_matched = False\\n\\n    while all_matched is False:\\n\\n        num = len(all_pairs)\\n\\n        if num < len(moving):\\n\\n            ml = find_missing(all_pairs, moving)\\n\\n            dist2 = dist[:][ml]\\n\\n            # dist2 has distance among unmatched streamlines of moving bundle\\n            # and all static bundle\\'s streamlines\\n\\n            matched_pairs2 = np.asarray(linear_sum_assignment(dist2)).T\\n\\n            for i in range(matched_pairs2.shape[0]):\\n                matched_pairs2[i][0] = ml[matched_pairs2[i][0]]\\n\\n            for mt in matched_pairs2:\\n                matched_pairs[mt[0]] = mt\\n\\n            all_pairs.extend(matched_pairs2[:, 0])\\n\\n            num2 = num + len(matched_pairs2)\\n            if num2 == len(moving):\\n                all_matched = True\\n                num = num2\\n        else:\\n            all_matched = True\\n\\n    deformed_bundle = Streamlines([])\\n    warp = []\\n\\n    # Iterate over each pair of streamlines and deform them\\n    # Append deformed streamlines in deformed_bundle\\n\\n    for _, pairs in enumerate(matched_pairs):\\n\\n        s1 = static[int(pairs[1])]\\n        s2 = moving_aligned[int(pairs[0])]\\n\\n        static_s = s1\\n        moving_s = s2', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamwarp.py.txt'}),\n",
       " Document(page_content='deformed_bundle = Streamlines([])\\n    warp = []\\n\\n    # Iterate over each pair of streamlines and deform them\\n    # Append deformed streamlines in deformed_bundle\\n\\n    for _, pairs in enumerate(matched_pairs):\\n\\n        s1 = static[int(pairs[1])]\\n        s2 = moving_aligned[int(pairs[0])]\\n\\n        static_s = s1\\n        moving_s = s2\\n\\n        reg = DeformableRegistration(X=static_s, Y=moving_s, alpha=alpha,\\n                                     beta=beta, max_iterations=max_iter)\\n        ty, pr = reg.register()\\n        ty = ty.astype(float)\\n        deformed_bundle.append(ty)\\n        warp.append(pr)\\n\\n    # Returns deformed bundle, affinely moved bundle, distance matrix,\\n    # streamline correspondences, and warp field\\n    return deformed_bundle, moving_aligned, dist, matched_pairs, warp\\n\\n\\ndef bundlewarp_vector_filed(moving_aligned, deformed_bundle):\\n    \"\"\"Calculate vector fields.\\n\\n    Vector field computation as the difference between each streamline point\\n    in the deformed and linearly aligned bundles\\n\\n    Parameters\\n    ----------\\n    moving_aligned : Streamlines\\n        Linearly (affinely) moved bundle\\n    deformed_bundle : Streamlines\\n        Nonlinearly (warped) bundle\\n\\n    Returns\\n    -------\\n    offsets : List\\n        Vector field modules\\n    directions : List\\n        Unitary vector directions\\n    colors : List\\n    \"\"\"\\n    points_aligned, _ = unlist_streamlines(moving_aligned)\\n    points_deformed, _ = unlist_streamlines(deformed_bundle)\\n    vector_field = points_deformed - points_aligned\\n\\n    offsets = np.sqrt(np.sum((vector_field)**2, 1))  # vector field modules\\n\\n    # Normalize vectors to be unitary (directions)\\n    directions = vector_field / np.array([offsets]).T', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamwarp.py.txt'}),\n",
       " Document(page_content='offsets = np.sqrt(np.sum((vector_field)**2, 1))  # vector field modules\\n\\n    # Normalize vectors to be unitary (directions)\\n    directions = vector_field / np.array([offsets]).T\\n\\n    # Define colors mapping the direction vectors to RGB.\\n    # Absolute value generates DTI-like colors\\n    colors = directions\\n\\n    return offsets, directions, colors\\n\\n\\ndef bundlewarp_shape_analysis(moving_aligned, deformed_bundle, no_disks=10,\\n                              plotting=False):\\n    \"\"\"Calculate bundle shape difference profile.\\n\\n    Bundle shape difference analysis using magnitude from BundleWarp\\n    displacements and BUAN.\\n\\n    Depending on the number of points of a streamline, and the number of\\n    segments requested, multiple points may be considered for the computation\\n    of a given segment; a segment may contain information from a single point;\\n    or some segments may not contain information from any points. In the latter\\n    case, the segment will contain an ``np.nan`` value. The point-to-segment\\n    mapping is defined by the :func:`assignment_map`: for each segment index,\\n    the point information of the matching index positions, as returned by\\n    :func:`assignment_map`, are considered for the computation.\\n\\n    Parameters\\n    ----------\\n    moving_aligned : Streamlines\\n        Linearly (affinely) moved bundle\\n    deformed_bundle : Streamlines\\n        Nonlinearly (warped) moved bundle\\n    no_disks : int, optional\\n        Number of segments to be created along the length of the bundle\\n    plotting : Boolean, optional\\n        Plot bundle shape profile', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamwarp.py.txt'}),\n",
       " Document(page_content='Parameters\\n    ----------\\n    moving_aligned : Streamlines\\n        Linearly (affinely) moved bundle\\n    deformed_bundle : Streamlines\\n        Nonlinearly (warped) moved bundle\\n    no_disks : int, optional\\n        Number of segments to be created along the length of the bundle\\n    plotting : Boolean, optional\\n        Plot bundle shape profile\\n\\n    Returns\\n    -------\\n    shape_profile : np.ndarray\\n        Float array containing bundlewarp displacement magnitudes along the\\n        length of the bundle\\n    stdv : np.ndarray\\n        Float array containing standard deviations\\n    \"\"\"\\n    n = no_disks\\n    offsets, directions, colors = bundlewarp_vector_filed(moving_aligned,\\n                                                          deformed_bundle)\\n\\n    indx = assignment_map(deformed_bundle, deformed_bundle, n)\\n    indx = np.array(indx)\\n\\n    rng = np.random.default_rng()\\n\\n    colors = rng.random((n, 3))\\n\\n    disks_color = []\\n    for _, ind in enumerate(indx):\\n\\n        disks_color.append(tuple(colors[ind]))\\n\\n    x = np.array(range(1, n+1))\\n    shape_profile = np.zeros(n)\\n    stdv = np.zeros(n)\\n\\n    for i in range(n):\\n\\n        mask = indx == i\\n        if sum(mask):\\n            shape_profile[i] = np.mean(offsets[mask])\\n            stdv[i] = np.std(offsets[mask])\\n        else:\\n            shape_profile[i] = np.nan\\n            stdv[i] = np.nan\\n\\n    if plotting:\\n        bundle_shape_profile(x, shape_profile, stdv)\\n\\n    return shape_profile, stdv', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\streamwarp.py.txt'}),\n",
       " Document(page_content='\"\"\" Utility functions used by the Sum of Squared Differences (SSD) metric \"\"\"\\n\\nimport numpy as np\\ncimport cython\\ncimport numpy as cnp\\nfrom dipy.align.fused_types cimport floating\\ncdef extern from \"dpy_math.h\" nogil:\\n    int dpy_isinf(double)\\n    double sqrt(double)\\n\\n\\n@cython.boundscheck(False)\\n@cython.wraparound(False)\\n@cython.cdivision(True)\\ncdef void _solve_2d_symmetric_positive_definite(double* A, double* y,\\n                                                double det,\\n                                                double* out) noexcept nogil:\\n    r\"\"\"Solves a 2-variable symmetric positive-definite linear system\\n\\n    The C implementation of the public-facing Python function\\n    ``solve_2d_symmetric_positive_definite``.\\n\\n    Solves the symmetric positive-definite linear system $Mx = y$ given by::\\n\\n        M = [[A[0], A[1]],\\n             [A[1], A[2]]]\\n\\n    Parameters\\n    ----------\\n    A : array, shape (3,)\\n        the array containing the entries of the symmetric 2x2 matrix\\n    y : array, shape (2,)\\n        right-hand side of the system to be solved\\n    out : array, shape (2,)\\n        the array the output will be stored in\\n    \"\"\"\\n    out[1] = (A[0] * y[1] - A[1] * y[0]) / det\\n    out[0] = (y[0] - A[1] * out[1]) / A[0]\\n\\n\\ndef solve_2d_symmetric_positive_definite(A, y, double det):\\n    r\"\"\"Solves a 2-variable symmetric positive-definite linear system\\n\\n    Solves the symmetric positive-definite linear system $Mx = y$ given by::\\n\\n        M = [[A[0], A[1]],\\n             [A[1], A[2]]]\\n\\n    Parameters\\n    ----------\\n    A : array, shape (3,)\\n        the array containing the entries of the symmetric 2x2 matrix\\n    y : array, shape (2,)\\n        right-hand side of the system to be solved', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\sumsqdiff.pyx.txt'}),\n",
       " Document(page_content='def solve_2d_symmetric_positive_definite(A, y, double det):\\n    r\"\"\"Solves a 2-variable symmetric positive-definite linear system\\n\\n    Solves the symmetric positive-definite linear system $Mx = y$ given by::\\n\\n        M = [[A[0], A[1]],\\n             [A[1], A[2]]]\\n\\n    Parameters\\n    ----------\\n    A : array, shape (3,)\\n        the array containing the entries of the symmetric 2x2 matrix\\n    y : array, shape (2,)\\n        right-hand side of the system to be solved\\n\\n    Returns\\n    -------\\n    out : array, shape (2,)\\n        the array the output will be stored in\\n    \"\"\"\\n    cdef:\\n        cnp.ndarray out = np.zeros(2, dtype=float)\\n\\n    _solve_2d_symmetric_positive_definite(\\n        <double*> cnp.PyArray_DATA(np.ascontiguousarray(A, float)),\\n        <double*> cnp.PyArray_DATA(np.ascontiguousarray(y, float)),\\n        det,\\n        <double*> cnp.PyArray_DATA(out))\\n    return np.asarray(out)\\n\\n\\n@cython.boundscheck(False)\\n@cython.wraparound(False)\\n@cython.cdivision(True)\\ncdef int _solve_3d_symmetric_positive_definite(double* g,\\n                                               double* y,\\n                                               double tau,\\n                                               double* out) nogil:\\n    r\"\"\"Solves a 3-variable symmetric positive-definite linear system\\n\\n    Solves the symmetric semi-positive-definite linear system $Mx = y$ given by\\n    $M = (g g^{T} + \\\\tau I)$\\n\\n    The C implementation of the public-facing Python function\\n    ``solve_3d_symmetric_positive_definite``.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\sumsqdiff.pyx.txt'}),\n",
       " Document(page_content='Solves the symmetric semi-positive-definite linear system $Mx = y$ given by\\n    $M = (g g^{T} + \\\\tau I)$\\n\\n    The C implementation of the public-facing Python function\\n    ``solve_3d_symmetric_positive_definite``.\\n\\n    Parameters\\n    ----------\\n    g : array, shape (3,)\\n        the vector in the outer product above\\n    y : array, shape (3,)\\n        right-hand side of the system to be solved\\n    tau : double\\n        $\\\\tau$ in $M = (g g^{T} + \\\\tau I)$\\n    out : array, shape (3,)\\n        the array the output will be stored in\\n\\n    Returns\\n    -------\\n    is_singular : int\\n        1 if M is singular, otherwise 0\\n    \"\"\"\\n    cdef:\\n        double a,b,c,d,e,f, y0, y1, y2, sub_det\\n    a = g[0] ** 2 + tau\\n    if a < 1e-9:\\n        return 1\\n    b = g[0] * g[1]\\n    sub_det = (a * (g[1] ** 2 + tau) - b * b)\\n    if sub_det < 1e-9:\\n        return 1\\n    c = g[0] * g[2]\\n    d = (a * (g[1] ** 2 + tau) - b * b) / a\\n    e = (a * (g[1] * g[2]) - b * c) / a\\n    f = (a * (g[2] ** 2 + tau) - c * c) / a - (e * e * a) / sub_det\\n    if f < 1e-9:\\n        return 1\\n    y0 = y[0]\\n    y1 = (y[1] * a - y0 * b) / a\\n    y2 = (y[2] * a - c * y0) / a - (e * (y[1] * a - b * y0)) / sub_det\\n    out[2] = y2 / f\\n    out[1] = (y1 - e * out[2]) / d\\n    out[0] = (y0 - b * out[1] - c * out[2]) / a\\n    return 0\\n\\n\\ndef solve_3d_symmetric_positive_definite(g, y, double tau):\\n    r\"\"\"Solves a 3-variable symmetric positive-definite linear system\\n\\n    Solves the symmetric semi-positive-definite linear system $Mx = y$ given by\\n    $M = (g g^{T} + \\\\tau I)$.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\sumsqdiff.pyx.txt'}),\n",
       " Document(page_content='def solve_3d_symmetric_positive_definite(g, y, double tau):\\n    r\"\"\"Solves a 3-variable symmetric positive-definite linear system\\n\\n    Solves the symmetric semi-positive-definite linear system $Mx = y$ given by\\n    $M = (g g^{T} + \\\\tau I)$.\\n\\n    Parameters\\n    ----------\\n    g : array, shape (3,)\\n        the vector in the outer product above\\n    y : array, shape (3,)\\n        right-hand side of the system to be solved\\n    tau : double\\n        $\\\\tau$ in $M = (g g^{T} + \\\\tau I)$\\n\\n    Returns\\n    -------\\n    out : array, shape (3,)\\n        the array the output will be stored in\\n    is_singular : int\\n        1 if M is singular, otherwise 0\\n    \"\"\"\\n    cdef:\\n        cnp.ndarray out = np.zeros(3, dtype=float)\\n        int is_singular\\n    is_singular = _solve_3d_symmetric_positive_definite(\\n        <double*> cnp.PyArray_DATA(np.ascontiguousarray(g, float)),\\n        <double*> cnp.PyArray_DATA(np.ascontiguousarray(y, float)),\\n        tau,\\n        <double*> cnp.PyArray_DATA(out))\\n    return np.asarray(out), is_singular\\n\\n\\n@cython.boundscheck(False)\\n@cython.wraparound(False)\\n@cython.cdivision(True)\\ncpdef double iterate_residual_displacement_field_ssd_2d(\\n                floating[:, :] delta_field, floating[:, :] sigmasq_field,\\n                floating[:, :, :] grad, floating[:, :, :] target,\\n                double lambda_param, floating[:, :, :] displacement_field):\\n    r\"\"\"One iteration of a large linear system solver for 2D SSD registration\\n\\n    Performs one iteration at one level of the Multi-resolution Gauss-Seidel\\n    solver proposed by Bruhn and Weickert [Bruhn05].', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\sumsqdiff.pyx.txt'}),\n",
       " Document(page_content=\"Performs one iteration at one level of the Multi-resolution Gauss-Seidel\\n    solver proposed by Bruhn and Weickert [Bruhn05].\\n\\n    Parameters\\n    ----------\\n    delta_field : array, shape (R, C)\\n        the difference between the static and moving image (the 'derivative\\n        w.r.t. time' in the optical flow model)\\n    sigmasq_field : array, shape (R, C)\\n        the variance of the gray level value at each voxel, according to the\\n        EM model (for SSD, it is 1 for all voxels). Inf and 0 values\\n        are processed specially to support infinite and zero variance.\\n    grad : array, shape (R, C, 2)\\n        the gradient of the moving image\\n    target : array, shape (R, C, 2)\\n        right-hand side of the linear system to be solved in the Weickert's\\n        multi-resolution algorithm\\n    lambda_param : float\\n        smoothness parameter of the objective function\\n    displacement_field : array, shape (R, C, 2)\\n        current displacement field to start the iteration from\\n\\n    Returns\\n    -------\\n    max_displacement : float\\n        the norm of the maximum change in the displacement field after the\\n        iteration\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\sumsqdiff.pyx.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    max_displacement : float\\n        the norm of the maximum change in the displacement field after the\\n        iteration\\n\\n    References\\n    ----------\\n    [Bruhn05] Andres Bruhn and Joachim Weickert, \"Towards ultimate motion\\n              estimation: combining highest accuracy with real-time\\n              performance\", 10th IEEE International Conference on Computer\\n              Vision, 2005. ICCV 2005.\\n    \"\"\"\\n    ftype = np.asarray(delta_field).dtype\\n    cdef:\\n        int NUM_NEIGHBORS = 4\\n        int* dRow = [-1, 0, 1,  0]\\n        int* dCol = [0, 1, 0, -1]\\n        cnp.npy_intp nrows = delta_field.shape[0]\\n        cnp.npy_intp ncols = delta_field.shape[1]\\n        cnp.npy_intp r, c, dr, dc, nn, k\\n\\n        double* b = [0, 0]\\n        double* d = [0, 0]\\n        double* y = [0, 0]\\n        double* A = [0, 0, 0]\\n\\n        double xx, yy, opt, nrm2, delta, sigmasq, max_displacement, det\\n\\n    max_displacement = 0\\n\\n    with nogil:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\sumsqdiff.pyx.txt'}),\n",
       " Document(page_content='for r in range(nrows):\\n            for c in range(ncols):\\n                delta = delta_field[r, c]\\n                sigmasq = sigmasq_field[r, c] if sigmasq_field is not None else 1\\n                if target is None:\\n                    b[0] = delta_field[r, c] * grad[r, c, 0]\\n                    b[1] = delta_field[r, c] * grad[r, c, 1]\\n                else:\\n                    b[0] = target[r, c, 0]\\n                    b[1] = target[r, c, 1]\\n                nn = 0\\n                y[0] = 0\\n                y[1] = 0\\n                for k in range(NUM_NEIGHBORS):\\n                    dr = r + dRow[k]\\n                    if dr < 0 or dr >= nrows:\\n                        continue\\n                    dc = c + dCol[k]\\n                    if dc < 0 or dc >= ncols:\\n                        continue\\n                    nn += 1\\n                    y[0] += displacement_field[dr, dc, 0]\\n                    y[1] += displacement_field[dr, dc, 1]\\n                if dpy_isinf(sigmasq) != 0:\\n                    xx = displacement_field[r, c, 0]\\n                    yy = displacement_field[r, c, 1]\\n                    displacement_field[r, c, 0] = y[0] / nn\\n                    displacement_field[r, c, 1] = y[1] / nn\\n                    xx -= displacement_field[r, c, 0]\\n                    yy -= displacement_field[r, c, 1]\\n                    opt = xx * xx + yy * yy\\n                    if max_displacement < opt:\\n                        max_displacement = opt\\n                else:\\n                    A[0] = grad[r, c, 0] ** 2 + sigmasq * lambda_param * nn\\n                    A[1] = grad[r, c, 0] * grad[r, c, 1]\\n                    A[2] = grad[r, c, 1] ** 2 + sigmasq * lambda_param * nn', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\sumsqdiff.pyx.txt'}),\n",
       " Document(page_content='xx -= displacement_field[r, c, 0]\\n                    yy -= displacement_field[r, c, 1]\\n                    opt = xx * xx + yy * yy\\n                    if max_displacement < opt:\\n                        max_displacement = opt\\n                else:\\n                    A[0] = grad[r, c, 0] ** 2 + sigmasq * lambda_param * nn\\n                    A[1] = grad[r, c, 0] * grad[r, c, 1]\\n                    A[2] = grad[r, c, 1] ** 2 + sigmasq * lambda_param * nn\\n                    det = A[0] * A[2] - A[1] * A[1]\\n                    if det < 1e-9:\\n                        nrm2 = (grad[r, c, 0] ** 2 +\\n                                grad[r, c, 1] ** 2)\\n                        if nrm2 < 1e-9:\\n                            displacement_field[r, c, 0] = 0\\n                            displacement_field[r, c, 1] = 0\\n                        else:\\n                            displacement_field[r, c, 0] = (b[0]) / nrm2\\n                            displacement_field[r, c, 1] = (b[1]) / nrm2\\n                    else:\\n                        y[0] = b[0] + sigmasq * lambda_param * y[0]\\n                        y[1] = b[1] + sigmasq * lambda_param * y[1]\\n                        _solve_2d_symmetric_positive_definite(A, y, det, d)\\n                        xx = displacement_field[r, c, 0] - d[0]\\n                        yy = displacement_field[r, c, 1] - d[1]\\n                        displacement_field[r, c, 0] = d[0]\\n                        displacement_field[r, c, 1] = d[1]\\n                        opt = xx * xx + yy * yy\\n                        if max_displacement < opt:\\n                            max_displacement = opt\\n    return sqrt(max_displacement)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\sumsqdiff.pyx.txt'}),\n",
       " Document(page_content='@cython.boundscheck(False)\\n@cython.wraparound(False)\\ncpdef double compute_energy_ssd_2d(floating[:, :] delta_field):\\n    r\"\"\"Sum of squared differences between two 2D images\\n\\n    Computes the Sum of Squared Differences between the static and moving image.\\n    Those differences are given by delta_field\\n\\n    Parameters\\n    ----------\\n    delta_field : array, shape (R, C)\\n        the difference between the static and moving image (the \\'derivative\\n        w.r.t. time\\' in the optical flow model)\\n\\n    Returns\\n    -------\\n    energy : float\\n        the SSD energy at this iteration\\n\\n    Notes\\n    -----\\n    The numeric value of the energy is used only to detect convergence.\\n    This function returns only the energy corresponding to the data term\\n    (excluding the energy corresponding to the regularization term) because\\n    the Greedy-SyN algorithm is an unconstrained gradient descent algorithm\\n    in the space of diffeomorphisms: in each iteration it makes a step\\n    along the negative smoothed gradient --of the data term-- and then makes\\n    sure the resulting diffeomorphisms are invertible using an explicit\\n    inversion algorithm. Since it is not clear how to reflect the energy\\n    corresponding to this re-projection to the space of diffeomorphisms,\\n    a more precise energy computation including the regularization term\\n    is useless. Instead, convergence is checked considering the data-term\\n    energy only and detecting oscilations in the energy profile.\\n\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp nrows = delta_field.shape[0]\\n        cnp.npy_intp ncols = delta_field.shape[1]\\n        cnp.npy_intp r, c\\n        double energy = 0', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\sumsqdiff.pyx.txt'}),\n",
       " Document(page_content='\"\"\"\\n    cdef:\\n        cnp.npy_intp nrows = delta_field.shape[0]\\n        cnp.npy_intp ncols = delta_field.shape[1]\\n        cnp.npy_intp r, c\\n        double energy = 0\\n\\n    with nogil:\\n        for r in range(nrows):\\n            for c in range(ncols):\\n                energy += delta_field[r, c] ** 2\\n    return energy\\n\\n\\n@cython.boundscheck(False)\\n@cython.wraparound(False)\\n@cython.cdivision(True)\\ncpdef double iterate_residual_displacement_field_ssd_3d(\\n                floating[:, :, :] delta_field, floating[:, :, :] sigmasq_field,\\n                floating[:, :, :, :] grad, floating[:, :, :, :] target,\\n                double lambda_param, floating[:, :, :, :] disp):\\n    r\"\"\"One iteration of a large linear system solver for 3D SSD registration\\n\\n    Performs one iteration at one level of the Multi-resolution Gauss-Seidel\\n    solver proposed by Bruhn and Weickert [Bruhn05].', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\sumsqdiff.pyx.txt'}),\n",
       " Document(page_content=\"Performs one iteration at one level of the Multi-resolution Gauss-Seidel\\n    solver proposed by Bruhn and Weickert [Bruhn05].\\n\\n    Parameters\\n    ----------\\n    delta_field : array, shape (S, R, C)\\n        the difference between the static and moving image (the 'derivative\\n        w.r.t. time' in the optical flow model)\\n    sigmasq_field : array, shape (S, R, C)\\n        the variance of the gray level value at each voxel, according to the\\n        EM model (for SSD, it is 1 for all voxels). Inf and 0 values\\n        are processed specially to support infinite and zero variance.\\n    grad : array, shape (S, R, C, 3)\\n        the gradient of the moving image\\n    target : array, shape (S, R, C, 3)\\n        right-hand side of the linear system to be solved in the Weickert's\\n        multi-resolution algorithm\\n    lambda_param : float\\n        smoothness parameter of the objective function\\n    disp : array, shape (S, R, C, 3)\\n        the displacement field to start the optimization from\\n\\n    Returns\\n    -------\\n    max_displacement : float\\n        the norm of the maximum change in the displacement field after the\\n        iteration\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\sumsqdiff.pyx.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    max_displacement : float\\n        the norm of the maximum change in the displacement field after the\\n        iteration\\n\\n    References\\n    ----------\\n    [Bruhn05] Andres Bruhn and Joachim Weickert, \"Towards ultimate motion\\n              estimation: combining highest accuracy with real-time\\n              performance\", 10th IEEE International Conference on Computer\\n              Vision, 2005. ICCV 2005.\\n    \"\"\"\\n    ftype = np.asarray(delta_field).dtype\\n    cdef:\\n        int NUM_NEIGHBORS = 6\\n        int* dSlice = [-1, 0, 0, 0,  0, 1]\\n        int* dRow = [0, -1, 0, 1,  0, 0]\\n        int* dCol = [0,  0, 1, 0, -1, 0]\\n        cnp.npy_intp nslices = delta_field.shape[0]\\n        cnp.npy_intp nrows = delta_field.shape[1]\\n        cnp.npy_intp ncols = delta_field.shape[2]\\n        int nn\\n        double* g = [0, 0, 0]\\n        double* b = [0, 0, 0]\\n        double* d = [0, 0, 0]\\n        double* y = [0, 0, 0]\\n        double* A = [0, 0, 0, 0, 0, 0]\\n        double xx, yy, zz, opt, nrm2, delta, sigmasq, max_displacement\\n        cnp.npy_intp dr, ds, dc, s, r, c\\n    max_displacement = 0\\n\\n    with nogil:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\sumsqdiff.pyx.txt'}),\n",
       " Document(page_content='for s in range(nslices):\\n            for r in range(nrows):\\n                for c in range(ncols):\\n                    g[0] = grad[s, r, c, 0]\\n                    g[1] = grad[s, r, c, 1]\\n                    g[2] = grad[s, r, c, 2]\\n                    delta = delta_field[s, r, c]\\n                    sigmasq = sigmasq_field[s, r, c] if sigmasq_field is not None else 1\\n                    if target is None:\\n                        b[0] = delta_field[s, r, c] * g[0]\\n                        b[1] = delta_field[s, r, c] * g[1]\\n                        b[2] = delta_field[s, r, c] * g[2]\\n                    else:\\n                        b[0] = target[s, r, c, 0]\\n                        b[1] = target[s, r, c, 1]\\n                        b[2] = target[s, r, c, 2]\\n                    nn = 0\\n                    y[0] = 0\\n                    y[1] = 0\\n                    y[2] = 0\\n                    for k in range(NUM_NEIGHBORS):\\n                        ds = s + dSlice[k]\\n                        if ds < 0 or ds >= nslices:\\n                            continue\\n                        dr = r + dRow[k]\\n                        if dr < 0 or dr >= nrows:\\n                            continue\\n                        dc = c + dCol[k]\\n                        if dc < 0 or dc >= ncols:\\n                            continue\\n                        nn += 1\\n                        y[0] += disp[ds, dr, dc, 0]\\n                        y[1] += disp[ds, dr, dc, 1]\\n                        y[2] += disp[ds, dr, dc, 2]\\n                    if dpy_isinf(sigmasq) != 0:\\n                        xx = disp[s, r, c, 0]\\n                        yy = disp[s, r, c, 1]\\n                        zz = disp[s, r, c, 2]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\sumsqdiff.pyx.txt'}),\n",
       " Document(page_content='if dc < 0 or dc >= ncols:\\n                            continue\\n                        nn += 1\\n                        y[0] += disp[ds, dr, dc, 0]\\n                        y[1] += disp[ds, dr, dc, 1]\\n                        y[2] += disp[ds, dr, dc, 2]\\n                    if dpy_isinf(sigmasq) != 0:\\n                        xx = disp[s, r, c, 0]\\n                        yy = disp[s, r, c, 1]\\n                        zz = disp[s, r, c, 2]\\n                        disp[s, r, c, 0] = y[0] / nn\\n                        disp[s, r, c, 1] = y[1] / nn\\n                        disp[s, r, c, 2] = y[2] / nn\\n                        xx -= disp[s, r, c, 0]\\n                        yy -= disp[s, r, c, 1]\\n                        zz -= disp[s, r, c, 2]\\n                        opt = xx * xx + yy * yy + zz * zz\\n                        if max_displacement < opt:\\n                            max_displacement = opt\\n                    elif sigmasq < 1e-9:\\n                            nrm2 = g[0] ** 2 + g[1] ** 2 + g[2] ** 2\\n                            if nrm2 < 1e-9:\\n                                disp[s, r, c, 0] = 0\\n                                disp[s, r, c, 1] = 0\\n                                disp[s, r, c, 2] = 0\\n                            else:\\n                                disp[s, r, c, 0] = (b[0]) / nrm2\\n                                disp[s, r, c, 1] = (b[1]) / nrm2\\n                                disp[s, r, c, 2] = (b[2]) / nrm2\\n                    else:\\n                        tau = sigmasq * lambda_param * nn\\n                        y[0] = b[0] + sigmasq * lambda_param * y[0]\\n                        y[1] = b[1] + sigmasq * lambda_param * y[1]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\sumsqdiff.pyx.txt'}),\n",
       " Document(page_content='else:\\n                                disp[s, r, c, 0] = (b[0]) / nrm2\\n                                disp[s, r, c, 1] = (b[1]) / nrm2\\n                                disp[s, r, c, 2] = (b[2]) / nrm2\\n                    else:\\n                        tau = sigmasq * lambda_param * nn\\n                        y[0] = b[0] + sigmasq * lambda_param * y[0]\\n                        y[1] = b[1] + sigmasq * lambda_param * y[1]\\n                        y[2] = b[2] + sigmasq * lambda_param * y[2]\\n                        is_singular = _solve_3d_symmetric_positive_definite(\\n                                                                g, y, tau, d)\\n                        if is_singular == 1:\\n                            nrm2 = g[0] ** 2 + g[1] ** 2 + g[2] ** 2\\n                            if nrm2 < 1e-9:\\n                                disp[s, r, c, 0] = 0\\n                                disp[s, r, c, 1] = 0\\n                                disp[s, r, c, 2] = 0\\n                            else:\\n                                disp[s, r, c, 0] = (b[0]) / nrm2\\n                                disp[s, r, c, 1] = (b[1]) / nrm2\\n                                disp[s, r, c, 2] = (b[2]) / nrm2\\n                        xx = disp[s, r, c, 0] - d[0]\\n                        yy = disp[s, r, c, 1] - d[1]\\n                        zz = disp[s, r, c, 2] - d[2]\\n                        disp[s, r, c, 0] = d[0]\\n                        disp[s, r, c, 1] = d[1]\\n                        disp[s, r, c, 2] = d[2]\\n                        opt = xx * xx + yy * yy + zz * zz\\n                        if max_displacement < opt:\\n                            max_displacement = opt', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\sumsqdiff.pyx.txt'}),\n",
       " Document(page_content='xx = disp[s, r, c, 0] - d[0]\\n                        yy = disp[s, r, c, 1] - d[1]\\n                        zz = disp[s, r, c, 2] - d[2]\\n                        disp[s, r, c, 0] = d[0]\\n                        disp[s, r, c, 1] = d[1]\\n                        disp[s, r, c, 2] = d[2]\\n                        opt = xx * xx + yy * yy + zz * zz\\n                        if max_displacement < opt:\\n                            max_displacement = opt\\n    return sqrt(max_displacement)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\sumsqdiff.pyx.txt'}),\n",
       " Document(page_content='@cython.boundscheck(False)\\n@cython.wraparound(False)\\ncpdef double compute_energy_ssd_3d(floating[:, :, :] delta_field):\\n    r\"\"\"Sum of squared differences between two 3D volumes\\n\\n    Computes the Sum of Squared Differences between the static and moving volume\\n    Those differences are given by delta_field\\n\\n    Parameters\\n    ----------\\n    delta_field : array, shape (R, C)\\n        the difference between the static and moving image (the \\'derivative\\n        w.r.t. time\\' in the optical flow model)\\n\\n    Returns\\n    -------\\n    energy : float\\n        the SSD energy at this iteration', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\sumsqdiff.pyx.txt'}),\n",
       " Document(page_content='Parameters\\n    ----------\\n    delta_field : array, shape (R, C)\\n        the difference between the static and moving image (the \\'derivative\\n        w.r.t. time\\' in the optical flow model)\\n\\n    Returns\\n    -------\\n    energy : float\\n        the SSD energy at this iteration\\n\\n    Notes\\n    -----\\n    The numeric value of the energy is used only to detect convergence.\\n    This function returns only the energy corresponding to the data term\\n    (excluding the energy corresponding to the regularization term) because\\n    the Greedy-SyN algorithm is an unconstrained gradient descent algorithm\\n    in the space of diffeomorphisms: in each iteration it makes a step\\n    along the negative smoothed gradient --of the data term-- and then makes\\n    sure the resulting diffeomorphisms are invertible using an explicit\\n    inversion algorithm. Since it is not clear how to reflect the energy\\n    corresponding to this re-projection to the space of diffeomorphisms,\\n    a more precise energy computation including the regularization term\\n    is useless. Instead, convergence is checked considering the data-term\\n    energy only and detecting oscilations in the energy profile.\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp nslices = delta_field.shape[0]\\n        cnp.npy_intp nrows = delta_field.shape[1]\\n        cnp.npy_intp ncols = delta_field.shape[2]\\n        cnp.npy_intp s, r, c\\n        double energy = 0\\n    with nogil:\\n        for s in range(nslices):\\n            for r in range(nrows):\\n                for c in range(ncols):\\n                    energy += delta_field[s, r, c] ** 2\\n    return energy', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\sumsqdiff.pyx.txt'}),\n",
       " Document(page_content='@cython.boundscheck(False)\\n@cython.wraparound(False)\\ndef compute_residual_displacement_field_ssd_3d(\\n        floating[:, :, :] delta_field, floating[:, :, :] sigmasq_field,\\n        floating[:, :, :, :] gradient_field, floating[:, :, :, :] target,\\n        double lambda_param, floating[:, :, :, :] disp,\\n        floating[:, :, :, :] residual):\\n    r\"\"\"The residual displacement field to be fit on the next iteration\\n\\n    Computes the residual displacement field corresponding to the current\\n    displacement field (given by \\'disp\\') in the Multi-resolution\\n    Gauss-Seidel solver proposed by Bruhn and Weickert [Bruhn].\\n\\n    Parameters\\n    ----------\\n    delta_field : array, shape (S, R, C)\\n        the difference between the static and moving image (the \\'derivative\\n        w.r.t. time\\' in the optical flow model)\\n    sigmasq_field : array, shape (S, R, C)\\n        the variance of the gray level value at each voxel, according to the\\n        EM model (for SSD, it is 1 for all voxels). Inf and 0 values\\n        are processed specially to support infinite and zero variance.\\n    gradient_field : array, shape (S, R, C, 3)\\n        the gradient of the moving image\\n    target : array, shape (S, R, C, 3)\\n        right-hand side of the linear system to be solved in the Weickert\\'s\\n        multi-resolution algorithm\\n    lambda_param : float\\n        smoothness parameter in the objective function\\n    disp : array, shape (S, R, C, 3)\\n        the current displacement field to compute the residual from\\n    residual : array, shape (S, R, C, 3)\\n        the displacement field to put the residual to', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\sumsqdiff.pyx.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    residual : array, shape (S, R, C, 3)\\n        the residual displacement field. If residual was None a input, then\\n        a new field is returned, otherwise the same array is returned\\n\\n    References\\n    ----------\\n    [Bruhn05] Andres Bruhn and Joachim Weickert, \"Towards ultimate motion\\n              estimation: combining highest accuracy with real-time\\n              performance\", 10th IEEE International Conference on Computer\\n              Vision, 2005. ICCV 2005.\\n    \"\"\"\\n    ftype = np.asarray(delta_field).dtype\\n    cdef:\\n        int NUM_NEIGHBORS = 6\\n        int* dSlice = [-1,  0, 0, 0,  0, 1]\\n        int* dRow = [0, -1, 0, 1,  0, 0]\\n        int* dCol = [0,  0, 1, 0, -1, 0]\\n        double* b = [0, 0, 0]\\n        double* y = [0, 0, 0]\\n\\n        cnp.npy_intp nslices = delta_field.shape[0]\\n        cnp.npy_intp nrows = delta_field.shape[1]\\n        cnp.npy_intp ncols = delta_field.shape[2]\\n        double delta, sigmasq, dotP\\n        cnp.npy_intp s, r, c, ds, dr, dc\\n    if residual is None:\\n        residual = np.empty(shape=(nslices, nrows, ncols, 3), dtype=ftype)\\n\\n    with nogil:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\sumsqdiff.pyx.txt'}),\n",
       " Document(page_content='for s in range(nslices):\\n            for r in range(nrows):\\n                for c in range(ncols):\\n                    delta = delta_field[s, r, c]\\n                    sigmasq = sigmasq_field[s, r, c] if sigmasq_field is not None else 1\\n                    if target is None:\\n                        b[0] = delta * gradient_field[s, r, c, 0]\\n                        b[1] = delta * gradient_field[s, r, c, 1]\\n                        b[2] = delta * gradient_field[s, r, c, 2]\\n                    else:\\n                        b[0] = target[s, r, c, 0]\\n                        b[1] = target[s, r, c, 1]\\n                        b[2] = target[s, r, c, 2]\\n                    y[0] = 0\\n                    y[1] = 0\\n                    y[2] = 0\\n                    for k in range(NUM_NEIGHBORS):\\n                        ds = s + dSlice[k]\\n                        if ds < 0 or ds >= nslices:\\n                            continue\\n                        dr = r + dRow[k]\\n                        if dr < 0 or dr >= nrows:\\n                            continue\\n                        dc = c + dCol[k]\\n                        if dc < 0 or dc >= ncols:\\n                            continue\\n                        y[0] += (disp[s, r, c, 0] - disp[ds, dr, dc, 0])\\n                        y[1] += (disp[s, r, c, 1] - disp[ds, dr, dc, 1])\\n                        y[2] += (disp[s, r, c, 2] - disp[ds, dr, dc, 2])\\n                    if dpy_isinf(sigmasq) != 0:\\n                        residual[s, r, c, 0] = -lambda_param * y[0]\\n                        residual[s, r, c, 1] = -lambda_param * y[1]\\n                        residual[s, r, c, 2] = -lambda_param * y[2]\\n                    else:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\sumsqdiff.pyx.txt'}),\n",
       " Document(page_content='y[0] += (disp[s, r, c, 0] - disp[ds, dr, dc, 0])\\n                        y[1] += (disp[s, r, c, 1] - disp[ds, dr, dc, 1])\\n                        y[2] += (disp[s, r, c, 2] - disp[ds, dr, dc, 2])\\n                    if dpy_isinf(sigmasq) != 0:\\n                        residual[s, r, c, 0] = -lambda_param * y[0]\\n                        residual[s, r, c, 1] = -lambda_param * y[1]\\n                        residual[s, r, c, 2] = -lambda_param * y[2]\\n                    else:\\n                        dotP = (gradient_field[s, r, c, 0] * disp[s, r, c, 0] +\\n                                gradient_field[s, r, c, 1] * disp[s, r, c, 1] +\\n                                gradient_field[s, r, c, 2] * disp[s, r, c, 2])\\n                        residual[s, r, c, 0] = (b[0] -\\n                                                (gradient_field[s, r, c, 0] * dotP +\\n                                                 sigmasq * lambda_param * y[0]))\\n                        residual[s, r, c, 1] = (b[1] -\\n                                                (gradient_field[s, r, c, 1] * dotP +\\n                                                 sigmasq * lambda_param * y[1]))\\n                        residual[s, r, c, 2] = (b[2] -\\n                                                (gradient_field[s, r, c, 2] * dotP +\\n                                                 sigmasq * lambda_param * y[2]))\\n    return np.asarray(residual)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\sumsqdiff.pyx.txt'}),\n",
       " Document(page_content='@cython.boundscheck(False)\\n@cython.wraparound(False)\\ncpdef compute_residual_displacement_field_ssd_2d(\\n        floating[:, :] delta_field, floating[:, :] sigmasq_field,\\n        floating[:, :, :] gradient_field, floating[:, :, :] target,\\n        double lambda_param, floating[:, :, :] d,\\n        floating[:, :, :] residual):\\n    r\"\"\"The residual displacement field to be fit on the next iteration\\n\\n    Computes the residual displacement field corresponding to the current\\n    displacement field in the Multi-resolution Gauss-Seidel solver proposed by\\n    Bruhn and Weickert [Bruhn05].\\n\\n    Parameters\\n    ----------\\n    delta_field : array, shape (R, C)\\n        the difference between the static and moving image (the \\'derivative\\n        w.r.t. time\\' in the optical flow model)\\n    sigmasq_field : array, shape (R, C)\\n        the variance of the gray level value at each voxel, according to the\\n        EM model (for SSD, it is 1 for all voxels). Inf and 0 values\\n        are processed specially to support infinite and zero variance.\\n    gradient_field : array, shape (R, C, 2)\\n        the gradient of the moving image\\n    target : array, shape (R, C, 2)\\n        right-hand side of the linear system to be solved in the Weickert\\'s\\n        multi-resolution algorithm\\n    lambda_param : float\\n        smoothness parameter in the objective function\\n    d : array, shape (R, C, 2)\\n        the current displacement field to compute the residual from\\n    residual : array, shape (R, C, 2)\\n        the displacement field to put the residual to', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\sumsqdiff.pyx.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    residual : array, shape (R, C, 2)\\n        the residual displacement field. If residual was None a input, then\\n        a new field is returned, otherwise the same array is returned\\n\\n    References\\n    ----------\\n    [Bruhn05] Andres Bruhn and Joachim Weickert, \"Towards ultimate motion\\n              estimation: combining highest accuracy with real-time\\n              performance\", 10th IEEE International Conference on Computer\\n              Vision, 2005. ICCV 2005.\\n    \"\"\"\\n    ftype = np.asarray(delta_field).dtype\\n    cdef:\\n        int NUM_NEIGHBORS = 4\\n        int* dRow = [-1, 0, 1,  0]\\n        int* dCol = [0, 1, 0, -1]\\n        double* b = [0, 0]\\n        double* y = [0, 0]\\n\\n        cnp.npy_intp nrows = delta_field.shape[0]\\n        cnp.npy_intp ncols = delta_field.shape[1]\\n        double delta, sigmasq, dotP\\n        cnp.npy_intp r, c, dr, dc\\n    if residual is None:\\n        residual = np.empty(shape=(nrows, ncols, 2), dtype=ftype)\\n\\n    with nogil:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\sumsqdiff.pyx.txt'}),\n",
       " Document(page_content='cnp.npy_intp nrows = delta_field.shape[0]\\n        cnp.npy_intp ncols = delta_field.shape[1]\\n        double delta, sigmasq, dotP\\n        cnp.npy_intp r, c, dr, dc\\n    if residual is None:\\n        residual = np.empty(shape=(nrows, ncols, 2), dtype=ftype)\\n\\n    with nogil:\\n\\n        for r in range(nrows):\\n            for c in range(ncols):\\n                delta = delta_field[r, c]\\n                sigmasq = sigmasq_field[r, c] if sigmasq_field is not None else 1\\n                if target is None:\\n                    b[0] = delta * gradient_field[r, c, 0]\\n                    b[1] = delta * gradient_field[r, c, 1]\\n                else:\\n                    b[0] = target[r, c, 0]\\n                    b[1] = target[r, c, 1]\\n                y[0] = 0  # reset y\\n                y[1] = 0\\n                nn=0\\n                for k in range(NUM_NEIGHBORS):\\n                    dr = r + dRow[k]\\n                    if dr < 0 or dr >= nrows:\\n                        continue\\n                    dc = c + dCol[k]\\n                    if dc < 0 or dc >= ncols:\\n                        continue\\n                    y[0] += (d[r, c, 0] - d[dr, dc, 0])\\n                    y[1] += (d[r, c, 1] - d[dr, dc, 1])', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\sumsqdiff.pyx.txt'}),\n",
       " Document(page_content='if dpy_isinf(sigmasq) != 0:\\n                    residual[r, c, 0] = -lambda_param * y[0]\\n                    residual[r, c, 1] = -lambda_param * y[1]\\n                else:\\n                    dotP = (gradient_field[r, c, 0] * d[r, c, 0] +\\n                            gradient_field[r, c, 1] * d[r, c, 1])\\n                    residual[r, c, 0] = (b[0] -\\n                                         (gradient_field[r, c, 0] * dotP +\\n                                          sigmasq * lambda_param * y[0]))\\n                    residual[r, c, 1] = (b[1] -\\n                                         (gradient_field[r, c, 1] * dotP +\\n                                          sigmasq * lambda_param * y[1]))\\n    return np.asarray(residual)\\n\\n\\n@cython.boundscheck(False)\\n@cython.wraparound(False)\\n@cython.cdivision(True)\\ndef compute_ssd_demons_step_2d(floating[:,:] delta_field,\\n                               floating[:,:,:] gradient_moving,\\n                               double sigma_sq_x,\\n                               floating[:,:,:] out):\\n    r\"\"\"Demons step for 2D SSD-driven registration\\n\\n    Computes the demons step for SSD-driven registration\\n    ( eq. 4 in [Bruhn05] )', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\sumsqdiff.pyx.txt'}),\n",
       " Document(page_content='@cython.boundscheck(False)\\n@cython.wraparound(False)\\n@cython.cdivision(True)\\ndef compute_ssd_demons_step_2d(floating[:,:] delta_field,\\n                               floating[:,:,:] gradient_moving,\\n                               double sigma_sq_x,\\n                               floating[:,:,:] out):\\n    r\"\"\"Demons step for 2D SSD-driven registration\\n\\n    Computes the demons step for SSD-driven registration\\n    ( eq. 4 in [Bruhn05] )\\n\\n    Parameters\\n    ----------\\n    delta_field : array, shape (R, C)\\n        the difference between the static and moving image (the \\'derivative\\n        w.r.t. time\\' in the optical flow model)\\n    gradient_field : array, shape (R, C, 2)\\n        the gradient of the moving image\\n    sigma_sq_x : float\\n        parameter controlling the amount of regularization. It corresponds to\\n        $\\\\sigma_x^2$ in algorithm 1 of Vercauteren et al.[Vercauteren09]\\n    out : array, shape (R, C, 2)\\n        if None, a new array will be created to store the demons step. Otherwise\\n        the provided array will be used.\\n\\n    Returns\\n    -------\\n    demons_step : array, shape (R, C, 2)\\n        the demons step to be applied for updating the current displacement\\n        field\\n    energy : float\\n        the current ssd energy (before applying the returned demons_step)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\sumsqdiff.pyx.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    demons_step : array, shape (R, C, 2)\\n        the demons step to be applied for updating the current displacement\\n        field\\n    energy : float\\n        the current ssd energy (before applying the returned demons_step)\\n\\n    References\\n    ----------\\n    [Bruhn05] Andres Bruhn and Joachim Weickert, \"Towards ultimate motion\\n              estimation: combining highest accuracy with real-time\\n              performance\", 10th IEEE International Conference on Computer\\n              Vision, 2005. ICCV 2005.\\n    [Vercauteren09] Vercauteren, T., Pennec, X., Perchant, A., & Ayache, N.\\n                    (2009). Diffeomorphic demons: efficient non-parametric\\n                    image registration. NeuroImage, 45(1 Suppl), S61-72.\\n                    doi:10.1016/j.neuroimage.2008.10.040\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp nr = delta_field.shape[0]\\n        cnp.npy_intp nc = delta_field.shape[1]\\n        cnp.npy_intp i, j\\n        double delta, delta_2, nrm2, energy, den\\n\\n    if out is None:\\n        out = np.zeros((nr, nc, 2), dtype=np.asarray(delta_field).dtype)\\n\\n    with nogil:\\n\\n        energy = 0\\n        for i in range(nr):\\n            for j in range(nc):\\n                delta = delta_field[i,j]\\n                delta_2 = delta**2\\n                energy += delta_2\\n                nrm2 = gradient_moving[i, j, 0]**2 + gradient_moving[i, j, 1]**2\\n                den = delta_2/sigma_sq_x + nrm2\\n                if den <1e-9:\\n                    out[i, j, 0] = 0\\n                    out[i, j, 1] = 0\\n                else:\\n                    out[i, j, 0] = delta * gradient_moving[i, j, 0] / den\\n                    out[i, j, 1] = delta * gradient_moving[i, j, 1] / den', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\sumsqdiff.pyx.txt'}),\n",
       " Document(page_content='return np.asarray(out), energy\\n\\n\\n@cython.boundscheck(False)\\n@cython.wraparound(False)\\n@cython.cdivision(True)\\ndef compute_ssd_demons_step_3d(floating[:,:,:] delta_field,\\n                               floating[:,:,:,:] gradient_moving,\\n                               double sigma_sq_x,\\n                               floating[:,:,:,:] out):\\n    r\"\"\"Demons step for 3D SSD-driven registration\\n\\n    Computes the demons step for SSD-driven registration\\n    ( eq. 4 in [Bruhn05] )\\n\\n    Parameters\\n    ----------\\n    delta_field : array, shape (S, R, C)\\n        the difference between the static and moving image (the \\'derivative\\n        w.r.t. time\\' in the optical flow model)\\n    gradient_field : array, shape (S, R, C, 2)\\n        the gradient of the moving image\\n    sigma_sq_x : float\\n        parameter controlling the amount of regularization. It corresponds to\\n        $\\\\sigma_x^2$ in algorithm 1 of Vercauteren et al.[Vercauteren09]\\n    out : array, shape (S, R, C, 2)\\n        if None, a new array will be created to store the demons step. Otherwise\\n        the provided array will be used.\\n\\n    Returns\\n    -------\\n    demons_step : array, shape (S, R, C, 3)\\n        the demons step to be applied for updating the current displacement\\n        field\\n    energy : float\\n        the current ssd energy (before applying the returned demons_step)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\sumsqdiff.pyx.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    demons_step : array, shape (S, R, C, 3)\\n        the demons step to be applied for updating the current displacement\\n        field\\n    energy : float\\n        the current ssd energy (before applying the returned demons_step)\\n\\n    References\\n    ----------\\n    [Bruhn05] Andres Bruhn and Joachim Weickert, \"Towards ultimate motion\\n              estimation: combining highest accuracy with real-time\\n              performance\", 10th IEEE International Conference on Computer\\n              Vision, 2005. ICCV 2005.\\n    [Vercauteren09] Vercauteren, T., Pennec, X., Perchant, A., & Ayache, N.\\n                    (2009). Diffeomorphic demons: efficient non-parametric\\n                    image registration. NeuroImage, 45(1 Suppl), S61-72.\\n                    doi:10.1016/j.neuroimage.2008.10.040\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp ns = delta_field.shape[0]\\n        cnp.npy_intp nr = delta_field.shape[1]\\n        cnp.npy_intp nc = delta_field.shape[2]\\n        cnp.npy_intp i, j, k\\n        double delta, delta_2, nrm2, energy, den\\n\\n    if out is None:\\n        out = np.zeros((ns, nr, nc, 3), dtype=np.asarray(delta_field).dtype)\\n\\n    with nogil:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\sumsqdiff.pyx.txt'}),\n",
       " Document(page_content='if out is None:\\n        out = np.zeros((ns, nr, nc, 3), dtype=np.asarray(delta_field).dtype)\\n\\n    with nogil:\\n\\n        energy = 0\\n        for k in range(ns):\\n            for i in range(nr):\\n                for j in range(nc):\\n                    delta = delta_field[k,i,j]\\n                    delta_2 = delta**2\\n                    energy += delta_2\\n                    nrm2 = (gradient_moving[k, i, j, 0]**2 +\\n                            gradient_moving[k, i, j, 1]**2 +\\n                            gradient_moving[k, i, j, 2]**2)\\n                    den = delta_2/sigma_sq_x + nrm2\\n                    if den < 1e-9:\\n                        out[k, i, j, 0] = 0\\n                        out[k, i, j, 1] = 0\\n                        out[k, i, j, 2] = 0\\n                    else:\\n                        out[k, i, j, 0] = (delta *\\n                                           gradient_moving[k, i, j, 0] / den)\\n                        out[k, i, j, 1] = (delta *\\n                                           gradient_moving[k, i, j, 1] / den)\\n                        out[k, i, j, 2] = (delta *\\n                                           gradient_moving[k, i, j, 2] / den)\\n    return np.asarray(out), energy', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\sumsqdiff.pyx.txt'}),\n",
       " Document(page_content='#!python\\n#cython: boundscheck=False\\n#cython: wraparound=False\\n#cython: cdivision=True\\n\\nimport numpy as np\\ncimport numpy as cnp\\n\\ncdef extern from \"dpy_math.h\" nogil:\\n    double cos(double)\\n    double sin(double)\\n    double log(double)\\n\\ncdef class Transform:\\n    r\"\"\" Base class (contract) for all transforms for affine image registration\\n    Each transform must define the following (fast, nogil) methods:\\n\\n    1. _jacobian(theta, x, J): receives a parameter vector theta, a point in\\n       x, and a matrix J with shape (dim, len(theta)). It must writes in J, the\\n       Jacobian of the transform with parameters theta evaluated at x.\\n\\n    2. _get_identity_parameters(theta): receives a vector theta whose length is\\n       the number of parameters of the transform and sets in theta the values\\n       that define the identity transform.\\n\\n    3. _param_to_matrix(theta, T): receives a parameter vector theta, and a\\n       matrix T of shape (dim + 1, dim + 1) and writes in T the matrix\\n       representation of the transform with parameters theta\\n\\n    This base class defines the (slow, convenient) python wrappers for each\\n    of the above functions, which also do parameter checking and raise\\n    a ValueError in case the provided parameters are invalid.\\n    \"\"\"\\n    def __cinit__(self):\\n        r\"\"\" Default constructor\\n        Sets transform dimension and number of parameter to invalid values (-1)\\n        \"\"\"\\n        self.dim = -1\\n        self.number_of_parameters = -1\\n\\n    cdef int _jacobian(self, double[:] theta, double[:] x,\\n                       double[:, :] J) noexcept nogil:\\n        return -1\\n\\n    cdef void _get_identity_parameters(self, double[:] theta) noexcept nogil:\\n        return', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='cdef int _jacobian(self, double[:] theta, double[:] x,\\n                       double[:, :] J) noexcept nogil:\\n        return -1\\n\\n    cdef void _get_identity_parameters(self, double[:] theta) noexcept nogil:\\n        return\\n\\n    cdef void _param_to_matrix(self, double[:] theta, double[:, :] T) noexcept nogil:\\n        return\\n\\n    def jacobian(self, double[:] theta, double[:] x):\\n        r\"\"\" Jacobian function of this transform\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (n,)\\n            vector containing the n parameters of this transform\\n        x : array, shape (dim,)\\n            vector containing the point where the Jacobian must be evaluated\\n\\n        Returns\\n        -------\\n        J : array, shape (dim, n)\\n            Jacobian matrix of the transform with parameters theta at point x\\n        \"\"\"\\n        n = theta.shape[0]\\n        if n != self.number_of_parameters:\\n            raise ValueError(\"Invalid number of parameters: %d\"%(n,))\\n        m = x.shape[0]\\n        if m < self.dim:\\n            raise ValueError(\"Invalid point dimension: %d\"%(m,))\\n        J = np.zeros((self.dim, n))\\n        ret = self._jacobian(theta, x, J)\\n        return np.asarray(J)\\n\\n    def get_identity_parameters(self):\\n        r\"\"\" Parameter values corresponding to the identity transform\\n\\n        Returns\\n        -------\\n        theta : array, shape (n,)\\n            the n parameter values corresponding to the identity transform\\n        \"\"\"\\n        if self.number_of_parameters < 0:\\n            raise ValueError(\"Invalid transform.\")\\n        theta = np.zeros(self.number_of_parameters)\\n        self._get_identity_parameters(theta)\\n        return np.asarray(theta)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='def get_identity_parameters(self):\\n        r\"\"\" Parameter values corresponding to the identity transform\\n\\n        Returns\\n        -------\\n        theta : array, shape (n,)\\n            the n parameter values corresponding to the identity transform\\n        \"\"\"\\n        if self.number_of_parameters < 0:\\n            raise ValueError(\"Invalid transform.\")\\n        theta = np.zeros(self.number_of_parameters)\\n        self._get_identity_parameters(theta)\\n        return np.asarray(theta)\\n\\n    def param_to_matrix(self, double[:] theta):\\n        r\"\"\" Matrix representation of this transform with the given parameters\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (n,)\\n            the parameter values of the transform\\n\\n        Returns\\n        -------\\n        T : array, shape (dim + 1, dim + 1)\\n            the matrix representation of this transform with parameters theta\\n        \"\"\"\\n        n = len(theta)\\n        if n != self.number_of_parameters:\\n            raise ValueError(\"Invalid number of parameters: %d\"%(n,))\\n        T = np.eye(self.dim + 1)\\n        self._param_to_matrix(theta, T)\\n        return np.asarray(T)\\n\\n    def get_number_of_parameters(self):\\n        return self.number_of_parameters\\n\\n    def get_dim(self):\\n        return self.dim\\n\\n\\ncdef class TranslationTransform2D(Transform):\\n    def __init__(self):\\n        r\"\"\" Translation transform in 2D\\n        \"\"\"\\n        self.dim = 2\\n        self.number_of_parameters = 2\\n\\n    cdef int _jacobian(self, double[:] theta, double[:] x,\\n                       double[:, :] J) noexcept nogil:\\n        r\"\"\" Jacobian matrix of the 2D translation transform\\n        The transformation is given by:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='def get_dim(self):\\n        return self.dim\\n\\n\\ncdef class TranslationTransform2D(Transform):\\n    def __init__(self):\\n        r\"\"\" Translation transform in 2D\\n        \"\"\"\\n        self.dim = 2\\n        self.number_of_parameters = 2\\n\\n    cdef int _jacobian(self, double[:] theta, double[:] x,\\n                       double[:, :] J) noexcept nogil:\\n        r\"\"\" Jacobian matrix of the 2D translation transform\\n        The transformation is given by:\\n\\n        T(x) = (T1(x), T2(x)) = (x0 + t0, x1 + t1)\\n\\n        The derivative w.r.t. t1 and t2 is given by\\n\\n        T\\'(x) = [[1, 0], # derivatives of [T1, T2] w.r.t. t0\\n                 [0, 1]] # derivatives of [T1, T2] w.r.t. t1\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (2,)\\n            the parameters of the 2D translation transform (the Jacobian does\\n            not depend on the parameters, but we receive the buffer so all\\n            Jacobian functions receive the same parameters)\\n        x : array, shape (2,)\\n            the point at which to compute the Jacobian (the Jacobian does not\\n            depend on x, but we receive the buffer so all Jacobian functions\\n            receive the same parameters)\\n        J : array, shape (2, 2)\\n            the buffer in which to write the Jacobian\\n\\n        Returns\\n        -------\\n        is_constant : int\\n            always returns 1, indicating that the Jacobian is constant\\n            (independent of x)\\n        \"\"\"\\n        J[0, 0], J[0, 1] = 1.0, 0.0\\n        J[1, 0], J[1, 1] = 0.0, 1.0\\n        # This Jacobian does not depend on x (it\\'s constant): return 1\\n        return 1', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='Returns\\n        -------\\n        is_constant : int\\n            always returns 1, indicating that the Jacobian is constant\\n            (independent of x)\\n        \"\"\"\\n        J[0, 0], J[0, 1] = 1.0, 0.0\\n        J[1, 0], J[1, 1] = 0.0, 1.0\\n        # This Jacobian does not depend on x (it\\'s constant): return 1\\n        return 1\\n\\n    cdef void _get_identity_parameters(self, double[:] theta) noexcept nogil:\\n        r\"\"\" Parameter values corresponding to the identity\\n        Sets in theta the parameter values corresponding to the identity\\n        transform\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (2,)\\n            buffer to write the parameters of the 2D translation transform\\n        \"\"\"\\n        theta[:2] = 0\\n\\n    cdef void _param_to_matrix(self, double[:] theta, double[:, :] R) noexcept nogil:\\n        r\"\"\" Matrix associated with the 2D translation transform\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (2,)\\n            the parameters of the 2D translation transform\\n        R : array, shape (3, 3)\\n            the buffer in which to write the translation matrix\\n        \"\"\"\\n        R[0, 0], R[0, 1], R[0, 2] = 1, 0, theta[0]\\n        R[1, 0], R[1, 1], R[1, 2] = 0, 1, theta[1]\\n        R[2, 0], R[2, 1], R[2, 2] = 0, 0, 1\\n\\n\\ncdef class TranslationTransform3D(Transform):\\n    def __init__(self):\\n        r\"\"\" Translation transform in 3D\\n        \"\"\"\\n        self.dim = 3\\n        self.number_of_parameters = 3\\n\\n    cdef int _jacobian(self, double[:] theta, double[:] x,\\n                       double[:, :] J) noexcept nogil:\\n        r\"\"\" Jacobian matrix of the 3D translation transform\\n        The transformation is given by:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='cdef class TranslationTransform3D(Transform):\\n    def __init__(self):\\n        r\"\"\" Translation transform in 3D\\n        \"\"\"\\n        self.dim = 3\\n        self.number_of_parameters = 3\\n\\n    cdef int _jacobian(self, double[:] theta, double[:] x,\\n                       double[:, :] J) noexcept nogil:\\n        r\"\"\" Jacobian matrix of the 3D translation transform\\n        The transformation is given by:\\n\\n        T(x) = (T1(x), T2(x), T3(x)) = (x0 + t0, x1 + t1, x2 + t2)\\n\\n        The derivative w.r.t. t1, t2 and t3 is given by\\n\\n        T\\'(x) = [[1, 0, 0], # derivatives of [T1, T2, T3] w.r.t. t0\\n                 [0, 1, 0], # derivatives of [T1, T2, T3] w.r.t. t1\\n                 [0, 0, 1]] # derivatives of [T1, T2, T3] w.r.t. t2\\n        Parameters\\n        ----------\\n        theta : array, shape (3,)\\n            the parameters of the 3D translation transform (the Jacobian does\\n            not depend on the parameters, but we receive the buffer so all\\n            Jacobian functions receive the same parameters)\\n        x : array, shape (3,)\\n            the point at which to compute the Jacobian (the Jacobian does not\\n            depend on x, but we receive the buffer so all Jacobian functions\\n            receive the same parameters)\\n        J : array, shape (3, 3)\\n            the buffer in which to write the Jacobian', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='Returns\\n        -------\\n        is_constant : int\\n            always returns 1, indicating that the Jacobian is constant\\n            (independent of x)\\n        \"\"\"\\n        J[0, 0], J[0, 1], J[0, 2] = 1.0, 0.0, 0.0\\n        J[1, 0], J[1, 1], J[1, 2] = 0.0, 1.0, 0.0\\n        J[2, 0], J[2, 1], J[2, 2] = 0.0, 0.0, 1.0\\n        # This Jacobian does not depend on x (it\\'s constant): return 1\\n        return 1\\n\\n    cdef void _get_identity_parameters(self, double[:] theta) noexcept nogil:\\n        r\"\"\" Parameter values corresponding to the identity\\n        Sets in theta the parameter values corresponding to the identity\\n        transform\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (3,)\\n            buffer to write the parameters of the 3D translation transform\\n        \"\"\"\\n        theta[:3] = 0\\n\\n    cdef void _param_to_matrix(self, double[:] theta, double[:, :] R) noexcept nogil:\\n        r\"\"\" Matrix associated with the 3D translation transform\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (3,)\\n            the parameters of the 3D translation transform\\n        R : array, shape (4, 4)\\n            the buffer in which to write the translation matrix\\n        \"\"\"\\n        R[0, 0], R[0, 1], R[0, 2], R[0, 3] = 1, 0, 0, theta[0]\\n        R[1, 0], R[1, 1], R[1, 2], R[1, 3] = 0, 1, 0, theta[1]\\n        R[2, 0], R[2, 1], R[2, 2], R[2, 3] = 0, 0, 1, theta[2]\\n        R[3, 0], R[3, 1], R[3, 2], R[3, 3] = 0, 0, 0, 1\\n\\n\\ncdef class RotationTransform2D(Transform):\\n    def __init__(self):\\n        r\"\"\" Rotation transform in 2D\\n        \"\"\"\\n        self.dim = 2\\n        self.number_of_parameters = 1', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='cdef class RotationTransform2D(Transform):\\n    def __init__(self):\\n        r\"\"\" Rotation transform in 2D\\n        \"\"\"\\n        self.dim = 2\\n        self.number_of_parameters = 1\\n\\n    cdef int _jacobian(self, double[:] theta, double[:] x,\\n                       double[:, :] J) noexcept nogil:\\n        r\"\"\" Jacobian matrix of a 2D rotation with parameter theta, at x\\n\\n        The transformation is given by:\\n\\n        T(x,y) = (T1(x,y), T2(x,y)) = (x cost - y sint, x sint + y cost)\\n\\n        The derivatives w.r.t. the rotation angle, t, are:\\n\\n        T\\'(x,y) = [-x sint - y cost, # derivative of T1 w.r.t. t\\n                    x cost - y sint] # derivative of T2 w.r.t. t\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (1,)\\n            the rotation angle\\n        x : array, shape (2,)\\n            the point at which to compute the Jacobian\\n        J : array, shape (2, 1)\\n            the buffer in which to write the Jacobian\\n\\n        Returns\\n        -------\\n        is_constant : int\\n            always returns 0, indicating that the Jacobian is not\\n            constant (it depends on the value of x)\\n        \"\"\"\\n        cdef:\\n            double st = sin(theta[0])\\n            double ct = cos(theta[0])\\n            double px = x[0], py = x[1]\\n\\n        J[0, 0] = -px * st - py * ct\\n        J[1, 0] = px * ct - py * st\\n        # This Jacobian depends on x (it\\'s not constant): return 0\\n        return 0\\n\\n    cdef void _get_identity_parameters(self, double[:] theta) noexcept nogil:\\n        r\"\"\" Parameter values corresponding to the identity\\n        Sets in theta the parameter values corresponding to the identity\\n        transform', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='J[0, 0] = -px * st - py * ct\\n        J[1, 0] = px * ct - py * st\\n        # This Jacobian depends on x (it\\'s not constant): return 0\\n        return 0\\n\\n    cdef void _get_identity_parameters(self, double[:] theta) noexcept nogil:\\n        r\"\"\" Parameter values corresponding to the identity\\n        Sets in theta the parameter values corresponding to the identity\\n        transform\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (1,)\\n            buffer to write the parameters of the 2D rotation transform\\n        \"\"\"\\n        theta[0] = 0\\n\\n    cdef void _param_to_matrix(self, double[:] theta, double[:, :] R) noexcept nogil:\\n        r\"\"\" Matrix associated with the 2D rotation transform\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (1,)\\n            the rotation angle\\n        R : array, shape (3,3)\\n            the buffer in which to write the matrix\\n        \"\"\"\\n        cdef:\\n            double ct = cos(theta[0])\\n            double st = sin(theta[0])\\n        R[0, 0], R[0, 1], R[0, 2] = ct, -st, 0\\n        R[1, 0], R[1, 1], R[1, 2] = st, ct, 0\\n        R[2, 0], R[2, 1], R[2, 2] = 0, 0, 1\\n\\n\\ncdef class RotationTransform3D(Transform):\\n    def __init__(self):\\n        r\"\"\" Rotation transform in 3D\\n        \"\"\"\\n        self.dim = 3\\n        self.number_of_parameters = 3\\n\\n    cdef int _jacobian(self, double[:] theta, double[:] x,\\n                       double[:, :] J) noexcept nogil:\\n        r\"\"\" Jacobian matrix of a 3D rotation with parameters theta, at x', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='cdef class RotationTransform3D(Transform):\\n    def __init__(self):\\n        r\"\"\" Rotation transform in 3D\\n        \"\"\"\\n        self.dim = 3\\n        self.number_of_parameters = 3\\n\\n    cdef int _jacobian(self, double[:] theta, double[:] x,\\n                       double[:, :] J) noexcept nogil:\\n        r\"\"\" Jacobian matrix of a 3D rotation with parameters theta, at x\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (3,)\\n            the rotation angles about the canonical axes\\n        x : array, shape (3,)\\n            the point at which to compute the Jacobian\\n        J : array, shape (3, 3)\\n            the buffer in which to write the Jacobian\\n\\n        Returns\\n        -------\\n        is_constant : int\\n            always returns 0, indicating that the Jacobian is not\\n            constant (it depends on the value of x)\\n        \"\"\"\\n        cdef:\\n            double sa = sin(theta[0])\\n            double ca = cos(theta[0])\\n            double sb = sin(theta[1])\\n            double cb = cos(theta[1])\\n            double sc = sin(theta[2])\\n            double cc = cos(theta[2])\\n            double px = x[0], py = x[1], z = x[2]\\n\\n        J[0, 0] = (-sc * ca * sb) * px + (sc * sa) * py + (sc * ca * cb) * z\\n        J[1, 0] = (cc * ca * sb) * px + (-cc * sa) * py + (-cc * ca * cb) * z\\n        J[2, 0] = (sa * sb) * px + ca * py + (-sa * cb) * z\\n\\n        J[0, 1] = (-cc * sb - sc * sa * cb) * px + (cc * cb - sc * sa * sb) * z\\n        J[1, 1] = (-sc * sb + cc * sa * cb) * px + (sc * cb + cc * sa * sb) * z\\n        J[2, 1] = (-ca * cb) * px + (-ca * sb) * z', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='J[0, 0] = (-sc * ca * sb) * px + (sc * sa) * py + (sc * ca * cb) * z\\n        J[1, 0] = (cc * ca * sb) * px + (-cc * sa) * py + (-cc * ca * cb) * z\\n        J[2, 0] = (sa * sb) * px + ca * py + (-sa * cb) * z\\n\\n        J[0, 1] = (-cc * sb - sc * sa * cb) * px + (cc * cb - sc * sa * sb) * z\\n        J[1, 1] = (-sc * sb + cc * sa * cb) * px + (sc * cb + cc * sa * sb) * z\\n        J[2, 1] = (-ca * cb) * px + (-ca * sb) * z\\n\\n        J[0, 2] = (-sc * cb - cc * sa * sb) * px + (-cc * ca) * py + \\\\\\n                  (-sc * sb + cc * sa * cb) * z\\n        J[1, 2] = (cc * cb - sc * sa * sb) * px + (-sc * ca) * py + \\\\\\n                  (cc * sb + sc * sa * cb) * z\\n        J[2, 2] = 0\\n        # This Jacobian depends on x (it\\'s not constant): return 0\\n        return 0\\n\\n    cdef void _get_identity_parameters(self, double[:] theta) noexcept nogil:\\n        r\"\"\" Parameter values corresponding to the identity\\n        Sets in theta the parameter values corresponding to the identity\\n        transform\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (3,)\\n            buffer to write the parameters of the 3D rotation transform\\n        \"\"\"\\n        theta[:3] = 0\\n\\n    cdef void _param_to_matrix(self, double[:] theta, double[:, :] R) noexcept nogil:\\n        r\"\"\" Matrix associated with the 3D rotation transform\\n\\n        The matrix is the product of rotation matrices of angles theta[0],\\n        theta[1], theta[2] around axes x, y, z applied in the following\\n        order: y, x, z. This order was chosen for consistency with ANTS.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='cdef void _param_to_matrix(self, double[:] theta, double[:, :] R) noexcept nogil:\\n        r\"\"\" Matrix associated with the 3D rotation transform\\n\\n        The matrix is the product of rotation matrices of angles theta[0],\\n        theta[1], theta[2] around axes x, y, z applied in the following\\n        order: y, x, z. This order was chosen for consistency with ANTS.\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (3,)\\n            the rotation angles about each axis:\\n            theta[0] : rotation angle around x axis\\n            theta[1] : rotation angle around y axis\\n            theta[2] : rotation angle around z axis\\n        R : array, shape (4, 4)\\n            buffer in which to write the rotation matrix\\n        \"\"\"\\n        cdef:\\n            double sa = sin(theta[0])\\n            double ca = cos(theta[0])\\n            double sb = sin(theta[1])\\n            double cb = cos(theta[1])\\n            double sc = sin(theta[2])\\n            double cc = cos(theta[2])\\n\\n        R[0,0], R[0,1], R[0,2] = cc*cb-sc*sa*sb, -sc*ca, cc*sb+sc*sa*cb\\n        R[1,0], R[1,1], R[1,2] = sc*cb+cc*sa*sb, cc*ca, sc*sb-cc*sa*cb\\n        R[2,0], R[2,1], R[2,2] = -ca*sb, sa, ca*cb\\n        R[3,0], R[3,1], R[3,2] = 0, 0, 0\\n        R[0, 3] = 0\\n        R[1, 3] = 0\\n        R[2, 3] = 0\\n        R[3, 3] = 1\\n\\n\\ncdef class RigidTransform2D(Transform):\\n    def __init__(self):\\n        r\"\"\" Rigid transform in 2D (rotation + translation)\\n        The parameter vector theta of length 3 is interpreted as follows:\\n        theta[0] : rotation angle\\n        theta[1] : translation along the x axis\\n        theta[2] : translation along the y axis\\n        \"\"\"\\n        self.dim = 2\\n        self.number_of_parameters = 3', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='cdef class RigidTransform2D(Transform):\\n    def __init__(self):\\n        r\"\"\" Rigid transform in 2D (rotation + translation)\\n        The parameter vector theta of length 3 is interpreted as follows:\\n        theta[0] : rotation angle\\n        theta[1] : translation along the x axis\\n        theta[2] : translation along the y axis\\n        \"\"\"\\n        self.dim = 2\\n        self.number_of_parameters = 3\\n\\n    cdef int _jacobian(self, double[:] theta, double[:] x,\\n                       double[:, :] J) noexcept nogil:\\n        r\"\"\" Jacobian matrix of a 2D rigid transform (rotation + translation)\\n\\n        The transformation is given by:\\n\\n        T(x,y) = (T1(x,y), T2(x,y)) =\\n                 (x cost - y sint + dx, x sint + y cost + dy)\\n\\n        The derivatives w.r.t. t, dx and dy are:\\n\\n        T\\'(x,y) = [-x sint - y cost, 1, 0, # derivative of T1 w.r.t. t, dx, dy\\n                    x cost - y sint, 0, 1] # derivative of T2 w.r.t. t, dx, dy\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (3,)\\n            the parameters of the 2D rigid transform\\n            theta[0] : rotation angle (t)\\n            theta[1] : translation along the x axis (dx)\\n            theta[2] : translation along the y axis (dy)\\n        x : array, shape (2,)\\n            the point at which to compute the Jacobian\\n        J : array, shape (2, 3)\\n            the buffer in which to write the Jacobian', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='Parameters\\n        ----------\\n        theta : array, shape (3,)\\n            the parameters of the 2D rigid transform\\n            theta[0] : rotation angle (t)\\n            theta[1] : translation along the x axis (dx)\\n            theta[2] : translation along the y axis (dy)\\n        x : array, shape (2,)\\n            the point at which to compute the Jacobian\\n        J : array, shape (2, 3)\\n            the buffer in which to write the Jacobian\\n\\n        Returns\\n        -------\\n        is_constant : int\\n            always returns 0, indicating that the Jacobian is not\\n            constant (it depends on the value of x)\\n        \"\"\"\\n        cdef:\\n            double st = sin(theta[0])\\n            double ct = cos(theta[0])\\n            double px = x[0], py = x[1]\\n\\n        J[0, 0], J[0, 1], J[0, 2] = -px * st - py * ct, 1, 0\\n        J[1, 0], J[1, 1], J[1, 2] = px * ct - py * st, 0, 1\\n        # This Jacobian depends on x (it\\'s not constant): return 0\\n        return 0\\n\\n    cdef void _get_identity_parameters(self, double[:] theta) noexcept nogil:\\n        r\"\"\" Parameter values corresponding to the identity\\n        Sets in theta the parameter values corresponding to the identity\\n        transform\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (3,)\\n            buffer to write the parameters of the 2D rigid transform\\n            theta[0] : rotation angle\\n            theta[1] : translation along the x axis\\n            theta[2] : translation along the y axis\\n        \"\"\"\\n        theta[:3] = 0\\n\\n    cdef void _param_to_matrix(self, double[:] theta, double[:, :] R) noexcept nogil:\\n        r\"\"\" Matrix associated with the 2D rigid transform', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='Parameters\\n        ----------\\n        theta : array, shape (3,)\\n            buffer to write the parameters of the 2D rigid transform\\n            theta[0] : rotation angle\\n            theta[1] : translation along the x axis\\n            theta[2] : translation along the y axis\\n        \"\"\"\\n        theta[:3] = 0\\n\\n    cdef void _param_to_matrix(self, double[:] theta, double[:, :] R) noexcept nogil:\\n        r\"\"\" Matrix associated with the 2D rigid transform\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (3,)\\n            the parameters of the 2D rigid transform\\n            theta[0] : rotation angle\\n            theta[1] : translation along the x axis\\n            theta[2] : translation along the y axis\\n        R : array, shape (3, 3)\\n            buffer in which to write the rigid matrix\\n        \"\"\"\\n        cdef:\\n            double ct = cos(theta[0])\\n            double st = sin(theta[0])\\n        R[0, 0], R[0, 1], R[0, 2] = ct, -st, theta[1]\\n        R[1, 0], R[1, 1], R[1, 2] = st, ct, theta[2]\\n        R[2, 0], R[2, 1], R[2, 2] = 0, 0, 1\\n\\n\\ncdef class RigidTransform3D(Transform):\\n    def __init__(self):\\n        r\"\"\" Rigid transform in 3D (rotation + translation)\\n        The parameter vector theta of length 6 is interpreted as follows:\\n        theta[0] : rotation about the x axis\\n        theta[1] : rotation about the y axis\\n        theta[2] : rotation about the z axis\\n        theta[3] : translation along the x axis\\n        theta[4] : translation along the y axis\\n        theta[5] : translation along the z axis\\n        \"\"\"\\n        self.dim = 3\\n        self.number_of_parameters = 6', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='cdef int _jacobian(self, double[:] theta, double[:] x,\\n                       double[:, :] J) noexcept nogil:\\n        r\"\"\" Jacobian matrix of a 3D rigid transform (rotation + translation)\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (6,)\\n            the parameters of the 3D rigid transform\\n            theta[0] : rotation about the x axis\\n            theta[1] : rotation about the y axis\\n            theta[2] : rotation about the z axis\\n            theta[3] : translation along the x axis\\n            theta[4] : translation along the y axis\\n            theta[5] : translation along the z axis\\n        x : array, shape (3,)\\n            the point at which to compute the Jacobian\\n        J : array, shape (3, 6)\\n            the buffer in which to write the Jacobian\\n\\n        Returns\\n        -------\\n        is_constant : int\\n            always returns 0, indicating that the Jacobian is not\\n            constant (it depends on the value of x)\\n        \"\"\"\\n        cdef:\\n            double sa = sin(theta[0])\\n            double ca = cos(theta[0])\\n            double sb = sin(theta[1])\\n            double cb = cos(theta[1])\\n            double sc = sin(theta[2])\\n            double cc = cos(theta[2])\\n            double px = x[0], py = x[1], pz = x[2]\\n\\n        J[0, 0] = (-sc * ca * sb) * px + (sc * sa) * py + (sc * ca * cb) * pz\\n        J[1, 0] = (cc * ca * sb) * px + (-cc * sa) * py + (-cc * ca * cb) * pz\\n        J[2, 0] = (sa * sb) * px + ca * py + (-sa * cb) * pz\\n\\n        J[0, 1] = (-cc * sb - sc * sa * cb) * px + (cc * cb - sc * sa * sb) * pz\\n        J[1, 1] = (-sc * sb + cc * sa * cb) * px + (sc * cb + cc * sa * sb) * pz\\n        J[2, 1] = (-ca * cb) * px + (-ca * sb) * pz', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='J[0, 0] = (-sc * ca * sb) * px + (sc * sa) * py + (sc * ca * cb) * pz\\n        J[1, 0] = (cc * ca * sb) * px + (-cc * sa) * py + (-cc * ca * cb) * pz\\n        J[2, 0] = (sa * sb) * px + ca * py + (-sa * cb) * pz\\n\\n        J[0, 1] = (-cc * sb - sc * sa * cb) * px + (cc * cb - sc * sa * sb) * pz\\n        J[1, 1] = (-sc * sb + cc * sa * cb) * px + (sc * cb + cc * sa * sb) * pz\\n        J[2, 1] = (-ca * cb) * px + (-ca * sb) * pz\\n\\n        J[0, 2] = (-sc * cb - cc * sa * sb) * px + (-cc * ca) * py + \\\\\\n                  (-sc * sb + cc * sa * cb) * pz\\n        J[1, 2] = (cc * cb - sc * sa * sb) * px + (-sc * ca) * py + \\\\\\n                  (cc * sb + sc * sa * cb) * pz\\n        J[2, 2] = 0\\n\\n        J[0, 3:6] = 0\\n        J[1, 3:6] = 0\\n        J[2, 3:6] = 0\\n        J[0, 3], J[1, 4], J[2, 5] = 1, 1, 1\\n        # This Jacobian depends on x (it\\'s not constant): return 0\\n        return 0\\n\\n    cdef void _get_identity_parameters(self, double[:] theta) noexcept nogil:\\n        r\"\"\" Parameter values corresponding to the identity\\n        Sets in theta the parameter values corresponding to the identity\\n        transform\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (6,)\\n            buffer to write the parameters of the 3D rigid transform\\n            theta[0] : rotation about the x axis\\n            theta[1] : rotation about the y axis\\n            theta[2] : rotation about the z axis\\n            theta[3] : translation along the x axis\\n            theta[4] : translation along the y axis\\n            theta[5] : translation along the z axis\\n        \"\"\"\\n        theta[:6] = 0', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='Parameters\\n        ----------\\n        theta : array, shape (6,)\\n            buffer to write the parameters of the 3D rigid transform\\n            theta[0] : rotation about the x axis\\n            theta[1] : rotation about the y axis\\n            theta[2] : rotation about the z axis\\n            theta[3] : translation along the x axis\\n            theta[4] : translation along the y axis\\n            theta[5] : translation along the z axis\\n        \"\"\"\\n        theta[:6] = 0\\n\\n    cdef void _param_to_matrix(self, double[:] theta, double[:, :] R) noexcept nogil:\\n        r\"\"\" Matrix associated with the 3D rigid transform\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (6,)\\n            the parameters of the 3D rigid transform\\n            theta[0] : rotation about the x axis\\n            theta[1] : rotation about the y axis\\n            theta[2] : rotation about the z axis\\n            theta[3] : translation along the x axis\\n            theta[4] : translation along the y axis\\n            theta[5] : translation along the z axis\\n        R : array, shape (4, 4)\\n            buffer in which to write the rigid matrix\\n        \"\"\"\\n        cdef:\\n            double sa = sin(theta[0])\\n            double ca = cos(theta[0])\\n            double sb = sin(theta[1])\\n            double cb = cos(theta[1])\\n            double sc = sin(theta[2])\\n            double cc = cos(theta[2])\\n            double dx = theta[3]\\n            double dy = theta[4]\\n            double dz = theta[5]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='R[0,0], R[0,1], R[0,2] = cc*cb-sc*sa*sb, -sc*ca, cc*sb+sc*sa*cb\\n        R[1,0], R[1,1], R[1,2] = sc*cb+cc*sa*sb, cc*ca, sc*sb-cc*sa*cb\\n        R[2,0], R[2,1], R[2,2] = -ca*sb, sa, ca*cb\\n        R[3,0], R[3,1], R[3,2] = 0, 0, 0\\n        R[0,3] = dx\\n        R[1,3] = dy\\n        R[2,3] = dz\\n        R[3,3] = 1\\n\\n\\ncdef class RigidIsoScalingTransform2D(Transform):\\n    def __init__(self):\\n        \"\"\" Rigid isoscaling transform in 2D.\\n\\n        (rotation + translation + scaling)\\n\\n        The parameter vector theta of length 4 is interpreted as follows:\\n        theta[0] : rotation angle\\n        theta[1] : translation along the x axis\\n        theta[2] : translation along the y axis\\n        theta[3] : isotropic scaling\\n        \"\"\"\\n        self.dim = 2\\n        self.number_of_parameters = 4\\n\\n    cdef int _jacobian(self, double[:] theta, double[:] x,\\n                       double[:, :] J) noexcept nogil:\\n        r\"\"\" Jacobian matrix of a 2D rigid isoscaling transform.\\n\\n        The transformation (rotation + translation + isoscaling) is given by:\\n\\n        T(x,y) = (T1(x,y), T2(x,y)) =\\n            (sx * x * cost -  sx * y * sint + dx, sy * x * sint + sy * y * cost + dy)\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (4,)\\n            the parameters of the 2D rigid isoscaling transform\\n            theta[0] : rotation angle (t)\\n            theta[1] : translation along the x axis (dx)\\n            theta[2] : translation along the y axis (dy)\\n            theta[3] : isotropic scaling (s)\\n        x : array, shape (2,)\\n            the point at which to compute the Jacobian\\n        J : array, shape (2, 4)\\n            the buffer in which to write the Jacobian', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='Returns\\n        -------\\n        is_constant : int\\n            always returns 0, indicating that the Jacobian is not\\n            constant (it depends on the value of x)\\n        \"\"\"\\n        cdef:\\n            double st = sin(theta[0])\\n            double ct = cos(theta[0])\\n            double px = x[0], py = x[1]\\n            double scale = theta[3]\\n\\n        J[0, 0], J[0, 1], J[0, 2] = -px * scale * st - py * scale * ct, 1, 0\\n        J[1, 0], J[1, 1], J[1, 2] = px * scale * ct - py * scale * st, 0, 1\\n\\n        J[0, 3] = px * ct - py * st\\n        J[1, 3] = px * st + py * ct\\n        # This Jacobian depends on x (it\\'s not constant): return 0\\n        return 0\\n\\n    cdef void _get_identity_parameters(self, double[:] theta) noexcept nogil:\\n        \"\"\" Parameter values corresponding to the identity\\n        Sets in theta the parameter values corresponding to the identity\\n        transform\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (4,)\\n            buffer to write the parameters of the 2D isoscaling rigid transform\\n            theta[0] : rotation angle\\n            theta[1] : translation along the x axis\\n            theta[2] : translation along the y axis\\n            theta[3] : isotropic scaling\\n        \"\"\"\\n        theta[:3] = 0\\n        theta[3] = 1\\n\\n    cdef void _param_to_matrix(self, double[:] theta, double[:, :] R) noexcept nogil:\\n        r\"\"\" Matrix associated with the 2D rigid isoscaling transform', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='cdef void _param_to_matrix(self, double[:] theta, double[:, :] R) noexcept nogil:\\n        r\"\"\" Matrix associated with the 2D rigid isoscaling transform\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (4,)\\n            the parameters of the 2D rigid isoscaling transform\\n            theta[0] : rotation angle\\n            theta[1] : translation along the x axis\\n            theta[2] : translation along the y axis\\n            theta[3] : isotropic scaling\\n        R : array, shape (3, 3)\\n            buffer in which to write the rigid isoscaling matrix\\n        \"\"\"\\n        cdef:\\n            double ct = cos(theta[0])\\n            double st = sin(theta[0])\\n        R[0, 0], R[0, 1], R[0, 2] = ct*theta[3], -st*theta[3], theta[1]\\n        R[1, 0], R[1, 1], R[1, 2] = st*theta[3], ct*theta[3], theta[2]\\n        R[2, 0], R[2, 1], R[2, 2] = 0, 0, 1\\n\\n\\ncdef class RigidIsoScalingTransform3D(Transform):\\n    def __init__(self):\\n        \"\"\"Rigid isoscaling transform in 3D.\\n\\n        (rotation + translation + scaling)\\n        The parameter vector theta of length 7 is interpreted as follows:\\n        theta[0] : rotation about the x axis\\n        theta[1] : rotation about the y axis\\n        theta[2] : rotation about the z axis\\n        theta[3] : translation along the x axis\\n        theta[4] : translation along the y axis\\n        theta[5] : translation along the z axis\\n        theta[6] : isotropic scaling\\n\\n        \"\"\"\\n        self.dim = 3\\n        self.number_of_parameters = 7\\n\\n    cdef int _jacobian(self, double[:] theta, double[:] x,\\n                       double[:, :] J) noexcept nogil:\\n        r\"\"\" Jacobian matrix of a 3D isoscaling rigid transform.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='\"\"\"\\n        self.dim = 3\\n        self.number_of_parameters = 7\\n\\n    cdef int _jacobian(self, double[:] theta, double[:] x,\\n                       double[:, :] J) noexcept nogil:\\n        r\"\"\" Jacobian matrix of a 3D isoscaling rigid transform.\\n\\n        (rotation + translation + scaling)\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (7,)\\n            the parameters of the 3D rigid isoscaling transform\\n            theta[0] : rotation about the x axis\\n            theta[1] : rotation about the y axis\\n            theta[2] : rotation about the z axis\\n            theta[3] : translation along the x axis\\n            theta[4] : translation along the y axis\\n            theta[5] : translation along the z axis\\n            theta[6] : isotropic scaling\\n        x : array, shape (3,)\\n            the point at which to compute the Jacobian\\n        J : array, shape (3, 7)\\n            the buffer in which to write the Jacobian\\n\\n        Returns\\n        -------\\n        is_constant : int\\n            always returns 0, indicating that the Jacobian is not\\n            constant (it depends on the value of x)\\n        \"\"\"\\n        cdef:\\n            double sa = sin(theta[0])\\n            double ca = cos(theta[0])\\n            double sb = sin(theta[1])\\n            double cb = cos(theta[1])\\n            double sc = sin(theta[2])\\n            double cc = cos(theta[2])\\n            double px = x[0], py = x[1], pz = x[2]\\n            double scale = theta[6]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='J[0, 0] = (-sc * ca * sb) * px * scale + (sc * sa) * py * scale + \\\\\\n                  (sc * ca * cb) * pz * scale\\n        J[1, 0] = (cc * ca * sb) * px * scale + (-cc * sa) * py * scale + \\\\\\n                  (-cc * ca * cb) * pz  * scale\\n        J[2, 0] = (sa * sb) * px * scale + ca * py * scale + \\\\\\n                  (-sa * cb) * pz * scale\\n\\n        J[0, 1] = (-cc * sb - sc * sa * cb) * px * scale + \\\\\\n                  (cc * cb - sc * sa * sb) * pz * scale\\n        J[1, 1] = (-sc * sb + cc * sa * cb) * px * scale + \\\\\\n                  (sc * cb + cc * sa * sb) * pz * scale\\n        J[2, 1] = (-ca * cb) * px * scale + (-ca * sb) * pz * scale\\n\\n        J[0, 2] = (-sc * cb - cc * sa * sb) * px * scale + \\\\\\n                  (-cc * ca) * py  * scale + \\\\\\n                  (-sc * sb + cc * sa * cb) * pz * scale\\n        J[1, 2] = (cc * cb - sc * sa * sb) * px * scale + \\\\\\n                  (-sc * ca) * py * scale + \\\\\\n                  (cc * sb + sc * sa * cb) * pz * scale\\n        J[2, 2] = 0\\n\\n        J[0, 3:6] = 0\\n        J[1, 3:6] = 0\\n        J[2, 3:6] = 0\\n        J[0, 3], J[1, 4], J[2, 5] = 1, 1, 1\\n        J[0, 6] = (cc*cb-sc*sa*sb) * px - sc*ca*py + (cc*sb+sc*sa*cb) * pz\\n        J[1, 6] = (sc*cb+cc*sa*sb) * px + cc*ca*py + (sc*sb-cc*sa*cb) * pz\\n        J[2, 6] = -ca*sb*px + sa*py + ca*cb*pz\\n        # This Jacobian depends on x (it\\'s not constant): return 0\\n        return 0\\n\\n    cdef void _get_identity_parameters(self, double[:] theta) noexcept nogil:\\n        \"\"\" Parameter values corresponding to the identity\\n\\n        Sets in theta the parameter values corresponding to the identity\\n        transform', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='cdef void _get_identity_parameters(self, double[:] theta) noexcept nogil:\\n        \"\"\" Parameter values corresponding to the identity\\n\\n        Sets in theta the parameter values corresponding to the identity\\n        transform\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (7,)\\n            buffer to write the parameters of the 3D rigid isoscaling transform\\n            theta[0] : rotation about the x axis\\n            theta[1] : rotation about the y axis\\n            theta[2] : rotation about the z axis\\n            theta[3] : translation along the x axis\\n            theta[4] : translation along the y axis\\n            theta[5] : translation along the z axis\\n            theta[6] : isotropic scaling\\n\\n        \"\"\"\\n        theta[:6] = 0\\n        theta[6] = 1\\n\\n    cdef void _param_to_matrix(self, double[:] theta, double[:, :] R) noexcept nogil:\\n        \"\"\" Matrix associated with the 3D rigid isoscaling transform', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='\"\"\"\\n        theta[:6] = 0\\n        theta[6] = 1\\n\\n    cdef void _param_to_matrix(self, double[:] theta, double[:, :] R) noexcept nogil:\\n        \"\"\" Matrix associated with the 3D rigid isoscaling transform\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (7,)\\n            the parameters of the 3D rigid isoscaling transform\\n            theta[0] : rotation about the x axis\\n            theta[1] : rotation about the y axis\\n            theta[2] : rotation about the z axis\\n            theta[3] : translation along the x axis\\n            theta[4] : translation along the y axis\\n            theta[5] : translation along the z axis\\n            theta[6] : isotropic scaling\\n        R : array, shape (4, 4)\\n            buffer in which to write the rigid isoscaling matrix\\n        \"\"\"\\n        cdef:\\n            double sa = sin(theta[0])\\n            double ca = cos(theta[0])\\n            double sb = sin(theta[1])\\n            double cb = cos(theta[1])\\n            double sc = sin(theta[2])\\n            double cc = cos(theta[2])\\n            double dx = theta[3]\\n            double dy = theta[4]\\n            double dz = theta[5]\\n            double sxyz = theta[6]\\n\\n        R[0,0], R[0,1], R[0,2] = (cc*cb-sc*sa*sb)*sxyz, -sc*ca*sxyz, (cc*sb+sc*sa*cb)*sxyz\\n        R[1,0], R[1,1], R[1,2] = (sc*cb+cc*sa*sb)*sxyz, cc*ca*sxyz, (sc*sb-cc*sa*cb)*sxyz\\n        R[2,0], R[2,1], R[2,2] = -ca*sb*sxyz, sa*sxyz, ca*cb*sxyz\\n        R[3,0], R[3,1], R[3,2] = 0, 0, 0\\n        R[0,3] = dx\\n        R[1,3] = dy\\n        R[2,3] = dz\\n        R[3,3] = 1\\n\\n\\ncdef class RigidScalingTransform2D(Transform):\\n    def __init__(self):\\n        \"\"\" Rigid scaling transform in 2D.\\n\\n        (rotation + translation + scaling)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='cdef class RigidScalingTransform2D(Transform):\\n    def __init__(self):\\n        \"\"\" Rigid scaling transform in 2D.\\n\\n        (rotation + translation + scaling)\\n\\n        The parameter vector theta of length 5 is interpreted as follows:\\n        theta[0] : rotation angle\\n        theta[1] : translation along the x axis\\n        theta[2] : translation along the y axis\\n        theta[3] : scaling along the x axis\\n        theta[4] : scaling along the y axis\\n        \"\"\"\\n        self.dim = 2\\n        self.number_of_parameters = 5\\n\\n    cdef int _jacobian(self, double[:] theta, double[:] x,\\n                       double[:, :] J) noexcept nogil:\\n        r\"\"\" Jacobian matrix of a 2D rigid scaling transform.\\n\\n        The transformation (rotation + translation + scaling) is given by:\\n\\n        T(x,y) = (T1(x,y), T2(x,y)) =\\n            (sx * x * cost -  sx * y * sint + dx, sy * x * sint + sy * y * cost + dy)\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (5,)\\n            the parameters of the 2D rigid isoscaling transform\\n            theta[0] : rotation angle (t)\\n            theta[1] : translation along the x axis (dx)\\n            theta[2] : translation along the y axis (dy)\\n            theta[3] : scaling along the x axis\\n            theta[4] : scaling along the y axis\\n        x : array, shape (2,)\\n            the point at which to compute the Jacobian\\n        J : array, shape (2, 5)\\n            the buffer in which to write the Jacobian', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='Returns\\n        -------\\n        is_constant : int\\n            always returns 0, indicating that the Jacobian is not\\n            constant (it depends on the value of x)\\n        \"\"\"\\n        cdef:\\n            double st = sin(theta[0])\\n            double ct = cos(theta[0])\\n            double px = x[0], py = x[1]\\n            double fx = theta[3]\\n            double fy = theta[4]\\n\\n        J[0, 0], J[0, 1], J[0, 2] = -px * fx * st - py * fx * ct, 1, 0\\n        J[1, 0], J[1, 1], J[1, 2] = px * fy * ct - py * fy * st, 0, 1\\n\\n        J[0, 3], J[0, 4] = px * ct - py * st, 0\\n        J[1, 3], J[1, 4] = 0, px * st + py * ct\\n        # This Jacobian depends on x (it\\'s not constant): return 0\\n        return 0\\n\\n    cdef void _get_identity_parameters(self, double[:] theta) noexcept nogil:\\n        \"\"\" Parameter values corresponding to the identity\\n        Sets in theta the parameter values corresponding to the identity\\n        transform\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (5,)\\n            buffer to write the parameters of the 2D scaling rigid transform\\n            theta[0] : rotation angle\\n            theta[1] : translation along the x axis\\n            theta[2] : translation along the y axis\\n            theta[3] : scaling along the x axis\\n            theta[4] : scaling along the y axis\\n        \"\"\"\\n        theta[:3] = 0\\n        theta[3], theta[4] = 1, 1\\n\\n    cdef void _param_to_matrix(self, double[:] theta, double[:, :] R) noexcept nogil:\\n        r\"\"\" Matrix associated with the 2D rigid scaling transform', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='cdef void _param_to_matrix(self, double[:] theta, double[:, :] R) noexcept nogil:\\n        r\"\"\" Matrix associated with the 2D rigid scaling transform\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (5,)\\n            the parameters of the 2D rigid scaling transform\\n            theta[0] : rotation angle\\n            theta[1] : translation along the x axis\\n            theta[2] : translation along the y axis\\n            theta[3] : scaling along the x axis\\n            theta[4] : scaling along the y axis\\n        R : array, shape (3, 3)\\n            buffer in which to write the rigid scaling matrix\\n        \"\"\"\\n        cdef:\\n            double ct = cos(theta[0])\\n            double st = sin(theta[0])\\n        R[0, 0], R[0, 1], R[0, 2] = ct*theta[3], -st*theta[3], theta[1]\\n        R[1, 0], R[1, 1], R[1, 2] = st*theta[4], ct*theta[4], theta[2]\\n        R[2, 0], R[2, 1], R[2, 2] = 0, 0, 1\\n\\n\\ncdef class RigidScalingTransform3D(Transform):\\n    def __init__(self):\\n        \"\"\"Rigid Scaling transform in 3D (rotation + translation + scaling).\\n\\n        The parameter vector theta of length 9 is interpreted as follows:\\n        theta[0] : rotation about the x axis\\n        theta[1] : rotation about the y axis\\n        theta[2] : rotation about the z axis\\n        theta[3] : translation along the x axis\\n        theta[4] : translation along the y axis\\n        theta[5] : translation along the z axis\\n        theta[6] : scaling in the x axis\\n        theta[7] : scaling in the y axis\\n        theta[8] : scaling in the z axis\\n\\n        \"\"\"\\n        self.dim = 3\\n        self.number_of_parameters = 9', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='\"\"\"\\n        self.dim = 3\\n        self.number_of_parameters = 9\\n\\n    cdef int _jacobian(self, double[:] theta, double[:] x,\\n                       double[:, :] J) noexcept nogil:\\n        \"\"\"Jacobian matrix of a 3D rigid scaling transform.\\n\\n        (rotation + translation + scaling)\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (9,)\\n            the parameters of the 3D rigid transform\\n            theta[0] : rotation about the x axis\\n            theta[1] : rotation about the y axis\\n            theta[2] : rotation about the z axis\\n            theta[3] : translation along the x axis\\n            theta[4] : translation along the y axis\\n            theta[5] : translation along the z axis\\n            theta[6] : scaling in the x axis\\n            theta[7] : scaling in the y axis\\n            theta[8] : scaling in the z axis\\n        x : array, shape (3,)\\n            the point at which to compute the Jacobian\\n        J : array, shape (3, 9)\\n            the buffer in which to write the Jacobian\\n\\n        Returns\\n        -------\\n        is_constant : int\\n            always returns 0, indicating that the Jacobian is not\\n            constant (it depends on the value of x)\\n        \"\"\"\\n        cdef:\\n            double sa = sin(theta[0])\\n            double ca = cos(theta[0])\\n            double sb = sin(theta[1])\\n            double cb = cos(theta[1])\\n            double sc = sin(theta[2])\\n            double cc = cos(theta[2])\\n            double px = x[0], py = x[1], pz = x[2]\\n            double fx = theta[6]\\n            double fy = theta[7]\\n            double fz = theta[8]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='J[0, 0] = (-sc * ca * sb) * px * fx + (sc * sa) * py * fx + \\\\\\n                  (sc * ca * cb) * pz * fx\\n        J[1, 0] = (cc * ca * sb) * px * fy + (-cc * sa) * py * fy + \\\\\\n                  (-cc * ca * cb) * pz * fy\\n        J[2, 0] = (sa * sb) * px * fz + ca * py * fz + (-sa * cb) * pz * fz\\n\\n        J[0, 1] = (-cc * sb - sc * sa * cb) * px * fx + \\\\\\n                  (cc * cb - sc * sa * sb) * pz * fx\\n        J[1, 1] = (-sc * sb + cc * sa * cb) * px * fy + \\\\\\n                  (sc * cb + cc * sa * sb) * pz * fy\\n        J[2, 1] = (-ca * cb) * px * fz + (-ca * sb) * pz * fz\\n\\n        J[0, 2] = (-sc * cb - cc * sa * sb) * px * fx + (-cc * ca) * py * fx + \\\\\\n                  (-sc * sb + cc * sa * cb) * pz * fx\\n        J[1, 2] = (cc * cb - sc * sa * sb) * px * fy + (-sc * ca) * py * fy + \\\\\\n                  (cc * sb + sc * sa * cb) * pz * fy\\n        J[2, 2] = 0\\n\\n        J[0, 3:6] = 0\\n        J[1, 3:6] = 0\\n        J[2, 3:6] = 0\\n        J[0, 3], J[1, 4], J[2, 5] = 1, 1, 1\\n        J[0, 6] = (cc*cb-sc*sa*sb) * px - sc*ca*py + (cc*sb+sc*sa*cb) * pz\\n        J[1, 6], J[2, 6] = 0, 0\\n        J[1, 7] = (sc*cb+cc*sa*sb) * px + cc*ca*py + (sc*sb-cc*sa*cb) * pz\\n        J[0, 7], J[2, 7] = 0, 0\\n        J[0, 8], J[1, 8] = 0, 0\\n        J[2, 8] = -ca*sb*px + sa*py + ca*cb*pz\\n        # This Jacobian depends on x (it\\'s not constant): return 0\\n        return 0\\n\\n    cdef void _get_identity_parameters(self, double[:] theta) noexcept nogil:\\n        r\"\"\" Parameter values corresponding to the identity\\n        Sets in theta the parameter values corresponding to the identity\\n        transform', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='cdef void _get_identity_parameters(self, double[:] theta) noexcept nogil:\\n        r\"\"\" Parameter values corresponding to the identity\\n        Sets in theta the parameter values corresponding to the identity\\n        transform\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (9,)\\n            buffer to write the parameters of the 3D rigid scaling transform\\n            theta[0] : rotation about the x axis\\n            theta[1] : rotation about the y axis\\n            theta[2] : rotation about the z axis\\n            theta[3] : translation along the x axis\\n            theta[4] : translation along the y axis\\n            theta[5] : translation along the z axis\\n            theta[6] : scaling in the x axis\\n            theta[7] : scaling in the y axis\\n            theta[8] : scaling in the z axis\\n        \"\"\"\\n        theta[:6] = 0\\n        theta[6:9] = 1\\n\\n    cdef void _param_to_matrix(self, double[:] theta, double[:, :] R) noexcept nogil:\\n        r\"\"\" Matrix associated with the 3D rigid scaling transform', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='cdef void _param_to_matrix(self, double[:] theta, double[:, :] R) noexcept nogil:\\n        r\"\"\" Matrix associated with the 3D rigid scaling transform\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (9,)\\n            the parameters of the 3D rigid scaling transform\\n            theta[0] : rotation about the x axis\\n            theta[1] : rotation about the y axis\\n            theta[2] : rotation about the z axis\\n            theta[3] : translation along the x axis\\n            theta[4] : translation along the y axis\\n            theta[5] : translation along the z axis\\n            theta[6] : scaling in the x axis\\n            theta[7] : scaling in the y axis\\n            theta[8] : scaling in the z axis\\n        R : array, shape (4, 4)\\n            buffer in which to write the rigid matrix\\n        \"\"\"\\n        cdef:\\n            double sa = sin(theta[0])\\n            double ca = cos(theta[0])\\n            double sb = sin(theta[1])\\n            double cb = cos(theta[1])\\n            double sc = sin(theta[2])\\n            double cc = cos(theta[2])\\n            double dx = theta[3]\\n            double dy = theta[4]\\n            double dz = theta[5]\\n            double fx = theta[6]\\n            double fy = theta[7]\\n            double fz = theta[8]\\n\\n        R[0,0], R[0,1], R[0,2] = (cc*cb-sc*sa*sb)*fx, -sc*ca*fx, (cc*sb+sc*sa*cb)*fx\\n        R[1,0], R[1,1], R[1,2] = (sc*cb+cc*sa*sb)*fy, cc*ca*fy, (sc*sb-cc*sa*cb)*fy\\n        R[2,0], R[2,1], R[2,2] = -ca*sb*fz, sa*fz, ca*cb*fz\\n        R[3,0], R[3,1], R[3,2] = 0, 0, 0\\n        R[0,3] = dx\\n        R[1,3] = dy\\n        R[2,3] = dz\\n        R[3,3] = 1', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='R[0,0], R[0,1], R[0,2] = (cc*cb-sc*sa*sb)*fx, -sc*ca*fx, (cc*sb+sc*sa*cb)*fx\\n        R[1,0], R[1,1], R[1,2] = (sc*cb+cc*sa*sb)*fy, cc*ca*fy, (sc*sb-cc*sa*cb)*fy\\n        R[2,0], R[2,1], R[2,2] = -ca*sb*fz, sa*fz, ca*cb*fz\\n        R[3,0], R[3,1], R[3,2] = 0, 0, 0\\n        R[0,3] = dx\\n        R[1,3] = dy\\n        R[2,3] = dz\\n        R[3,3] = 1\\n\\n\\ncdef class ScalingTransform2D(Transform):\\n    def __init__(self):\\n        r\"\"\" Scaling transform in 2D\\n        \"\"\"\\n        self.dim = 2\\n        self.number_of_parameters = 1\\n\\n    cdef int _jacobian(self, double[:] theta, double[:] x,\\n                       double[:, :] J) noexcept nogil:\\n        r\"\"\" Jacobian matrix of the isotropic 2D scale transform\\n        The transformation is given by:\\n\\n        T(x) = (s*x0, s*x1)\\n\\n        The derivative w.r.t. s is T\\'(x) = [x0, x1]\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (1,)\\n            the scale factor (the Jacobian does not depend on the scale factor,\\n            but we receive the buffer to make it consistent with other Jacobian\\n            functions)\\n        x : array, shape (2,)\\n            the point at which to compute the Jacobian\\n        J : array, shape (2, 1)\\n            the buffer in which to write the Jacobian\\n\\n        Returns\\n        -------\\n        is_constant : int\\n            always returns 0, indicating that the Jacobian is not\\n            constant (it depends on the value of x)\\n        \"\"\"\\n        J[0, 0], J[1, 0] = x[0], x[1]\\n        # This Jacobian depends on x (it\\'s not constant): return 0\\n        return 0', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='Returns\\n        -------\\n        is_constant : int\\n            always returns 0, indicating that the Jacobian is not\\n            constant (it depends on the value of x)\\n        \"\"\"\\n        J[0, 0], J[1, 0] = x[0], x[1]\\n        # This Jacobian depends on x (it\\'s not constant): return 0\\n        return 0\\n\\n    cdef void _get_identity_parameters(self, double[:] theta) noexcept nogil:\\n        r\"\"\" Parameter values corresponding to the identity\\n        Sets in theta the parameter values corresponding to the identity\\n        transform\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (1,)\\n            buffer to write the parameters of the 2D scale transform\\n        \"\"\"\\n        theta[0] = 1\\n\\n    cdef void _param_to_matrix(self, double[:] theta, double[:, :] R) noexcept nogil:\\n        r\"\"\" Matrix associated with the 2D (isotropic) scaling transform\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (1,)\\n            the scale factor\\n        R : array, shape (3, 3)\\n            the buffer in which to write the scaling matrix\\n        \"\"\"\\n        R[0, 0], R[0, 1], R[0, 2] = theta[0], 0, 0\\n        R[1, 0], R[1, 1], R[1, 2] = 0, theta[0], 0\\n        R[2, 0], R[2, 1], R[2, 2] = 0, 0, 1\\n\\n\\ncdef class ScalingTransform3D(Transform):\\n    def __init__(self):\\n        r\"\"\" Scaling transform in 3D\\n        \"\"\"\\n        self.dim = 3\\n        self.number_of_parameters = 1\\n\\n    cdef int _jacobian(self, double[:] theta, double[:] x,\\n                       double[:, :] J) noexcept nogil:\\n        r\"\"\" Jacobian matrix of the isotropic 3D scale transform\\n        The transformation is given by:\\n\\n        T(x) = (s*x0, s*x1, s*x2)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='cdef class ScalingTransform3D(Transform):\\n    def __init__(self):\\n        r\"\"\" Scaling transform in 3D\\n        \"\"\"\\n        self.dim = 3\\n        self.number_of_parameters = 1\\n\\n    cdef int _jacobian(self, double[:] theta, double[:] x,\\n                       double[:, :] J) noexcept nogil:\\n        r\"\"\" Jacobian matrix of the isotropic 3D scale transform\\n        The transformation is given by:\\n\\n        T(x) = (s*x0, s*x1, s*x2)\\n\\n        The derivative w.r.t. s is T\\'(x) = [x0, x1, x2]\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (1,)\\n            the scale factor (the Jacobian does not depend on the scale factor,\\n            but we receive the buffer to make it consistent with other Jacobian\\n            functions)\\n        x : array, shape (3,)\\n            the point at which to compute the Jacobian\\n        J : array, shape (3, 1)\\n            the buffer in which to write the Jacobian\\n\\n        Returns\\n        -------\\n        is_constant : int\\n            always returns 0, indicating that the Jacobian is not\\n            constant (it depends on the value of x)\\n        \"\"\"\\n        J[0, 0], J[1, 0], J[2, 0]= x[0], x[1], x[2]\\n        # This Jacobian depends on x (it\\'s not constant): return 0\\n        return 0\\n\\n    cdef void _get_identity_parameters(self, double[:] theta) noexcept nogil:\\n        r\"\"\" Parameter values corresponding to the identity\\n        Sets in theta the parameter values corresponding to the identity\\n        transform\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (1,)\\n            buffer to write the parameters of the 3D scale transform\\n        \"\"\"\\n        theta[0] = 1', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='cdef void _get_identity_parameters(self, double[:] theta) noexcept nogil:\\n        r\"\"\" Parameter values corresponding to the identity\\n        Sets in theta the parameter values corresponding to the identity\\n        transform\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (1,)\\n            buffer to write the parameters of the 3D scale transform\\n        \"\"\"\\n        theta[0] = 1\\n\\n    cdef void _param_to_matrix(self, double[:] theta, double[:, :] R) noexcept nogil:\\n        r\"\"\" Matrix associated with the 3D (isotropic) scaling transform\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (1,)\\n            the scale factor\\n        R : array, shape (4, 4)\\n            the buffer in which to write the scaling matrix\\n        \"\"\"\\n        R[0, 0], R[0, 1], R[0, 2], R[0, 3] = theta[0], 0, 0, 0\\n        R[1, 0], R[1, 1], R[1, 2], R[1, 3] = 0, theta[0], 0, 0\\n        R[2, 0], R[2, 1], R[2, 2], R[2, 3] = 0, 0, theta[0], 0\\n        R[3, 0], R[3, 1], R[3, 2], R[3, 3] = 0, 0, 0, 1\\n\\n\\ncdef class AffineTransform2D(Transform):\\n    def __init__(self):\\n        r\"\"\" Affine transform in 2D\\n        \"\"\"\\n        self.dim = 2\\n        self.number_of_parameters = 6\\n\\n    cdef int _jacobian(self, double[:] theta, double[:] x,\\n                       double[:, :] J) noexcept nogil:\\n        r\"\"\" Jacobian matrix of the 2D affine transform\\n        The transformation is given by:\\n\\n        T(x) = |a0, a1, a2 |   |x0|   | T1(x) |   |a0*x0 + a1*x1 + a2|\\n               |a3, a4, a5 | * |x1| = | T2(x) | = |a3*x0 + a4*x1 + a5|\\n                               | 1|\\n\\n        The derivatives w.r.t. each parameter are given by', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='cdef int _jacobian(self, double[:] theta, double[:] x,\\n                       double[:, :] J) noexcept nogil:\\n        r\"\"\" Jacobian matrix of the 2D affine transform\\n        The transformation is given by:\\n\\n        T(x) = |a0, a1, a2 |   |x0|   | T1(x) |   |a0*x0 + a1*x1 + a2|\\n               |a3, a4, a5 | * |x1| = | T2(x) | = |a3*x0 + a4*x1 + a5|\\n                               | 1|\\n\\n        The derivatives w.r.t. each parameter are given by\\n\\n        T\\'(x) = [[x0,  0], #derivatives of [T1, T2] w.r.t a0\\n                 [x1,  0], #derivatives of [T1, T2] w.r.t a1\\n                 [ 1,  0], #derivatives of [T1, T2] w.r.t a2\\n                 [ 0, x0], #derivatives of [T1, T2] w.r.t a3\\n                 [ 0, x1], #derivatives of [T1, T2] w.r.t a4\\n                 [ 0,  1]] #derivatives of [T1, T2, T3] w.r.t a5\\n\\n        The Jacobian matrix is the transpose of the above matrix.\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (6,)\\n            the parameters of the 2D affine transform\\n        x : array, shape (2,)\\n            the point at which to compute the Jacobian\\n        J : array, shape (2, 6)\\n            the buffer in which to write the Jacobian\\n\\n        Returns\\n        -------\\n        is_constant : int\\n            always returns 0, indicating that the Jacobian is not\\n            constant (it depends on the value of x)\\n        \"\"\"\\n        J[0, :6] = 0\\n        J[1, :6] = 0\\n\\n        J[0, :2] = x[:2]\\n        J[0, 2] = 1\\n        J[1, 3:5] = x[:2]\\n        J[1, 5] = 1\\n        # This Jacobian depends on x (it\\'s not constant): return 0\\n        return 0', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='Returns\\n        -------\\n        is_constant : int\\n            always returns 0, indicating that the Jacobian is not\\n            constant (it depends on the value of x)\\n        \"\"\"\\n        J[0, :6] = 0\\n        J[1, :6] = 0\\n\\n        J[0, :2] = x[:2]\\n        J[0, 2] = 1\\n        J[1, 3:5] = x[:2]\\n        J[1, 5] = 1\\n        # This Jacobian depends on x (it\\'s not constant): return 0\\n        return 0\\n\\n    cdef void _get_identity_parameters(self, double[:] theta) noexcept nogil:\\n        r\"\"\" Parameter values corresponding to the identity\\n        Sets in theta the parameter values corresponding to the identity\\n        transform\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (6,)\\n            buffer to write the parameters of the 2D affine transform\\n        \"\"\"\\n        theta[0], theta[1], theta[2] = 1, 0, 0\\n        theta[3], theta[4], theta[5] = 0, 1, 0\\n\\n    cdef void _param_to_matrix(self, double[:] theta, double[:, :] R) noexcept nogil:\\n        r\"\"\" Matrix associated with a general 2D affine transform\\n\\n        The transformation is given by the matrix:\\n\\n        A = [[a0, a1, a2],\\n             [a3, a4, a5],\\n             [ 0,  0,  1]]\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (6,)\\n            the parameters of the 2D affine transform\\n        R : array, shape (3,3)\\n            the buffer in which to write the matrix\\n        \"\"\"\\n        R[0, 0], R[0, 1], R[0, 2] = theta[0], theta[1], theta[2]\\n        R[1, 0], R[1, 1], R[1, 2] = theta[3], theta[4], theta[5]\\n        R[2, 0], R[2, 1], R[2, 2] = 0, 0, 1', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='A = [[a0, a1, a2],\\n             [a3, a4, a5],\\n             [ 0,  0,  1]]\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (6,)\\n            the parameters of the 2D affine transform\\n        R : array, shape (3,3)\\n            the buffer in which to write the matrix\\n        \"\"\"\\n        R[0, 0], R[0, 1], R[0, 2] = theta[0], theta[1], theta[2]\\n        R[1, 0], R[1, 1], R[1, 2] = theta[3], theta[4], theta[5]\\n        R[2, 0], R[2, 1], R[2, 2] = 0, 0, 1\\n\\n\\ncdef class AffineTransform3D(Transform):\\n    def __init__(self):\\n        r\"\"\" Affine transform in 3D\\n        \"\"\"\\n        self.dim = 3\\n        self.number_of_parameters = 12\\n\\n    cdef int _jacobian(self, double[:] theta, double[:] x,\\n                       double[:, :] J) noexcept nogil:\\n        r\"\"\" Jacobian matrix of the 3D affine transform\\n        The transformation is given by:\\n\\n        T(x)= |a0, a1, a2,  a3 |  |x0|  | T1(x) |  |a0*x0 + a1*x1 + a2*x2 + a3|\\n              |a4, a5, a6,  a7 |* |x1|= | T2(x) |= |a4*x0 + a5*x1 + a6*x2 + a7|\\n              |a8, a9, a10, a11|  |x2|  | T3(x) |  |a8*x0 + a9*x1 + a10*x2+a11|\\n                                  | 1|\\n\\n        The derivatives w.r.t. each parameter are given by', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content=\"T(x)= |a0, a1, a2,  a3 |  |x0|  | T1(x) |  |a0*x0 + a1*x1 + a2*x2 + a3|\\n              |a4, a5, a6,  a7 |* |x1|= | T2(x) |= |a4*x0 + a5*x1 + a6*x2 + a7|\\n              |a8, a9, a10, a11|  |x2|  | T3(x) |  |a8*x0 + a9*x1 + a10*x2+a11|\\n                                  | 1|\\n\\n        The derivatives w.r.t. each parameter are given by\\n\\n        T'(x) = [[x0,  0,  0], #derivatives of [T1, T2, T3] w.r.t a0\\n                 [x1,  0,  0], #derivatives of [T1, T2, T3] w.r.t a1\\n                 [x2,  0,  0], #derivatives of [T1, T2, T3] w.r.t a2\\n                 [ 1,  0,  0], #derivatives of [T1, T2, T3] w.r.t a3\\n                 [ 0, x0,  0], #derivatives of [T1, T2, T3] w.r.t a4\\n                 [ 0, x1,  0], #derivatives of [T1, T2, T3] w.r.t a5\\n                 [ 0, x2,  0], #derivatives of [T1, T2, T3] w.r.t a6\\n                 [ 0,  1,  0], #derivatives of [T1, T2, T3] w.r.t a7\\n                 [ 0,  0, x0], #derivatives of [T1, T2, T3] w.r.t a8\\n                 [ 0,  0, x1], #derivatives of [T1, T2, T3] w.r.t a9\\n                 [ 0,  0, x2], #derivatives of [T1, T2, T3] w.r.t a10\\n                 [ 0,  0,  1]] #derivatives of [T1, T2, T3] w.r.t a11\\n\\n        The Jacobian matrix is the transpose of the above matrix.\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (12,)\\n            the parameters of the 3D affine transform\\n        x : array, shape (3,)\\n            the point at which to compute the Jacobian\\n        J : array, shape (3, 12)\\n            the buffer in which to write the Jacobian\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='The Jacobian matrix is the transpose of the above matrix.\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (12,)\\n            the parameters of the 3D affine transform\\n        x : array, shape (3,)\\n            the point at which to compute the Jacobian\\n        J : array, shape (3, 12)\\n            the buffer in which to write the Jacobian\\n\\n        Returns\\n        -------\\n        is_constant : int\\n            always returns 0, indicating that the Jacobian is not\\n            constant (it depends on the value of x)\\n        \"\"\"\\n        cdef:\\n            cnp.npy_intp j\\n\\n        for j in range(3):\\n            J[j, :12] = 0\\n        J[0, :3] = x[:3]\\n        J[0, 3] = 1\\n        J[1, 4:7] = x[:3]\\n        J[1, 7] = 1\\n        J[2, 8:11] = x[:3]\\n        J[2, 11] = 1\\n        # This Jacobian depends on x (it\\'s not constant): return 0\\n        return 0\\n\\n    cdef void _get_identity_parameters(self, double[:] theta) noexcept nogil:\\n        r\"\"\" Parameter values corresponding to the identity\\n        Sets in theta the parameter values corresponding to the identity\\n        transform\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (12,)\\n            buffer to write the parameters of the 3D affine transform\\n        \"\"\"\\n        theta[0], theta[1], theta[2], theta[3] = 1, 0, 0, 0\\n        theta[4], theta[5], theta[6], theta[7] = 0, 1, 0, 0\\n        theta[8], theta[9], theta[10], theta[11] = 0, 0, 1, 0\\n\\n    cdef void _param_to_matrix(self, double[:] theta, double[:, :] R) noexcept nogil:\\n        r\"\"\" Matrix associated with a general 3D affine transform\\n\\n        The transformation is given by the matrix:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='cdef void _param_to_matrix(self, double[:] theta, double[:, :] R) noexcept nogil:\\n        r\"\"\" Matrix associated with a general 3D affine transform\\n\\n        The transformation is given by the matrix:\\n\\n        A = [[a0, a1, a2, a3],\\n             [a4, a5, a6, a7],\\n             [a8, a9, a10, a11],\\n             [ 0,   0,   0,   1]]\\n\\n        Parameters\\n        ----------\\n        theta : array, shape (12,)\\n            the parameters of the 3D affine transform\\n        R : array, shape (4,4)\\n            the buffer in which to write the matrix\\n        \"\"\"\\n        R[0, 0], R[0, 1], R[0, 2] = theta[0], theta[1], theta[2]\\n        R[1, 0], R[1, 1], R[1, 2] = theta[4], theta[5], theta[6]\\n        R[2, 0], R[2, 1], R[2, 2] = theta[8], theta[9], theta[10]\\n        R[3, 0], R[3, 1], R[3, 2] = 0, 0, 0\\n        R[0, 3] = theta[3]\\n        R[1, 3] = theta[7]\\n        R[2, 3] = theta[11]\\n        R[3, 3] = 1', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content=\"regtransforms = dict()\\nregtransforms [('TRANSLATION', 2)] = TranslationTransform2D()\\nregtransforms [('TRANSLATION', 3)] = TranslationTransform3D()\\nregtransforms [('ROTATION', 2)] = RotationTransform2D()\\nregtransforms [('ROTATION', 3)] = RotationTransform3D()\\nregtransforms [('RIGID', 2)] = RigidTransform2D()\\nregtransforms [('RIGID', 3)] = RigidTransform3D()\\nregtransforms [('SCALING', 2)] = ScalingTransform2D()\\nregtransforms [('SCALING', 3)] = ScalingTransform3D()\\nregtransforms [('AFFINE', 2)] = AffineTransform2D()\\nregtransforms [('AFFINE', 3)] = AffineTransform3D()\\nregtransforms [('RIGIDSCALING', 2)] = RigidScalingTransform2D()\\nregtransforms [('RIGIDSCALING', 3)] = RigidScalingTransform3D()\\nregtransforms [('RIGIDISOSCALING', 2)] = RigidIsoScalingTransform2D()\\nregtransforms [('RIGIDISOSCALING', 3)] = RigidIsoScalingTransform3D()\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\transforms.pyx.txt'}),\n",
       " Document(page_content='#!python\\n#cython: boundscheck=False\\n#cython: wraparound=False\\n#cython: cdivision=True\\n\\nimport numpy as np\\ncimport numpy as cnp\\n\\nfrom dipy.align.fused_types cimport floating, number\\nfrom dipy.core.interpolation cimport (_interpolate_scalar_2d,\\n                                      _interpolate_scalar_3d,\\n                                      _interpolate_vector_2d,\\n                                      _interpolate_vector_3d,\\n                                      _interpolate_scalar_nn_2d,\\n                                      _interpolate_scalar_nn_3d)\\n\\n\\ncdef extern from \"dpy_math.h\" nogil:\\n    double floor(double)\\n    double sqrt(double)\\n    double cos(double)\\n    double atan2(double, double)\\n\\n\\ndef is_valid_affine(double[:, :] M, int dim):\\n    if M is None:\\n        return True\\n    if M.shape[0] < dim or M.shape[1] < dim + 1:\\n        return False\\n    if not np.all(np.isfinite(M)):\\n        return False\\n    return True\\n\\n\\ncdef void _compose_vector_fields_2d(floating[:, :, :] d1, floating[:, :, :] d2,\\n                                    double[:, :] premult_index,\\n                                    double[:, :] premult_disp,\\n                                    double time_scaling,\\n                                    floating[:, :, :] comp,\\n                                    double[:] stats) noexcept nogil:\\n    r\"\"\"Computes the composition of two 2D displacement fields\\n\\n    Computes the composition of the two 2-D displacements d1 and d2. The\\n    evaluation of d2 at non-lattice points is computed using tri-linear\\n    interpolation. The actual composition is computed as:\\n\\n    comp[i] = d1[i] + t * d2[ A * i + B * d1[i] ]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content=\"Computes the composition of the two 2-D displacements d1 and d2. The\\n    evaluation of d2 at non-lattice points is computed using tri-linear\\n    interpolation. The actual composition is computed as:\\n\\n    comp[i] = d1[i] + t * d2[ A * i + B * d1[i] ]\\n\\n    where t = time_scaling, A = premult_index and B=premult_disp and i denotes\\n    the voxel coordinates of a voxel in d1's grid. Using this parameters it is\\n    possible to compose vector fields with arbitrary discretization: let R and\\n    S be the voxel-to-space transformation associated to d1 and d2,\\n    respectively then the composition at a voxel with coordinates i in d1's\\n    grid is given by:\\n\\n    comp[i] = d1[i] + R*i + d2[Sinv*(R*i + d1[i])] - R*i\\n\\n    (the R*i terms cancel each other) where Sinv = S^{-1}\\n    we can then define A = Sinv * R and B = Sinv to compute the composition\\n    using this function.\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content=\"comp[i] = d1[i] + R*i + d2[Sinv*(R*i + d1[i])] - R*i\\n\\n    (the R*i terms cancel each other) where Sinv = S^{-1}\\n    we can then define A = Sinv * R and B = Sinv to compute the composition\\n    using this function.\\n\\n    Parameters\\n    ----------\\n    d1 : array, shape (R, C, 2)\\n        first displacement field to be applied. R, C are the number of rows\\n        and columns of the displacement field, respectively.\\n    d2 : array, shape (R', C', 2)\\n        second displacement field to be applied. R', C' are the number of rows\\n        and columns of the displacement field, respectively.\\n    premult_index : array, shape (3, 3)\\n        the matrix A in the explanation above\\n    premult_disp : array, shape (3, 3)\\n        the matrix B in the explanation above\\n    time_scaling : float\\n        this corresponds to the time scaling 't' in the above explanation\\n    comp : array, shape (R, C, 2), same dimension as d1\\n        on output, this array will contain the composition of the two fields\\n    stats : array, shape (3,)\\n        on output, this array will contain three statistics of the vector norms\\n        of the composition (maximum, mean, standard_deviation)\\n\\n    Returns\\n    -------\\n    comp : array, shape (R, C, 2), same dimension as d1\\n        on output, this array will contain the composition of the two fields\\n    stats : array, shape (3,)\\n        on output, this array will contain three statistics of the vector norms\\n        of the composition (maximum, mean, standard_deviation)\\n\\n    Notes\\n    -----\\n    If d1[r,c] lies outside the domain of d2, then comp[r,c] will contain a\\n    zero vector.\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    comp : array, shape (R, C, 2), same dimension as d1\\n        on output, this array will contain the composition of the two fields\\n    stats : array, shape (3,)\\n        on output, this array will contain three statistics of the vector norms\\n        of the composition (maximum, mean, standard_deviation)\\n\\n    Notes\\n    -----\\n    If d1[r,c] lies outside the domain of d2, then comp[r,c] will contain a\\n    zero vector.\\n\\n    Warning: it is possible to use the same array reference for d1 and comp to\\n    effectively update d1 to the composition of d1 and d2 because previously\\n    updated values from d1 are no longer used (this is done to save memory and\\n    time). However, using the same array for d2 and comp may not be the\\n    intended operation (see comment below).\\n\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp nr1 = d1.shape[0]\\n        cnp.npy_intp nc1 = d1.shape[1]\\n        cnp.npy_intp nr2 = d2.shape[0]\\n        cnp.npy_intp nc2 = d2.shape[1]\\n        int inside, cnt = 0\\n        double maxNorm = 0\\n        double meanNorm = 0\\n        double stdNorm = 0\\n        double nn\\n        cnp.npy_intp i, j\\n        double di, dj, dii, djj, diii, djjj\\n\\n    for i in range(nr1):\\n        for j in range(nc1):\\n\\n            # This is the only place we access d1[i, j]\\n            dii = d1[i, j, 0]\\n            djj = d1[i, j, 1]\\n\\n            if premult_disp is None:\\n                di = dii\\n                dj = djj\\n            else:\\n                di = _apply_affine_2d_x0(dii, djj, 0, premult_disp)\\n                dj = _apply_affine_2d_x1(dii, djj, 0, premult_disp)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='for i in range(nr1):\\n        for j in range(nc1):\\n\\n            # This is the only place we access d1[i, j]\\n            dii = d1[i, j, 0]\\n            djj = d1[i, j, 1]\\n\\n            if premult_disp is None:\\n                di = dii\\n                dj = djj\\n            else:\\n                di = _apply_affine_2d_x0(dii, djj, 0, premult_disp)\\n                dj = _apply_affine_2d_x1(dii, djj, 0, premult_disp)\\n\\n            if premult_index is None:\\n                diii = i\\n                djjj = j\\n            else:\\n                diii = _apply_affine_2d_x0(i, j, 1, premult_index)\\n                djjj = _apply_affine_2d_x1(i, j, 1, premult_index)\\n\\n            diii += di\\n            djjj += dj\\n\\n            # If d1 and comp are the same array, this will correctly update\\n            # d1[i,j], which will never be accessed again\\n            # If d2 and comp are the same array, then (diii, djjj) may be\\n            # in the neighborhood of a previously updated vector from d2,\\n            # which may be problematic\\n            inside = _interpolate_vector_2d[floating](d2, diii, djjj,\\n                                                      &comp[i, j, 0])\\n\\n            if inside == 1:\\n                comp[i, j, 0] = time_scaling * comp[i, j, 0] + dii\\n                comp[i, j, 1] = time_scaling * comp[i, j, 1] + djj\\n                nn = comp[i, j, 0] ** 2 + comp[i, j, 1] ** 2\\n                meanNorm += nn\\n                stdNorm += nn * nn\\n                cnt += 1\\n                if maxNorm < nn:\\n                    maxNorm = nn\\n            else:\\n                comp[i, j, 0] = 0\\n                comp[i, j, 1] = 0', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='if inside == 1:\\n                comp[i, j, 0] = time_scaling * comp[i, j, 0] + dii\\n                comp[i, j, 1] = time_scaling * comp[i, j, 1] + djj\\n                nn = comp[i, j, 0] ** 2 + comp[i, j, 1] ** 2\\n                meanNorm += nn\\n                stdNorm += nn * nn\\n                cnt += 1\\n                if maxNorm < nn:\\n                    maxNorm = nn\\n            else:\\n                comp[i, j, 0] = 0\\n                comp[i, j, 1] = 0\\n\\n    meanNorm /= cnt\\n    stats[0] = sqrt(maxNorm)\\n    stats[1] = sqrt(meanNorm)\\n    stats[2] = sqrt(stdNorm / cnt - meanNorm * meanNorm)\\n\\n\\ndef compose_vector_fields_2d(floating[:, :, :] d1, floating[:, :, :] d2,\\n                             double[:, :] premult_index,\\n                             double[:, :] premult_disp,\\n                             double time_scaling,\\n                             floating[:, :, :] comp):\\n    r\"\"\"Computes the composition of two 2D displacement fields\\n\\n    Computes the composition of the two 2-D displacements d1 and d2. The\\n    evaluation of d2 at non-lattice points is computed using tri-linear\\n    interpolation. The actual composition is computed as:\\n\\n    comp[i] = d1[i] + t * d2[ A * i + B * d1[i] ]\\n\\n    where t = time_scaling, A = premult_index and B=premult_disp and i denotes\\n    the voxel coordinates of a voxel in d1\\'s grid. Using this parameters it is\\n    possible to compose vector fields with arbitrary discretizations: let R and\\n    S be the voxel-to-space transformation associated to d1 and d2,\\n    respectively then the composition at a voxel with coordinates i in d1\\'s\\n    grid is given by:\\n\\n    comp[i] = d1[i] + R*i + d2[Sinv*(R*i + d1[i])] - R*i', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content=\"where t = time_scaling, A = premult_index and B=premult_disp and i denotes\\n    the voxel coordinates of a voxel in d1's grid. Using this parameters it is\\n    possible to compose vector fields with arbitrary discretizations: let R and\\n    S be the voxel-to-space transformation associated to d1 and d2,\\n    respectively then the composition at a voxel with coordinates i in d1's\\n    grid is given by:\\n\\n    comp[i] = d1[i] + R*i + d2[Sinv*(R*i + d1[i])] - R*i\\n\\n    (the R*i terms cancel each other) where Sinv = S^{-1}\\n    we can then define A = Sinv * R and B = Sinv to compute the composition\\n    using this function.\\n\\n    Parameters\\n    ----------\\n    d1 : array, shape (R, C, 2)\\n        first displacement field to be applied. R, C are the number of rows\\n        and columns of the displacement field, respectively.\\n    d2 : array, shape (R', C', 2)\\n        second displacement field to be applied. R', C' are the number of rows\\n        and columns of the displacement field, respectively.\\n    premult_index : array, shape (3, 3)\\n        the matrix A in the explanation above\\n    premult_disp : array, shape (3, 3)\\n        the matrix B in the explanation above\\n    time_scaling : float\\n        this corresponds to the time scaling 't' in the above explanation\\n    comp : array, shape (R, C, 2)\\n        the buffer to write the composition to. If None, the buffer is created\\n        internally\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    comp : array, shape (R, C, 2), same dimension as d1\\n        on output, this array will contain the composition of the two fields\\n    stats : array, shape (3,)\\n        on output, this array will contain three statistics of the vector norms\\n        of the composition (maximum, mean, standard_deviation)\\n    \"\"\"\\n    cdef:\\n        double[:] stats = np.zeros(shape=(3,), dtype=np.float64)\\n\\n    if comp is None:\\n        comp = np.zeros_like(d1)\\n\\n    if not is_valid_affine(premult_index, 2):\\n        raise ValueError(\"Invalid index multiplication matrix\")\\n    if not is_valid_affine(premult_disp, 2):\\n        raise ValueError(\"Invalid displacement multiplication matrix\")\\n\\n    _compose_vector_fields_2d[floating](d1, d2, premult_index, premult_disp,\\n                                        time_scaling, comp, stats)\\n    return np.asarray(comp), np.asarray(stats)\\n\\n\\ncdef void _compose_vector_fields_3d(floating[:, :, :, :] d1,\\n                                    floating[:, :, :, :] d2,\\n                                    double[:, :] premult_index,\\n                                    double[:, :] premult_disp,\\n                                    double t,\\n                                    floating[:, :, :, :] comp,\\n                                    double[:] stats) noexcept nogil:\\n    r\"\"\"Computes the composition of two 3D displacement fields\\n\\n    Computes the composition of the two 3-D displacements d1 and d2. The\\n    evaluation of d2 at non-lattice points is computed using tri-linear\\n    interpolation. The actual composition is computed as:\\n\\n    comp[i] = d1[i] + t * d2[ A * i + B * d1[i] ]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content=\"Computes the composition of the two 3-D displacements d1 and d2. The\\n    evaluation of d2 at non-lattice points is computed using tri-linear\\n    interpolation. The actual composition is computed as:\\n\\n    comp[i] = d1[i] + t * d2[ A * i + B * d1[i] ]\\n\\n    where t = time_scaling, A = premult_index and B=premult_disp and i denotes\\n    the voxel coordinates of a voxel in d1's grid. Using this parameters it is\\n    possible to compose vector fields with arbitrary discretization: let R and\\n    S be the voxel-to-space transformation associated to d1 and d2,\\n    respectively then the composition at a voxel with coordinates i in d1's\\n    grid is given by:\\n\\n    comp[i] = d1[i] + R*i + d2[Sinv*(R*i + d1[i])] - R*i\\n\\n    (the R*i terms cancel each other) where Sinv = S^{-1}\\n    we can then define A = Sinv * R and B = Sinv to compute the composition\\n    using this function.\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content=\"comp[i] = d1[i] + R*i + d2[Sinv*(R*i + d1[i])] - R*i\\n\\n    (the R*i terms cancel each other) where Sinv = S^{-1}\\n    we can then define A = Sinv * R and B = Sinv to compute the composition\\n    using this function.\\n\\n    Parameters\\n    ----------\\n    d1 : array, shape (S, R, C, 3)\\n        first displacement field to be applied. S, R, C are the number of\\n        slices, rows and columns of the displacement field, respectively.\\n    d2 : array, shape (S', R', C', 3)\\n        second displacement field to be applied. R', C' are the number of rows\\n        and columns of the displacement field, respectively.\\n    premult_index : array, shape (4, 4)\\n        the matrix A in the explanation above\\n    premult_disp : array, shape (4, 4)\\n        the matrix B in the explanation above\\n    time_scaling : float\\n        this corresponds to the time scaling 't' in the above explanation\\n    comp : array, shape (S, R, C, 3), same dimension as d1\\n        on output, this array will contain the composition of the two fields\\n    stats : array, shape (3,)\\n        on output, this array will contain three statistics of the vector norms\\n        of the composition (maximum, mean, standard_deviation)\\n\\n    Returns\\n    -------\\n    comp : array, shape (S, R, C, 3), same dimension as d1\\n        on output, this array will contain the composition of the two fields\\n    stats : array, shape (3,)\\n        on output, this array will contain three statistics of the vector norms\\n        of the composition (maximum, mean, standard_deviation)\\n\\n    Notes\\n    -----\\n    If d1[s,r,c] lies outside the domain of d2, then comp[s,r,c] will contain\\n    a zero vector.\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    comp : array, shape (S, R, C, 3), same dimension as d1\\n        on output, this array will contain the composition of the two fields\\n    stats : array, shape (3,)\\n        on output, this array will contain three statistics of the vector norms\\n        of the composition (maximum, mean, standard_deviation)\\n\\n    Notes\\n    -----\\n    If d1[s,r,c] lies outside the domain of d2, then comp[s,r,c] will contain\\n    a zero vector.\\n\\n    Warning: it is possible to use the same array reference for d1 and comp to\\n    effectively update d1 to the composition of d1 and d2 because previously\\n    updated values from d1 are no longer used (this is done to save memory and\\n    time). However, using the same array for d2 and comp may not be the\\n    intended operation (see comment below).\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp ns1 = d1.shape[0]\\n        cnp.npy_intp nr1 = d1.shape[1]\\n        cnp.npy_intp nc1 = d1.shape[2]\\n        cnp.npy_intp ns2 = d2.shape[0]\\n        cnp.npy_intp nr2 = d2.shape[1]\\n        cnp.npy_intp nc2 = d2.shape[2]\\n        int inside, cnt = 0\\n        double maxNorm = 0\\n        double meanNorm = 0\\n        double stdNorm = 0\\n        double nn\\n        cnp.npy_intp i, j, k\\n        double di, dj, dk, dii, djj, dkk, diii, djjj, dkkk\\n    for k in range(ns1):\\n        for i in range(nr1):\\n            for j in range(nc1):\\n\\n                # This is the only place we access d1[k, i, j]\\n                dkk = d1[k, i, j, 0]\\n                dii = d1[k, i, j, 1]\\n                djj = d1[k, i, j, 2]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='# This is the only place we access d1[k, i, j]\\n                dkk = d1[k, i, j, 0]\\n                dii = d1[k, i, j, 1]\\n                djj = d1[k, i, j, 2]\\n\\n                if premult_disp is None:\\n                    dk = dkk\\n                    di = dii\\n                    dj = djj\\n                else:\\n                    dk = _apply_affine_3d_x0(dkk, dii, djj, 0, premult_disp)\\n                    di = _apply_affine_3d_x1(dkk, dii, djj, 0, premult_disp)\\n                    dj = _apply_affine_3d_x2(dkk, dii, djj, 0, premult_disp)\\n\\n                if premult_index is None:\\n                    dkkk = k\\n                    diii = i\\n                    djjj = j\\n                else:\\n                    dkkk = _apply_affine_3d_x0(k, i, j, 1, premult_index)\\n                    diii = _apply_affine_3d_x1(k, i, j, 1, premult_index)\\n                    djjj = _apply_affine_3d_x2(k, i, j, 1, premult_index)\\n\\n                dkkk += dk\\n                diii += di\\n                djjj += dj\\n\\n                # If d1 and comp are the same array, this will correctly update\\n                # d1[k,i,j], which will never be accessed again\\n                # If d2 and comp are the same array, then (dkkk, diii, djjj)\\n                # may be in the neighborhood of a previously updated vector\\n                # from d2, which may be problematic\\n                inside = _interpolate_vector_3d[floating](d2, dkkk, diii, djjj,\\n                                                          &comp[k, i, j, 0])', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='if inside == 1:\\n                    comp[k, i, j, 0] = t * comp[k, i, j, 0] + dkk\\n                    comp[k, i, j, 1] = t * comp[k, i, j, 1] + dii\\n                    comp[k, i, j, 2] = t * comp[k, i, j, 2] + djj\\n                    nn = (comp[k, i, j, 0] ** 2 + comp[k, i, j, 1] ** 2 +\\n                          comp[k, i, j, 2]**2)\\n                    meanNorm += nn\\n                    stdNorm += nn * nn\\n                    cnt += 1\\n                    if maxNorm < nn:\\n                        maxNorm = nn\\n                else:\\n                    comp[k, i, j, 0] = 0\\n                    comp[k, i, j, 1] = 0\\n                    comp[k, i, j, 2] = 0\\n    meanNorm /= cnt\\n    stats[0] = sqrt(maxNorm)\\n    stats[1] = sqrt(meanNorm)\\n    stats[2] = sqrt(stdNorm / cnt - meanNorm * meanNorm)\\n\\n\\ndef compose_vector_fields_3d(floating[:, :, :, :] d1, floating[:, :, :, :] d2,\\n                             double[:, :] premult_index,\\n                             double[:, :] premult_disp,\\n                             double time_scaling,\\n                             floating[:, :, :, :] comp):\\n    r\"\"\"Computes the composition of two 3D displacement fields\\n\\n    Computes the composition of the two 3-D displacements d1 and d2. The\\n    evaluation of d2 at non-lattice points is computed using tri-linear\\n    interpolation. The actual composition is computed as:\\n\\n    comp[i] = d1[i] + t * d2[ A * i + B * d1[i] ]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content=\"Computes the composition of the two 3-D displacements d1 and d2. The\\n    evaluation of d2 at non-lattice points is computed using tri-linear\\n    interpolation. The actual composition is computed as:\\n\\n    comp[i] = d1[i] + t * d2[ A * i + B * d1[i] ]\\n\\n    where t = time_scaling, A = premult_index and B=premult_disp and i denotes\\n    the voxel coordinates of a voxel in d1's grid. Using this parameters it is\\n    possible to compose vector fields with arbitrary discretization: let R and\\n    S be the voxel-to-space transformation associated to d1 and d2,\\n    respectively then the composition at a voxel with coordinates i in d1's\\n    grid is given by:\\n\\n    comp[i] = d1[i] + R*i + d2[Sinv*(R*i + d1[i])] - R*i\\n\\n    (the R*i terms cancel each other) where Sinv = S^{-1}\\n    we can then define A = Sinv * R and B = Sinv to compute the composition\\n    using this function.\\n\\n    Parameters\\n    ----------\\n    d1 : array, shape (S, R, C, 3)\\n        first displacement field to be applied. S, R, C are the number of\\n        slices, rows and columns of the displacement field, respectively.\\n    d2 : array, shape (S', R', C', 3)\\n        second displacement field to be applied. R', C' are the number of rows\\n        and columns of the displacement field, respectively.\\n    premult_index : array, shape (4, 4)\\n        the matrix A in the explanation above\\n    premult_disp : array, shape (4, 4)\\n        the matrix B in the explanation above\\n    time_scaling : float\\n        this corresponds to the time scaling 't' in the above explanation\\n    comp : array, shape (S, R, C, 3), same dimension as d1\\n        the buffer to write the composition to. If None, the buffer will be\\n        created internally\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    comp : array, shape (S, R, C, 3), same dimension as d1\\n        on output, this array will contain the composition of the two fields\\n    stats : array, shape (3,)\\n        on output, this array will contain three statistics of the vector norms\\n        of the composition (maximum, mean, standard_deviation)\\n\\n    Notes\\n    -----\\n    If d1[s,r,c] lies outside the domain of d2, then comp[s,r,c] will contain\\n    a zero vector.\\n    \"\"\"\\n    cdef:\\n        double[:] stats = np.zeros(shape=(3,), dtype=np.float64)\\n\\n    if comp is None:\\n        comp = np.zeros_like(d1)\\n\\n    if not is_valid_affine(premult_index, 3):\\n        raise ValueError(\"Invalid index pre-multiplication matrix\")\\n    if not is_valid_affine(premult_disp, 3):\\n        raise ValueError(\"Invalid displacement pre-multiplication matrix\")\\n\\n    _compose_vector_fields_3d[floating](d1, d2, premult_index, premult_disp,\\n                                        time_scaling, comp, stats)\\n    return np.asarray(comp), np.asarray(stats)\\n\\n\\ndef invert_vector_field_fixed_point_2d(floating[:, :, :] d,\\n                                       double[:, :] d_world2grid,\\n                                       double[:] spacing,\\n                                       int max_iter, double tolerance,\\n                                       floating[:, :, :] start=None):\\n    r\"\"\"Computes the inverse of a 2D displacement fields\\n\\n    Computes the inverse of the given 2-D displacement field d using the\\n    fixed-point algorithm [1].\\n\\n    [1] Chen, M., Lu, W., Chen, Q., Ruchala, K. J., & Olivera, G. H. (2008).\\n        A simple fixed-point approach to invert a deformation field.\\n        Medical Physics, 35(1), 81. doi:10.1118/1.2816107', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='Computes the inverse of the given 2-D displacement field d using the\\n    fixed-point algorithm [1].\\n\\n    [1] Chen, M., Lu, W., Chen, Q., Ruchala, K. J., & Olivera, G. H. (2008).\\n        A simple fixed-point approach to invert a deformation field.\\n        Medical Physics, 35(1), 81. doi:10.1118/1.2816107\\n\\n    Parameters\\n    ----------\\n    d : array, shape (R, C, 2)\\n        the 2-D displacement field to be inverted\\n    d_world2grid : array, shape (3, 3)\\n        the space-to-grid transformation associated to the displacement field\\n        d (transforming physical space coordinates to voxel coordinates of the\\n        displacement field grid)\\n    spacing :array, shape (2,)\\n        the spacing between voxels (voxel size along each axis)\\n    max_iter : int\\n        maximum number of iterations to be performed\\n    tolerance : float\\n        maximum tolerated inversion error\\n    start : array, shape (R, C)\\n        an approximation to the inverse displacement field (if no approximation\\n        is available, None can be provided and the start displacement field\\n        will be zero)\\n\\n    Returns\\n    -------\\n    p : array, shape (R, C, 2)\\n        the inverse displacement field', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    p : array, shape (R, C, 2)\\n        the inverse displacement field\\n\\n    Notes\\n    -----\\n    We assume that the displacement field is an endomorphism so that the shape\\n    and voxel-to-space transformation of the inverse field\\'s discretization is\\n    the same as those of the input displacement field. The \\'inversion error\\' at\\n    iteration t is defined as the mean norm of the displacement vectors of the\\n    input displacement field composed with the inverse at iteration t.\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp nr = d.shape[0]\\n        cnp.npy_intp nc = d.shape[1]\\n        int iter_count, current, flag\\n        double difmag, mag, maxlen, step_factor\\n        double epsilon\\n        double error = 1 + tolerance\\n        double di, dj, dii, djj\\n        double sr = spacing[0], sc = spacing[1]\\n\\n    ftype = np.asarray(d).dtype\\n    cdef:\\n        double[:] stats = np.zeros(shape=(2,), dtype=np.float64)\\n        double[:] substats = np.empty(shape=(3,), dtype=np.float64)\\n        double[:, :] norms = np.zeros(shape=(nr, nc), dtype=np.float64)\\n        floating[:, :, :] p = np.zeros(shape=(nr, nc, 2), dtype=ftype)\\n        floating[:, :, :] q = np.zeros(shape=(nr, nc, 2), dtype=ftype)\\n\\n    if not is_valid_affine(d_world2grid, 2):\\n        raise ValueError(\"Invalid world-to-image transform\")\\n\\n    if start is not None:\\n        p[...] = start', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='if not is_valid_affine(d_world2grid, 2):\\n        raise ValueError(\"Invalid world-to-image transform\")\\n\\n    if start is not None:\\n        p[...] = start\\n\\n    with nogil:\\n        iter_count = 0\\n        while (iter_count < max_iter) and (tolerance < error):\\n            if iter_count == 0:\\n                epsilon = 0.75\\n            else:\\n                epsilon = 0.5\\n            _compose_vector_fields_2d[floating](p, d, None, d_world2grid,\\n                                                1.0, q, substats)\\n            difmag = 0\\n            error = 0\\n            for i in range(nr):\\n                for j in range(nc):\\n                    mag = sqrt((q[i, j, 0]/sr) ** 2 + (q[i, j, 1]/sc) ** 2)\\n                    norms[i, j] = mag\\n                    error += mag\\n                    if difmag < mag:\\n                        difmag = mag\\n            maxlen = difmag * epsilon\\n            for i in range(nr):\\n                for j in range(nc):\\n                    if norms[i, j] > maxlen:\\n                        step_factor = epsilon * maxlen / norms[i, j]\\n                    else:\\n                        step_factor = epsilon\\n                    p[i, j, 0] = p[i, j, 0] - step_factor * q[i, j, 0]\\n                    p[i, j, 1] = p[i, j, 1] - step_factor * q[i, j, 1]\\n            error /= (nr * nc)\\n            iter_count += 1\\n        stats[0] = substats[1]\\n        stats[1] = iter_count\\n    return np.asarray(p)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='def invert_vector_field_fixed_point_3d(floating[:, :, :, :] d,\\n                                       double[:, :] d_world2grid,\\n                                       double[:] spacing,\\n                                       int max_iter, double tol,\\n                                       floating[:, :, :, :] start=None):\\n    r\"\"\"Computes the inverse of a 3D displacement fields\\n\\n    Computes the inverse of the given 3-D displacement field d using the\\n    fixed-point algorithm [1].\\n\\n    [1] Chen, M., Lu, W., Chen, Q., Ruchala, K. J., & Olivera, G. H. (2008).\\n        A simple fixed-point approach to invert a deformation field.\\n        Medical Physics, 35(1), 81. doi:10.1118/1.2816107\\n\\n    Parameters\\n    ----------\\n    d : array, shape (S, R, C, 3)\\n        the 3-D displacement field to be inverted\\n    d_world2grid : array, shape (4, 4)\\n        the space-to-grid transformation associated to the displacement field\\n        d (transforming physical space coordinates to voxel coordinates of the\\n        displacement field grid)\\n    spacing :array, shape (3,)\\n        the spacing between voxels (voxel size along each axis)\\n    max_iter : int\\n        maximum number of iterations to be performed\\n    tol : float\\n        maximum tolerated inversion error\\n    start : array, shape (S, R, C)\\n        an approximation to the inverse displacement field (if no approximation\\n        is available, None can be provided and the start displacement field\\n        will be zero)\\n\\n    Returns\\n    -------\\n    p : array, shape (S, R, C, 3)\\n        the inverse displacement field', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    p : array, shape (S, R, C, 3)\\n        the inverse displacement field\\n\\n    Notes\\n    -----\\n    We assume that the displacement field is an endomorphism so that the shape\\n    and voxel-to-space transformation of the inverse field\\'s discretization is\\n    the same as those of the input displacement field. The \\'inversion error\\' at\\n    iteration t is defined as the mean norm of the displacement vectors of the\\n    input displacement field composed with the inverse at iteration t.\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp ns = d.shape[0]\\n        cnp.npy_intp nr = d.shape[1]\\n        cnp.npy_intp nc = d.shape[2]\\n        int iter_count, current\\n        double dkk, dii, djj, dk, di, dj\\n        double difmag, mag, maxlen, step_factor\\n        double epsilon = 0.5\\n        double error = 1 + tol\\n        double ss = spacing[0], sr = spacing[1], sc = spacing[2]\\n\\n    ftype = np.asarray(d).dtype\\n    cdef:\\n        double[:] stats = np.zeros(shape=(2,), dtype=np.float64)\\n        double[:] substats = np.zeros(shape=(3,), dtype=np.float64)\\n        double[:, :, :] norms = np.zeros(shape=(ns, nr, nc), dtype=np.float64)\\n        floating[:, :, :, :] p = np.zeros(shape=(ns, nr, nc, 3), dtype=ftype)\\n        floating[:, :, :, :] q = np.zeros(shape=(ns, nr, nc, 3), dtype=ftype)\\n\\n    if not is_valid_affine(d_world2grid, 3):\\n        raise ValueError(\"Invalid world-to-image transform\")\\n\\n    if start is not None:\\n        p[...] = start', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='with nogil:\\n        iter_count = 0\\n        difmag = 1\\n        while (0.1 < difmag) and (iter_count < max_iter) and (tol < error):\\n            if iter_count == 0:\\n                epsilon = 0.75\\n            else:\\n                epsilon = 0.5\\n            _compose_vector_fields_3d[floating](p, d, None, d_world2grid,\\n                                                1.0, q, substats)\\n            difmag = 0\\n            error = 0\\n            for k in range(ns):\\n                for i in range(nr):\\n                    for j in range(nc):\\n                        mag = sqrt((q[k, i, j, 0]/ss) ** 2 +\\n                                   (q[k, i, j, 1]/sr) ** 2 +\\n                                   (q[k, i, j, 2]/sc) ** 2)\\n                        norms[k, i, j] = mag\\n                        error += mag\\n                        if difmag < mag:\\n                            difmag = mag\\n            maxlen = difmag*epsilon\\n            for k in range(ns):\\n                for i in range(nr):\\n                    for j in range(nc):\\n                        if norms[k, i, j] > maxlen:\\n                            step_factor = epsilon * maxlen / norms[k, i, j]\\n                        else:\\n                            step_factor = epsilon\\n                        p[k, i, j, 0] = (p[k, i, j, 0] -\\n                                         step_factor * q[k, i, j, 0])\\n                        p[k, i, j, 1] = (p[k, i, j, 1] -\\n                                         step_factor * q[k, i, j, 1])\\n                        p[k, i, j, 2] = (p[k, i, j, 2] -\\n                                         step_factor * q[k, i, j, 2])\\n            error /= (ns * nr * nc)\\n            iter_count += 1\\n        stats[0] = error', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='p[k, i, j, 0] = (p[k, i, j, 0] -\\n                                         step_factor * q[k, i, j, 0])\\n                        p[k, i, j, 1] = (p[k, i, j, 1] -\\n                                         step_factor * q[k, i, j, 1])\\n                        p[k, i, j, 2] = (p[k, i, j, 2] -\\n                                         step_factor * q[k, i, j, 2])\\n            error /= (ns * nr * nc)\\n            iter_count += 1\\n        stats[0] = error\\n        stats[1] = iter_count\\n    return np.asarray(p)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='def simplify_warp_function_2d(floating[:, :, :] d,\\n                              double[:, :] affine_idx_in,\\n                              double[:, :] affine_idx_out,\\n                              double[:, :] affine_disp,\\n                              int[:] out_shape):\\n    r\"\"\"\\n    Simplifies a nonlinear warping function combined with an affine transform\\n\\n    Modifies the given deformation field by incorporating into it a\\n    an affine transformation and voxel-to-space transforms associated to\\n    the discretization of its domain and codomain.\\n    The resulting transformation may be regarded as operating on the\\n    image spaces given by the domain and codomain discretization.\\n    More precisely, the resulting transform is of the form:\\n\\n    (1) T[i] = W * d[U * i] + V * i\\n\\n    Where U = affine_idx_in, V = affine_idx_out, W = affine_disp.\\n\\n    Parameters\\n    ----------\\n    d : array, shape (R\\', C\\', 2)\\n        the non-linear part of the transformation (displacement field)\\n    affine_idx_in : array, shape (3, 3)\\n        the matrix U in eq. (1) above\\n    affine_idx_out : array, shape (3, 3)\\n        the matrix V in eq. (1) above\\n    affine_disp : array, shape (3, 3)\\n        the matrix W in eq. (1) above\\n    out_shape : array, shape (2,)\\n        the number of rows and columns of the sampling grid\\n\\n    Returns\\n    -------\\n    out : array, shape = out_shape\\n        the deformation field `out` associated with `T` in eq. (1) such that:\\n        T[i] = i + out[i]\\n\\n    Notes\\n    -----\\n    Both the direct and inverse transforms of a DiffeomorphicMap can be written\\n    in this form:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content=\"Returns\\n    -------\\n    out : array, shape = out_shape\\n        the deformation field `out` associated with `T` in eq. (1) such that:\\n        T[i] = i + out[i]\\n\\n    Notes\\n    -----\\n    Both the direct and inverse transforms of a DiffeomorphicMap can be written\\n    in this form:\\n\\n    Direct:  Let D be the voxel-to-space transform of the domain's\\n             discretization, P be the pre-align matrix, Rinv the space-to-voxel\\n             transform of the reference grid (the grid the displacement field\\n             is defined on) and Cinv be the space-to-voxel transform of the\\n             codomain's discretization. Then, for each i in the domain's grid,\\n             the direct transform is given by\\n\\n             (2) T[i] = Cinv * d[Rinv * P * D * i] + Cinv * P * D * i\\n\\n             and we identify U = Rinv * P * D, V = Cinv * P * D, W = Cinv\\n\\n    Inverse: Let C be the voxel-to-space transform of the codomain's\\n             discretization, Pinv be the inverse of the pre-align matrix, Rinv\\n             the space-to-voxel transform of the reference grid (the grid the\\n             displacement field is defined on) and Dinv be the space-to-voxel\\n             transform of the domain's discretization. Then, for each j in the\\n             codomain's grid, the inverse transform is given by\\n\\n             (3) Tinv[j] = Dinv * Pinv * d[Rinv * C * j] + Dinv * Pinv * C * j\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='(3) Tinv[j] = Dinv * Pinv * d[Rinv * C * j] + Dinv * Pinv * C * j\\n\\n             and we identify U = Rinv * C, V = Dinv * Pinv * C, W = Dinv * Pinv\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp nrows = out_shape[0]\\n        cnp.npy_intp ncols = out_shape[1]\\n        cnp.npy_intp i, j\\n        double di, dj, dii, djj\\n        floating[:] tmp = np.zeros((2,), dtype=np.asarray(d).dtype)\\n        floating[:, :, :] out = np.zeros(shape=(nrows, ncols, 2),\\n                                         dtype=np.asarray(d).dtype)\\n\\n    if not is_valid_affine(affine_idx_in, 2):\\n        raise ValueError(\"Invalid inner index multiplication matrix\")\\n    if not is_valid_affine(affine_idx_out, 2):\\n        raise ValueError(\"Invalid outer index multiplication matrix\")\\n    if not is_valid_affine(affine_disp, 2):\\n        raise ValueError(\"Invalid displacement multiplication matrix\")\\n\\n    with nogil:\\n\\n        for i in range(nrows):\\n            for j in range(ncols):\\n                # Apply inner index pre-multiplication\\n                if affine_idx_in is None:\\n                    dii = d[i, j, 0]\\n                    djj = d[i, j, 1]\\n                else:\\n                    di = _apply_affine_2d_x0(\\n                        i, j, 1, affine_idx_in)\\n                    dj = _apply_affine_2d_x1(\\n                        i, j, 1, affine_idx_in)\\n                    _interpolate_vector_2d[floating](d, di, dj, &tmp[0])\\n                    dii = tmp[0]\\n                    djj = tmp[1]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='# Apply displacement multiplication\\n                if affine_disp is not None:\\n                    di = _apply_affine_2d_x0(\\n                        dii, djj, 0, affine_disp)\\n                    dj = _apply_affine_2d_x1(\\n                        dii, djj, 0, affine_disp)\\n                else:\\n                    di = dii\\n                    dj = djj\\n\\n                # Apply outer index multiplication and add the displacements\\n                if affine_idx_out is not None:\\n                    out[i, j, 0] = di + _apply_affine_2d_x0(i, j, 1,\\n                                                            affine_idx_out) - i\\n                    out[i, j, 1] = dj + _apply_affine_2d_x1(i, j, 1,\\n                                                            affine_idx_out) - j\\n                else:\\n                    out[i, j, 0] = di\\n                    out[i, j, 1] = dj\\n    return np.asarray(out)\\n\\n\\ndef simplify_warp_function_3d(floating[:, :, :, :] d,\\n                              double[:, :] affine_idx_in,\\n                              double[:, :] affine_idx_out,\\n                              double[:, :] affine_disp,\\n                              int[:] out_shape):\\n    r\"\"\"\\n    Simplifies a nonlinear warping function combined with an affine transform\\n\\n    Modifies the given deformation field by incorporating into it\\n    an affine transformation and voxel-to-space transforms associated with\\n    the discretization of its domain and codomain.\\n\\n    The resulting transformation may be regarded as operating on the\\n    image spaces given by the domain and codomain discretization.\\n    More precisely, the resulting transform is of the form:\\n\\n    (1) T[i] = W * d[U * i] + V * i', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content=\"Modifies the given deformation field by incorporating into it\\n    an affine transformation and voxel-to-space transforms associated with\\n    the discretization of its domain and codomain.\\n\\n    The resulting transformation may be regarded as operating on the\\n    image spaces given by the domain and codomain discretization.\\n    More precisely, the resulting transform is of the form:\\n\\n    (1) T[i] = W * d[U * i] + V * i\\n\\n    Where U = affine_idx_in, V = affine_idx_out, W = affine_disp.\\n\\n    Parameters\\n    ----------\\n    d : array, shape (S', R', C', 3)\\n        the non-linear part of the transformation (displacement field)\\n    affine_idx_in : array, shape (4, 4)\\n        the matrix U in eq. (1) above\\n    affine_idx_out : array, shape (4, 4)\\n        the matrix V in eq. (1) above\\n    affine_disp : array, shape (4, 4)\\n        the matrix W in eq. (1) above\\n    out_shape : array, shape (3,)\\n        the number of slices, rows and columns of the sampling grid\\n\\n    Returns\\n    -------\\n    out : array, shape = out_shape\\n        the deformation field `out` associated with `T` in eq. (1) such that:\\n        T[i] = i + out[i]\\n\\n    Notes\\n    -----\\n    Both the direct and inverse transforms of a DiffeomorphicMap can be written\\n    in this form:\\n\\n    Direct:  Let D be the voxel-to-space transform of the domain's\\n             discretization, P be the pre-align matrix, Rinv the space-to-voxel\\n             transform of the reference grid (the grid the displacement field\\n             is defined on) and Cinv be the space-to-voxel transform of the\\n             codomain's discretization. Then, for each i in the domain's grid,\\n             the direct transform is given by\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='Direct:  Let D be the voxel-to-space transform of the domain\\'s\\n             discretization, P be the pre-align matrix, Rinv the space-to-voxel\\n             transform of the reference grid (the grid the displacement field\\n             is defined on) and Cinv be the space-to-voxel transform of the\\n             codomain\\'s discretization. Then, for each i in the domain\\'s grid,\\n             the direct transform is given by\\n\\n             (2) T[i] = Cinv * d[Rinv * P * D * i] + Cinv * P * D * i\\n\\n             and we identify U = Rinv * P * D, V = Cinv * P * D, W = Cinv\\n\\n    Inverse: Let C be the voxel-to-space transform of the codomain\\'s\\n             discretization, Pinv be the inverse of the pre-align matrix, Rinv\\n             the space-to-voxel transform of the reference grid (the grid the\\n             displacement field is defined on) and Dinv be the space-to-voxel\\n             transform of the domain\\'s discretization. Then, for each j in the\\n             codomain\\'s grid, the inverse transform is given by\\n\\n             (3) Tinv[j] = Dinv * Pinv * d[Rinv * C * j] + Dinv * Pinv * C * j\\n\\n             and we identify U = Rinv * C, V = Dinv * Pinv * C, W = Dinv * Pinv\\n\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp nslices = out_shape[0]\\n        cnp.npy_intp nrows = out_shape[1]\\n        cnp.npy_intp ncols = out_shape[2]\\n        cnp.npy_intp i, j, k, inside\\n        double di, dj, dk, dii, djj, dkk\\n        floating[:] tmp = np.zeros((3,), dtype=np.asarray(d).dtype)\\n        floating[:, :, :, :] out = np.zeros(shape=(nslices, nrows, ncols, 3),\\n                                            dtype=np.asarray(d).dtype)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='\"\"\"\\n    cdef:\\n        cnp.npy_intp nslices = out_shape[0]\\n        cnp.npy_intp nrows = out_shape[1]\\n        cnp.npy_intp ncols = out_shape[2]\\n        cnp.npy_intp i, j, k, inside\\n        double di, dj, dk, dii, djj, dkk\\n        floating[:] tmp = np.zeros((3,), dtype=np.asarray(d).dtype)\\n        floating[:, :, :, :] out = np.zeros(shape=(nslices, nrows, ncols, 3),\\n                                            dtype=np.asarray(d).dtype)\\n\\n    if not is_valid_affine(affine_idx_in, 3):\\n        raise ValueError(\"Invalid inner index multiplication matrix\")\\n    if not is_valid_affine(affine_idx_out, 3):\\n        raise ValueError(\"Invalid outer index multiplication matrix\")\\n    if not is_valid_affine(affine_disp, 3):\\n        raise ValueError(\"Invalid displacement multiplication matrix\")\\n\\n    with nogil:\\n\\n        for k in range(nslices):\\n            for i in range(nrows):\\n                for j in range(ncols):\\n                    if affine_idx_in is None:\\n                        dkk = d[k, i, j, 0]\\n                        dii = d[k, i, j, 1]\\n                        djj = d[k, i, j, 2]\\n                    else:\\n                        dk = _apply_affine_3d_x0(\\n                            k, i, j, 1, affine_idx_in)\\n                        di = _apply_affine_3d_x1(\\n                            k, i, j, 1, affine_idx_in)\\n                        dj = _apply_affine_3d_x2(\\n                            k, i, j, 1, affine_idx_in)\\n                        inside = _interpolate_vector_3d[floating](d, dk, di,\\n                                                                  dj, &tmp[0])\\n                        dkk = tmp[0]\\n                        dii = tmp[1]\\n                        djj = tmp[2]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='if affine_disp is not None:\\n                        dk = _apply_affine_3d_x0(\\n                            dkk, dii, djj, 0, affine_disp)\\n                        di = _apply_affine_3d_x1(\\n                            dkk, dii, djj, 0, affine_disp)\\n                        dj = _apply_affine_3d_x2(\\n                            dkk, dii, djj, 0, affine_disp)\\n                    else:\\n                        dk = dkk\\n                        di = dii\\n                        dj = djj\\n\\n                    if affine_idx_out is not None:\\n                        out[k, i, j, 0] = dk +\\\\\\n                            _apply_affine_3d_x0(k, i, j, 1, affine_idx_out) - k\\n                        out[k, i, j, 1] = di +\\\\\\n                            _apply_affine_3d_x1(k, i, j, 1, affine_idx_out) - i\\n                        out[k, i, j, 2] = dj +\\\\\\n                            _apply_affine_3d_x2(k, i, j, 1, affine_idx_out) - j\\n                    else:\\n                        out[k, i, j, 0] = dk\\n                        out[k, i, j, 1] = di\\n                        out[k, i, j, 2] = dj\\n    return np.asarray(out)\\n\\n\\ndef reorient_vector_field_2d(floating[:, :, :] d,\\n                             double[:, :] affine):\\n    r\"\"\"Linearly transforms all vectors of a 2D displacement field\\n\\n    Modifies the input displacement field by multiplying each displacement\\n    vector by the given matrix. Note that the elements of the displacement\\n    field are vectors, not points, so their last homogeneous coordinate is\\n    zero, not one, and therefore the translation component of the affine\\n    transform will not have any effect on them.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='Modifies the input displacement field by multiplying each displacement\\n    vector by the given matrix. Note that the elements of the displacement\\n    field are vectors, not points, so their last homogeneous coordinate is\\n    zero, not one, and therefore the translation component of the affine\\n    transform will not have any effect on them.\\n\\n    Parameters\\n    ----------\\n    d : array, shape (R, C, 2)\\n        the displacement field to be re-oriented\\n    affine: array, shape (3, 3)\\n        the matrix to be applied\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp nrows = d.shape[0]\\n        cnp.npy_intp ncols = d.shape[1]\\n        cnp.npy_intp i, j\\n        double di, dj\\n\\n    if not is_valid_affine(affine, 2):\\n        raise ValueError(\"Invalid affine transform matrix\")\\n\\n    if affine is None:\\n        return\\n\\n    with nogil:\\n        for i in range(nrows):\\n            for j in range(ncols):\\n                di = d[i, j, 0]\\n                dj = d[i, j, 1]\\n                d[i, j, 0] = _apply_affine_2d_x0(di, dj, 0, affine)\\n                d[i, j, 1] = _apply_affine_2d_x1(di, dj, 0, affine)\\n\\n\\ndef reorient_vector_field_3d(floating[:, :, :, :] d,\\n                             double[:, :] affine):\\n    r\"\"\"Linearly transforms all vectors of a 3D displacement field\\n\\n    Modifies the input displacement field by multiplying each displacement\\n    vector by the given matrix. Note that the elements of the displacement\\n    field are vectors, not points, so their last homogeneous coordinate is\\n    zero, not one, and therefore the translation component of the affine\\n    transform will not have any effect on them.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='Modifies the input displacement field by multiplying each displacement\\n    vector by the given matrix. Note that the elements of the displacement\\n    field are vectors, not points, so their last homogeneous coordinate is\\n    zero, not one, and therefore the translation component of the affine\\n    transform will not have any effect on them.\\n\\n    Parameters\\n    ----------\\n    d : array, shape (S, R, C, 3)\\n        the displacement field to be re-oriented\\n    affine : array, shape (4, 4)\\n        the matrix to be applied\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp nslices = d.shape[0]\\n        cnp.npy_intp nrows = d.shape[1]\\n        cnp.npy_intp ncols = d.shape[2]\\n        cnp.npy_intp i, j, k\\n        double di, dj, dk\\n\\n    if not is_valid_affine(affine, 3):\\n        raise ValueError(\"Invalid affine transform matrix\")\\n\\n    if affine is None:\\n        return\\n\\n    with nogil:\\n        for k in range(nslices):\\n            for i in range(nrows):\\n                for j in range(ncols):\\n                    dk = d[k, i, j, 0]\\n                    di = d[k, i, j, 1]\\n                    dj = d[k, i, j, 2]\\n                    d[k, i, j, 0] = _apply_affine_3d_x0(dk, di, dj, 0, affine)\\n                    d[k, i, j, 1] = _apply_affine_3d_x1(dk, di, dj, 0, affine)\\n                    d[k, i, j, 2] = _apply_affine_3d_x2(dk, di, dj, 0, affine)\\n\\n\\ndef downsample_scalar_field_3d(floating[:, :, :] field):\\n    r\"\"\"Down-samples the input volume by a factor of 2\\n\\n    Down-samples the input volume by a factor of 2. The value at each voxel\\n    of the resulting volume is the average of its surrounding voxels in the\\n    original volume.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='def downsample_scalar_field_3d(floating[:, :, :] field):\\n    r\"\"\"Down-samples the input volume by a factor of 2\\n\\n    Down-samples the input volume by a factor of 2. The value at each voxel\\n    of the resulting volume is the average of its surrounding voxels in the\\n    original volume.\\n\\n    Parameters\\n    ----------\\n    field : array, shape (S, R, C)\\n        the volume to be down-sampled\\n\\n    Returns\\n    -------\\n    down : array, shape (S\\', R\\', C\\')\\n        the down-sampled displacement field, where S\\' = ceil(S/2),\\n        R\\'= ceil(R/2), C\\'=ceil(C/2)\\n    \"\"\"\\n    ftype = np.asarray(field).dtype\\n    cdef:\\n        cnp.npy_intp ns = field.shape[0]\\n        cnp.npy_intp nr = field.shape[1]\\n        cnp.npy_intp nc = field.shape[2]\\n        cnp.npy_intp nns = (ns + 1) // 2\\n        cnp.npy_intp nnr = (nr + 1) // 2\\n        cnp.npy_intp nnc = (nc + 1) // 2\\n        cnp.npy_intp i, j, k, ii, jj, kk\\n        floating[:, :, :] down = np.zeros((nns, nnr, nnc), dtype=ftype)\\n        int[:, :, :] cnt = np.zeros((nns, nnr, nnc), dtype=np.int32)\\n\\n    with nogil:\\n        for k in range(ns):\\n            for i in range(nr):\\n                for j in range(nc):\\n                    kk = k // 2\\n                    ii = i // 2\\n                    jj = j // 2\\n                    down[kk, ii, jj] += field[k, i, j]\\n                    cnt[kk, ii, jj] += 1\\n        for k in range(nns):\\n            for i in range(nnr):\\n                for j in range(nnc):\\n                    if cnt[k, i, j] > 0:\\n                        down[k, i, j] /= cnt[k, i, j]\\n    return np.asarray(down)\\n\\n\\ndef downsample_displacement_field_3d(floating[:, :, :, :] field):\\n    r\"\"\"Down-samples the input 3D vector field by a factor of 2', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='def downsample_displacement_field_3d(floating[:, :, :, :] field):\\n    r\"\"\"Down-samples the input 3D vector field by a factor of 2\\n\\n    Down-samples the input vector field by a factor of 2. This operation\\n    is equivalent to dividing the input image into 2x2x2 cubes and averaging\\n    the 8 vectors. The resulting field consists of these average vectors.\\n\\n    Parameters\\n    ----------\\n    field : array, shape (S, R, C)\\n        the vector field to be down-sampled\\n\\n    Returns\\n    -------\\n    down : array, shape (S\\', R\\', C\\')\\n        the down-sampled displacement field, where S\\' = ceil(S/2),\\n        R\\'= ceil(R/2), C\\'=ceil(C/2)\\n    \"\"\"\\n    ftype = np.asarray(field).dtype\\n    cdef:\\n        cnp.npy_intp ns = field.shape[0]\\n        cnp.npy_intp nr = field.shape[1]\\n        cnp.npy_intp nc = field.shape[2]\\n        cnp.npy_intp nns = (ns + 1) // 2\\n        cnp.npy_intp nnr = (nr + 1) // 2\\n        cnp.npy_intp nnc = (nc + 1) // 2\\n        cnp.npy_intp i, j, k, ii, jj, kk\\n        floating[:, :, :, :] down = np.zeros((nns, nnr, nnc, 3), dtype=ftype)\\n        int[:, :, :] cnt = np.zeros((nns, nnr, nnc), dtype=np.int32)\\n\\n    with nogil:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='with nogil:\\n\\n        for k in range(ns):\\n            for i in range(nr):\\n                for j in range(nc):\\n                    kk = k // 2\\n                    ii = i // 2\\n                    jj = j // 2\\n                    down[kk, ii, jj, 0] += field[k, i, j, 0]\\n                    down[kk, ii, jj, 1] += field[k, i, j, 1]\\n                    down[kk, ii, jj, 2] += field[k, i, j, 2]\\n                    cnt[kk, ii, jj] += 1\\n        for k in range(nns):\\n            for i in range(nnr):\\n                for j in range(nnc):\\n                    if cnt[k, i, j] > 0:\\n                        down[k, i, j, 0] /= cnt[k, i, j]\\n                        down[k, i, j, 1] /= cnt[k, i, j]\\n                        down[k, i, j, 2] /= cnt[k, i, j]\\n    return np.asarray(down)\\n\\n\\ndef downsample_scalar_field_2d(floating[:, :] field):\\n    r\"\"\"Down-samples the input 2D image by a factor of 2\\n\\n    Down-samples the input image by a factor of 2. The value at each pixel\\n    of the resulting image is the average of its surrounding pixels in the\\n    original image.\\n\\n    Parameters\\n    ----------\\n    field : array, shape (R, C)\\n        the image to be down-sampled\\n\\n    Returns\\n    -------\\n    down : array, shape (R\\', C\\')\\n        the down-sampled displacement field, where R\\'= ceil(R/2), C\\'=ceil(C/2)\\n    \"\"\"\\n    ftype = np.asarray(field).dtype\\n    cdef:\\n        cnp.npy_intp nr = field.shape[0]\\n        cnp.npy_intp nc = field.shape[1]\\n        cnp.npy_intp nnr = (nr + 1) // 2\\n        cnp.npy_intp nnc = (nc + 1) // 2\\n        cnp.npy_intp i, j, ii, jj\\n        floating[:, :] down = np.zeros(shape=(nnr, nnc), dtype=ftype)\\n        int[:, :] cnt = np.zeros(shape=(nnr, nnc), dtype=np.int32)\\n    with nogil:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='for i in range(nr):\\n            for j in range(nc):\\n                ii = i // 2\\n                jj = j // 2\\n                down[ii, jj] += field[i, j]\\n                cnt[ii, jj] += 1\\n        for i in range(nnr):\\n            for j in range(nnc):\\n                if cnt[i, j] > 0:\\n                    down[i, j] /= cnt[i, j]\\n    return np.asarray(down)\\n\\n\\ndef downsample_displacement_field_2d(floating[:, :, :] field):\\n    r\"\"\"Down-samples the 2D input vector field by a factor of 2\\n    Down-samples the input vector field by a factor of 2. The value at each\\n    pixel of the resulting field is the average of its surrounding pixels in\\n    the original field.\\n\\n    Parameters\\n    ----------\\n    field : array, shape (R, C)\\n        the vector field to be down-sampled\\n\\n    Returns\\n    -------\\n    down : array, shape (R\\', C\\')\\n        the down-sampled displacement field, where R\\'= ceil(R/2), C\\'=ceil(C/2),\\n    \"\"\"\\n    ftype = np.asarray(field).dtype\\n    cdef:\\n        cnp.npy_intp nr = field.shape[0]\\n        cnp.npy_intp nc = field.shape[1]\\n        cnp.npy_intp nnr = (nr + 1) // 2\\n        cnp.npy_intp nnc = (nc + 1) // 2\\n        cnp.npy_intp i, j, ii, jj\\n        floating[:, :, :] down = np.zeros((nnr, nnc, 2), dtype=ftype)\\n        int[:, :] cnt = np.zeros((nnr, nnc), dtype=np.int32)\\n\\n    with nogil:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='with nogil:\\n\\n        for i in range(nr):\\n            for j in range(nc):\\n                ii = i // 2\\n                jj = j // 2\\n                down[ii, jj, 0] += field[i, j, 0]\\n                down[ii, jj, 1] += field[i, j, 1]\\n                cnt[ii, jj] += 1\\n        for i in range(nnr):\\n            for j in range(nnc):\\n                if cnt[i, j] > 0:\\n                    down[i, j, 0] /= cnt[i, j]\\n                    down[i, j, 1] /= cnt[i, j]\\n    return np.asarray(down)\\n\\n\\ndef warp_coordinates_3d(points,  floating[:, :, :, :] d1,\\n                        double[:, :] in2world,\\n                        double[:, :] world2out,\\n                        double[:, :] field_world2grid):\\n    r\"\"\"\\n    Parameters\\n    ----------\\n    points : array, shape (n, 3)\\n    d1 : array, shape (S, R, C, 3)\\n    in2world : array, shape (4, 4)\\n    world2out : array, shape (4, 4)\\n    field_world2grid : array, shape (4, 4)\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp n = points.shape[0]\\n        cnp.npy_intp i\\n        double x, y, z, wx, wy, wz, gx, gy, gz\\n        double[:, :] out = np.zeros(shape=(n, 3), dtype=np.float64)\\n        double[:, :] _points = np.array(points, dtype=np.float64)\\n        double[:, :] in2grid\\n        int inside\\n        floating[:] tmp = np.zeros(shape=(3,), dtype=np.asarray(d1).dtype)\\n    # in2grid maps points to displacement\\'s grid\\n    if in2world is None:  # then points are already in world coordinates\\n        in2grid = field_world2grid\\n    elif field_world2grid is None:  # then the grid is in world coordinates\\n        in2grid = in2world\\n    else:\\n        in2grid = np.dot(field_world2grid, in2world)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content=\"with nogil:\\n        for i in range(n):\\n            x = _points[i, 0]\\n            y = _points[i, 1]\\n            z = _points[i, 2]\\n\\n            # Map points to world coordinates\\n            if in2world is not None:\\n                wx = _apply_affine_3d_x0(x, y, z, 1, in2world)\\n                wy = _apply_affine_3d_x1(x, y, z, 1, in2world)\\n                wz = _apply_affine_3d_x2(x, y, z, 1, in2world)\\n            else:\\n                wx = x\\n                wy = y\\n                wz = z\\n\\n            # Map points to deformation field's grid\\n            if in2grid is not None:\\n                gx = _apply_affine_3d_x0(x, y, z, 1, in2grid)\\n                gy = _apply_affine_3d_x1(x, y, z, 1, in2grid)\\n                gz = _apply_affine_3d_x2(x, y, z, 1, in2grid)\\n            else:\\n                gx = x\\n                gy = y\\n                gz = z\\n\\n            # Interpolate deformation field at (gx, gy, gz)\\n            inside = _interpolate_vector_3d[floating](d1, gx, gy, gz, &tmp[0])\\n\\n            # Warp input point\\n            wx += tmp[0]\\n            wy += tmp[1]\\n            wz += tmp[2]\\n\\n            # Map warped point to requested out coordinates\\n            if world2out is not None:\\n                out[i, 0] = _apply_affine_3d_x0(wx, wy, wz, 1, world2out)\\n                out[i, 1] = _apply_affine_3d_x1(wx, wy, wz, 1, world2out)\\n                out[i, 2] = _apply_affine_3d_x2(wx, wy, wz, 1, world2out)\\n            else:\\n                out[i, 0] = wx\\n                out[i, 1] = wy\\n                out[i, 2] = wz\\n    return np.asarray(out)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='# Map warped point to requested out coordinates\\n            if world2out is not None:\\n                out[i, 0] = _apply_affine_3d_x0(wx, wy, wz, 1, world2out)\\n                out[i, 1] = _apply_affine_3d_x1(wx, wy, wz, 1, world2out)\\n                out[i, 2] = _apply_affine_3d_x2(wx, wy, wz, 1, world2out)\\n            else:\\n                out[i, 0] = wx\\n                out[i, 1] = wy\\n                out[i, 2] = wz\\n    return np.asarray(out)\\n\\n\\ndef warp_coordinates_2d(points,  floating[:, :, :] d1,\\n                        double[:, :] in2world,\\n                        double[:, :] world2out,\\n                        double[:, :] field_world2grid):\\n    r\"\"\"\\n    Parameters\\n    ----------\\n    points : array, shape (n, 2)\\n    d1 : array, shape (S, R, C, 2)\\n    in2world : array, shape (3, 3)\\n    world2out : array, shape (3, 3)\\n    field_world2grid : array, shape (3, 3)\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp n = points.shape[0]\\n        cnp.npy_intp i\\n        double x, y, wx, wy, gx, gy\\n        double[:, :] out = np.zeros(shape=(n, 2), dtype=np.float64)\\n        double[:, :] _points = np.array(points, dtype=np.float64)\\n        double[:, :] in2grid\\n        int inside\\n        floating[:] tmp = np.zeros(shape=(2,), dtype=np.asarray(d1).dtype)\\n    # in2grid maps points to displacement\\'s grid\\n    if in2world is None:  # then points are already in world coordinates\\n        in2grid = field_world2grid\\n    elif field_world2grid is None:  # then the grid is in world coordinates\\n        in2grid = in2world\\n    else:\\n        in2grid = np.dot(field_world2grid, in2world)\\n    with nogil:\\n        for i in range(n):\\n            x = _points[i, 0]\\n            y = _points[i, 1]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='# Map points to world coordinates\\n            if in2world is not None:\\n                wx = _apply_affine_2d_x0(x, y, 1, in2world)\\n                wy = _apply_affine_2d_x1(x, y, 1, in2world)\\n            else:\\n                wx = x\\n                wy = y\\n\\n            # Map points to deformation field\\'s grid\\n            if in2grid is not None:\\n                gx = _apply_affine_2d_x0(x, y, 1, in2grid)\\n                gy = _apply_affine_2d_x1(x, y, 1, in2grid)\\n            else:\\n                gx = x\\n                gy = y\\n\\n            # Interpolate deformation field at (gx, gy, gz)\\n            inside = _interpolate_vector_2d[floating](d1, gx, gy, &tmp[0])\\n\\n            # Warp input point\\n            wx += tmp[0]\\n            wy += tmp[1]\\n\\n            # Map warped point to requested out coordinates\\n            if world2out is not None:\\n                out[i, 0] = _apply_affine_2d_x0(wx, wy, 1, world2out)\\n                out[i, 1] = _apply_affine_2d_x1(wx, wy, 1, world2out)\\n            else:\\n                out[i, 0] = wx\\n                out[i, 1] = wy\\n    return np.asarray(out)\\n\\n\\ndef warp_3d(floating[:, :, :] volume, floating[:, :, :, :] d1,\\n            double[:, :] affine_idx_in=None,\\n            double[:, :] affine_idx_out=None,\\n            double[:, :] affine_disp=None,\\n            int[:] out_shape=None):\\n    r\"\"\"Warps a 3D volume using trilinear interpolation\\n\\n    Deforms the input volume under the given transformation. The warped volume\\n    is computed using tri-linear interpolation and is given by:\\n\\n    (1) warped[i] = volume[ C * d1[A*i] + B*i ]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='def warp_3d(floating[:, :, :] volume, floating[:, :, :, :] d1,\\n            double[:, :] affine_idx_in=None,\\n            double[:, :] affine_idx_out=None,\\n            double[:, :] affine_disp=None,\\n            int[:] out_shape=None):\\n    r\"\"\"Warps a 3D volume using trilinear interpolation\\n\\n    Deforms the input volume under the given transformation. The warped volume\\n    is computed using tri-linear interpolation and is given by:\\n\\n    (1) warped[i] = volume[ C * d1[A*i] + B*i ]\\n\\n    where A = affine_idx_in, B = affine_idx_out, C = affine_disp and i denotes\\n    the discrete coordinates of a voxel in the sampling grid of\\n    shape = out_shape.\\n\\n    Parameters\\n    ----------\\n    volume : array, shape (S, R, C)\\n        the input volume to be transformed\\n    d1 : array, shape (S\\', R\\', C\\', 3)\\n        the displacement field driving the transformation\\n    affine_idx_in : array, shape (4, 4)\\n        the matrix A in eq. (1) above\\n    affine_idx_out : array, shape (4, 4)\\n        the matrix B in eq. (1) above\\n    affine_disp : array, shape (4, 4)\\n        the matrix C in eq. (1) above\\n    out_shape : array, shape (3,)\\n        the number of slices, rows and columns of the sampling grid\\n\\n    Returns\\n    -------\\n    warped : array, shape = out_shape\\n        the transformed volume\\n\\n    Notes\\n    -----\\n    To illustrate the use of this function, consider a displacement field d1\\n    with grid-to-space transformation R, a volume with grid-to-space\\n    transformation T and let\\'s say we want to sample the warped volume on a\\n    grid with grid-to-space transformation S (sampling grid). For each voxel\\n    in the sampling grid with discrete coordinates i, the warped volume is\\n    given by:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='Notes\\n    -----\\n    To illustrate the use of this function, consider a displacement field d1\\n    with grid-to-space transformation R, a volume with grid-to-space\\n    transformation T and let\\'s say we want to sample the warped volume on a\\n    grid with grid-to-space transformation S (sampling grid). For each voxel\\n    in the sampling grid with discrete coordinates i, the warped volume is\\n    given by:\\n\\n    (2) warped[i] = volume[Tinv * ( d1[Rinv * S * i] + S * i ) ]\\n\\n    where Tinv = T^{-1} and Rinv = R^{-1}. By identifying A = Rinv * S,\\n    B = Tinv * S, C = Tinv we can use this function to efficiently warp the\\n    input image.\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp nslices = volume.shape[0]\\n        cnp.npy_intp nrows = volume.shape[1]\\n        cnp.npy_intp ncols = volume.shape[2]\\n        cnp.npy_intp nsVol = volume.shape[0]\\n        cnp.npy_intp nrVol = volume.shape[1]\\n        cnp.npy_intp ncVol = volume.shape[2]\\n        cnp.npy_intp i, j, k\\n        int inside\\n        double dkk, dii, djj, dk, di, dj\\n\\n    if not is_valid_affine(affine_idx_in, 3):\\n        raise ValueError(\"Invalid inner index multiplication matrix\")\\n    if not is_valid_affine(affine_idx_out, 3):\\n        raise ValueError(\"Invalid outer index multiplication matrix\")\\n    if not is_valid_affine(affine_disp, 3):\\n        raise ValueError(\"Invalid displacement multiplication matrix\")\\n\\n    if out_shape is not None:\\n        nslices = out_shape[0]\\n        nrows = out_shape[1]\\n        ncols = out_shape[2]\\n    elif d1 is not None:\\n        nslices = d1.shape[0]\\n        nrows = d1.shape[1]\\n        ncols = d1.shape[2]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='if out_shape is not None:\\n        nslices = out_shape[0]\\n        nrows = out_shape[1]\\n        ncols = out_shape[2]\\n    elif d1 is not None:\\n        nslices = d1.shape[0]\\n        nrows = d1.shape[1]\\n        ncols = d1.shape[2]\\n\\n    cdef floating[:, :, :] warped = np.zeros(shape=(nslices, nrows, ncols),\\n                                             dtype=np.asarray(volume).dtype)\\n    cdef floating[:] tmp = np.zeros(shape=(3,), dtype=np.asarray(d1).dtype)\\n\\n    with nogil:\\n\\n        for k in range(nslices):\\n            for i in range(nrows):\\n                for j in range(ncols):\\n                    if affine_idx_in is None:\\n                        dkk = d1[k, i, j, 0]\\n                        dii = d1[k, i, j, 1]\\n                        djj = d1[k, i, j, 2]\\n                    else:\\n                        dk = _apply_affine_3d_x0(\\n                            k, i, j, 1, affine_idx_in)\\n                        di = _apply_affine_3d_x1(\\n                            k, i, j, 1, affine_idx_in)\\n                        dj = _apply_affine_3d_x2(\\n                            k, i, j, 1, affine_idx_in)\\n                        inside = _interpolate_vector_3d[floating](d1, dk, di,\\n                                                                  dj, &tmp[0])\\n                        dkk = tmp[0]\\n                        dii = tmp[1]\\n                        djj = tmp[2]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='if affine_disp is not None:\\n                        dk = _apply_affine_3d_x0(\\n                            dkk, dii, djj, 0, affine_disp)\\n                        di = _apply_affine_3d_x1(\\n                            dkk, dii, djj, 0, affine_disp)\\n                        dj = _apply_affine_3d_x2(\\n                            dkk, dii, djj, 0, affine_disp)\\n                    else:\\n                        dk = dkk\\n                        di = dii\\n                        dj = djj\\n\\n                    if affine_idx_out is not None:\\n                        dkk = dk + _apply_affine_3d_x0(k, i, j, 1,\\n                                                       affine_idx_out)\\n                        dii = di + _apply_affine_3d_x1(k, i, j, 1,\\n                                                       affine_idx_out)\\n                        djj = dj + _apply_affine_3d_x2(k, i, j, 1,\\n                                                       affine_idx_out)\\n                    else:\\n                        dkk = dk + k\\n                        dii = di + i\\n                        djj = dj + j\\n\\n                    inside = _interpolate_scalar_3d[floating](volume, dkk,\\n                                                              dii, djj,\\n                                                              &warped[k, i, j])\\n    return np.asarray(warped)\\n\\n\\ndef transform_3d_affine(floating[:, :, :] volume, int[:] ref_shape,\\n                        double[:, :] affine):\\n    r\"\"\"Transforms a 3D volume by an affine transform with trilinear interp.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='inside = _interpolate_scalar_3d[floating](volume, dkk,\\n                                                              dii, djj,\\n                                                              &warped[k, i, j])\\n    return np.asarray(warped)\\n\\n\\ndef transform_3d_affine(floating[:, :, :] volume, int[:] ref_shape,\\n                        double[:, :] affine):\\n    r\"\"\"Transforms a 3D volume by an affine transform with trilinear interp.\\n\\n    Deforms the input volume under the given affine transformation using\\n    tri-linear interpolation. The shape of the resulting transformation\\n    is given by ref_shape. If the affine matrix is None, it is taken as the\\n    identity.\\n\\n    Parameters\\n    ----------\\n    volume : array, shape (S, R, C)\\n        the input volume to be transformed\\n    ref_shape : array, shape (3,)\\n        the shape of the resulting volume\\n    affine : array, shape (4, 4)\\n        the affine transform to be applied\\n\\n    Returns\\n    -------\\n    out : array, shape (S\\', R\\', C\\')\\n        the transformed volume', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='Parameters\\n    ----------\\n    volume : array, shape (S, R, C)\\n        the input volume to be transformed\\n    ref_shape : array, shape (3,)\\n        the shape of the resulting volume\\n    affine : array, shape (4, 4)\\n        the affine transform to be applied\\n\\n    Returns\\n    -------\\n    out : array, shape (S\\', R\\', C\\')\\n        the transformed volume\\n\\n    Notes\\n    -----\\n    The reason it is necessary to provide the intended shape of the resulting\\n    volume is because the affine transformation is defined on all R^{3}\\n    but we must sample a finite lattice. Also the resulting shape may not be\\n    necessarily equal to the input shape, unless we are interested on\\n    endomorphisms only and not general diffeomorphisms.\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp nslices = ref_shape[0]\\n        cnp.npy_intp nrows = ref_shape[1]\\n        cnp.npy_intp ncols = ref_shape[2]\\n        cnp.npy_intp nsVol = volume.shape[0]\\n        cnp.npy_intp nrVol = volume.shape[1]\\n        cnp.npy_intp ncVol = volume.shape[2]\\n        cnp.npy_intp i, j, k, ii, jj, kk\\n        int inside\\n        double dkk, dii, djj, tmp0, tmp1\\n        double alpha, beta, gamma, calpha, cbeta, cgamma\\n        floating[:, :, :] out = np.zeros(shape=(nslices, nrows, ncols),\\n                                         dtype=np.asarray(volume).dtype)\\n\\n    if not is_valid_affine(affine, 3):\\n        raise ValueError(\"Invalid affine transform matrix\")\\n\\n    with nogil:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='if not is_valid_affine(affine, 3):\\n        raise ValueError(\"Invalid affine transform matrix\")\\n\\n    with nogil:\\n\\n        for k in range(nslices):\\n            for i in range(nrows):\\n                for j in range(ncols):\\n                    if affine is not None:\\n                        dkk = _apply_affine_3d_x0(k, i, j, 1, affine)\\n                        dii = _apply_affine_3d_x1(k, i, j, 1, affine)\\n                        djj = _apply_affine_3d_x2(k, i, j, 1, affine)\\n                    else:\\n                        dkk = k\\n                        dii = i\\n                        djj = j\\n                    inside = _interpolate_scalar_3d[floating](volume, dkk,\\n                        dii, djj, &out[k, i, j])\\n    return np.asarray(out)\\n\\n\\ndef warp_3d_nn(number[:, :, :] volume, floating[:, :, :, :] d1,\\n               double[:, :] affine_idx_in=None,\\n               double[:, :] affine_idx_out=None,\\n               double[:, :] affine_disp=None,\\n               int[:] out_shape=None):\\n    r\"\"\"Warps a 3D volume using using nearest-neighbor interpolation\\n\\n    Deforms the input volume under the given transformation. The warped volume\\n    is computed using nearest-neighbor interpolation and is given by:\\n\\n    (1) warped[i] = volume[ C * d1[A*i] + B*i ]\\n\\n    where A = affine_idx_in, B = affine_idx_out, C = affine_disp and i denotes\\n    the discrete coordinates of a voxel in the sampling grid of\\n    shape = out_shape.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content=\"Deforms the input volume under the given transformation. The warped volume\\n    is computed using nearest-neighbor interpolation and is given by:\\n\\n    (1) warped[i] = volume[ C * d1[A*i] + B*i ]\\n\\n    where A = affine_idx_in, B = affine_idx_out, C = affine_disp and i denotes\\n    the discrete coordinates of a voxel in the sampling grid of\\n    shape = out_shape.\\n\\n    Parameters\\n    ----------\\n    volume : array, shape (S, R, C)\\n        the input volume to be transformed\\n    d1 : array, shape (S', R', C', 3)\\n        the displacement field driving the transformation\\n    affine_idx_in : array, shape (4, 4)\\n        the matrix A in eq. (1) above\\n    affine_idx_out : array, shape (4, 4)\\n        the matrix B in eq. (1) above\\n    affine_disp : array, shape (4, 4)\\n        the matrix C in eq. (1) above\\n    out_shape : array, shape (3,)\\n        the number of slices, rows and columns of the sampling grid\\n\\n    Returns\\n    -------\\n    warped : array, shape = out_shape\\n        the transformed volume\\n\\n    Notes\\n    -----\\n    To illustrate the use of this function, consider a displacement field d1\\n    with grid-to-space transformation R, a volume with grid-to-space\\n    transformation T and let's say we want to sample the warped volume on a\\n    grid with grid-to-space transformation S (sampling grid). For each voxel\\n    in the sampling grid with discrete coordinates i, the warped volume is\\n    given by:\\n\\n    (2) warped[i] = volume[Tinv * ( d1[Rinv * S * i] + S * i ) ]\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='Notes\\n    -----\\n    To illustrate the use of this function, consider a displacement field d1\\n    with grid-to-space transformation R, a volume with grid-to-space\\n    transformation T and let\\'s say we want to sample the warped volume on a\\n    grid with grid-to-space transformation S (sampling grid). For each voxel\\n    in the sampling grid with discrete coordinates i, the warped volume is\\n    given by:\\n\\n    (2) warped[i] = volume[Tinv * ( d1[Rinv * S * i] + S * i ) ]\\n\\n    where Tinv = T^{-1} and Rinv = R^{-1}. By identifying A = Rinv * S,\\n    B = Tinv * S, C = Tinv we can use this function to efficiently warp the\\n    input image.\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp nslices = volume.shape[0]\\n        cnp.npy_intp nrows = volume.shape[1]\\n        cnp.npy_intp ncols = volume.shape[2]\\n        cnp.npy_intp nsVol = volume.shape[0]\\n        cnp.npy_intp nrVol = volume.shape[1]\\n        cnp.npy_intp ncVol = volume.shape[2]\\n        cnp.npy_intp i, j, k\\n        int inside\\n        double dkk, dii, djj, dk, di, dj\\n\\n    if not is_valid_affine(affine_idx_in, 3):\\n        raise ValueError(\"Invalid inner index multiplication matrix\")\\n    if not is_valid_affine(affine_idx_out, 3):\\n        raise ValueError(\"Invalid outer index multiplication matrix\")\\n    if not is_valid_affine(affine_disp, 3):\\n        raise ValueError(\"Invalid displacement multiplication matrix\")\\n\\n    if out_shape is not None:\\n        nslices = out_shape[0]\\n        nrows = out_shape[1]\\n        ncols = out_shape[2]\\n    elif d1 is not None:\\n        nslices = d1.shape[0]\\n        nrows = d1.shape[1]\\n        ncols = d1.shape[2]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='if out_shape is not None:\\n        nslices = out_shape[0]\\n        nrows = out_shape[1]\\n        ncols = out_shape[2]\\n    elif d1 is not None:\\n        nslices = d1.shape[0]\\n        nrows = d1.shape[1]\\n        ncols = d1.shape[2]\\n\\n    cdef number[:, :, :] warped = np.zeros(shape=(nslices, nrows, ncols),\\n                                           dtype=np.asarray(volume).dtype)\\n    cdef floating[:] tmp = np.zeros(shape=(3,), dtype = np.asarray(d1).dtype)\\n\\n    with nogil:\\n\\n        for k in range(nslices):\\n            for i in range(nrows):\\n                for j in range(ncols):\\n                    if affine_idx_in is None:\\n                        dkk = d1[k, i, j, 0]\\n                        dii = d1[k, i, j, 1]\\n                        djj = d1[k, i, j, 2]\\n                    else:\\n                        dk = _apply_affine_3d_x0(\\n                            k, i, j, 1, affine_idx_in)\\n                        di = _apply_affine_3d_x1(\\n                            k, i, j, 1, affine_idx_in)\\n                        dj = _apply_affine_3d_x2(\\n                            k, i, j, 1, affine_idx_in)\\n                        inside = _interpolate_vector_3d[floating](d1, dk, di,\\n                                                                  dj, &tmp[0])\\n                        dkk = tmp[0]\\n                        dii = tmp[1]\\n                        djj = tmp[2]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='if affine_disp is not None:\\n                        dk = _apply_affine_3d_x0(\\n                            dkk, dii, djj, 0, affine_disp)\\n                        di = _apply_affine_3d_x1(\\n                            dkk, dii, djj, 0, affine_disp)\\n                        dj = _apply_affine_3d_x2(\\n                            dkk, dii, djj, 0, affine_disp)\\n                    else:\\n                        dk = dkk\\n                        di = dii\\n                        dj = djj\\n\\n                    if affine_idx_out is not None:\\n                        dkk = dk + _apply_affine_3d_x0(k, i, j, 1,\\n                                                       affine_idx_out)\\n                        dii = di + _apply_affine_3d_x1(k, i, j, 1,\\n                                                       affine_idx_out)\\n                        djj = dj + _apply_affine_3d_x2(k, i, j, 1,\\n                                                       affine_idx_out)\\n                    else:\\n                        dkk = dk + k\\n                        dii = di + i\\n                        djj = dj + j\\n\\n                    inside = _interpolate_scalar_nn_3d[number](volume,\\n                                        dkk, dii, djj, &warped[k, i, j])\\n    return np.asarray(warped)\\n\\n\\ndef transform_3d_affine_nn(number[:, :, :] volume, int[:] ref_shape,\\n                           double[:, :] affine=None):\\n    r\"\"\"Transforms a 3D volume by an affine transform with NN interpolation', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='inside = _interpolate_scalar_nn_3d[number](volume,\\n                                        dkk, dii, djj, &warped[k, i, j])\\n    return np.asarray(warped)\\n\\n\\ndef transform_3d_affine_nn(number[:, :, :] volume, int[:] ref_shape,\\n                           double[:, :] affine=None):\\n    r\"\"\"Transforms a 3D volume by an affine transform with NN interpolation\\n\\n    Deforms the input volume under the given affine transformation using\\n    nearest neighbor interpolation. The shape of the resulting volume\\n    is given by ref_shape. If the affine matrix is None, it is taken as the\\n    identity.\\n\\n    Parameters\\n    ----------\\n    volume : array, shape (S, R, C)\\n        the input volume to be transformed\\n    ref_shape : array, shape (3,)\\n        the shape of the resulting volume\\n    affine : array, shape (4, 4)\\n        the affine transform to be applied\\n\\n    Returns\\n    -------\\n    out : array, shape (S\\', R\\', C\\')\\n        the transformed volume', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='Parameters\\n    ----------\\n    volume : array, shape (S, R, C)\\n        the input volume to be transformed\\n    ref_shape : array, shape (3,)\\n        the shape of the resulting volume\\n    affine : array, shape (4, 4)\\n        the affine transform to be applied\\n\\n    Returns\\n    -------\\n    out : array, shape (S\\', R\\', C\\')\\n        the transformed volume\\n\\n    Notes\\n    -----\\n    The reason it is necessary to provide the intended shape of the resulting\\n    volume is because the affine transformation is defined on all R^{3}\\n    but we must sample a finite lattice. Also the resulting shape may not be\\n    necessarily equal to the input shape, unless we are interested on\\n    endomorphisms only and not general diffeomorphisms.\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp nslices = ref_shape[0]\\n        cnp.npy_intp nrows = ref_shape[1]\\n        cnp.npy_intp ncols = ref_shape[2]\\n        cnp.npy_intp nsVol = volume.shape[0]\\n        cnp.npy_intp nrVol = volume.shape[1]\\n        cnp.npy_intp ncVol = volume.shape[2]\\n        double dkk, dii, djj, tmp0, tmp1\\n        double alpha, beta, gamma, calpha, cbeta, cgamma\\n        cnp.npy_intp k, i, j, kk, ii, jj\\n        number[:, :, :] out = np.zeros((nslices, nrows, ncols),\\n                                        dtype=np.asarray(volume).dtype)\\n\\n    if not is_valid_affine(affine, 3):\\n        raise ValueError(\"Invalid affine transform matrix\")\\n\\n    with nogil:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='if not is_valid_affine(affine, 3):\\n        raise ValueError(\"Invalid affine transform matrix\")\\n\\n    with nogil:\\n\\n        for k in range(nslices):\\n            for i in range(nrows):\\n                for j in range(ncols):\\n                    if affine is not None:\\n                        dkk = _apply_affine_3d_x0(k, i, j, 1, affine)\\n                        dii = _apply_affine_3d_x1(k, i, j, 1, affine)\\n                        djj = _apply_affine_3d_x2(k, i, j, 1, affine)\\n                    else:\\n                        dkk = k\\n                        dii = i\\n                        djj = j\\n                    _interpolate_scalar_nn_3d[number](volume, dkk, dii, djj,\\n                                                      &out[k, i, j])\\n    return np.asarray(out)\\n\\n\\ndef warp_2d(floating[:, :] image, floating[:, :, :] d1,\\n            double[:, :] affine_idx_in=None,\\n            double[:, :] affine_idx_out=None,\\n            double[:, :] affine_disp=None,\\n            int[:] out_shape=None):\\n    r\"\"\"Warps a 2D image using bilinear interpolation\\n\\n    Deforms the input image under the given transformation. The warped image\\n    is computed using bi-linear interpolation and is given by:\\n\\n    (1) warped[i] = image[ C * d1[A*i] + B*i ]\\n\\n    where A = affine_idx_in, B = affine_idx_out, C = affine_disp and i denotes\\n    the discrete coordinates of a voxel in the sampling grid of\\n    shape = out_shape.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content=\"Deforms the input image under the given transformation. The warped image\\n    is computed using bi-linear interpolation and is given by:\\n\\n    (1) warped[i] = image[ C * d1[A*i] + B*i ]\\n\\n    where A = affine_idx_in, B = affine_idx_out, C = affine_disp and i denotes\\n    the discrete coordinates of a voxel in the sampling grid of\\n    shape = out_shape.\\n\\n    Parameters\\n    ----------\\n    image : array, shape (R, C)\\n        the input image to be transformed\\n    d1 : array, shape (R', C', 2)\\n        the displacement field driving the transformation\\n    affine_idx_in : array, shape (3, 3)\\n        the matrix A in eq. (1) above\\n    affine_idx_out : array, shape (3, 3)\\n        the matrix B in eq. (1) above\\n    affine_disp : array, shape (3, 3)\\n        the matrix C in eq. (1) above\\n    out_shape : array, shape (2,)\\n        the number of rows and columns of the sampling grid\\n\\n    Returns\\n    -------\\n    warped : array, shape = out_shape\\n        the transformed image\\n\\n    Notes\\n    -----\\n    To illustrate the use of this function, consider a displacement field d1\\n    with grid-to-space transformation R, an image with grid-to-space\\n    transformation T and let's say we want to sample the warped image on a\\n    grid with grid-to-space transformation S (sampling grid). For each voxel\\n    in the sampling grid with discrete coordinates i, the warped image is\\n    given by:\\n\\n    (2) warped[i] = image[Tinv * ( d1[Rinv * S * i] + S * i ) ]\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='Notes\\n    -----\\n    To illustrate the use of this function, consider a displacement field d1\\n    with grid-to-space transformation R, an image with grid-to-space\\n    transformation T and let\\'s say we want to sample the warped image on a\\n    grid with grid-to-space transformation S (sampling grid). For each voxel\\n    in the sampling grid with discrete coordinates i, the warped image is\\n    given by:\\n\\n    (2) warped[i] = image[Tinv * ( d1[Rinv * S * i] + S * i ) ]\\n\\n    where Tinv = T^{-1} and Rinv = R^{-1}. By identifying A = Rinv * S,\\n    B = Tinv * S, C = Tinv we can use this function to efficiently warp the\\n    input image.\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp nrows = image.shape[0]\\n        cnp.npy_intp ncols = image.shape[1]\\n        cnp.npy_intp nrVol = image.shape[0]\\n        cnp.npy_intp ncVol = image.shape[1]\\n        cnp.npy_intp i, j, ii, jj\\n        double di, dj, dii, djj\\n\\n    if not is_valid_affine(affine_idx_in, 2):\\n        raise ValueError(\"Invalid inner index multiplication matrix\")\\n    if not is_valid_affine(affine_idx_out, 2):\\n        raise ValueError(\"Invalid outer index multiplication matrix\")\\n    if not is_valid_affine(affine_disp, 2):\\n        raise ValueError(\"Invalid displacement multiplication matrix\")\\n\\n    if out_shape is not None:\\n        nrows = out_shape[0]\\n        ncols = out_shape[1]\\n    elif d1 is not None:\\n        nrows = d1.shape[0]\\n        ncols = d1.shape[1]\\n    cdef floating[:, :] warped = np.zeros(shape=(nrows, ncols),\\n                                          dtype=np.asarray(image).dtype)\\n    cdef floating[:] tmp = np.zeros(shape=(2,), dtype=np.asarray(d1).dtype)\\n\\n    with nogil:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='if out_shape is not None:\\n        nrows = out_shape[0]\\n        ncols = out_shape[1]\\n    elif d1 is not None:\\n        nrows = d1.shape[0]\\n        ncols = d1.shape[1]\\n    cdef floating[:, :] warped = np.zeros(shape=(nrows, ncols),\\n                                          dtype=np.asarray(image).dtype)\\n    cdef floating[:] tmp = np.zeros(shape=(2,), dtype=np.asarray(d1).dtype)\\n\\n    with nogil:\\n\\n        for i in range(nrows):\\n            for j in range(ncols):\\n                # Apply inner index pre-multiplication\\n                if affine_idx_in is None:\\n                    dii = d1[i, j, 0]\\n                    djj = d1[i, j, 1]\\n                else:\\n                    di = _apply_affine_2d_x0(\\n                        i, j, 1, affine_idx_in)\\n                    dj = _apply_affine_2d_x1(\\n                        i, j, 1, affine_idx_in)\\n                    _interpolate_vector_2d[floating](d1, di, dj, &tmp[0])\\n                    dii = tmp[0]\\n                    djj = tmp[1]\\n\\n                # Apply displacement multiplication\\n                if affine_disp is not None:\\n                    di = _apply_affine_2d_x0(\\n                        dii, djj, 0, affine_disp)\\n                    dj = _apply_affine_2d_x1(\\n                        dii, djj, 0, affine_disp)\\n                else:\\n                    di = dii\\n                    dj = djj', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='# Apply displacement multiplication\\n                if affine_disp is not None:\\n                    di = _apply_affine_2d_x0(\\n                        dii, djj, 0, affine_disp)\\n                    dj = _apply_affine_2d_x1(\\n                        dii, djj, 0, affine_disp)\\n                else:\\n                    di = dii\\n                    dj = djj\\n\\n                # Apply outer index multiplication and add the displacements\\n                if affine_idx_out is not None:\\n                    dii = di + _apply_affine_2d_x0(i, j, 1, affine_idx_out)\\n                    djj = dj + _apply_affine_2d_x1(i, j, 1, affine_idx_out)\\n                else:\\n                    dii = di + i\\n                    djj = dj + j\\n\\n                # Interpolate the input image at the resulting location\\n                _interpolate_scalar_2d[floating](image, dii, djj,\\n                                                 &warped[i, j])\\n    return np.asarray(warped)\\n\\n\\ndef transform_2d_affine(floating[:, :] image, int[:] ref_shape,\\n                        double[:, :] affine=None):\\n    r\"\"\"Transforms a 2D image by an affine transform with bilinear interp.\\n\\n    Deforms the input image under the given affine transformation using\\n    tri-linear interpolation. The shape of the resulting image\\n    is given by ref_shape. If the affine matrix is None, it is taken as the\\n    identity.\\n\\n    Parameters\\n    ----------\\n    image : array, shape (R, C)\\n        the input image to be transformed\\n    ref_shape : array, shape (2,)\\n        the shape of the resulting image\\n    affine : array, shape (3, 3)\\n        the affine transform to be applied', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='Deforms the input image under the given affine transformation using\\n    tri-linear interpolation. The shape of the resulting image\\n    is given by ref_shape. If the affine matrix is None, it is taken as the\\n    identity.\\n\\n    Parameters\\n    ----------\\n    image : array, shape (R, C)\\n        the input image to be transformed\\n    ref_shape : array, shape (2,)\\n        the shape of the resulting image\\n    affine : array, shape (3, 3)\\n        the affine transform to be applied\\n\\n    Returns\\n    -------\\n    out : array, shape (R\\', C\\')\\n        the transformed image\\n\\n    Notes\\n    -----\\n    The reason it is necessary to provide the intended shape of the resulting\\n    image is because the affine transformation is defined on all R^{2}\\n    but we must sample a finite lattice. Also the resulting shape may not be\\n    necessarily equal to the input shape, unless we are interested on\\n    endomorphisms only and not general diffeomorphisms.\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp nrows = ref_shape[0]\\n        cnp.npy_intp ncols = ref_shape[1]\\n        cnp.npy_intp nrVol = image.shape[0]\\n        cnp.npy_intp ncVol = image.shape[1]\\n        cnp.npy_intp i, j, ii, jj\\n        double dii, djj, tmp0\\n        double alpha, beta, calpha, cbeta\\n        floating[:, :] out = np.zeros(shape=(nrows, ncols),\\n                                      dtype=np.asarray(image).dtype)\\n\\n    if not is_valid_affine(affine, 2):\\n        raise ValueError(\"Invalid affine transform matrix\")\\n\\n    with nogil:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='if not is_valid_affine(affine, 2):\\n        raise ValueError(\"Invalid affine transform matrix\")\\n\\n    with nogil:\\n\\n        for i in range(nrows):\\n            for j in range(ncols):\\n                if affine is not None:\\n                    dii = _apply_affine_2d_x0(i, j, 1, affine)\\n                    djj = _apply_affine_2d_x1(i, j, 1, affine)\\n                else:\\n                    dii = i\\n                    djj = j\\n                _interpolate_scalar_2d[floating](image, dii, djj,\\n                                                 &out[i, j])\\n    return np.asarray(out)\\n\\n\\ndef warp_2d_nn(number[:, :] image, floating[:, :, :] d1,\\n               double[:, :] affine_idx_in=None,\\n               double[:, :] affine_idx_out=None,\\n               double[:, :] affine_disp=None,\\n               int[:] out_shape=None):\\n    r\"\"\"Warps a 2D image using nearest neighbor interpolation\\n\\n    Deforms the input image under the given transformation. The warped image\\n    is computed using nearest-neighbor interpolation and is given by:\\n\\n    (1) warped[i] = image[ C * d1[A*i] + B*i ]\\n\\n    where A = affine_idx_in, B = affine_idx_out, C = affine_disp and i denotes\\n    the discrete coordinates of a voxel in the sampling grid of\\n    shape = out_shape.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content=\"Deforms the input image under the given transformation. The warped image\\n    is computed using nearest-neighbor interpolation and is given by:\\n\\n    (1) warped[i] = image[ C * d1[A*i] + B*i ]\\n\\n    where A = affine_idx_in, B = affine_idx_out, C = affine_disp and i denotes\\n    the discrete coordinates of a voxel in the sampling grid of\\n    shape = out_shape.\\n\\n    Parameters\\n    ----------\\n    image : array, shape (R, C)\\n        the input image to be transformed\\n    d1 : array, shape (R', C', 2)\\n        the displacement field driving the transformation\\n    affine_idx_in : array, shape (3, 3)\\n        the matrix A in eq. (1) above\\n    affine_idx_out : array, shape (3, 3)\\n        the matrix B in eq. (1) above\\n    affine_disp : array, shape (3, 3)\\n        the matrix C in eq. (1) above\\n    out_shape : array, shape (2,)\\n        the number of rows and columns of the sampling grid\\n\\n    Returns\\n    -------\\n    warped : array, shape = out_shape\\n        the transformed image\\n\\n    Notes\\n    -----\\n    To illustrate the use of this function, consider a displacement field d1\\n    with grid-to-space transformation R, an image with grid-to-space\\n    transformation T and let's say we want to sample the warped image on a\\n    grid with grid-to-space transformation S (sampling grid). For each voxel\\n    in the sampling grid with discrete coordinates i, the warped image is\\n    given by:\\n\\n    (2) warped[i] = image[Tinv * ( d1[Rinv * S * i] + S * i ) ]\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='Notes\\n    -----\\n    To illustrate the use of this function, consider a displacement field d1\\n    with grid-to-space transformation R, an image with grid-to-space\\n    transformation T and let\\'s say we want to sample the warped image on a\\n    grid with grid-to-space transformation S (sampling grid). For each voxel\\n    in the sampling grid with discrete coordinates i, the warped image is\\n    given by:\\n\\n    (2) warped[i] = image[Tinv * ( d1[Rinv * S * i] + S * i ) ]\\n\\n    where Tinv = T^{-1} and Rinv = R^{-1}. By identifying A = Rinv * S,\\n    B = Tinv * S, C = Tinv we can use this function to efficiently warp the\\n    input image.\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp nrows = image.shape[0]\\n        cnp.npy_intp ncols = image.shape[1]\\n        cnp.npy_intp nrVol = image.shape[0]\\n        cnp.npy_intp ncVol = image.shape[1]\\n        cnp.npy_intp i, j, ii, jj\\n        double di, dj, dii, djj\\n\\n    if not is_valid_affine(affine_idx_in, 2):\\n        raise ValueError(\"Invalid inner index multiplication matrix\")\\n    if not is_valid_affine(affine_idx_out, 2):\\n        raise ValueError(\"Invalid outer index multiplication matrix\")\\n    if not is_valid_affine(affine_disp, 2):\\n        raise ValueError(\"Invalid displacement multiplication matrix\")\\n\\n    if out_shape is not None:\\n        nrows = out_shape[0]\\n        ncols = out_shape[1]\\n    elif d1 is not None:\\n        nrows = d1.shape[0]\\n        ncols = d1.shape[1]\\n    cdef number[:, :] warped = np.zeros(shape=(nrows, ncols),\\n                                        dtype=np.asarray(image).dtype)\\n    cdef floating[:] tmp = np.zeros(shape=(2,), dtype=np.asarray(d1).dtype)\\n\\n    with nogil:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='if out_shape is not None:\\n        nrows = out_shape[0]\\n        ncols = out_shape[1]\\n    elif d1 is not None:\\n        nrows = d1.shape[0]\\n        ncols = d1.shape[1]\\n    cdef number[:, :] warped = np.zeros(shape=(nrows, ncols),\\n                                        dtype=np.asarray(image).dtype)\\n    cdef floating[:] tmp = np.zeros(shape=(2,), dtype=np.asarray(d1).dtype)\\n\\n    with nogil:\\n\\n        for i in range(nrows):\\n            for j in range(ncols):\\n                # Apply inner index pre-multiplication\\n                if affine_idx_in is None:\\n                    dii = d1[i, j, 0]\\n                    djj = d1[i, j, 1]\\n                else:\\n                    di = _apply_affine_2d_x0(\\n                        i, j, 1, affine_idx_in)\\n                    dj = _apply_affine_2d_x1(\\n                        i, j, 1, affine_idx_in)\\n                    _interpolate_vector_2d[floating](d1, di, dj, &tmp[0])\\n                    dii = tmp[0]\\n                    djj = tmp[1]\\n\\n                # Apply displacement multiplication\\n                if affine_disp is not None:\\n                    di = _apply_affine_2d_x0(\\n                        dii, djj, 0, affine_disp)\\n                    dj = _apply_affine_2d_x1(\\n                        dii, djj, 0, affine_disp)\\n                else:\\n                    di = dii\\n                    dj = djj', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='# Apply displacement multiplication\\n                if affine_disp is not None:\\n                    di = _apply_affine_2d_x0(\\n                        dii, djj, 0, affine_disp)\\n                    dj = _apply_affine_2d_x1(\\n                        dii, djj, 0, affine_disp)\\n                else:\\n                    di = dii\\n                    dj = djj\\n\\n                # Apply outer index multiplication and add the displacements\\n                if affine_idx_out is not None:\\n                    dii = di + _apply_affine_2d_x0(i, j, 1, affine_idx_out)\\n                    djj = dj + _apply_affine_2d_x1(i, j, 1, affine_idx_out)\\n                else:\\n                    dii = di + i\\n                    djj = dj + j\\n\\n                # Interpolate the input image at the resulting location\\n                _interpolate_scalar_nn_2d[number](image, dii, djj,\\n                                                  &warped[i, j])\\n    return np.asarray(warped)\\n\\n\\ndef transform_2d_affine_nn(number[:, :] image, int[:] ref_shape,\\n                           double[:, :] affine=None):\\n    r\"\"\"Transforms a 2D image by an affine transform with NN interpolation\\n\\n    Deforms the input image under the given affine transformation using\\n    nearest neighbor interpolation. The shape of the resulting image\\n    is given by ref_shape. If the affine matrix is None, it is taken as the\\n    identity.\\n\\n    Parameters\\n    ----------\\n    image : array, shape (R, C)\\n        the input image to be transformed\\n    ref_shape : array, shape (2,)\\n        the shape of the resulting image\\n    affine : array, shape (3, 3)\\n        the affine transform to be applied', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='Deforms the input image under the given affine transformation using\\n    nearest neighbor interpolation. The shape of the resulting image\\n    is given by ref_shape. If the affine matrix is None, it is taken as the\\n    identity.\\n\\n    Parameters\\n    ----------\\n    image : array, shape (R, C)\\n        the input image to be transformed\\n    ref_shape : array, shape (2,)\\n        the shape of the resulting image\\n    affine : array, shape (3, 3)\\n        the affine transform to be applied\\n\\n    Returns\\n    -------\\n    out : array, shape (R\\', C\\')\\n        the transformed image\\n\\n    Notes\\n    -----\\n    The reason it is necessary to provide the intended shape of the resulting\\n    image is because the affine transformation is defined on all R^{2}\\n    but we must sample a finite lattice. Also the resulting shape may not be\\n    necessarily equal to the input shape, unless we are interested on\\n    endomorphisms only and not general diffeomorphisms.\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp nrows = ref_shape[0]\\n        cnp.npy_intp ncols = ref_shape[1]\\n        cnp.npy_intp nrVol = image.shape[0]\\n        cnp.npy_intp ncVol = image.shape[1]\\n        double dii, djj, tmp0\\n        double alpha, beta, calpha, cbeta\\n        cnp.npy_intp i, j, ii, jj\\n        number[:, :] out = np.zeros((nrows, ncols),\\n                                    dtype=np.asarray(image).dtype)\\n\\n    if not is_valid_affine(affine, 2):\\n        raise ValueError(\"Invalid affine transform matrix\")\\n\\n    with nogil:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='if not is_valid_affine(affine, 2):\\n        raise ValueError(\"Invalid affine transform matrix\")\\n\\n    with nogil:\\n\\n        for i in range(nrows):\\n            for j in range(ncols):\\n                if affine is not None:\\n                    dii = _apply_affine_2d_x0(i, j, 1, affine)\\n                    djj = _apply_affine_2d_x1(i, j, 1, affine)\\n                else:\\n                    dii = i\\n                    djj = j\\n                _interpolate_scalar_nn_2d[number](image, dii, djj,\\n                                                  &out[i, j])\\n    return np.asarray(out)\\n\\n\\ndef resample_displacement_field_3d(floating[:, :, :, :] field,\\n                                   double[:] factors, int[:] out_shape):\\n    r\"\"\"Resamples a 3D vector field to a custom target shape\\n\\n    Resamples the given 3D displacement field on a grid of the requested shape,\\n    using the given scale factors. More precisely, the resulting displacement\\n    field at each grid cell i is given by\\n\\n    D[i] = field[Diag(factors) * i]\\n\\n    Parameters\\n    ----------\\n    factors : array, shape (3,)\\n        the scaling factors mapping (integer) grid coordinates in the resampled\\n        grid to (floating point) grid coordinates in the original grid\\n    out_shape : array, shape (3,)\\n        the desired shape of the resulting grid', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='D[i] = field[Diag(factors) * i]\\n\\n    Parameters\\n    ----------\\n    factors : array, shape (3,)\\n        the scaling factors mapping (integer) grid coordinates in the resampled\\n        grid to (floating point) grid coordinates in the original grid\\n    out_shape : array, shape (3,)\\n        the desired shape of the resulting grid\\n\\n    Returns\\n    -------\\n    expanded : array, shape = out_shape + (3, )\\n        the resampled displacement field\\n    \"\"\"\\n    ftype = np.asarray(field).dtype\\n    cdef:\\n        cnp.npy_intp tslices = out_shape[0]\\n        cnp.npy_intp trows = out_shape[1]\\n        cnp.npy_intp tcols = out_shape[2]\\n        cnp.npy_intp k, i, j\\n        int inside\\n        double dkk, dii, djj\\n        floating[:, :, :, :] expanded = np.zeros((tslices, trows, tcols, 3),\\n                                                 dtype=ftype)\\n\\n    for k in range(tslices):\\n        for i in range(trows):\\n            for j in range(tcols):\\n                dkk = <double> k * factors[0]\\n                dii = <double> i * factors[1]\\n                djj = <double> j * factors[2]\\n                _interpolate_vector_3d[floating](field, dkk, dii, djj,\\n                                                 &expanded[k, i, j, 0])\\n    return np.asarray(expanded)\\n\\n\\ndef resample_displacement_field_2d(floating[:, :, :] field, double[:] factors,\\n                                   int[:] out_shape):\\n    r\"\"\"Resamples a 2D vector field to a custom target shape\\n\\n    Resamples the given 2D displacement field on a grid of the requested shape,\\n    using the given scale factors. More precisely, the resulting displacement\\n    field at each grid cell i is given by\\n\\n    D[i] = field[Diag(factors) * i]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='def resample_displacement_field_2d(floating[:, :, :] field, double[:] factors,\\n                                   int[:] out_shape):\\n    r\"\"\"Resamples a 2D vector field to a custom target shape\\n\\n    Resamples the given 2D displacement field on a grid of the requested shape,\\n    using the given scale factors. More precisely, the resulting displacement\\n    field at each grid cell i is given by\\n\\n    D[i] = field[Diag(factors) * i]\\n\\n    Parameters\\n    ----------\\n    factors : array, shape (2,)\\n        the scaling factors mapping (integer) grid coordinates in the resampled\\n        grid to (floating point) grid coordinates in the original grid\\n    out_shape : array, shape (2,)\\n        the desired shape of the resulting grid\\n\\n    Returns\\n    -------\\n    expanded : array, shape = out_shape + (2, )\\n        the resampled displacement field\\n    \"\"\"\\n    ftype = np.asarray(field).dtype\\n    cdef:\\n        cnp.npy_intp trows = out_shape[0]\\n        cnp.npy_intp tcols = out_shape[1]\\n        cnp.npy_intp i, j\\n        int inside\\n        double dii, djj\\n        floating[:, :, :] expanded = np.zeros((trows, tcols, 2), dtype=ftype)\\n\\n    for i in range(trows):\\n        for j in range(tcols):\\n            dii = i*factors[0]\\n            djj = j*factors[1]\\n            inside = _interpolate_vector_2d[floating](field, dii, djj,\\n                                                      &expanded[i, j, 0])\\n    return np.asarray(expanded)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='for i in range(trows):\\n        for j in range(tcols):\\n            dii = i*factors[0]\\n            djj = j*factors[1]\\n            inside = _interpolate_vector_2d[floating](field, dii, djj,\\n                                                      &expanded[i, j, 0])\\n    return np.asarray(expanded)\\n\\n\\ndef create_random_displacement_2d(int[:] from_shape,\\n                                  double[:, :] from_grid2world,\\n                                  int[:] to_shape,\\n                                  double[:, :] to_grid2world,\\n                                  object rng=None):\\n    r\"\"\"Creates a random 2D displacement \\'exactly\\' mapping points of two grids\\n\\n    Creates a random 2D displacement field mapping points of an input discrete\\n    domain (with dimensions given by from_shape) to points of an output\\n    discrete domain (with shape given by to_shape). The affine matrices\\n    bringing discrete coordinates to physical space are given by\\n    from_grid2world (for the displacement field discretization) and\\n    to_grid2world (for the target discretization). Since this function is\\n    intended to be used for testing, voxels in the input domain will never be\\n    assigned to boundary voxels on the output domain.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='Parameters\\n    ----------\\n    from_shape : array, shape (2,)\\n        the grid shape where the displacement field will be defined on.\\n    from_grid2world : array, shape (3,3)\\n        the grid-to-space transformation of the displacement field\\n    to_shape : array, shape (2,)\\n        the grid shape where the deformation field will map the input grid to.\\n    to_grid2world : array, shape (3,3)\\n        the grid-to-space transformation of the mapped grid\\n    rng : numpy.rnadom.Generator class, optional\\n        Numpy\\'s random generator for setting seed values when needed.\\n        Default is None.\\n\\n    Returns\\n    -------\\n    output : array, shape = from_shape\\n        the random displacement field in the physical domain\\n    int_field : array, shape = from_shape\\n        the assignment of each point in the input grid to the target grid\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp i, j, ri, rj\\n        double di, dj, dii, djj\\n        int[:, :, :] int_field = np.empty(tuple(from_shape) + (2,),\\n                                          dtype=np.int32)\\n        double[:, :, :] output = np.zeros(tuple(from_shape) + (2,),\\n                                          dtype=np.float64)\\n        cnp.npy_intp dom_size = from_shape[0]*from_shape[1]\\n\\n    if not is_valid_affine(from_grid2world, 2):\\n        raise ValueError(\"Invalid \\'from\\' affine transform matrix\")\\n    if not is_valid_affine(to_grid2world, 2):\\n        raise ValueError(\"Invalid \\'to\\' affine transform matrix\")\\n\\n    if rng is None:\\n        rng = np.random.default_rng()', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='if not is_valid_affine(from_grid2world, 2):\\n        raise ValueError(\"Invalid \\'from\\' affine transform matrix\")\\n    if not is_valid_affine(to_grid2world, 2):\\n        raise ValueError(\"Invalid \\'to\\' affine transform matrix\")\\n\\n    if rng is None:\\n        rng = np.random.default_rng()\\n\\n    # compute the actual displacement field in the physical space\\n    for i in range(from_shape[0]):\\n        for j in range(from_shape[1]):\\n            # randomly choose where each input grid point will be mapped to in\\n            # the target grid\\n            ri = rng.integers(1, to_shape[0]-1)\\n            rj = rng.integers(1, to_shape[1]-1)\\n            int_field[i, j, 0] = ri\\n            int_field[i, j, 1] = rj\\n\\n            # convert the input point to physical coordinates\\n            if from_grid2world is not None:\\n                di = _apply_affine_2d_x0(i, j, 1, from_grid2world)\\n                dj = _apply_affine_2d_x1(i, j, 1, from_grid2world)\\n            else:\\n                di = i\\n                dj = j\\n\\n            # convert the output point to physical coordinates\\n            if to_grid2world is not None:\\n                dii = _apply_affine_2d_x0(ri, rj, 1, to_grid2world)\\n                djj = _apply_affine_2d_x1(ri, rj, 1, to_grid2world)\\n            else:\\n                dii = ri\\n                djj = rj\\n\\n            # the displacement vector at (i,j) must be the target point minus\\n            # the original point, both in physical space\\n\\n            output[i, j, 0] = dii - di\\n            output[i, j, 1] = djj - dj\\n\\n    return np.asarray(output), np.asarray(int_field)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='# the displacement vector at (i,j) must be the target point minus\\n            # the original point, both in physical space\\n\\n            output[i, j, 0] = dii - di\\n            output[i, j, 1] = djj - dj\\n\\n    return np.asarray(output), np.asarray(int_field)\\n\\n\\ndef create_random_displacement_3d(int[:] from_shape,\\n                                  double[:, :] from_grid2world,\\n                                  int[:] to_shape,\\n                                  double[:, :] to_grid2world,\\n                                  object rng=None):\\n    r\"\"\"Creates a random 3D displacement \\'exactly\\' mapping points of two grids\\n\\n    Creates a random 3D displacement field mapping points of an input discrete\\n    domain (with dimensions given by from_shape) to points of an output\\n    discrete domain (with shape given by to_shape). The affine matrices\\n    bringing discrete coordinates to physical space are given by\\n    from_grid2world (for the displacement field discretization) and\\n    to_grid2world (for the target discretization). Since this function is\\n    intended to be used for testing, voxels in the input domain will never be\\n    assigned to boundary voxels on the output domain.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='Parameters\\n    ----------\\n    from_shape : array, shape (3,)\\n        the grid shape where the displacement field will be defined on.\\n    from_grid2world : array, shape (4,4)\\n        the grid-to-space transformation of the displacement field\\n    to_shape : array, shape (3,)\\n        the grid shape where the deformation field will map the input grid to.\\n    to_grid2world : array, shape (4,4)\\n        the grid-to-space transformation of the mapped grid\\n    rng : numpy.rnadom.Generator class, optional\\n        Numpy\\'s random generator for setting seed values when needed.\\n        Default is None.\\n\\n    Returns\\n    -------\\n    output : array, shape = from_shape\\n        the random displacement field in the physical domain\\n    int_field : array, shape = from_shape\\n        the assignment of each point in the input grid to the target grid\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp i, j, k, ri, rj, rk\\n        double di, dj, dk, dii, djj, dkk\\n        int[:, :, :, :] int_field = np.empty(tuple(from_shape) + (3,),\\n                                             dtype=np.int32)\\n        double[:, :, :, :] output = np.zeros(tuple(from_shape) + (3,),\\n                                             dtype=np.float64)\\n        cnp.npy_intp dom_size = from_shape[0]*from_shape[1]*from_shape[2]\\n\\n    if not is_valid_affine(from_grid2world, 3):\\n        raise ValueError(\"Invalid \\'from\\' affine transform matrix\")\\n    if not is_valid_affine(to_grid2world, 3):\\n        raise ValueError(\"Invalid \\'to\\' affine transform matrix\")\\n\\n    if rng is None:\\n        rng = np.random.default_rng()', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='if not is_valid_affine(from_grid2world, 3):\\n        raise ValueError(\"Invalid \\'from\\' affine transform matrix\")\\n    if not is_valid_affine(to_grid2world, 3):\\n        raise ValueError(\"Invalid \\'to\\' affine transform matrix\")\\n\\n    if rng is None:\\n        rng = np.random.default_rng()\\n\\n    # compute the actual displacement field in the physical space\\n    for k in range(from_shape[0]):\\n        for i in range(from_shape[1]):\\n            for j in range(from_shape[2]):\\n                # randomly choose the location of each point on the target grid\\n                rk = rng.integers(1, to_shape[0]-1)\\n                ri = rng.integers(1, to_shape[1]-1)\\n                rj = rng.integers(1, to_shape[2]-1)\\n                int_field[k, i, j, 0] = rk\\n                int_field[k, i, j, 1] = ri\\n                int_field[k, i, j, 2] = rj\\n\\n                # convert the input point to physical coordinates\\n                if from_grid2world is not None:\\n                    dk = _apply_affine_3d_x0(k, i, j, 1, from_grid2world)\\n                    di = _apply_affine_3d_x1(k, i, j, 1, from_grid2world)\\n                    dj = _apply_affine_3d_x2(k, i, j, 1, from_grid2world)\\n                else:\\n                    dk = k\\n                    di = i\\n                    dj = j', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='# convert the input point to physical coordinates\\n                if from_grid2world is not None:\\n                    dk = _apply_affine_3d_x0(k, i, j, 1, from_grid2world)\\n                    di = _apply_affine_3d_x1(k, i, j, 1, from_grid2world)\\n                    dj = _apply_affine_3d_x2(k, i, j, 1, from_grid2world)\\n                else:\\n                    dk = k\\n                    di = i\\n                    dj = j\\n\\n                # convert the output point to physical coordinates\\n                if to_grid2world is not None:\\n                    dkk = _apply_affine_3d_x0(rk, ri, rj, 1, to_grid2world)\\n                    dii = _apply_affine_3d_x1(rk, ri, rj, 1, to_grid2world)\\n                    djj = _apply_affine_3d_x2(rk, ri, rj, 1, to_grid2world)\\n                else:\\n                    dkk = rk\\n                    dii = ri\\n                    djj = rj\\n\\n                # the displacement vector at (i,j) must be the target point\\n                # minus the original point, both in physical space\\n\\n                output[k, i, j, 0] = dkk - dk\\n                output[k, i, j, 1] = dii - di\\n                output[k, i, j, 2] = djj - dj\\n\\n    return np.asarray(output), np.asarray(int_field)\\n\\n\\ndef create_harmonic_fields_2d(cnp.npy_intp nrows, cnp.npy_intp ncols,\\n                              double b, double m):\\n    r\"\"\"Creates an invertible 2D displacement field\\n\\n    Creates the invertible displacement fields used in Chen et al. eqs.\\n    9 and 10 [1]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='output[k, i, j, 0] = dkk - dk\\n                output[k, i, j, 1] = dii - di\\n                output[k, i, j, 2] = djj - dj\\n\\n    return np.asarray(output), np.asarray(int_field)\\n\\n\\ndef create_harmonic_fields_2d(cnp.npy_intp nrows, cnp.npy_intp ncols,\\n                              double b, double m):\\n    r\"\"\"Creates an invertible 2D displacement field\\n\\n    Creates the invertible displacement fields used in Chen et al. eqs.\\n    9 and 10 [1]\\n\\n    Parameters\\n    ----------\\n    nrows : int\\n        number of rows in the resulting harmonic field\\n    ncols : int\\n        number of columns in the resulting harmonic field\\n    b, m : float\\n        parameters of the harmonic field (as in [1]). To understand the effect\\n        of these parameters, please consider plotting a deformed image\\n        (a circle or a grid) under the deformation field, or see examples\\n        in [1]\\n\\n    Returns\\n    -------\\n    d : array, shape (nrows, ncols, 2)\\n        the harmonic displacement field\\n    inv : array, shape (nrows, ncols, 2)\\n        the analytical inverse of the harmonic displacement field', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    d : array, shape (nrows, ncols, 2)\\n        the harmonic displacement field\\n    inv : array, shape (nrows, ncols, 2)\\n        the analytical inverse of the harmonic displacement field\\n\\n    [1] Chen, M., Lu, W., Chen, Q., Ruchala, K. J., & Olivera, G. H. (2008).\\n        A simple fixed-point approach to invert a deformation field.\\n        Medical Physics, 35(1), 81. doi:10.1118/1.2816107\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp mid_row = nrows/2\\n        cnp.npy_intp mid_col = ncols/2\\n        cnp.npy_intp i, j, ii, jj\\n        double theta\\n        double[:, :, :] d = np.zeros((nrows, ncols, 2), dtype=np.float64)\\n        double[:, :, :] inv = np.zeros((nrows, ncols, 2), dtype=np.float64)\\n    for i in range(nrows):\\n        for j in range(ncols):\\n            ii = i - mid_row\\n            jj = j - mid_col\\n            theta = atan2(ii, jj)\\n            d[i, j, 0] = ii * (1.0 / (1 + b * cos(m * theta)) - 1.0)\\n            d[i, j, 1] = jj * (1.0 / (1 + b * cos(m * theta)) - 1.0)\\n            inv[i, j, 0] = b * cos(m * theta) * ii\\n            inv[i, j, 1] = b * cos(m * theta) * jj\\n\\n    return np.asarray(d), np.asarray(inv)\\n\\n\\ndef create_harmonic_fields_3d(int nslices, cnp.npy_intp nrows,\\n                              cnp.npy_intp ncols, double b, double m):\\n    r\"\"\"Creates an invertible 3D displacement field\\n\\n    Creates the invertible displacement fields used in Chen et al. eqs.\\n    9 and 10 [1] computing the angle theta along z-slides.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='return np.asarray(d), np.asarray(inv)\\n\\n\\ndef create_harmonic_fields_3d(int nslices, cnp.npy_intp nrows,\\n                              cnp.npy_intp ncols, double b, double m):\\n    r\"\"\"Creates an invertible 3D displacement field\\n\\n    Creates the invertible displacement fields used in Chen et al. eqs.\\n    9 and 10 [1] computing the angle theta along z-slides.\\n\\n    Parameters\\n    ----------\\n    nslices : int\\n        number of slices in the resulting harmonic field\\n    nrows : int\\n        number of rows in the resulting harmonic field\\n    ncols : int\\n        number of columns in the resulting harmonic field\\n    b, f : float\\n        parameters of the harmonic field (as in [1]). To understand the effect\\n        of these parameters, please consider plotting a deformed image\\n        (e.g. a circle or a grid) under the deformation field, or see examples\\n        in [1]\\n\\n    Returns\\n    -------\\n    d : array, shape (nslices, nrows, ncols, 3)\\n        the harmonic displacement field\\n    inv : array, shape (nslices, nrows, ncols, 3)\\n        the analytical inverse of the harmonic displacement field', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    d : array, shape (nslices, nrows, ncols, 3)\\n        the harmonic displacement field\\n    inv : array, shape (nslices, nrows, ncols, 3)\\n        the analytical inverse of the harmonic displacement field\\n\\n    [1] Chen, M., Lu, W., Chen, Q., Ruchala, K. J., & Olivera, G. H. (2008).\\n        A simple fixed-point approach to invert a deformation field.\\n        Medical Physics, 35(1), 81. doi:10.1118/1.2816107\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp mid_slice = nslices / 2\\n        cnp.npy_intp mid_row = nrows / 2\\n        cnp.npy_intp mid_col = ncols / 2\\n        cnp.npy_intp i, j, k, ii, jj, kk\\n        double theta\\n        double[:, :, :, :] d = np.zeros((nslices, nrows, ncols, 3),\\n                                        dtype=np.float64)\\n        double[:, :, :, :] inv = np.zeros((nslices, nrows, ncols, 3),\\n                                          dtype=np.float64)\\n    for k in range(nslices):\\n        for i in range(nrows):\\n            for j in range(ncols):\\n                kk = k - mid_slice\\n                ii = i - mid_row\\n                jj = j - mid_col\\n                theta = atan2(ii, jj)\\n                d[k, i, j, 0] = kk * (1.0 / (1 + b * cos(m * theta)) - 1.0)\\n                d[k, i, j, 1] = ii * (1.0 / (1 + b * cos(m * theta)) - 1.0)\\n                d[k, i, j, 2] = jj * (1.0 / (1 + b * cos(m * theta)) - 1.0)\\n                inv[k, i, j, 0] = b * cos(m * theta) * kk\\n                inv[k, i, j, 1] = b * cos(m * theta) * ii\\n                inv[k, i, j, 2] = b * cos(m * theta) * jj\\n\\n    return np.asarray(d), np.asarray(inv)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='return np.asarray(d), np.asarray(inv)\\n\\n\\ndef create_circle(cnp.npy_intp nrows, cnp.npy_intp ncols, cnp.npy_intp radius):\\n    r\"\"\"\\n    Create a binary 2D image where pixel values are 1 iff their distance\\n    to the center of the image is less than or equal to radius.\\n\\n    Parameters\\n    ----------\\n    nrows : int\\n        number of rows of the resulting image\\n    ncols : int\\n        number of columns of the resulting image\\n    radius : int\\n        the radius of the circle\\n\\n    Returns\\n    -------\\n    c : array, shape (nrows, ncols)\\n        the binary image of the circle with the requested dimensions\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp mid_row = nrows/2\\n        cnp.npy_intp mid_col = ncols/2\\n        cnp.npy_intp i, j, ii, jj\\n        double r\\n        double[:, :] c = np.zeros((nrows, ncols), dtype=np.float64)\\n    for i in range(nrows):\\n        for j in range(ncols):\\n            ii = i - mid_row\\n            jj = j - mid_col\\n            r = sqrt(ii*ii + jj*jj)\\n            if r <= radius:\\n                c[i, j] = 1\\n            else:\\n                c[i, j] = 0\\n    return np.asarray(c)\\n\\n\\ndef create_sphere(cnp.npy_intp nslices, cnp.npy_intp nrows,\\n                  cnp.npy_intp ncols, cnp.npy_intp radius):\\n    r\"\"\"\\n    Create a binary 3D image where voxel values are 1 iff their distance\\n    to the center of the image is less than or equal to radius.\\n\\n    Parameters\\n    ----------\\n    nslices : int\\n        number if slices of the resulting image\\n    nrows : int\\n        number of rows of the resulting image\\n    ncols : int\\n        number of columns of the resulting image\\n    radius : int\\n        the radius of the sphere', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='Parameters\\n    ----------\\n    nslices : int\\n        number if slices of the resulting image\\n    nrows : int\\n        number of rows of the resulting image\\n    ncols : int\\n        number of columns of the resulting image\\n    radius : int\\n        the radius of the sphere\\n\\n    Returns\\n    -------\\n    c : array, shape (nslices, nrows, ncols)\\n        the binary image of the sphere with the requested dimensions\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp mid_slice = nslices/2\\n        cnp.npy_intp mid_row = nrows/2\\n        cnp.npy_intp mid_col = ncols/2\\n        cnp.npy_intp i, j, k, ii, jj, kk\\n        double r\\n        double[:, :, :] s = np.zeros((nslices, nrows, ncols), dtype=np.float64)\\n\\n    for k in range(nslices):\\n        for i in range(nrows):\\n            for j in range(ncols):\\n                kk = k - mid_slice\\n                ii = i - mid_row\\n                jj = j - mid_col\\n                r = sqrt(ii*ii + jj*jj + kk*kk)\\n                if r <= radius:\\n                    s[k, i, j] = 1\\n                else:\\n                    s[k, i, j] = 0\\n    return np.asarray(s)\\n\\n\\ndef _gradient_3d(floating[:, :, :] img, double[:, :] img_world2grid,\\n                 double[:] img_spacing, double[:, :] out_grid2world,\\n                 floating[:, :, :, :] out, int[:, :, :] inside):\\n    r\"\"\" Gradient of a 3D image in physical space coordinates\\n\\n    Each grid cell (i, j, k) in the sampling grid (determined by\\n    out.shape) is mapped to its corresponding physical point (x, y, z) by\\n    multiplying out_grid2world (its grid-to-space transform) by (i, j, k),\\n    then the image is interpolated, at', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='Each grid cell (i, j, k) in the sampling grid (determined by\\n    out.shape) is mapped to its corresponding physical point (x, y, z) by\\n    multiplying out_grid2world (its grid-to-space transform) by (i, j, k),\\n    then the image is interpolated, at\\n\\n    P1=(x + h, y, z), Q1=(x - h, y, z)\\n    P2=(x, y + h, z), Q2=(x, y - h, z)\\n    P3=(x, y, z + h), Q3=(x, y, z - h)\\n\\n    (by mapping Pi and Qi to the grid using img_world2grid: the inverse of the\\n    grid-to-space transform of img). The displacement parameter h is of\\n    magnitude 0.5 (in physical space units), therefore the approximated partial\\n    derivatives are given by the difference between the image interpolated at\\n    Pi and Qi.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='Parameters\\n    ----------\\n    img : array, shape (S, R, C)\\n        the input volume whose gradient will be computed\\n    img_world2grid : array, shape (4, 4)\\n        the space-to-grid transform matrix associated to img\\n    img_spacing : array, shape (3,)\\n        the spacing between voxels (voxel size along each axis) of the input\\n        volume\\n    out_grid2world : array, shape (4, 4)\\n        the grid-to-space transform associated to the sampling grid\\n    out : array, shape (S\\', R\\', C\\', 3)\\n        the buffer in which to store the image gradient\\n    inside : array, shape (S\\', R\\', C\\')\\n        the buffer in which to store the flags indicating whether the sample\\n        point lies inside (=1) or outside (=0) the image grid\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp nslices = out.shape[0]\\n        cnp.npy_intp nrows = out.shape[1]\\n        cnp.npy_intp ncols = out.shape[2]\\n        cnp.npy_intp i, j, k, in_flag\\n        double tmp\\n        double[:] x = np.empty(shape=(3,), dtype=np.float64)\\n        double[:] dx = np.empty(shape=(3,), dtype=np.float64)\\n        double[:] h = np.empty(shape=(3,), dtype=np.float64)\\n        double[:] q = np.empty(shape=(3,), dtype=np.float64)\\n    with nogil:\\n        h[0] = 0.5 * img_spacing[0]\\n        h[1] = 0.5 * img_spacing[1]\\n        h[2] = 0.5 * img_spacing[2]\\n        for k in range(nslices):\\n            for i in range(nrows):\\n                for j in range(ncols):\\n                    inside[k, i, j] = 1\\n                    # Compute coordinates of index (k, i, j) in physical space\\n                    x[0] = _apply_affine_3d_x0(k, i, j, 1, out_grid2world)\\n                    x[1] = _apply_affine_3d_x1(k, i, j, 1, out_grid2world)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content=\"with nogil:\\n        h[0] = 0.5 * img_spacing[0]\\n        h[1] = 0.5 * img_spacing[1]\\n        h[2] = 0.5 * img_spacing[2]\\n        for k in range(nslices):\\n            for i in range(nrows):\\n                for j in range(ncols):\\n                    inside[k, i, j] = 1\\n                    # Compute coordinates of index (k, i, j) in physical space\\n                    x[0] = _apply_affine_3d_x0(k, i, j, 1, out_grid2world)\\n                    x[1] = _apply_affine_3d_x1(k, i, j, 1, out_grid2world)\\n                    x[2] = _apply_affine_3d_x2(k, i, j, 1, out_grid2world)\\n                    dx[:] = x[:]\\n                    for p in range(3):\\n                        # Compute coordinates of point dx on img's grid\\n                        dx[p] = x[p] - h[p]\\n                        q[0] = _apply_affine_3d_x0(dx[0], dx[1], dx[2], 1,\\n                                                   img_world2grid)\\n                        q[1] = _apply_affine_3d_x1(dx[0], dx[1], dx[2], 1,\\n                                                   img_world2grid)\\n                        q[2] = _apply_affine_3d_x2(dx[0], dx[1], dx[2], 1,\\n                                                   img_world2grid)\\n                        # Interpolate img at q\\n                        in_flag = _interpolate_scalar_3d[floating](img, q[0],\\n                            q[1], q[2], &out[k, i, j, p])\\n                        if in_flag == 0:\\n                            out[k, i, j, p] = 0\\n                            inside[k, i, j] = 0\\n                            continue\\n                        tmp = out[k, i, j, p]\\n                        # Compute coordinates of point dx on img's grid\\n                        dx[p] = x[p] + h[p]\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content=\"in_flag = _interpolate_scalar_3d[floating](img, q[0],\\n                            q[1], q[2], &out[k, i, j, p])\\n                        if in_flag == 0:\\n                            out[k, i, j, p] = 0\\n                            inside[k, i, j] = 0\\n                            continue\\n                        tmp = out[k, i, j, p]\\n                        # Compute coordinates of point dx on img's grid\\n                        dx[p] = x[p] + h[p]\\n                        q[0] = _apply_affine_3d_x0(dx[0], dx[1], dx[2], 1,\\n                                                   img_world2grid)\\n                        q[1] = _apply_affine_3d_x1(dx[0], dx[1], dx[2], 1,\\n                                                   img_world2grid)\\n                        q[2] = _apply_affine_3d_x2(dx[0], dx[1], dx[2], 1,\\n                                                   img_world2grid)\\n                        # Interpolate img at q\\n                        in_flag = _interpolate_scalar_3d[floating](img, q[0],\\n                                                q[1], q[2], &out[k, i, j, p])\\n                        if in_flag == 0:\\n                            out[k, i, j, p] = 0\\n                            inside[k, i, j] = 0\\n                            continue\\n                        out[k, i, j, p] = ((out[k, i, j, p] - tmp) /\\n                                           img_spacing[p])\\n                        dx[p] = x[p]\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='def _sparse_gradient_3d(floating[:, :, :] img,\\n                        double[:, :] img_world2grid,\\n                        double[:] img_spacing,\\n                        double[:, :] sample_points,\\n                        floating[:, :] out, int[:] inside):\\n    r\"\"\" Gradient of a 3D image evaluated at a set of points in physical space\\n\\n    For each row (x_i, y_i, z_i) in sample_points, the image is interpolated at\\n\\n    P1=(x_i + h, y_i, z_i), Q1=(x_i - h, y_i, z_i)\\n    P2=(x_i, y_i + h, z_i), Q2=(x_i, y_i - h, z_i)\\n    P3=(x_i, y_i, z_i + h), Q3=(x_i, y_i, z_i - h)\\n\\n    (by mapping Pi and Qi to the grid using img_world2grid: the inverse of the\\n    grid-to-space transform of img). The displacement parameter h is of\\n    magnitude 0.5 (in physical space units), therefore the approximated partial\\n    derivatives are given by the difference between the image interpolated at\\n    Pi and Qi.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='Parameters\\n    ----------\\n    img : array, shape (S, R, C)\\n        the input volume whose gradient will be computed\\n    img_world2grid : array, shape (4, 4)\\n        the space-to-grid transform matrix associated to img\\n    img_spacing : array, shape (3,)\\n        the spacing between voxels (voxel size along each axis) of the input\\n    sample_points: array, shape (n, 3)\\n        list of points where the derivative will be evaluated\\n        (one point per row)\\n    out : array, shape (n, 3)\\n        the buffer in which to store the image gradient\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp n = sample_points.shape[0]\\n        cnp.npy_intp i, in_flag\\n        double tmp\\n        double[:] dx = np.empty(shape=(3,), dtype=np.float64)\\n        double[:] h = np.empty(shape=(3,), dtype=np.float64)\\n        double[:] q = np.empty(shape=(3,), dtype=np.float64)\\n    with nogil:\\n        h[0] = 0.5 * img_spacing[0]\\n        h[1] = 0.5 * img_spacing[1]\\n        h[2] = 0.5 * img_spacing[2]\\n        for i in range(n):\\n            inside[i] = 1\\n            dx[0] = sample_points[i, 0]\\n            dx[1] = sample_points[i, 1]\\n            dx[2] = sample_points[i, 2]\\n            for p in range(3):\\n                # Compute coordinates of point dx on img\\'s grid\\n                dx[p] = sample_points[i, p] - h[p]\\n                q[0] = _apply_affine_3d_x0(dx[0], dx[1], dx[2], 1,\\n                                           img_world2grid)\\n                q[1] = _apply_affine_3d_x1(dx[0], dx[1], dx[2], 1,\\n                                           img_world2grid)\\n                q[2] = _apply_affine_3d_x2(dx[0], dx[1], dx[2], 1,\\n                                           img_world2grid)\\n                # Interpolate img at q', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content=\"dx[p] = sample_points[i, p] - h[p]\\n                q[0] = _apply_affine_3d_x0(dx[0], dx[1], dx[2], 1,\\n                                           img_world2grid)\\n                q[1] = _apply_affine_3d_x1(dx[0], dx[1], dx[2], 1,\\n                                           img_world2grid)\\n                q[2] = _apply_affine_3d_x2(dx[0], dx[1], dx[2], 1,\\n                                           img_world2grid)\\n                # Interpolate img at q\\n                in_flag = _interpolate_scalar_3d[floating](img, q[0], q[1],\\n                                                           q[2], &out[i, p])\\n                if in_flag == 0:\\n                    out[i, p] = 0\\n                    inside[i] = 0\\n                    continue\\n                tmp = out[i, p]\\n                # Compute coordinates of point dx on img's grid\\n                dx[p] = sample_points[i, p] + h[p]\\n                q[0] = _apply_affine_3d_x0(dx[0], dx[1], dx[2], 1,\\n                                           img_world2grid)\\n                q[1] = _apply_affine_3d_x1(dx[0], dx[1], dx[2], 1,\\n                                           img_world2grid)\\n                q[2] = _apply_affine_3d_x2(dx[0], dx[1], dx[2], 1,\\n                                           img_world2grid)\\n                # Interpolate img at q\\n                in_flag = _interpolate_scalar_3d[floating](img,\\n                                                q[0], q[1], q[2], &out[i, p])\\n                if in_flag == 0:\\n                    out[i, p] = 0\\n                    inside[i] = 0\\n                    continue\\n                out[i, p] = (out[i, p] - tmp) / img_spacing[p]\\n                dx[p] = sample_points[i, p]\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='def _gradient_2d(floating[:, :] img, double[:, :] img_world2grid,\\n                 double[:] img_spacing, double[:, :] out_grid2world,\\n                 floating[:, :, :] out, int[:, :] inside):\\n    r\"\"\" Gradient of a 2D image in physical space coordinates\\n\\n    Each grid cell (i, j) in the sampling grid (determined by\\n    out.shape) is mapped to its corresponding physical point (x, y) by\\n    multiplying out_grid2world (its grid-to-space transform) by (i, j), then\\n    the image is interpolated, at\\n\\n    P1=(x + h, y), Q1=(x - h, y)\\n    P2=(x, y + h), Q2=(x, y - h)\\n\\n    (by mapping Pi and Qi to the grid using img_world2grid: the inverse of the\\n    grid-to-space transform of img). The displacement parameter h is of\\n    magnitude 0.5 (in physical space units), therefore the approximated partial\\n    derivatives are given by the difference between the image interpolated at\\n    Pi and Qi.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='Parameters\\n    ----------\\n    img : array, shape (R, C)\\n        the input image whose gradient will be computed\\n    img_world2grid : array, shape (3, 3)\\n        the space-to-grid transform matrix associated to img\\n    img_spacing : array, shape (2,)\\n        the spacing between pixels (pixel size along each axis) of the input\\n        image\\n    out_grid2world : array, shape (3, 3)\\n        the grid-to-space transform associated to the sampling grid\\n    out : array, shape (S\\', R\\', 2)\\n        the buffer in which to store the image gradient\\n    inside : array, shape (S\\', R\\')\\n        the buffer in which to store the flags indicating whether the sample\\n        point lies inside (=1) or outside (=0) the image grid\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp nrows = out.shape[0]\\n        cnp.npy_intp ncols = out.shape[1]\\n        cnp.npy_intp i, j, k, in_flag\\n        double tmp\\n        double[:] x = np.empty(shape=(2,), dtype=np.float64)\\n        double[:] dx = np.empty(shape=(2,), dtype=np.float64)\\n        double[:] h = np.empty(shape=(2,), dtype=np.float64)\\n        double[:] q = np.empty(shape=(2,), dtype=np.float64)\\n    with nogil:\\n        h[0] = 0.5 * img_spacing[0]\\n        h[1] = 0.5 * img_spacing[1]\\n        for i in range(nrows):\\n            for j in range(ncols):\\n                inside[i, j] = 1\\n                # Compute coordinates of index (i, j) in physical space\\n                x[0] = _apply_affine_2d_x0(i, j, 1, out_grid2world)\\n                x[1] = _apply_affine_2d_x1(i, j, 1, out_grid2world)\\n                dx[:] = x[:]\\n                for p in range(2):\\n                    # Compute coordinates of point dx on img\\'s grid\\n                    dx[p] = x[p] - h[p]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content=\"for i in range(nrows):\\n            for j in range(ncols):\\n                inside[i, j] = 1\\n                # Compute coordinates of index (i, j) in physical space\\n                x[0] = _apply_affine_2d_x0(i, j, 1, out_grid2world)\\n                x[1] = _apply_affine_2d_x1(i, j, 1, out_grid2world)\\n                dx[:] = x[:]\\n                for p in range(2):\\n                    # Compute coordinates of point dx on img's grid\\n                    dx[p] = x[p] - h[p]\\n                    q[0] = _apply_affine_2d_x0(dx[0], dx[1], 1, img_world2grid)\\n                    q[1] = _apply_affine_2d_x1(dx[0], dx[1], 1, img_world2grid)\\n                    # Interpolate img at q\\n                    in_flag = _interpolate_scalar_2d[floating](img, q[0],\\n                                                    q[1], &out[i, j, p])\\n                    if in_flag == 0:\\n                        out[i, j, p] = 0\\n                        inside[i, j] = 0\\n                        continue\\n                    tmp = out[i, j, p]\\n                    # Compute coordinates of point dx on img's grid\\n                    dx[p] = x[p] + h[p]\\n                    q[0] = _apply_affine_2d_x0(dx[0], dx[1], 1, img_world2grid)\\n                    q[1] = _apply_affine_2d_x1(dx[0], dx[1], 1, img_world2grid)\\n                    # Interpolate img at q\\n                    in_flag = _interpolate_scalar_2d[floating](img, q[0],\\n                                                    q[1], &out[i, j, p])\\n                    if in_flag == 0:\\n                        out[i, j, p] = 0\\n                        inside[i, j] = 0\\n                        continue\\n                    out[i, j, p] = (out[i, j, p] - tmp) / img_spacing[p]\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='q[1] = _apply_affine_2d_x1(dx[0], dx[1], 1, img_world2grid)\\n                    # Interpolate img at q\\n                    in_flag = _interpolate_scalar_2d[floating](img, q[0],\\n                                                    q[1], &out[i, j, p])\\n                    if in_flag == 0:\\n                        out[i, j, p] = 0\\n                        inside[i, j] = 0\\n                        continue\\n                    out[i, j, p] = (out[i, j, p] - tmp) / img_spacing[p]\\n                    dx[p] = x[p]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='def _sparse_gradient_2d(floating[:, :] img, double[:, :] img_world2grid,\\n                        double[:] img_spacing,\\n                        double[:, :] sample_points,\\n                        floating[:, :] out, int[:] inside):\\n    r\"\"\" Gradient of a 2D image evaluated at a set of points in physical space\\n\\n    For each row (x_i, y_i) in sample_points, the image is interpolated at\\n\\n    P1=(x_i + h, y_i), Q1=(x_i - h, y_i)\\n    P2=(x_i, y_i + h), Q2=(x_i, y_i - h)\\n\\n    (by mapping Pi and Qi to the grid using img_world2grid: the inverse of the\\n    grid-to-space transform of img). The displacement parameter h is of\\n    magnitude 0.5 (in physical space units), therefore the approximated partial\\n    derivatives are given by the difference between the image interpolated at\\n    Pi and Qi.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='Parameters\\n    ----------\\n    img : array, shape (R, C)\\n        the input volume whose gradient will be computed\\n    img_world2grid : array, shape (3, 3)\\n        the space-to-grid transform matrix associated to img\\n    img_spacing : array, shape (2,)\\n        the spacing between pixels (pixel size along each axis) of the input\\n    sample_points: array, shape (n, 2)\\n        list of points where the derivative will be evaluated\\n        (one point per row)\\n    out : array, shape (n, 2)\\n        the buffer in which to store the image gradient\\n    inside : array, shape (n,)\\n        the buffer in which to store the flags indicating whether the sample\\n        point lies inside (=1) or outside (=0) the image grid\\n    \"\"\"\\n    cdef:\\n        cnp.npy_intp n = sample_points.shape[0]\\n        cnp.npy_intp i, in_flag\\n        double tmp\\n        double[:] dx = np.empty(shape=(2,), dtype=np.float64)\\n        double[:] h = np.empty(shape=(2,), dtype=np.float64)\\n        double[:] q = np.empty(shape=(2,), dtype=np.float64)\\n    with nogil:\\n        h[0] = 0.5 * img_spacing[0]\\n        h[1] = 0.5 * img_spacing[1]\\n        for i in range(n):\\n            inside[i] = 1\\n            dx[0] = sample_points[i, 0]\\n            dx[1] = sample_points[i, 1]\\n            for p in range(2):\\n                # Compute coordinates of point dx on img\\'s grid\\n                dx[p] = sample_points[i, p] - h[p]\\n                q[0] = _apply_affine_2d_x0(dx[0], dx[1], 1, img_world2grid)\\n                q[1] = _apply_affine_2d_x1(dx[0], dx[1], 1, img_world2grid)\\n                # Interpolate img at q\\n                in_flag = _interpolate_scalar_2d[floating](img, q[0], q[1],', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content=\"dx[0] = sample_points[i, 0]\\n            dx[1] = sample_points[i, 1]\\n            for p in range(2):\\n                # Compute coordinates of point dx on img's grid\\n                dx[p] = sample_points[i, p] - h[p]\\n                q[0] = _apply_affine_2d_x0(dx[0], dx[1], 1, img_world2grid)\\n                q[1] = _apply_affine_2d_x1(dx[0], dx[1], 1, img_world2grid)\\n                # Interpolate img at q\\n                in_flag = _interpolate_scalar_2d[floating](img, q[0], q[1],\\n                                                           &out[i, p])\\n                if in_flag == 0:\\n                    out[i, p] = 0\\n                    inside[i] = 0\\n                    continue\\n                tmp = out[i, p]\\n                # Compute coordinates of point dx on img's grid\\n                dx[p] = sample_points[i, p] + h[p]\\n                q[0] = _apply_affine_2d_x0(dx[0], dx[1], 1, img_world2grid)\\n                q[1] = _apply_affine_2d_x1(dx[0], dx[1], 1, img_world2grid)\\n                # Interpolate img at q\\n                in_flag = _interpolate_scalar_2d[floating](img, q[0], q[1],\\n                                                           &out[i, p])\\n                if in_flag == 0:\\n                    out[i, p] = 0\\n                    inside[i] = 0\\n                    continue\\n                out[i, p] = (out[i, p] - tmp) / img_spacing[p]\\n                dx[p] = sample_points[i, p]\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='def gradient(img, img_world2grid, img_spacing, out_shape,\\n             out_grid2world):\\n    r\"\"\" Gradient of an image in physical space\\n\\n    Parameters\\n    ----------\\n    img : 2D or 3D array, shape (R, C) or (S, R, C)\\n        the input image whose gradient will be computed\\n    img_world2grid : array, shape (dim+1, dim+1)\\n        the space-to-grid transform matrix associated to img\\n    img_spacing : array, shape (dim,)\\n        the spacing between voxels (voxel size along each axis) of the input\\n        image\\n    out_shape : array, shape (dim,)\\n        the number of (slices), rows and columns of the sampling grid\\n    out_grid2world : array, shape (dim+1, dim+1)\\n        the grid-to-space transform associated to the sampling grid', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    out : array, shape (R\\', C\\', 2) or (S\\', R\\', C\\', 3)\\n        the buffer in which to store the image gradient, where\\n        (S\\'), R\\', C\\' are given by out_shape\\n    \"\"\"\\n    dim = len(img.shape)\\n    if not is_valid_affine(img_world2grid, dim):\\n        raise ValueError(\"Invalid image affine transform\")\\n    if not is_valid_affine(out_grid2world, dim):\\n        raise ValueError(\"Invalid sampling grid affine transform\")\\n    if len(img_spacing) < dim:\\n        raise ValueError(\"Invalid spacings\")\\n    ftype = img.dtype.type\\n    out = np.empty(tuple(out_shape)+(dim,), dtype=ftype)\\n    inside = np.empty(tuple(out_shape), dtype=np.int32)\\n    # Select joint density gradient 2D or 3D\\n    if dim == 2:\\n        jd_grad = _gradient_2d\\n    elif dim == 3:\\n        jd_grad = _gradient_3d\\n    else:\\n        raise ValueError(\\'Undefined gradient for image dimension %d\\' % (dim,))\\n    if img_world2grid.dtype != np.float64:\\n        img_world2grid = img_world2grid.astype(np.float64)\\n    if img_spacing.dtype != np.float64:\\n        img_spacing = img_spacing.astype(np.float64)\\n    if out_grid2world.dtype != np.float64:\\n        out_grid2world = out_grid2world.astype(np.float64)\\n    jd_grad(img, img_world2grid, img_spacing, out_grid2world, out, inside)\\n    return np.asarray(out), np.asarray(inside)\\n\\n\\ndef sparse_gradient(img, img_world2grid, img_spacing, sample_points):\\n    r\"\"\" Gradient of an image in physical space', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='def sparse_gradient(img, img_world2grid, img_spacing, sample_points):\\n    r\"\"\" Gradient of an image in physical space\\n\\n    Parameters\\n    ----------\\n    img : 2D or 3D array, shape (R, C) or (S, R, C)\\n        the input image whose gradient will be computed\\n    img_world2grid : array, shape (dim+1, dim+1)\\n        the space-to-grid transform matrix associated to img\\n    img_spacing : array, shape (dim,)\\n        the spacing between voxels (voxel size along each axis) of the input\\n        image\\n    sample_points: array, shape (n, dim)\\n        list of points where the derivative will be evaluated\\n        (one point per row)\\n\\n    Returns\\n    -------\\n    out : array, shape (n, dim)\\n        the gradient at each point stored at its corresponding row\\n    \"\"\"\\n    dim = len(img.shape)\\n    if not is_valid_affine(img_world2grid, dim):\\n        raise ValueError(\"Invalid affine transform matrix\")\\n    if len(img_spacing) < dim:\\n        raise ValueError(\"Invalid spacings\")\\n\\n    ftype = img.dtype.type\\n    n = sample_points.shape[0]\\n    out = np.empty(shape=(n, dim), dtype=ftype)\\n    inside = np.empty(shape=(n,), dtype=np.int32)\\n    # Select joint density gradient 2D or 3D\\n    if dim == 2:\\n        jd_grad = _sparse_gradient_2d\\n    else:\\n        jd_grad = _sparse_gradient_3d\\n    if img_world2grid.dtype != np.float64:\\n        img_world2grid = img_world2grid.astype(np.float64)\\n    if img_spacing.dtype != np.float64:\\n        img_spacing = img_spacing.astype(np.float64)\\n    jd_grad(img, img_world2grid, img_spacing, sample_points, out, inside)\\n    return np.asarray(out), np.asarray(inside)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\vector_fields.pyx.txt'}),\n",
       " Document(page_content='\"\"\"\\n\\nRegistration API: simplified API for registration of MRI data and of\\nstreamlines.\\n\\n\\n\"\"\"\\nfrom warnings import warn\\nimport re\\nimport collections.abc\\nfrom functools import partial\\nimport numbers\\nimport numpy as np\\nimport nibabel as nib\\nfrom dipy.align.metrics import CCMetric, EMMetric, SSDMetric\\nfrom dipy.align.imwarp import (SymmetricDiffeomorphicRegistration,\\n                               DiffeomorphicMap)\\n\\nfrom dipy.align.imaffine import (transform_centers_of_mass,\\n                                 AffineMap,\\n                                 MutualInformationMetric,\\n                                 AffineRegistration)\\n\\nfrom dipy.align.transforms import (TranslationTransform3D,\\n                                   RigidTransform3D,\\n                                   RigidScalingTransform3D,\\n                                   RigidIsoScalingTransform3D,\\n                                   AffineTransform3D)\\n\\n\\nimport dipy.core.gradients as dpg\\nimport dipy.data as dpd\\nfrom dipy.align.streamlinear import StreamlineLinearRegistration\\nfrom dipy.tracking.streamline import set_number_of_points\\nfrom dipy.tracking.utils import transform_tracking_output\\nfrom dipy.io.streamline import load_trk\\nfrom dipy.io.utils import read_img_arr_or_path\\nfrom dipy.io.image import load_nifti, save_nifti\\n\\n__all__ = [\"syn_registration\", \"register_dwi_to_template\",\\n           \"write_mapping\", \"read_mapping\", \"resample\",\\n           \"center_of_mass\", \"translation\",\\n           \"rigid_isoscaling\", \"rigid_scaling\",\\n           \"rigid\", \"affine\", \"motion_correction\",\\n           \"affine_registration\", \"register_series\",\\n           \"register_dwi_series\", \"streamline_registration\"]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\_public.py.txt'}),\n",
       " Document(page_content='__all__ = [\"syn_registration\", \"register_dwi_to_template\",\\n           \"write_mapping\", \"read_mapping\", \"resample\",\\n           \"center_of_mass\", \"translation\",\\n           \"rigid_isoscaling\", \"rigid_scaling\",\\n           \"rigid\", \"affine\", \"motion_correction\",\\n           \"affine_registration\", \"register_series\",\\n           \"register_dwi_series\", \"streamline_registration\"]\\n\\n# Global dicts for choosing metrics for registration:\\nsyn_metric_dict = {\\'CC\\': CCMetric,\\n                   \\'EM\\': EMMetric,\\n                   \\'SSD\\': SSDMetric}\\n\\naffine_metric_dict = {\\'MI\\': MutualInformationMetric}\\n\\n\\ndef _handle_pipeline_inputs(moving, static, static_affine=None,\\n                            moving_affine=None, starting_affine=None):\\n    \"\"\"\\n    Helper function to prepare inputs for pipeline functions\\n\\n    Parameters\\n    ----------\\n    moving, static: Either as a 3D/4D array or as a nifti image object, or as\\n        a string containing the full path to a nifti file.\\n\\n    static_affine, moving_affine: 2D arrays.\\n        The array associated with the static/moving images.\\n\\n    starting_affine : 2D array, optional.\\n        This is the registration matrix that is inherited from previous steps\\n        in the pipeline. Default: 4-by-4 identity matrix.\\n    \"\"\"\\n    static, static_affine = read_img_arr_or_path(static,\\n                                                 affine=static_affine)\\n    moving, moving_affine = read_img_arr_or_path(moving,\\n                                                 affine=moving_affine)\\n    if starting_affine is None:\\n        starting_affine = np.eye(4)\\n\\n    return static, static_affine, moving, moving_affine, starting_affine', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\_public.py.txt'}),\n",
       " Document(page_content='return static, static_affine, moving, moving_affine, starting_affine\\n\\n\\ndef syn_registration(moving, static,\\n                     moving_affine=None,\\n                     static_affine=None,\\n                     step_length=0.25,\\n                     metric=\\'CC\\',\\n                     dim=3,\\n                     level_iters=None,\\n                     prealign=None,\\n                     **metric_kwargs):\\n    \"\"\"Register a 2D/3D source image (moving) to a 2D/3D target image (static).\\n\\n    Parameters\\n    ----------\\n    moving, static : array or nib.Nifti1Image or str.\\n        Either as a 2D/3D array or as a nifti image object, or as\\n        a string containing the full path to a nifti file.\\n    moving_affine, static_affine : 4x4 array, optional.\\n        Must be provided for `data` provided as an array. If provided together\\n        with Nifti1Image or str `data`, this input will over-ride the affine\\n        that is stored in the `data` input. Default: use the affine stored\\n        in `data`.\\n    metric : string, optional\\n        The metric to be optimized. One of `CC`, `EM`, `SSD`,\\n        Default: \\'CC\\' => CCMetric.\\n    dim: int (either 2 or 3), optional\\n       The dimensions of the image domain. Default: 3\\n    level_iters : list of int, optional\\n        the number of iterations at each level of the Gaussian Pyramid (the\\n        length of the list defines the number of pyramid levels to be\\n        used). Default: [10, 10, 5].\\n    metric_kwargs : dict, optional\\n        Parameters for initialization of the metric object. If not provided,\\n        uses the default settings of each metric.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\_public.py.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    warped_moving : ndarray\\n        The data in `moving`, warped towards the `static` data.\\n    forward : ndarray (..., 3)\\n        The vector field describing the forward warping from the source to the\\n        target.\\n    backward : ndarray (..., 3)\\n        The vector field describing the backward warping from the target to the\\n        source.\\n\\n    \"\"\"\\n    level_iters = level_iters or [10, 10, 5]\\n\\n    static, static_affine, moving, moving_affine, _ = \\\\\\n        _handle_pipeline_inputs(moving, static,\\n                                moving_affine=moving_affine,\\n                                static_affine=static_affine,\\n                                starting_affine=None)\\n\\n    use_metric = syn_metric_dict[metric.upper()](dim, **metric_kwargs)\\n\\n    sdr = SymmetricDiffeomorphicRegistration(use_metric, level_iters,\\n                                             step_length=step_length)\\n\\n    mapping = sdr.optimize(static, moving,\\n                           static_grid2world=static_affine,\\n                           moving_grid2world=moving_affine,\\n                           prealign=prealign)\\n\\n    warped_moving = mapping.transform(moving)\\n    return warped_moving, mapping\\n\\n\\ndef register_dwi_to_template(dwi, gtab, dwi_affine=None, template=None,\\n                             template_affine=None, reg_method=\"syn\",\\n                             **reg_kwargs):\\n    \"\"\" Register DWI data to a template through the B0 volumes.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\_public.py.txt'}),\n",
       " Document(page_content='warped_moving = mapping.transform(moving)\\n    return warped_moving, mapping\\n\\n\\ndef register_dwi_to_template(dwi, gtab, dwi_affine=None, template=None,\\n                             template_affine=None, reg_method=\"syn\",\\n                             **reg_kwargs):\\n    \"\"\" Register DWI data to a template through the B0 volumes.\\n\\n    Parameters\\n    ----------\\n    dwi : 4D array, nifti image or str\\n        Containing the DWI data, or full path to a nifti file with DWI.\\n    gtab : GradientTable or sequence of strings\\n        The gradients associated with the DWI data, or a sequence with\\n        (fbval, fbvec), full paths to bvals and bvecs files.\\n    dwi_affine : 4x4 array, optional\\n        An affine transformation associated with the DWI. Required if data\\n        is provided as an array. If provided together with nifti/path,\\n        will over-ride the affine that is in the nifti.\\n    template : 3D array, nifti image or str\\n        Containing the data for the template, or full path to a nifti file\\n        with the template data.\\n    template_affine : 4x4 array, optional\\n        An affine transformation associated with the template. Required if data\\n        is provided as an array. If provided together with nifti/path,\\n        will over-ride the affine that is in the nifti.\\n\\n    reg_method : str,\\n        One of \"syn\" or \"aff\", which designates which registration method is\\n        used. Either syn, which uses the :func:`syn_registration` function\\n        or :func:`affine_registration` function. Default: \"syn\".\\n    reg_kwargs : key-word arguments for :func:`syn_registration` or\\n        :func:`affine_registration`', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\_public.py.txt'}),\n",
       " Document(page_content='reg_method : str,\\n        One of \"syn\" or \"aff\", which designates which registration method is\\n        used. Either syn, which uses the :func:`syn_registration` function\\n        or :func:`affine_registration` function. Default: \"syn\".\\n    reg_kwargs : key-word arguments for :func:`syn_registration` or\\n        :func:`affine_registration`\\n\\n    Returns\\n    -------\\n    warped_b0, mapping: The fist is an array with the b0 volume warped to the\\n    template. If reg_method is \"syn\", the second is a DiffeomorphicMap class\\n    instance that can be used to transform between the two spaces. Otherwise,\\n    if reg_method is \"aff\", this is a 4x4 matrix encoding the affine transform.\\n\\n    Notes\\n    -----\\n    This function assumes that the DWI data is already internally registered.\\n    See :func:`register_dwi_series`.\\n\\n    \"\"\"\\n    dwi_data, dwi_affine = read_img_arr_or_path(dwi, affine=dwi_affine)\\n\\n    if template is None:\\n        template = dpd.read_mni_template()\\n\\n    template_data, template_affine = read_img_arr_or_path(\\n                                       template,\\n                                       affine=template_affine)\\n\\n    if not isinstance(gtab, dpg.GradientTable):\\n        gtab = dpg.gradient_table(*gtab)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\_public.py.txt'}),\n",
       " Document(page_content='\"\"\"\\n    dwi_data, dwi_affine = read_img_arr_or_path(dwi, affine=dwi_affine)\\n\\n    if template is None:\\n        template = dpd.read_mni_template()\\n\\n    template_data, template_affine = read_img_arr_or_path(\\n                                       template,\\n                                       affine=template_affine)\\n\\n    if not isinstance(gtab, dpg.GradientTable):\\n        gtab = dpg.gradient_table(*gtab)\\n\\n    mean_b0 = np.mean(dwi_data[..., gtab.b0s_mask], -1)\\n    if reg_method.lower() == \"syn\":\\n        warped_b0, mapping = syn_registration(mean_b0, template_data,\\n                                              moving_affine=dwi_affine,\\n                                              static_affine=template_affine,\\n                                              **reg_kwargs)\\n    elif reg_method.lower() == \"aff\":\\n        warped_b0, mapping = affine_registration(mean_b0, template_data,\\n                                                 moving_affine=dwi_affine,\\n                                                 static_affine=template_affine,\\n                                                 **reg_kwargs)\\n    else:\\n        raise ValueError(\"reg_method should be one of \\'aff\\' or \\'syn\\', but you\"\\n                         \" provided %s\" % reg_method)\\n\\n    return warped_b0, mapping\\n\\n\\ndef write_mapping(mapping, fname):\\n    \"\"\" Write out a syn registration mapping to a nifti file.\\n\\n    Parameters\\n    ----------\\n    mapping : a DiffeomorphicMap object derived from :func:`syn_registration`\\n    fname : str\\n        Full path to the nifti file storing the mapping', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\_public.py.txt'}),\n",
       " Document(page_content='return warped_b0, mapping\\n\\n\\ndef write_mapping(mapping, fname):\\n    \"\"\" Write out a syn registration mapping to a nifti file.\\n\\n    Parameters\\n    ----------\\n    mapping : a DiffeomorphicMap object derived from :func:`syn_registration`\\n    fname : str\\n        Full path to the nifti file storing the mapping\\n\\n    Notes\\n    -----\\n    The data in the file is organized with shape (X, Y, Z, 3, 2), such\\n    that the forward mapping in each voxel is in `data[i, j, k, :, 0]` and\\n    the backward mapping in each voxel is in `data[i, j, k, :, 1]`.\\n\\n    \"\"\"\\n    mapping_data = np.array([mapping.forward.T, mapping.backward.T]).T\\n    save_nifti(fname, mapping_data, mapping.codomain_world2grid)\\n\\n\\ndef read_mapping(disp, domain_img, codomain_img, prealign=None):\\n    \"\"\" Read a syn registration mapping from a nifti file.\\n\\n    Parameters\\n    ----------\\n    disp : str or Nifti1Image\\n        A file of image containing the mapping displacement field in each voxel\\n        Shape (x, y, z, 3, 2)\\n\\n    domain_img : str or Nifti1Image\\n\\n    codomain_img : str or Nifti1Image\\n\\n    Returns\\n    -------\\n    A :class:`DiffeomorphicMap` object.\\n\\n    Notes\\n    -----\\n    See :func:`write_mapping` for the data format expected.\\n\\n    \"\"\"\\n    if isinstance(disp, str):\\n        disp_data, disp_affine = load_nifti(disp)\\n\\n    if isinstance(domain_img, str):\\n        domain_img = nib.load(domain_img)\\n\\n    if isinstance(codomain_img, str):\\n        codomain_img = nib.load(codomain_img)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\_public.py.txt'}),\n",
       " Document(page_content='domain_img : str or Nifti1Image\\n\\n    codomain_img : str or Nifti1Image\\n\\n    Returns\\n    -------\\n    A :class:`DiffeomorphicMap` object.\\n\\n    Notes\\n    -----\\n    See :func:`write_mapping` for the data format expected.\\n\\n    \"\"\"\\n    if isinstance(disp, str):\\n        disp_data, disp_affine = load_nifti(disp)\\n\\n    if isinstance(domain_img, str):\\n        domain_img = nib.load(domain_img)\\n\\n    if isinstance(codomain_img, str):\\n        codomain_img = nib.load(codomain_img)\\n\\n    mapping = DiffeomorphicMap(3, disp_data.shape[:3],\\n                               disp_grid2world=np.linalg.inv(disp_affine),\\n                               domain_shape=domain_img.shape[:3],\\n                               domain_grid2world=domain_img.affine,\\n                               codomain_shape=codomain_img.shape,\\n                               codomain_grid2world=codomain_img.affine,\\n                               prealign=prealign)\\n\\n    mapping.forward = disp_data[..., 0]\\n    mapping.backward = disp_data[..., 1]\\n    mapping.is_inverse = True\\n\\n    return mapping\\n\\n\\ndef resample(moving, static, moving_affine=None, static_affine=None,\\n             between_affine=None):\\n    \"\"\"Resample an image (moving) from one space to another (static).\\n\\n    Parameters\\n    ----------\\n    moving : array, nifti image or str\\n        Containing the data for the moving object, or full path to a nifti file\\n        with the moving data.\\n\\n    moving_affine : 4x4 array, optional\\n        An affine transformation associated with the moving object. Required if\\n        data is provided as an array. If provided together with nifti/path,\\n        will over-ride the affine that is in the nifti.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\_public.py.txt'}),\n",
       " Document(page_content='Parameters\\n    ----------\\n    moving : array, nifti image or str\\n        Containing the data for the moving object, or full path to a nifti file\\n        with the moving data.\\n\\n    moving_affine : 4x4 array, optional\\n        An affine transformation associated with the moving object. Required if\\n        data is provided as an array. If provided together with nifti/path,\\n        will over-ride the affine that is in the nifti.\\n\\n    static : array, nifti image or str\\n        Containing the data for the static object, or full path to a nifti file\\n        with the moving data.\\n\\n    static_affine : 4x4 array, optional\\n        An affine transformation associated with the static object. Required if\\n        data is provided as an array. If provided together with nifti/path,\\n        will over-ride the affine that is in the nifti.\\n\\n    between_affine: 4x4 array, optional\\n        If an additional affine is needed between the two spaces.\\n        Default: identity (no additional registration).\\n\\n    Returns\\n    -------\\n    A Nifti1Image class instance with the data from the moving object\\n    resampled into the space of the static object.\\n\\n    \"\"\"\\n\\n    static, static_affine, moving, moving_affine, between_affine = \\\\\\n        _handle_pipeline_inputs(moving, static,\\n                                moving_affine=moving_affine,\\n                                static_affine=static_affine,\\n                                starting_affine=between_affine)\\n    affine_map = AffineMap(between_affine,\\n                           static.shape, static_affine,\\n                           moving.shape, moving_affine)\\n    resampled = affine_map.transform(moving)\\n    return nib.Nifti1Image(resampled, static_affine)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\_public.py.txt'}),\n",
       " Document(page_content='def affine_registration(moving, static,\\n                        moving_affine=None,\\n                        static_affine=None,\\n                        pipeline=None,\\n                        starting_affine=None,\\n                        metric=\\'MI\\',\\n                        level_iters=None,\\n                        sigmas=None,\\n                        factors=None,\\n                        ret_metric=False,\\n                        moving_mask=None,\\n                        static_mask=None,\\n                        **metric_kwargs):\\n    \"\"\"\\n    Find the affine transformation between two 3D images. Alternatively, find\\n    the combination of several linear transformations.\\n\\n    Parameters\\n    ----------\\n    moving : array, nifti image or str\\n        Containing the data for the moving object, or full path to a nifti file\\n        with the moving data.\\n\\n    static : array, nifti image or str\\n        Containing the data for the static object, or full path to a nifti file\\n        with the moving data.\\n\\n    moving_affine : 4x4 array, optional\\n        An affine transformation associated with the moving object. Required if\\n        data is provided as an array. If provided together with nifti/path,\\n        will over-ride the affine that is in the nifti.\\n\\n    static_affine : 4x4 array, optional\\n        An affine transformation associated with the static object. Required if\\n        data is provided as an array. If provided together with nifti/path,\\n        will over-ride the affine that is in the nifti.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\_public.py.txt'}),\n",
       " Document(page_content='static_affine : 4x4 array, optional\\n        An affine transformation associated with the static object. Required if\\n        data is provided as an array. If provided together with nifti/path,\\n        will over-ride the affine that is in the nifti.\\n\\n    pipeline : list of str, optional\\n        Sequence of transforms to use in the gradual fitting. Default: gradual\\n        fit of the full affine (executed from left to right):\\n        ``[\"center_of_mass\", \"translation\", \"rigid\", \"affine\"]``\\n        Alternatively, any other combination of the following registration\\n        methods might be used: center_of_mass, translation, rigid,\\n        rigid_isoscaling, rigid_scaling and affine.\\n\\n    starting_affine: 4x4 array, optional\\n        Initial guess for the transformation between the spaces.\\n        Default: identity.\\n\\n    metric : str, optional.\\n        Currently only supports \\'MI\\' for MutualInformationMetric.\\n\\n    level_iters : sequence, optional\\n        AffineRegistration key-word argument: the number of iterations at each\\n        scale of the scale space. `level_iters[0]` corresponds to the coarsest\\n        scale, `level_iters[-1]` the finest, where n is the length of the\\n        sequence. By default, a 3-level scale space with iterations\\n        sequence equal to [10000, 1000, 100] will be used.\\n\\n    sigmas : sequence of floats, optional\\n        AffineRegistration key-word argument: custom smoothing parameter to\\n        build the scale space (one parameter for each scale). By default,\\n        the sequence of sigmas will be [3, 1, 0].', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\_public.py.txt'}),\n",
       " Document(page_content=\"sigmas : sequence of floats, optional\\n        AffineRegistration key-word argument: custom smoothing parameter to\\n        build the scale space (one parameter for each scale). By default,\\n        the sequence of sigmas will be [3, 1, 0].\\n\\n    factors : sequence of floats, optional\\n        AffineRegistration key-word argument: custom scale factors to build the\\n        scale space (one factor for each scale). By default, the sequence of\\n        factors will be [4, 2, 1].\\n\\n    ret_metric : boolean, optional\\n        Set it to True to return the value of the optimized coefficients and\\n        the optimization quality metric.\\n\\n    moving_mask : array, shape (S', R', C') or (R', C'), optional\\n        moving image mask that defines which pixels in the moving image\\n        are used to calculate the mutual information.\\n\\n    static_mask : array, shape (S, R, C) or (R, C), optional\\n        static image mask that defines which pixels in the static image\\n        are used to calculate the mutual information.\\n\\n    nbins : int, optional\\n        MutualInformationMetric key-word argument: the number of bins to be\\n        used for computing the intensity histograms. The default is 32.\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\_public.py.txt'}),\n",
       " Document(page_content='static_mask : array, shape (S, R, C) or (R, C), optional\\n        static image mask that defines which pixels in the static image\\n        are used to calculate the mutual information.\\n\\n    nbins : int, optional\\n        MutualInformationMetric key-word argument: the number of bins to be\\n        used for computing the intensity histograms. The default is 32.\\n\\n    sampling_proportion : None or float in interval (0, 1], optional\\n        MutualInformationMetric key-word argument: There are two types of\\n        sampling: dense and sparse. Dense sampling uses all voxels for\\n        estimating the (joint and marginal) intensity histograms, while\\n        sparse sampling uses a subset of them. If `sampling_proportion` is\\n        None, then dense sampling is used. If `sampling_proportion` is a\\n        floating point value in (0,1] then sparse sampling is used,\\n        where `sampling_proportion` specifies the proportion of voxels to\\n        be used. The default is None (dense sampling).\\n\\n    Returns\\n    -------\\n    transformed : array with moving data resampled to the static space\\n    after computing the affine transformation\\n    affine : the affine 4x4 associated with the transformation.\\n    xopt : the value of the optimized coefficients.\\n    fopt : the value of the optimization quality metric.\\n\\n    Notes\\n    -----\\n    Performs a gradual registration between the two inputs, using a pipeline\\n    that gradually approximates the final registration. If the final default\\n    step (`affine`) is omitted, the resulting affine may not have all 12\\n    degrees of freedom adjusted.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\_public.py.txt'}),\n",
       " Document(page_content='Notes\\n    -----\\n    Performs a gradual registration between the two inputs, using a pipeline\\n    that gradually approximates the final registration. If the final default\\n    step (`affine`) is omitted, the resulting affine may not have all 12\\n    degrees of freedom adjusted.\\n\\n    \"\"\"\\n    pipeline = pipeline or [\"center_of_mass\", \"translation\", \"rigid\", \"affine\"]\\n    level_iters = level_iters or [10000, 1000, 100]\\n    sigmas = sigmas or [3, 1, 0.0]\\n    factors = factors or [4, 2, 1]\\n\\n    starting_was_supplied = starting_affine is not None\\n    static, static_affine, moving, moving_affine, starting_affine = \\\\\\n        _handle_pipeline_inputs(moving, static,\\n                                moving_affine=moving_affine,\\n                                static_affine=static_affine,\\n                                starting_affine=starting_affine)\\n\\n    # Define the Affine registration object we\\'ll use with the chosen metric.\\n    # For now, there is only one metric (mutual information)\\n    use_metric = affine_metric_dict[metric](**metric_kwargs)\\n\\n    affreg = AffineRegistration(metric=use_metric,\\n                                level_iters=level_iters,\\n                                sigmas=sigmas,\\n                                factors=factors)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\_public.py.txt'}),\n",
       " Document(page_content='# Define the Affine registration object we\\'ll use with the chosen metric.\\n    # For now, there is only one metric (mutual information)\\n    use_metric = affine_metric_dict[metric](**metric_kwargs)\\n\\n    affreg = AffineRegistration(metric=use_metric,\\n                                level_iters=level_iters,\\n                                sigmas=sigmas,\\n                                factors=factors)\\n\\n    # Convert pipeline to sanitized list of str\\n    pipeline = list(pipeline)\\n    for fi, func in enumerate(pipeline):\\n        if callable(func):\\n            for key, val in _METHOD_DICT.items():\\n                if func is val[0]:  # if they passed the callable equiv.\\n                    pipeline[fi] = func = key\\n                    break\\n        if not isinstance(func, str) or func not in _METHOD_DICT:\\n            raise ValueError(f\\'pipeline[{fi}] must be one of \\'\\n                             f\\'{list(_METHOD_DICT)}, got {func!r}\\')\\n\\n    if pipeline == [\"center_of_mass\"] and ret_metric:\\n        raise ValueError(\"center of mass registration cannot return any \"\\n                         \"quality metric.\")\\n\\n    # Go through the selected transformation:\\n    for func in pipeline:\\n        if func == \"center_of_mass\":\\n\\n            if starting_affine is not None and starting_was_supplied:\\n                wm = \"starting_affine overwritten by center_of_mass transform\"\\n                warn(wm, UserWarning, stacklevel=2)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\_public.py.txt'}),\n",
       " Document(page_content='if pipeline == [\"center_of_mass\"] and ret_metric:\\n        raise ValueError(\"center of mass registration cannot return any \"\\n                         \"quality metric.\")\\n\\n    # Go through the selected transformation:\\n    for func in pipeline:\\n        if func == \"center_of_mass\":\\n\\n            if starting_affine is not None and starting_was_supplied:\\n                wm = \"starting_affine overwritten by center_of_mass transform\"\\n                warn(wm, UserWarning, stacklevel=2)\\n\\n            # multiply images by masks for transform_centers_of_mass\\n            static_masked, moving_masked = static, moving\\n            if static_mask is not None:\\n                static_masked = static*static_mask\\n            if moving_mask is not None:\\n                moving_masked = moving*moving_mask\\n\\n            transform = transform_centers_of_mass(static_masked, static_affine,\\n                                                  moving_masked, moving_affine)\\n            starting_affine = transform.affine\\n\\n        else:\\n            transform = _METHOD_DICT[func][1]()\\n            xform, xopt, fopt \\\\\\n                = affreg.optimize(static, moving, transform, None,\\n                                  static_affine, moving_affine,\\n                                  starting_affine=starting_affine,\\n                                  ret_metric=True,\\n                                  static_mask=static_mask,\\n                                  moving_mask=moving_mask)\\n            starting_affine = xform.affine', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\_public.py.txt'}),\n",
       " Document(page_content='# After doing all that, resample once at the end:\\n    affine_map = AffineMap(starting_affine,\\n                           static.shape, static_affine,\\n                           moving.shape, moving_affine)\\n\\n    resampled = affine_map.transform(moving)\\n\\n    # Return the optimization metric only if requested\\n    if ret_metric:\\n        return resampled, starting_affine, xopt, fopt\\n    return resampled, starting_affine\\n\\n\\ncenter_of_mass = partial(affine_registration, pipeline=[\\'center_of_mass\\'])\\ncenter_of_mass.__doc__ = (\"Implements a center of mass transform. \"\\n                          \"Based on `affine_registration()`.\")\\n\\ntranslation = partial(affine_registration, pipeline=[\\'translation\\'])\\ntranslation.__doc__ = (\"Implements a translation transform. \"\\n                       \"Based on `affine_registration()`.\")\\n\\nrigid = partial(affine_registration, pipeline=[\\'rigid\\'])\\nrigid.__doc__ = (\"Implements a rigid transform. \"\\n                 \"Based on `affine_registration()`.\")\\n\\nrigid_isoscaling = partial(affine_registration, pipeline=[\\'rigid_isoscaling\\'])\\nrigid_isoscaling.__doc__ = (\"Implements a rigid isoscaling transform. \"\\n                            \"Based on `affine_registration()`.\")\\n\\nrigid_scaling = partial(affine_registration, pipeline=[\\'rigid_scaling\\'])\\nrigid_scaling.__doc__ = (\"Implements a rigid scaling transform. \"\\n                         \"Based on `affine_registration()`.\")\\n\\naffine = partial(affine_registration, pipeline=[\\'affine\\'])\\naffine.__doc__ = (\"Implements an affine transform. \"\\n                  \"Based on `affine_registration()`.\")', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\_public.py.txt'}),\n",
       " Document(page_content='rigid_scaling = partial(affine_registration, pipeline=[\\'rigid_scaling\\'])\\nrigid_scaling.__doc__ = (\"Implements a rigid scaling transform. \"\\n                         \"Based on `affine_registration()`.\")\\n\\naffine = partial(affine_registration, pipeline=[\\'affine\\'])\\naffine.__doc__ = (\"Implements an affine transform. \"\\n                  \"Based on `affine_registration()`.\")\\n\\n\\n_METHOD_DICT = dict(  # mapping from str key -> (callable, class) tuple\\n    center_of_mass=(center_of_mass, None),\\n    translation=(translation, TranslationTransform3D),\\n    rigid_isoscaling=(rigid_isoscaling, RigidIsoScalingTransform3D),\\n    rigid_scaling=(rigid_scaling, RigidScalingTransform3D),\\n    rigid=(rigid, RigidTransform3D),\\n    affine=(affine, AffineTransform3D))\\n\\n\\ndef register_series(series, ref, pipeline=None, series_affine=None,\\n                    ref_affine=None, static_mask=None):\\n    \"\"\"Register a series to a reference image.\\n\\n    Parameters\\n    ----------\\n    series : 4D array or nib.Nifti1Image class instance or str\\n        The data is 4D with the last dimension separating different 3D volumes\\n\\n    ref : int or 3D array or nib.Nifti1Image class instance or str\\n        If this is an int, this is the index of the reference image within the\\n        series. Otherwise it is an array of data to register with (associated\\n        with a `ref_affine` required) or a nifti img or full path to a file\\n        containing one.\\n\\n    pipeline : sequence, optional\\n        Sequence of transforms to do for each volume in the series.\\n        Default: (executed from left to right):\\n        `[center_of_mass, translation, rigid, affine]`', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\_public.py.txt'}),\n",
       " Document(page_content='pipeline : sequence, optional\\n        Sequence of transforms to do for each volume in the series.\\n        Default: (executed from left to right):\\n        `[center_of_mass, translation, rigid, affine]`\\n\\n    series_affine, ref_affine : 4x4 arrays, optional.\\n        The affine. If provided, this input will over-ride the affine provided\\n        together with the nifti img or file.\\n\\n    static_mask : array, shape (S, R, C) or (R, C), optional\\n        static image mask that defines which pixels in the static image\\n        are used to calculate the mutual information.\\n\\n    Returns\\n    -------\\n    xformed, affines : 4D array with transformed data and a (4,4,n) array\\n    with 4x4 matrices associated with each of the volumes of the input moving\\n    data that was used to transform it into register with the static data.\\n\\n    \"\"\"\\n    pipeline = pipeline or [\"center_of_mass\", \"translation\", \"rigid\", \"affine\"]\\n\\n    series, series_affine = read_img_arr_or_path(series,\\n                                                 affine=series_affine)\\n    if isinstance(ref, numbers.Number):\\n        ref_as_idx = ref\\n        idxer = np.zeros(series.shape[-1]).astype(bool)\\n        idxer[ref] = True\\n        ref = series[..., idxer].squeeze()\\n        ref_affine = series_affine\\n    else:\\n        ref_as_idx = False\\n        ref, ref_affine = read_img_arr_or_path(ref, affine=ref_affine)\\n        if len(ref.shape) != 3:\\n            raise ValueError(\"The reference image should be a single volume\",\\n                             \" or the index of one or more volumes\")', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\_public.py.txt'}),\n",
       " Document(page_content='xformed = np.zeros(series.shape)\\n    affines = np.zeros((4, 4, series.shape[-1]))\\n    for ii in range(series.shape[-1]):\\n        this_moving = series[..., ii]\\n        if isinstance(ref_as_idx, numbers.Number) and ii == ref_as_idx:\\n            # This is the reference! No need to move and the xform is I(4):\\n            xformed[..., ii] = this_moving\\n            affines[..., ii] = np.eye(4)\\n        else:\\n            transformed, reg_affine = affine_registration(\\n                this_moving, ref,\\n                moving_affine=series_affine,\\n                static_affine=ref_affine,\\n                pipeline=pipeline,\\n                static_mask=static_mask)\\n            xformed[..., ii] = transformed\\n            affines[..., ii] = reg_affine\\n\\n    return xformed, affines\\n\\n\\ndef register_dwi_series(data, gtab, affine=None, b0_ref=0, pipeline=None,\\n                        static_mask=None):\\n    \"\"\"Register a DWI series to the mean of the B0 images in that series.\\n\\n    all first registered to the first B0 volume\\n\\n    Parameters\\n    ----------\\n    data : 4D array or nibabel Nifti1Image class instance or str\\n        Diffusion data. Either as a 4D array or as a nifti image object, or as\\n        a string containing the full path to a nifti file.\\n\\n    gtab : a GradientTable class instance or tuple of strings\\n        If provided as a tuple of strings, these are assumed to be full paths\\n        to the bvals and bvecs files (in that order).', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\_public.py.txt'}),\n",
       " Document(page_content='all first registered to the first B0 volume\\n\\n    Parameters\\n    ----------\\n    data : 4D array or nibabel Nifti1Image class instance or str\\n        Diffusion data. Either as a 4D array or as a nifti image object, or as\\n        a string containing the full path to a nifti file.\\n\\n    gtab : a GradientTable class instance or tuple of strings\\n        If provided as a tuple of strings, these are assumed to be full paths\\n        to the bvals and bvecs files (in that order).\\n\\n    affine : 4x4 array, optional.\\n        Must be provided for `data` provided as an array. If provided together\\n        with Nifti1Image or str `data`, this input will over-ride the affine\\n        that is stored in the `data` input. Default: use the affine stored\\n        in `data`.\\n\\n    b0_ref : int, optional.\\n        Which b0 volume to use as reference. Default: 0\\n\\n    pipeline : list of callables, optional.\\n        The transformations to perform in sequence (from left to right):\\n        Default: ``[center_of_mass, translation, rigid, affine]``\\n\\n    static_mask : array, shape (S, R, C) or (R, C), optional\\n        static image mask that defines which pixels in the static image\\n        are used to calculate the mutual information.\\n\\n    Returns\\n    -------\\n    xform_img, affine_array: a Nifti1Image containing the registered data and\\n    using the affine of the original data and a list containing the affine\\n    transforms associated with each of the\\n\\n    \"\"\"\\n    pipeline = pipeline or [\"center_of_mass\", \"translation\", \"rigid\", \"affine\"]\\n\\n    data, affine = read_img_arr_or_path(data, affine=affine)\\n    if isinstance(gtab, collections.abc.Sequence):\\n        gtab = dpg.gradient_table(*gtab)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\_public.py.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    xform_img, affine_array: a Nifti1Image containing the registered data and\\n    using the affine of the original data and a list containing the affine\\n    transforms associated with each of the\\n\\n    \"\"\"\\n    pipeline = pipeline or [\"center_of_mass\", \"translation\", \"rigid\", \"affine\"]\\n\\n    data, affine = read_img_arr_or_path(data, affine=affine)\\n    if isinstance(gtab, collections.abc.Sequence):\\n        gtab = dpg.gradient_table(*gtab)\\n\\n    if np.sum(gtab.b0s_mask) > 1:\\n        # First, register the b0s into one image and average:\\n        b0_img = nib.Nifti1Image(data[..., gtab.b0s_mask], affine)\\n        trans_b0, b0_affines = register_series(b0_img, ref=b0_ref,\\n                                               pipeline=pipeline,\\n                                               static_mask=static_mask)\\n        ref_data = np.mean(trans_b0, -1, keepdims=True)\\n    else:\\n        # There\\'s only one b0 and we register everything to it\\n        trans_b0 = ref_data = data[..., gtab.b0s_mask]\\n        b0_affines = np.eye(4)[..., np.newaxis]\\n\\n    # Construct a series out of the DWI and the registered mean B0:\\n    moving_data = data[..., ~gtab.b0s_mask]\\n    series_arr = np.concatenate([ref_data, moving_data], -1)\\n    series = nib.Nifti1Image(series_arr, affine)\\n\\n    xformed, affines = register_series(series, ref=0, pipeline=pipeline,\\n                                       static_mask=static_mask)\\n    # Cut out the part pertaining to that first volume:\\n    affines = affines[..., 1:]\\n    xformed = xformed[..., 1:]\\n    affine_array = np.zeros((4, 4, data.shape[-1]))\\n    affine_array[..., gtab.b0s_mask] = b0_affines\\n    affine_array[..., ~gtab.b0s_mask] = affines', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\_public.py.txt'}),\n",
       " Document(page_content='xformed, affines = register_series(series, ref=0, pipeline=pipeline,\\n                                       static_mask=static_mask)\\n    # Cut out the part pertaining to that first volume:\\n    affines = affines[..., 1:]\\n    xformed = xformed[..., 1:]\\n    affine_array = np.zeros((4, 4, data.shape[-1]))\\n    affine_array[..., gtab.b0s_mask] = b0_affines\\n    affine_array[..., ~gtab.b0s_mask] = affines\\n\\n    data_array = np.zeros(data.shape)\\n    data_array[..., gtab.b0s_mask] = trans_b0\\n    data_array[..., ~gtab.b0s_mask] = xformed\\n\\n    return nib.Nifti1Image(data_array, affine), affine_array\\n\\n\\nmotion_correction = partial(register_dwi_series, pipeline=[\"center_of_mass\",\\n                                                           \"translation\",\\n                                                           \"rigid\", \"affine\"])\\nmotion_correction.__doc__ = re.sub(\\'Register.*?volume\\', \\'Apply a motion \\'\\n                                   \\'correction to a DWI dataset \\'\\n                                   \\'(Between-Volumes Motion correction)\\',\\n                                   register_dwi_series.__doc__,\\n                                   flags=re.DOTALL)\\n\\n\\ndef streamline_registration(moving, static, n_points=100,\\n                            native_resampled=False):\\n    \"\"\" Register two collections of streamlines (\\'bundles\\') to each other.\\n\\n    Parameters\\n    ----------\\n    moving, static : lists of 3 by n, or str\\n        The two bundles to be registered. Given either as lists of arrays with\\n        3D coordinates, or strings containing full paths to these files.\\n\\n    n_points : int, optional\\n        How many points to resample to. Default: 100.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\_public.py.txt'}),\n",
       " Document(page_content='def streamline_registration(moving, static, n_points=100,\\n                            native_resampled=False):\\n    \"\"\" Register two collections of streamlines (\\'bundles\\') to each other.\\n\\n    Parameters\\n    ----------\\n    moving, static : lists of 3 by n, or str\\n        The two bundles to be registered. Given either as lists of arrays with\\n        3D coordinates, or strings containing full paths to these files.\\n\\n    n_points : int, optional\\n        How many points to resample to. Default: 100.\\n\\n    native_resampled : bool, optional\\n        Whether to return the moving bundle in the original space, but\\n        resampled in the static space to n_points.\\n\\n    Returns\\n    -------\\n    aligned : list\\n        Streamlines from the moving group, moved to be closely matched to\\n        the static group.\\n\\n    matrix : array (4, 4)\\n        The affine transformation that takes us from \\'moving\\' to \\'static\\'\\n\\n    \"\"\"\\n    # Load the streamlines, if you were given a file-name\\n    if isinstance(moving, str):\\n        moving = load_trk(moving, \\'same\\', bbox_valid_check=False).streamlines\\n    if isinstance(static, str):\\n        static = load_trk(static, \\'same\\', bbox_valid_check=False).streamlines\\n\\n    srr = StreamlineLinearRegistration()\\n    srm = srr.optimize(static=set_number_of_points(static, n_points),\\n                       moving=set_number_of_points(moving, n_points))\\n\\n    aligned = srm.transform(moving)\\n    if native_resampled:\\n        aligned = set_number_of_points(aligned, n_points)\\n        aligned = transform_tracking_output(aligned, np.linalg.inv(srm.matrix))\\n\\n    return aligned, srm.matrix', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\_public.py.txt'}),\n",
       " Document(page_content='import numpy as np\\nfloating = np.float32\\n\\n\\nclass Bunch:\\n    def __init__(self, **kwds):\\n        r\"\"\"A \\'bunch\\' of values (a replacement of Enum)\\n\\n        This is a temporary replacement of Enum, which is not available\\n        on all versions of Python 2\\n        \"\"\"\\n        self.__dict__.update(kwds)\\n\\n\\nVerbosityLevels = Bunch(NONE=0, STATUS=1, DIAGNOSE=2, DEBUG=3)\\nr\"\"\" VerbosityLevels\\nThis enum defines the four levels of verbosity we use in the align\\nmodule.\\nNONE : do not print anything\\nSTATUS : print information about the current status of the algorithm\\nDIAGNOSE : print high level information of the components involved in the\\nregistration that can be used to detect a failing component.\\nDEBUG : print as much information as possible to isolate the cause of a bug.\\n\"\"\"\\n\\nfrom dipy.align._public import (syn_registration, register_dwi_to_template, # noqa\\n                                write_mapping, read_mapping, resample,\\n                                center_of_mass, translation,\\n                                rigid_isoscaling, rigid_scaling,\\n                                rigid, affine, motion_correction,\\n                                affine_registration, register_series,\\n                                register_dwi_series, streamline_registration)\\n\\n__all__ = [\"syn_registration\", \"register_dwi_to_template\",\\n           \"write_mapping\", \"read_mapping\", \"resample\",\\n           \"center_of_mass\", \"translation\",\\n           \"rigid_isoscaling\", \"rigid_scaling\",\\n           \"rigid\", \"affine\", \"motion_correction\",\\n           \"affine_registration\", \"register_series\",\\n           \"register_dwi_series\", \"streamline_registration\"]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\__init__.py.txt'}),\n",
       " Document(page_content=\"python_sources = ['__init__.py',\\n  'test_api.py',\\n  'test_crosscorr.py',\\n  'test_expectmax.py',\\n  'test_imaffine.py',\\n  'test_imwarp.py',\\n  'test_metrics.py',\\n  'test_parzenhist.py',\\n  'test_reslice.py',\\n  'test_scalespace.py',\\n  'test_streamlinear.py',\\n  'test_streamwarp.py',\\n  'test_sumsqdiff.py',\\n  'test_transforms.py',\\n  'test_vector_fields.py',\\n  'test_whole_brain_slr.py',\\n  ]\\n\\npy3.install_sources(\\n  python_sources,\\n  pure: false,\\n  subdir: 'dipy/align/tests'\\n)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\meson.build.txt'}),\n",
       " Document(page_content='import os.path as op\\nimport pytest\\nfrom tempfile import TemporaryDirectory\\n\\nimport numpy as np\\nimport numpy.testing as npt\\n\\nimport nibabel as nib\\n\\nimport dipy.data as dpd\\nimport dipy.core.gradients as dpg\\n\\nfrom dipy.align import (syn_registration, register_series, register_dwi_series,\\n                        center_of_mass, translation, rigid_isoscaling,\\n                        rigid_scaling, rigid, affine, motion_correction,\\n                        affine_registration, streamline_registration,\\n                        write_mapping, read_mapping, register_dwi_to_template)\\n\\nfrom dipy.align.imwarp import DiffeomorphicMap\\n\\nfrom dipy.tracking.utils import transform_tracking_output\\nfrom dipy.io.streamline import save_trk\\nfrom dipy.io.stateful_tractogram import StatefulTractogram, Space\\nfrom dipy.io.gradients import read_bvals_bvecs\\nfrom dipy.io.image import load_nifti\\nfrom dipy.testing.decorators import set_random_number_generator\\n\\n\\ndef setup_module():\\n    global subset_b0, subset_dwi_data, subset_t2, subset_b0_img, \\\\\\n           subset_t2_img, gtab, hardi_affine, MNI_T2_affine\\n    MNI_T2 = dpd.read_mni_template()\\n    hardi_img, gtab = dpd.read_stanford_hardi()\\n    MNI_T2_data = MNI_T2.get_fdata()\\n    MNI_T2_affine = MNI_T2.affine\\n    hardi_data = hardi_img.get_fdata()\\n    hardi_affine = hardi_img.affine\\n    b0 = hardi_data[..., gtab.b0s_mask]\\n    mean_b0 = np.mean(b0, -1)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_api.py.txt'}),\n",
       " Document(page_content=\"def setup_module():\\n    global subset_b0, subset_dwi_data, subset_t2, subset_b0_img, \\\\\\n           subset_t2_img, gtab, hardi_affine, MNI_T2_affine\\n    MNI_T2 = dpd.read_mni_template()\\n    hardi_img, gtab = dpd.read_stanford_hardi()\\n    MNI_T2_data = MNI_T2.get_fdata()\\n    MNI_T2_affine = MNI_T2.affine\\n    hardi_data = hardi_img.get_fdata()\\n    hardi_affine = hardi_img.affine\\n    b0 = hardi_data[..., gtab.b0s_mask]\\n    mean_b0 = np.mean(b0, -1)\\n\\n    # We select some arbitrary chunk of data so this goes quicker:\\n    subset_b0 = mean_b0[40:45, 40:45, 40:45]\\n    subset_dwi_data = nib.Nifti1Image(hardi_data[40:45, 40:45, 40:45],\\n                                      hardi_affine)\\n    subset_t2 = MNI_T2_data[40:50, 40:50, 40:50]\\n    subset_b0_img = nib.Nifti1Image(subset_b0, hardi_affine)\\n    subset_t2_img = nib.Nifti1Image(subset_t2, MNI_T2_affine)\\n\\n\\ndef test_syn_registration():\\n    with TemporaryDirectory() as tmpdir:\\n        warped_moving, mapping = syn_registration(subset_b0,\\n                                                  subset_t2,\\n                                                  moving_affine=hardi_affine,\\n                                                  static_affine=MNI_T2_affine,\\n                                                  step_length=0.1,\\n                                                  metric='CC',\\n                                                  dim=3,\\n                                                  level_iters=[5, 5, 5],\\n                                                  sigma_diff=2.0,\\n                                                  radius=1,\\n                                                  prealign=None)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_api.py.txt'}),\n",
       " Document(page_content=\"npt.assert_equal(warped_moving.shape, subset_t2.shape)\\n        mapping_fname = op.join(tmpdir, 'mapping.nii.gz')\\n        write_mapping(mapping, mapping_fname)\\n        file_mapping = read_mapping(mapping_fname,\\n                                    subset_b0_img,\\n                                    subset_t2_img)\\n\\n        # Test that it has the same effect on the data:\\n        warped_from_file = file_mapping.transform(subset_b0)\\n        npt.assert_equal(warped_from_file, warped_moving)\\n\\n        # Test that it is, attribute by attribute, identical:\\n        for k in mapping.__dict__:\\n            npt.assert_((np.all(mapping.__getattribute__(k) ==\\n                                file_mapping.__getattribute__(k))))\\n\\n\\ndef test_register_dwi_to_template():\\n    # Default is syn registration:\\n    warped_b0, mapping = register_dwi_to_template(subset_dwi_data, gtab,\\n                                                  template=subset_t2_img,\\n                                                  level_iters=[5, 5, 5],\\n                                                  sigma_diff=2.0,\\n                                                  radius=1)\\n    npt.assert_(isinstance(mapping, DiffeomorphicMap))\\n    npt.assert_equal(warped_b0.shape, subset_t2_img.shape)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_api.py.txt'}),\n",
       " Document(page_content='# Use affine registration (+ don\\'t provide a template and inputs as\\n    # strings):\\n    fdata, fbval, fbvec = dpd.get_fnames(\\'small_64D\\')\\n    warped_data, affine_mat = register_dwi_to_template(fdata, (fbval, fbvec),\\n                                                       reg_method=\"aff\",\\n                                                       level_iters=[5, 5, 5],\\n                                                       sigmas=[3, 1, 0],\\n                                                       factors=[4, 2, 1])\\n    npt.assert_(isinstance(affine_mat, np.ndarray))\\n    npt.assert_(affine_mat.shape == (4, 4))\\n\\n\\ndef test_affine_registration():\\n    moving = subset_b0\\n    static = subset_b0\\n    moving_affine = static_affine = np.eye(4)\\n    xformed, affine_mat = affine_registration(moving, static,\\n                                              moving_affine=moving_affine,\\n                                              static_affine=static_affine,\\n                                              level_iters=[5, 5],\\n                                              sigmas=[3, 1],\\n                                              factors=[2, 1])\\n\\n    # We don\\'t ask for much:\\n    npt.assert_almost_equal(affine_mat[:3, :3], np.eye(3), decimal=1)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_api.py.txt'}),\n",
       " Document(page_content='# We don\\'t ask for much:\\n    npt.assert_almost_equal(affine_mat[:3, :3], np.eye(3), decimal=1)\\n\\n    # [center_of_mass] + ret_metric=True should raise an error\\n    with pytest.raises(ValueError):\\n        # For array input, must provide affines:\\n        xformed, affine_mat = affine_registration(moving, static,\\n                                                  moving_affine=moving_affine,\\n                                                  static_affine=static_affine,\\n                                                  pipeline=[\"center_of_mass\"],\\n                                                  ret_metric=True)\\n\\n    # Define list of methods\\n    reg_methods = [\"center_of_mass\", \"translation\", \"rigid\",\\n                   \"rigid_isoscaling\", \"rigid_scaling\", \"affine\",\\n                   center_of_mass, translation, rigid,\\n                   rigid_isoscaling, rigid_scaling, affine]\\n\\n    # Test methods individually (without returning any metric)\\n    for func in reg_methods:\\n        xformed, affine_mat = affine_registration(moving, static,\\n                                                  moving_affine=moving_affine,\\n                                                  static_affine=static_affine,\\n                                                  level_iters=[5, 5],\\n                                                  sigmas=[3, 1],\\n                                                  factors=[2, 1],\\n                                                  pipeline=[func])\\n        # We don\\'t ask for much:\\n        npt.assert_almost_equal(affine_mat[:3, :3], np.eye(3), decimal=1)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_api.py.txt'}),\n",
       " Document(page_content=\"# Bad method\\n    with pytest.raises(ValueError, match=r'^pipeline\\\\[0\\\\] must be one.*foo.*'):\\n        affine_registration(\\n            moving, static, moving_affine, static_affine, pipeline=['foo'])\\n\\n    # Test methods individually (returning quality metric)\\n    expected_nparams = [0, 3, 6, 7, 9, 12] * 2\\n    assert len(expected_nparams) == len(reg_methods)\\n    for i, func in enumerate(reg_methods):\\n        if func in ('center_of_mass', center_of_mass):\\n            # can't return metric\\n            with pytest.raises(ValueError, match='cannot return any quality'):\\n                affine_registration(\\n                    moving, static, moving_affine, static_affine,\\n                    pipeline=[func], ret_metric=True)\\n            continue\\n\\n        xformed, affine_mat, \\\\\\n            xopt, fopt = affine_registration(moving, static,\\n                                             moving_affine=moving_affine,\\n                                             static_affine=static_affine,\\n                                             level_iters=[5, 5],\\n                                             sigmas=[3, 1],\\n                                             factors=[2, 1],\\n                                             pipeline=[func],\\n                                             ret_metric=True)\\n        # Expected number of optimization parameters\\n        npt.assert_equal(len(xopt), expected_nparams[i])\\n        # Optimization metric must be a single numeric value\\n        npt.assert_equal(isinstance(fopt, (int, float)), True)\\n\\n    with pytest.raises(ValueError):\\n        # For array input, must provide affines:\\n        xformed, affine_mat = affine_registration(moving, static)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_api.py.txt'}),\n",
       " Document(page_content='with pytest.raises(ValueError):\\n        # For array input, must provide affines:\\n        xformed, affine_mat = affine_registration(moving, static)\\n\\n    # Not supported transform names should raise an error\\n    npt.assert_raises(ValueError, affine_registration, moving, static,\\n                      moving_affine, static_affine,\\n                      pipeline=[\"wrong_transform\"])\\n\\n    # If providing nifti image objects, don\\'t need to provide affines:\\n    moving_img = nib.Nifti1Image(moving, moving_affine)\\n    static_img = nib.Nifti1Image(static, static_affine)\\n    xformed, affine_mat = affine_registration(moving_img, static_img)\\n    npt.assert_almost_equal(affine_mat[:3, :3], np.eye(3), decimal=1)\\n\\n    # Using strings with full paths as inputs also works:\\n    t1_name, b0_name = dpd.get_fnames(\\'syn_data\\')\\n    moving = b0_name\\n    static = t1_name\\n    xformed, affine_mat = affine_registration(moving, static,\\n                                              level_iters=[5, 5],\\n                                              sigmas=[3, 1],\\n                                              factors=[4, 2])\\n    npt.assert_almost_equal(affine_mat[:3, :3], np.eye(3), decimal=1)\\n\\n\\ndef test_single_transforms():\\n    moving = subset_b0\\n    static = subset_b0\\n    moving_affine = static_affine = np.eye(4)\\n\\n    reg_methods = [center_of_mass, translation, rigid_isoscaling,\\n                   rigid_scaling, rigid, affine]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_api.py.txt'}),\n",
       " Document(page_content=\"def test_single_transforms():\\n    moving = subset_b0\\n    static = subset_b0\\n    moving_affine = static_affine = np.eye(4)\\n\\n    reg_methods = [center_of_mass, translation, rigid_isoscaling,\\n                   rigid_scaling, rigid, affine]\\n\\n    for func in reg_methods:\\n        xformed, affine_mat = func(moving, static, moving_affine,\\n                                   static_affine, level_iters=[5, 5],\\n                                   sigmas=[3, 1], factors=[2, 1])\\n        # We don't ask for much:\\n        npt.assert_almost_equal(affine_mat[:3, :3], np.eye(3), decimal=1)\\n\\n\\ndef test_register_series():\\n    fdata, fbval, fbvec = dpd.get_fnames('small_64D')\\n    img = nib.load(fdata)\\n    gtab = dpg.gradient_table(fbval, fbvec)\\n    ref_idx = np.where(gtab.b0s_mask)[0][0]\\n    xformed, affines = register_series(img, ref_idx)\\n    npt.assert_(np.all(affines[..., ref_idx] == np.eye(4)))\\n    npt.assert_(np.all(xformed[..., ref_idx] == img.get_fdata()[..., ref_idx]))\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_api.py.txt'}),\n",
       " Document(page_content=\"def test_register_series():\\n    fdata, fbval, fbvec = dpd.get_fnames('small_64D')\\n    img = nib.load(fdata)\\n    gtab = dpg.gradient_table(fbval, fbvec)\\n    ref_idx = np.where(gtab.b0s_mask)[0][0]\\n    xformed, affines = register_series(img, ref_idx)\\n    npt.assert_(np.all(affines[..., ref_idx] == np.eye(4)))\\n    npt.assert_(np.all(xformed[..., ref_idx] == img.get_fdata()[..., ref_idx]))\\n\\n\\ndef test_register_dwi_series_and_motion_correction():\\n    fdata, fbval, fbvec = dpd.get_fnames('small_64D')\\n    with TemporaryDirectory() as tmpdir:\\n        # Use an abbreviated data-set:\\n        img = nib.load(fdata)\\n        data = img.get_fdata()[..., :10]\\n        nib.save(nib.Nifti1Image(data, img.affine),\\n                 op.join(tmpdir, 'data.nii.gz'))\\n        # Save a subset:\\n        bvals = np.loadtxt(fbval)\\n        bvecs = np.loadtxt(fbvec)\\n        np.savetxt(op.join(tmpdir, 'bvals.txt'), bvals[:10])\\n        np.savetxt(op.join(tmpdir, 'bvecs.txt'), bvecs[:10])\\n        gtab = dpg.gradient_table(op.join(tmpdir, 'bvals.txt'),\\n                                  op.join(tmpdir, 'bvecs.txt'))\\n        reg_img, reg_affines = register_dwi_series(data, gtab, img.affine)\\n        reg_img_2, reg_affines_2 = motion_correction(data, gtab, img.affine)\\n        npt.assert_(isinstance(reg_img, nib.Nifti1Image))\\n\\n        npt.assert_array_equal(reg_img.get_fdata(), reg_img_2.get_fdata())\\n        npt.assert_array_equal(reg_affines, reg_affines_2)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_api.py.txt'}),\n",
       " Document(page_content=\"npt.assert_array_equal(reg_img.get_fdata(), reg_img_2.get_fdata())\\n        npt.assert_array_equal(reg_affines, reg_affines_2)\\n\\n\\n@set_random_number_generator()\\ndef test_streamline_registration(rng):\\n    sl1 = [np.array([[0, 0, 0], [0, 0, 0.5], [0, 0, 1], [0, 0, 1.5]]),\\n           np.array([[0, 0, 0], [0, 0.5, 0.5], [0, 1, 1]])]\\n    affine_mat = np.eye(4)\\n    affine_mat[:3, 3] = rng.standard_normal(3)\\n    sl2 = list(transform_tracking_output(sl1, affine_mat))\\n    aligned, matrix = streamline_registration(sl2, sl1)\\n    npt.assert_almost_equal(matrix, np.linalg.inv(affine_mat))\\n    npt.assert_almost_equal(aligned[0], sl1[0])\\n    npt.assert_almost_equal(aligned[1], sl1[1])\\n\\n    # We assume the two tracks come from the same space, but it might have\\n    # some affine associated with it:\\n    base_aff = np.eye(4) * rng.random()\\n    base_aff[:3, 3] = np.array([1, 2, 3])\\n    base_aff[3, 3] = 1\\n\\n    with TemporaryDirectory() as tmpdir:\\n        for use_aff in [None, base_aff]:\\n            fname1 = op.join(tmpdir, 'sl1.trk')\\n            fname2 = op.join(tmpdir, 'sl2.trk')\\n            if use_aff is not None:\\n                img = nib.Nifti1Image(np.zeros((2, 2, 2)), use_aff)\\n                # Move the streamlines to this other space, and report it:\\n                tgm1 = StatefulTractogram(\\n                    transform_tracking_output(sl1, np.linalg.inv(use_aff)),\\n                    img,\\n                    Space.VOX)\\n\\n                save_trk(tgm1, fname1, bbox_valid_check=False)\\n\\n                tgm2 = StatefulTractogram(\\n                    transform_tracking_output(sl2, np.linalg.inv(use_aff)),\\n                    img,\\n                    Space.VOX)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_api.py.txt'}),\n",
       " Document(page_content=\"save_trk(tgm1, fname1, bbox_valid_check=False)\\n\\n                tgm2 = StatefulTractogram(\\n                    transform_tracking_output(sl2, np.linalg.inv(use_aff)),\\n                    img,\\n                    Space.VOX)\\n\\n                save_trk(tgm2, fname2, bbox_valid_check=False)\\n\\n            else:\\n                img = nib.Nifti1Image(np.zeros((2, 2, 2)), np.eye(4))\\n                tgm1 = StatefulTractogram(sl1, img, Space.RASMM)\\n                tgm2 = StatefulTractogram(sl2, img, Space.RASMM)\\n                save_trk(tgm1, fname1, bbox_valid_check=False)\\n                save_trk(tgm2, fname2, bbox_valid_check=False)\\n\\n            aligned, matrix = streamline_registration(fname2, fname1)\\n            npt.assert_almost_equal(aligned[0], sl1[0], decimal=5)\\n            npt.assert_almost_equal(aligned[1], sl1[1], decimal=5)\\n\\n\\ndef test_register_dwi_series_multi_b0():\\n    # Test if register_dwi_series works with multiple b0 images\\n    dwi_fname, dwi_bval_fname, \\\\\\n        dwi_bvec_fname = dpd.get_fnames('sherbrooke_3shell')\\n    data, affine = load_nifti(dwi_fname)\\n    bvals, bvecs = read_bvals_bvecs(dwi_bval_fname, dwi_bvec_fname)\\n\\n    data_small = data[..., :3]\\n    data_small = np.concatenate([data[..., :1], data_small], axis=-1)\\n    bvals_small = np.concatenate([bvals[:1], bvals[:3]], axis=0)\\n    bvecs_small = np.concatenate([bvecs[:1], bvecs[:3]], axis=0)\\n    gtab = dpg.gradient_table(bvals_small, bvecs_small)\\n    _ = motion_correction(data_small, gtab, affine)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_api.py.txt'}),\n",
       " Document(page_content='import numpy as np\\nfrom numpy.testing import assert_array_almost_equal\\nfrom dipy.align import floating\\nfrom dipy.align import crosscorr as cc\\nfrom dipy.testing.decorators import set_random_number_generator\\n\\n\\ndef test_cc_factors_2d():\\n    r\"\"\"\\n    Compares the output of the optimized function to compute the cross-\\n    correlation factors against a direct (not optimized, but less error prone)\\n    implementation.\\n    \"\"\"\\n    a = np.array(range(20*20), dtype=floating).reshape(20, 20)\\n    b = np.array(range(20*20)[::-1], dtype=floating).reshape(20, 20)\\n    a /= a.max()\\n    b /= b.max()\\n    for radius in [0, 1, 3, 6]:\\n        factors = np.asarray(cc.precompute_cc_factors_2d(a, b, radius))\\n        expected = np.asarray(cc.precompute_cc_factors_2d_test(a, b, radius))\\n        assert_array_almost_equal(factors, expected)\\n\\n\\ndef test_cc_factors_3d():\\n    r\"\"\"\\n    Compares the output of the optimized function to compute the cross-\\n    correlation factors against a direct (not optimized, but less error prone)\\n    implementation.\\n    \"\"\"\\n    a = np.array(range(20*20*20), dtype=floating).reshape(20, 20, 20)\\n    b = np.array(range(20*20*20)[::-1], dtype=floating).reshape(20, 20, 20)\\n    a /= a.max()\\n    b /= b.max()\\n    for radius in [0, 1, 3, 6]:\\n        factors = np.asarray(cc.precompute_cc_factors_3d(a, b, radius))\\n        expected = np.asarray(cc.precompute_cc_factors_3d_test(a, b, radius))\\n        assert_array_almost_equal(factors, expected, decimal=5)\\n\\n\\n@set_random_number_generator(1147572)\\ndef test_compute_cc_steps_2d(rng):\\n    # Select arbitrary images\\' shape (same shape for both images)\\n    sh = (32, 32)\\n    radius = 2', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_crosscorr.py.txt'}),\n",
       " Document(page_content=\"@set_random_number_generator(1147572)\\ndef test_compute_cc_steps_2d(rng):\\n    # Select arbitrary images' shape (same shape for both images)\\n    sh = (32, 32)\\n    radius = 2\\n\\n    # Select arbitrary centers\\n    c_f = (np.asarray(sh)/2) + 1.25\\n    c_g = c_f + 2.5\\n\\n    # Compute the identity vector field I(x) = x in R^2\\n    x_0 = np.asarray(range(sh[0]))\\n    x_1 = np.asarray(range(sh[1]))\\n    X = np.ndarray(sh + (2,), dtype=np.float64)\\n    O = np.ones(sh)\\n    X[..., 0] = x_0[:, None] * O\\n    X[..., 1] = x_1[None, :] * O\\n\\n    # Compute the gradient fields of F and G\\n    gradF = np.array(X - c_f, dtype=floating)\\n    gradG = np.array(X - c_g, dtype=floating)\\n\\n    sz = np.size(gradF)\\n    Fnoise = rng.random(sz).reshape(gradF.shape) * gradF.max() * 0.1\\n    Fnoise = Fnoise.astype(floating)\\n    gradF += Fnoise\\n\\n    sz = np.size(gradG)\\n    Gnoise = rng.random(sz).reshape(gradG.shape) * gradG.max() * 0.1\\n    Gnoise = Gnoise.astype(floating)\\n    gradG += Gnoise\\n\\n    sq_norm_grad_G = np.sum(gradG**2, -1)\\n\\n    F = np.array(0.5*np.sum(gradF**2, -1), dtype=floating)\\n    G = np.array(0.5*sq_norm_grad_G, dtype=floating)\\n\\n    Fnoise = rng.random(np.size(F)).reshape(F.shape) * F.max() * 0.1\\n    Fnoise = Fnoise.astype(floating)\\n    F += Fnoise\\n\\n    Gnoise = rng.random(np.size(G)).reshape(G.shape) * G.max() * 0.1\\n    Gnoise = Gnoise.astype(floating)\\n    G += Gnoise\\n\\n    # precompute the cross correlation factors\\n    factors = cc.precompute_cc_factors_2d_test(F, G, radius)\\n    factors = np.array(factors, dtype=floating)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_crosscorr.py.txt'}),\n",
       " Document(page_content='Fnoise = rng.random(np.size(F)).reshape(F.shape) * F.max() * 0.1\\n    Fnoise = Fnoise.astype(floating)\\n    F += Fnoise\\n\\n    Gnoise = rng.random(np.size(G)).reshape(G.shape) * G.max() * 0.1\\n    Gnoise = Gnoise.astype(floating)\\n    G += Gnoise\\n\\n    # precompute the cross correlation factors\\n    factors = cc.precompute_cc_factors_2d_test(F, G, radius)\\n    factors = np.array(factors, dtype=floating)\\n\\n    # test the forward step against the exact expression\\n    I = factors[..., 0]\\n    J = factors[..., 1]\\n    sfm = factors[..., 2]\\n    sff = factors[..., 3]\\n    smm = factors[..., 4]\\n    expected = np.ndarray(shape=sh + (2,), dtype=floating)\\n    factor = (-2.0 * sfm / (sff * smm)) * (J - (sfm / sff) * I)\\n    expected[..., 0] = factor * gradF[..., 0]\\n    factor = (-2.0 * sfm / (sff * smm)) * (J - (sfm / sff) * I)\\n    expected[..., 1] = factor * gradF[..., 1]\\n    actual, energy = cc.compute_cc_forward_step_2d(gradF, factors, 0)\\n    assert_array_almost_equal(actual, expected)\\n    for radius in range(1, 5):\\n        expected[:radius, ...] = 0\\n        expected[:, :radius, ...] = 0\\n        expected[-radius::, ...] = 0\\n        expected[:, -radius::, ...] = 0\\n        actual, energy = cc.compute_cc_forward_step_2d(gradF, factors, radius)\\n        assert_array_almost_equal(actual, expected)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_crosscorr.py.txt'}),\n",
       " Document(page_content='# test the backward step against the exact expression\\n    factor = (-2.0 * sfm / (sff * smm)) * (I - (sfm / smm) * J)\\n    expected[..., 0] = factor * gradG[..., 0]\\n    factor = (-2.0 * sfm / (sff * smm)) * (I - (sfm / smm) * J)\\n    expected[..., 1] = factor * gradG[..., 1]\\n    actual, energy = cc.compute_cc_backward_step_2d(gradG, factors, 0)\\n    assert_array_almost_equal(actual, expected)\\n    for radius in range(1, 5):\\n        expected[:radius, ...] = 0\\n        expected[:, :radius, ...] = 0\\n        expected[-radius::, ...] = 0\\n        expected[:, -radius::, ...] = 0\\n        actual, energy = cc.compute_cc_backward_step_2d(gradG, factors, radius)\\n        assert_array_almost_equal(actual, expected)\\n\\n\\n@set_random_number_generator(12465825)\\ndef test_compute_cc_steps_3d(rng):\\n    sh = (32, 32, 32)\\n    radius = 2\\n\\n    # Select arbitrary centers\\n    c_f = (np.asarray(sh)/2) + 1.25\\n    c_g = c_f + 2.5\\n\\n    # Compute the identity vector field I(x) = x in R^2\\n    x_0 = np.asarray(range(sh[0]))\\n    x_1 = np.asarray(range(sh[1]))\\n    x_2 = np.asarray(range(sh[2]))\\n    X = np.ndarray(sh + (3,), dtype=np.float64)\\n    O = np.ones(sh)\\n    X[..., 0] = x_0[:, None, None] * O\\n    X[..., 1] = x_1[None, :, None] * O\\n    X[..., 2] = x_2[None, None, :] * O\\n\\n    # Compute the gradient fields of F and G\\n    gradF = np.array(X - c_f, dtype=floating)\\n    gradG = np.array(X - c_g, dtype=floating)\\n\\n    sz = np.size(gradF)\\n    Fnoise = rng.random(sz).reshape(gradF.shape) * gradF.max() * 0.1\\n    Fnoise = Fnoise.astype(floating)\\n    gradF += Fnoise\\n\\n    sz = np.size(gradG)\\n    Gnoise = rng.random(sz).reshape(gradG.shape) * gradG.max() * 0.1\\n    Gnoise = Gnoise.astype(floating)\\n    gradG += Gnoise', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_crosscorr.py.txt'}),\n",
       " Document(page_content='# Compute the gradient fields of F and G\\n    gradF = np.array(X - c_f, dtype=floating)\\n    gradG = np.array(X - c_g, dtype=floating)\\n\\n    sz = np.size(gradF)\\n    Fnoise = rng.random(sz).reshape(gradF.shape) * gradF.max() * 0.1\\n    Fnoise = Fnoise.astype(floating)\\n    gradF += Fnoise\\n\\n    sz = np.size(gradG)\\n    Gnoise = rng.random(sz).reshape(gradG.shape) * gradG.max() * 0.1\\n    Gnoise = Gnoise.astype(floating)\\n    gradG += Gnoise\\n\\n    sq_norm_grad_G = np.sum(gradG**2, -1)\\n\\n    F = np.array(0.5*np.sum(gradF**2, -1), dtype=floating)\\n    G = np.array(0.5*sq_norm_grad_G, dtype=floating)\\n\\n    Fnoise = rng.random(np.size(F)).reshape(F.shape) * F.max() * 0.1\\n    Fnoise = Fnoise.astype(floating)\\n    F += Fnoise\\n\\n    Gnoise = rng.random(np.size(G)).reshape(G.shape) * G.max() * 0.1\\n    Gnoise = Gnoise.astype(floating)\\n    G += Gnoise\\n\\n    # precompute the cross correlation factors\\n    factors = cc.precompute_cc_factors_3d_test(F, G, radius)\\n    factors = np.array(factors, dtype=floating)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_crosscorr.py.txt'}),\n",
       " Document(page_content='Fnoise = rng.random(np.size(F)).reshape(F.shape) * F.max() * 0.1\\n    Fnoise = Fnoise.astype(floating)\\n    F += Fnoise\\n\\n    Gnoise = rng.random(np.size(G)).reshape(G.shape) * G.max() * 0.1\\n    Gnoise = Gnoise.astype(floating)\\n    G += Gnoise\\n\\n    # precompute the cross correlation factors\\n    factors = cc.precompute_cc_factors_3d_test(F, G, radius)\\n    factors = np.array(factors, dtype=floating)\\n\\n    # test the forward step against the exact expression\\n    I = factors[..., 0]\\n    J = factors[..., 1]\\n    sfm = factors[..., 2]\\n    sff = factors[..., 3]\\n    smm = factors[..., 4]\\n    expected = np.ndarray(shape=sh + (3,), dtype=floating)\\n    factor = (-2.0 * sfm / (sff * smm)) * (J - (sfm / sff) * I)\\n    expected[..., 0] = factor * gradF[..., 0]\\n    expected[..., 1] = factor * gradF[..., 1]\\n    expected[..., 2] = factor * gradF[..., 2]\\n    actual, energy = cc.compute_cc_forward_step_3d(gradF, factors, 0)\\n    assert_array_almost_equal(actual, expected)\\n    for radius in range(1, 5):\\n        expected[:radius, ...] = 0\\n        expected[:, :radius, ...] = 0\\n        expected[:, :, :radius, :] = 0\\n        expected[-radius::, ...] = 0\\n        expected[:, -radius::, ...] = 0\\n        expected[:, :, -radius::, ...] = 0\\n        actual, energy = cc.compute_cc_forward_step_3d(gradF, factors, radius)\\n        assert_array_almost_equal(actual, expected)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_crosscorr.py.txt'}),\n",
       " Document(page_content='# test the backward step against the exact expression\\n    factor = (-2.0 * sfm / (sff * smm)) * (I - (sfm / smm) * J)\\n    expected[..., 0] = factor * gradG[..., 0]\\n    expected[..., 1] = factor * gradG[..., 1]\\n    expected[..., 2] = factor * gradG[..., 2]\\n    actual, energy = cc.compute_cc_backward_step_3d(gradG, factors, 0)\\n    assert_array_almost_equal(actual, expected)\\n    for radius in range(1, 5):\\n        expected[:radius, ...] = 0\\n        expected[:, :radius, ...] = 0\\n        expected[:, :, :radius, :] = 0\\n        expected[-radius::, ...] = 0\\n        expected[:, -radius::, ...] = 0\\n        expected[:, :, -radius::, ...] = 0\\n        actual, energy = cc.compute_cc_backward_step_3d(gradG, factors, radius)\\n        assert_array_almost_equal(actual, expected)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_crosscorr.py.txt'}),\n",
       " Document(page_content='import numpy as np\\nfrom numpy.testing import (assert_equal,\\n                           assert_array_equal,\\n                           assert_array_almost_equal,\\n                           assert_raises)\\nfrom dipy.align import floating\\nfrom dipy.align import expectmax as em\\nfrom dipy.testing.decorators import set_random_number_generator\\n\\n\\n@set_random_number_generator(1346491)\\ndef test_compute_em_demons_step_2d(rng):\\n    r\"\"\"\\n    Compares the output of the demons step in 2d against an analytical\\n    step. The fixed image is given by $F(x) = \\\\frac{1}{2}||x - c_f||^2$, the\\n    moving image is given by $G(x) = \\\\frac{1}{2}||x - c_g||^2$,\\n    $x, c_f, c_g \\\\in R^{2}$\\n\\n    References\\n    ----------\\n    [Vercauteren09] Vercauteren, T., Pennec, X., Perchant, A., & Ayache, N.\\n                    (2009). Diffeomorphic demons: efficient non-parametric\\n                    image registration. NeuroImage, 45(1 Suppl), S61-72.\\n                    doi:10.1016/j.neuroimage.2008.10.040\\n    \"\"\"\\n    # Select arbitrary images\\' shape (same shape for both images)\\n    sh = (30, 20)\\n\\n    # Select arbitrary centers\\n    c_f = np.asarray(sh) / 2\\n    c_g = c_f + 0.5\\n\\n    # Compute the identity vector field I(x) = x in R^2\\n    x_0 = np.asarray(range(sh[0]))\\n    x_1 = np.asarray(range(sh[1]))\\n    X = np.ndarray(sh + (2,), dtype=np.float64)\\n    O = np.ones(sh)\\n    X[..., 0] = x_0[:, None] * O\\n    X[..., 1] = x_1[None, :] * O\\n\\n    # Compute the gradient fields of F and G\\n    grad_F = X - c_f\\n    grad_G = X - c_g\\n\\n    # The squared norm of grad_G to be used later\\n    sq_norm_grad_G = np.sum(grad_G**2, -1)\\n\\n    # Compute F and G\\n    F = 0.5 * np.sum(grad_F**2, -1)\\n    G = 0.5 * sq_norm_grad_G\\n    delta_field = G - F', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_expectmax.py.txt'}),\n",
       " Document(page_content=\"# Compute the gradient fields of F and G\\n    grad_F = X - c_f\\n    grad_G = X - c_g\\n\\n    # The squared norm of grad_G to be used later\\n    sq_norm_grad_G = np.sum(grad_G**2, -1)\\n\\n    # Compute F and G\\n    F = 0.5 * np.sum(grad_F**2, -1)\\n    G = 0.5 * sq_norm_grad_G\\n    delta_field = G - F\\n\\n    # Now select an arbitrary parameter for\\n    # $\\\\sigma_x$ (eq 4 in [Vercauteren09])\\n    sigma_x_sq = 1.5\\n\\n    # Set arbitrary values for $\\\\sigma_i$ (eq. 4 in [Vercauteren09])\\n    # The original Demons algorithm used simply |F(x) - G(x)| as an\\n    # estimator, so let's use it as well\\n    sigma_i_sq = (F - G)**2\\n\\n    # Select some pixels to have special values\\n    random_labels = rng.integers(0, 5, sh[0] * sh[1])\\n    random_labels = random_labels.reshape(sh)\\n\\n    # this label is used to set sigma_i_sq == 0 below\\n    random_labels[sigma_i_sq == 0] = 2\\n    # this label is used to set gradient == 0 below\\n    random_labels[sq_norm_grad_G == 0] = 2\\n\\n    expected = np.zeros_like(grad_G)\\n    # Pixels with sigma_i_sq = inf\\n    sigma_i_sq[random_labels == 0] = np.inf\\n    expected[random_labels == 0, ...] = 0\\n\\n    # Pixels with gradient!=0 and sigma_i_sq=0\\n    sqnrm = sq_norm_grad_G[random_labels == 1]\\n    sigma_i_sq[random_labels == 1] = 0\\n    expected[random_labels == 1, 0] = (delta_field[random_labels == 1] *\\n                                       grad_G[random_labels == 1, 0] / sqnrm)\\n    expected[random_labels == 1, 1] = (delta_field[random_labels == 1] *\\n                                       grad_G[random_labels == 1, 1] / sqnrm)\\n\\n    # Pixels with gradient=0 and sigma_i_sq=0\\n    sigma_i_sq[random_labels == 2] = 0\\n    grad_G[random_labels == 2, ...] = 0\\n    expected[random_labels == 2, ...] = 0\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_expectmax.py.txt'}),\n",
       " Document(page_content='# Pixels with gradient=0 and sigma_i_sq=0\\n    sigma_i_sq[random_labels == 2] = 0\\n    grad_G[random_labels == 2, ...] = 0\\n    expected[random_labels == 2, ...] = 0\\n\\n    # Pixels with gradient=0 and sigma_i_sq!=0\\n    grad_G[random_labels == 3, ...] = 0\\n\\n    # Directly compute the demons step according to eq. 4 in [Vercauteren09]\\n    num = (sigma_x_sq * (F - G))[random_labels >= 3]\\n    den = (sigma_x_sq * sq_norm_grad_G + sigma_i_sq)[random_labels >= 3]\\n\\n    # This is $J^{P}$ in eq. 4 [Vercauteren09]\\n    expected[random_labels >= 3] = -1 * np.array(grad_G[random_labels >= 3])\\n    expected[random_labels >= 3, ...] *= (num / den)[..., None]\\n\\n    # Now compute it using the implementation under test\\n\\n    actual = np.empty_like(expected, dtype=floating)\\n    em.compute_em_demons_step_2d(np.array(delta_field, dtype=floating),\\n                                 np.array(sigma_i_sq, dtype=floating),\\n                                 np.array(grad_G, dtype=floating),\\n                                 sigma_x_sq,\\n                                 actual)\\n\\n    # Test sigma_i_sq == inf\\n    try:\\n        assert_array_almost_equal(actual[random_labels == 0],\\n                                  expected[random_labels == 0])\\n    except AssertionError:\\n        raise AssertionError(\"Failed for sigma_i_sq == inf\")\\n\\n    # Test sigma_i_sq == 0 and gradient != 0\\n    try:\\n        assert_array_almost_equal(actual[random_labels == 1],\\n                                  expected[random_labels == 1])\\n    except AssertionError:\\n        raise AssertionError(\"Failed for sigma_i_sq == 0 and gradient != 0\")', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_expectmax.py.txt'}),\n",
       " Document(page_content='# Test sigma_i_sq == 0 and gradient != 0\\n    try:\\n        assert_array_almost_equal(actual[random_labels == 1],\\n                                  expected[random_labels == 1])\\n    except AssertionError:\\n        raise AssertionError(\"Failed for sigma_i_sq == 0 and gradient != 0\")\\n\\n    # Test sigma_i_sq == 0 and gradient == 0\\n    try:\\n        assert_array_almost_equal(actual[random_labels == 2],\\n                                  expected[random_labels == 2])\\n    except AssertionError:\\n        raise AssertionError(\"Failed for sigma_i_sq == 0 and gradient == 0\")\\n\\n    # Test sigma_i_sq != 0 and gradient == 0\\n    try:\\n        assert_array_almost_equal(actual[random_labels == 3],\\n                                  expected[random_labels == 3])\\n    except AssertionError:\\n        raise AssertionError(\"Failed for sigma_i_sq != 0 and gradient == 0 \")\\n\\n    # Test sigma_i_sq != 0 and gradient != 0\\n    try:\\n        assert_array_almost_equal(actual[random_labels == 4],\\n                                  expected[random_labels == 4])\\n    except AssertionError:\\n        raise AssertionError(\"Failed for sigma_i_sq != 0 and gradient != 0\")\\n\\n\\n@set_random_number_generator(1346491)\\ndef test_compute_em_demons_step_3d(rng):\\n    r\"\"\"\\n    Compares the output of the demons step in 3d against an analytical\\n    step. The fixed image is given by $F(x) = \\\\frac{1}{2}||x - c_f||^2$, the\\n    moving image is given by $G(x) = \\\\frac{1}{2}||x - c_g||^2$,\\n    $x, c_f, c_g \\\\in R^{3}$', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_expectmax.py.txt'}),\n",
       " Document(page_content='@set_random_number_generator(1346491)\\ndef test_compute_em_demons_step_3d(rng):\\n    r\"\"\"\\n    Compares the output of the demons step in 3d against an analytical\\n    step. The fixed image is given by $F(x) = \\\\frac{1}{2}||x - c_f||^2$, the\\n    moving image is given by $G(x) = \\\\frac{1}{2}||x - c_g||^2$,\\n    $x, c_f, c_g \\\\in R^{3}$\\n\\n    References\\n    ----------\\n    [Vercauteren09] Vercauteren, T., Pennec, X., Perchant, A., & Ayache, N.\\n                    (2009). Diffeomorphic demons: efficient non-parametric\\n                    image registration. NeuroImage, 45(1 Suppl), S61-72.\\n                    doi:10.1016/j.neuroimage.2008.10.040\\n    \"\"\"\\n\\n    # Select arbitrary images\\' shape (same shape for both images)\\n    sh = (20, 15, 10)\\n\\n    # Select arbitrary centers\\n    c_f = np.asarray(sh) / 2\\n    c_g = c_f + 0.5\\n\\n    # Compute the identity vector field I(x) = x in R^2\\n    x_0 = np.asarray(range(sh[0]))\\n    x_1 = np.asarray(range(sh[1]))\\n    x_2 = np.asarray(range(sh[2]))\\n    X = np.ndarray(sh + (3,), dtype=np.float64)\\n    O = np.ones(sh)\\n    X[..., 0] = x_0[:, None, None] * O\\n    X[..., 1] = x_1[None, :, None] * O\\n    X[..., 2] = x_2[None, None, :] * O\\n\\n    # Compute the gradient fields of F and G\\n    grad_F = X - c_f\\n    grad_G = X - c_g\\n\\n    # The squared norm of grad_G to be used later\\n    sq_norm_grad_G = np.sum(grad_G**2, -1)\\n\\n    # Compute F and G\\n    F = 0.5 * np.sum(grad_F**2, -1)\\n    G = 0.5 * sq_norm_grad_G\\n    delta_field = G - F\\n\\n    # Now select an arbitrary parameter for\\n    # $\\\\sigma_x$ (eq 4 in [Vercauteren09])\\n    sigma_x_sq = 1.5', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_expectmax.py.txt'}),\n",
       " Document(page_content=\"# Compute the gradient fields of F and G\\n    grad_F = X - c_f\\n    grad_G = X - c_g\\n\\n    # The squared norm of grad_G to be used later\\n    sq_norm_grad_G = np.sum(grad_G**2, -1)\\n\\n    # Compute F and G\\n    F = 0.5 * np.sum(grad_F**2, -1)\\n    G = 0.5 * sq_norm_grad_G\\n    delta_field = G - F\\n\\n    # Now select an arbitrary parameter for\\n    # $\\\\sigma_x$ (eq 4 in [Vercauteren09])\\n    sigma_x_sq = 1.5\\n\\n    # Set arbitrary values for $\\\\sigma_i$ (eq. 4 in [Vercauteren09])\\n    # The original Demons algorithm used simply |F(x) - G(x)| as an\\n    # estimator, so let's use it as well\\n    sigma_i_sq = (F - G)**2\\n\\n    # Select some pixels to have special values\\n    random_labels = rng.integers(0, 5, sh[0] * sh[1] * sh[2])\\n    random_labels = random_labels.reshape(sh)\\n\\n    # this label is used to set sigma_i_sq == 0 below\\n    random_labels[sigma_i_sq == 0] = 2\\n    # this label is used to set gradient == 0 below\\n    random_labels[sq_norm_grad_G == 0] = 2\\n\\n    expected = np.zeros_like(grad_G)\\n    # Pixels with sigma_i_sq = inf\\n    sigma_i_sq[random_labels == 0] = np.inf\\n    expected[random_labels == 0, ...] = 0\\n\\n    # Pixels with gradient!=0 and sigma_i_sq=0\\n    sqnrm = sq_norm_grad_G[random_labels == 1]\\n    sigma_i_sq[random_labels == 1] = 0\\n    expected[random_labels == 1, 0] = (delta_field[random_labels == 1] *\\n                                       grad_G[random_labels == 1, 0] / sqnrm)\\n    expected[random_labels == 1, 1] = (delta_field[random_labels == 1] *\\n                                       grad_G[random_labels == 1, 1] / sqnrm)\\n    expected[random_labels == 1, 2] = (delta_field[random_labels == 1] *\\n                                       grad_G[random_labels == 1, 2] / sqnrm)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_expectmax.py.txt'}),\n",
       " Document(page_content='# Pixels with gradient=0 and sigma_i_sq=0\\n    sigma_i_sq[random_labels == 2] = 0\\n    grad_G[random_labels == 2, ...] = 0\\n    expected[random_labels == 2, ...] = 0\\n\\n    # Pixels with gradient=0 and sigma_i_sq!=0\\n    grad_G[random_labels == 3, ...] = 0\\n\\n    # Directly compute the demons step according to eq. 4 in [Vercauteren09]\\n    num = (sigma_x_sq * (F - G))[random_labels >= 3]\\n    den = (sigma_x_sq * sq_norm_grad_G + sigma_i_sq)[random_labels >= 3]\\n\\n    # This is $J^{P}$ in eq. 4 [Vercauteren09]\\n    expected[random_labels >= 3] = -1 * np.array(grad_G[random_labels >= 3])\\n    expected[random_labels >= 3, ...] *= (num / den)[..., None]\\n\\n    # Now compute it using the implementation under test\\n    actual = np.empty_like(expected, dtype=floating)\\n    em.compute_em_demons_step_3d(np.array(delta_field, dtype=floating),\\n                                 np.array(sigma_i_sq, dtype=floating),\\n                                 np.array(grad_G, dtype=floating),\\n                                 sigma_x_sq,\\n                                 actual)\\n\\n    # Test sigma_i_sq == inf\\n    try:\\n        assert_array_almost_equal(actual[random_labels == 0],\\n                                  expected[random_labels == 0])\\n    except AssertionError:\\n        raise AssertionError(\"Failed for sigma_i_sq == inf\")\\n\\n    # Test sigma_i_sq == 0 and gradient != 0\\n    try:\\n        assert_array_almost_equal(actual[random_labels == 1],\\n                                  expected[random_labels == 1])\\n    except AssertionError:\\n        raise AssertionError(\"Failed for sigma_i_sq == 0 and gradient != 0\")', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_expectmax.py.txt'}),\n",
       " Document(page_content='# Test sigma_i_sq == 0 and gradient != 0\\n    try:\\n        assert_array_almost_equal(actual[random_labels == 1],\\n                                  expected[random_labels == 1])\\n    except AssertionError:\\n        raise AssertionError(\"Failed for sigma_i_sq == 0 and gradient != 0\")\\n\\n    # Test sigma_i_sq == 0 and gradient == 0\\n    try:\\n        assert_array_almost_equal(actual[random_labels == 2],\\n                                  expected[random_labels == 2])\\n    except AssertionError:\\n        raise AssertionError(\"Failed for sigma_i_sq == 0 and gradient == 0\")\\n\\n    # Test sigma_i_sq != 0 and gradient == 0\\n    try:\\n        assert_array_almost_equal(actual[random_labels == 3],\\n                                  expected[random_labels == 3])\\n    except AssertionError:\\n        raise AssertionError(\"Failed for sigma_i_sq != 0 and gradient == 0 \")\\n\\n    # Test sigma_i_sq != 0 and gradient != 0\\n    try:\\n        assert_array_almost_equal(actual[random_labels == 4],\\n                                  expected[random_labels == 4])\\n    except AssertionError:\\n        raise AssertionError(\"Failed for sigma_i_sq != 0 and gradient != 0\")\\n\\n\\n@set_random_number_generator(1246592)\\ndef test_quantize_positive_2d(rng):\\n    # an arbitrary number of quantization levels\\n    num_levels = 11\\n    # arbitrary test image shape (must contain at least 3 elements)\\n    img_shape = (15, 20)\\n    min_positive = 0.1\\n    max_positive = 1.0\\n    epsilon = 1e-8', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_expectmax.py.txt'}),\n",
       " Document(page_content='@set_random_number_generator(1246592)\\ndef test_quantize_positive_2d(rng):\\n    # an arbitrary number of quantization levels\\n    num_levels = 11\\n    # arbitrary test image shape (must contain at least 3 elements)\\n    img_shape = (15, 20)\\n    min_positive = 0.1\\n    max_positive = 1.0\\n    epsilon = 1e-8\\n\\n    delta = (max_positive - min_positive + epsilon) / (num_levels - 1)\\n    true_levels = np.zeros((num_levels,), dtype=np.float32)\\n    # put the intensities at the centers of the bins\\n    true_levels[1:] = np.linspace(min_positive + delta * 0.5,\\n                                  max_positive - delta * 0.5, num_levels - 1)\\n    # generate a target quantization image\\n    true_quantization = np.empty(img_shape, dtype=np.int32)\\n    random_labels = rng.integers(0, num_levels,\\n                                      np.size(true_quantization))\\n\\n    # make sure there is at least one element equal to 0, 1 and num_levels-1\\n    random_labels[0] = 0\\n    random_labels[1] = 1\\n    random_labels[2] = num_levels - 1\\n    true_quantization[...] = random_labels.reshape(img_shape)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_expectmax.py.txt'}),\n",
       " Document(page_content=\"# make sure there is at least one element equal to 0, 1 and num_levels-1\\n    random_labels[0] = 0\\n    random_labels[1] = 1\\n    random_labels[2] = num_levels - 1\\n    true_quantization[...] = random_labels.reshape(img_shape)\\n\\n    # make sure additive noise doesn't change the quantization result\\n    noise_amplitude = np.min([delta / 4.0, min_positive / 4.0])\\n    sz = np.size(true_quantization)\\n    noise = rng.random(sz).reshape(img_shape) * noise_amplitude\\n    noise = noise.astype(floating)\\n    input_image = np.ndarray(img_shape, dtype=floating)\\n    # assign intensities plus noise\\n    input_image[...] = true_levels[true_quantization] + noise\\n    # preserve original zeros\\n    input_image[true_quantization == 0] = 0\\n    # preserve min positive value\\n    input_image[true_quantization == 1] = min_positive\\n    # preserve max positive value\\n    input_image[true_quantization == num_levels - 1] = max_positive\\n\\n    out, levels, hist = em.quantize_positive_2d(input_image, num_levels)\\n    levels = np.asarray(levels)\\n    assert_array_equal(out, true_quantization)\\n    assert_array_almost_equal(levels, true_levels)\\n    for i in range(num_levels):\\n        current_bin = np.asarray(true_quantization == i).sum()\\n        assert_equal(hist[i], current_bin)\\n\\n    # test num_levels<2 and input image with zeros and non-zeros everywhere\\n    assert_raises(ValueError, em.quantize_positive_2d, input_image, 0)\\n    assert_raises(ValueError, em.quantize_positive_2d, input_image, 1)\\n\\n    out, levels, hist = em.quantize_positive_2d(\\n        np.zeros(img_shape, dtype=floating), 2)\\n    assert_equal(out, np.zeros(img_shape, dtype=np.int32))\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_expectmax.py.txt'}),\n",
       " Document(page_content='# test num_levels<2 and input image with zeros and non-zeros everywhere\\n    assert_raises(ValueError, em.quantize_positive_2d, input_image, 0)\\n    assert_raises(ValueError, em.quantize_positive_2d, input_image, 1)\\n\\n    out, levels, hist = em.quantize_positive_2d(\\n        np.zeros(img_shape, dtype=floating), 2)\\n    assert_equal(out, np.zeros(img_shape, dtype=np.int32))\\n\\n    out, levels, hist = em.quantize_positive_2d(\\n        np.ones(img_shape, dtype=floating), 2)\\n    assert_equal(out, np.ones(img_shape, dtype=np.int32))\\n\\n\\n@set_random_number_generator(1246592)\\ndef test_quantize_positive_3d(rng):\\n    # an arbitrary number of quantization levels\\n    num_levels = 11\\n    # arbitrary test image shape (must contain at least 3 elements)\\n    img_shape = (5, 10, 15)\\n    min_positive = 0.1\\n    max_positive = 1.0\\n    epsilon = 1e-8\\n\\n    delta = (max_positive - min_positive + epsilon) / (num_levels - 1)\\n    true_levels = np.zeros((num_levels,), dtype=np.float32)\\n    # put the intensities at the centers of the bins\\n    true_levels[1:] = np.linspace(min_positive + delta * 0.5,\\n                                  max_positive - delta * 0.5,\\n                                  num_levels - 1)\\n    # generate a target quantization image\\n    true_quantization = np.empty(img_shape, dtype=np.int32)\\n    random_labels = rng.integers(0, num_levels,\\n                                      np.size(true_quantization))\\n\\n    # make sure there is at least one element equal to 0, 1 and num_levels-1\\n    random_labels[0] = 0\\n    random_labels[1] = 1\\n    random_labels[2] = num_levels - 1\\n    true_quantization[...] = random_labels.reshape(img_shape)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_expectmax.py.txt'}),\n",
       " Document(page_content=\"# make sure there is at least one element equal to 0, 1 and num_levels-1\\n    random_labels[0] = 0\\n    random_labels[1] = 1\\n    random_labels[2] = num_levels - 1\\n    true_quantization[...] = random_labels.reshape(img_shape)\\n\\n    # make sure additive noise doesn't change the quantization result\\n    noise_amplitude = np.min([delta / 4.0, min_positive / 4.0])\\n    sz = np.size(true_quantization)\\n    noise = rng.random(sz).reshape(img_shape) * noise_amplitude\\n    noise = noise.astype(floating)\\n    input_image = np.ndarray(img_shape, dtype=floating)\\n    # assign intensities plus noise\\n    input_image[...] = true_levels[true_quantization] + noise\\n    # preserve original zeros\\n    input_image[true_quantization == 0] = 0\\n    # preserve min positive value\\n    input_image[true_quantization == 1] = min_positive\\n    # preserve max positive value\\n    input_image[true_quantization == num_levels - 1] = max_positive\\n\\n    out, levels, hist = em.quantize_positive_3d(input_image, num_levels)\\n    levels = np.asarray(levels)\\n    assert_array_equal(out, true_quantization)\\n    assert_array_almost_equal(levels, true_levels)\\n    for i in range(num_levels):\\n        current_bin = np.asarray(true_quantization == i).sum()\\n        assert_equal(hist[i], current_bin)\\n\\n    # test num_levels<2 and input image with zeros and non-zeros everywhere\\n    assert_raises(ValueError, em.quantize_positive_3d, input_image, 0)\\n    assert_raises(ValueError, em.quantize_positive_3d, input_image, 1)\\n\\n    out, levels, hist = em.quantize_positive_3d(np.zeros(img_shape,\\n                                                         dtype=floating), 2)\\n    assert_equal(out, np.zeros(img_shape, dtype=np.int32))\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_expectmax.py.txt'}),\n",
       " Document(page_content='# test num_levels<2 and input image with zeros and non-zeros everywhere\\n    assert_raises(ValueError, em.quantize_positive_3d, input_image, 0)\\n    assert_raises(ValueError, em.quantize_positive_3d, input_image, 1)\\n\\n    out, levels, hist = em.quantize_positive_3d(np.zeros(img_shape,\\n                                                         dtype=floating), 2)\\n    assert_equal(out, np.zeros(img_shape, dtype=np.int32))\\n\\n    out, levels, hist = em.quantize_positive_3d(np.ones(img_shape,\\n                                                        dtype=floating), 2)\\n    assert_equal(out, np.ones(img_shape, dtype=np.int32))\\n\\n\\n@set_random_number_generator(1246592)\\ndef test_compute_masked_class_stats_2d(rng):\\n    shape = (32, 32)\\n\\n    # Create random labels\\n    labels = np.ndarray(shape, dtype=np.int32)\\n    labels[...] = rng.integers(2, 10, np.size(labels)).reshape(shape)\\n    # now label 0 is not present and label 1 occurs once\\n    labels[0, 0] = 1\\n\\n    # Create random values\\n    values = rng.standard_normal((shape[0], shape[1])).astype(floating)\\n    values *= labels\\n    values += labels\\n\\n    expected_means = [0, values[0, 0]] + \\\\\\n        [values[labels == i].mean() for i in range(2, 10)]\\n    expected_vars = [np.inf, np.inf] + \\\\\\n        [values[labels == i].var() for i in range(2, 10)]\\n\\n    mask = np.ones(shape, dtype=np.int32)\\n    means, std_dev = em.compute_masked_class_stats_2d(mask, values, 10, labels)\\n    assert_array_almost_equal(means, expected_means, decimal=4)\\n    assert_array_almost_equal(std_dev, expected_vars, decimal=4)\\n\\n\\n@set_random_number_generator(1246592)\\ndef test_compute_masked_class_stats_3d(rng):\\n    shape = (32, 32, 32)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_expectmax.py.txt'}),\n",
       " Document(page_content='mask = np.ones(shape, dtype=np.int32)\\n    means, std_dev = em.compute_masked_class_stats_2d(mask, values, 10, labels)\\n    assert_array_almost_equal(means, expected_means, decimal=4)\\n    assert_array_almost_equal(std_dev, expected_vars, decimal=4)\\n\\n\\n@set_random_number_generator(1246592)\\ndef test_compute_masked_class_stats_3d(rng):\\n    shape = (32, 32, 32)\\n\\n    # Create random labels\\n    labels = np.ndarray(shape, dtype=np.int32)\\n    labels[...] = rng.integers(2, 10, np.size(labels)).reshape(shape)\\n\\n    # now label 0 is not present and label 1 occurs once\\n    labels[0, 0, 0] = 1\\n\\n    # Create random values\\n    values = rng.standard_normal((shape[0], shape[1],\\n                                  shape[2])).astype(floating)\\n    values *= labels\\n    values += labels\\n\\n    expected_means = [0, values[0, 0, 0]] + \\\\\\n        [values[labels == i].mean() for i in range(2, 10)]\\n    expected_vars = [np.inf, np.inf] + \\\\\\n        [values[labels == i].var() for i in range(2, 10)]\\n\\n    mask = np.ones(shape, dtype=np.int32)\\n    means, std_dev = em.compute_masked_class_stats_3d(mask, values, 10, labels)\\n    assert_array_almost_equal(means, expected_means, decimal=4)\\n    assert_array_almost_equal(std_dev, expected_vars, decimal=4)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_expectmax.py.txt'}),\n",
       " Document(page_content='import numpy as np\\nimport numpy.linalg as npl\\nfrom numpy.testing import (assert_array_equal,\\n                           assert_array_almost_equal,\\n                           assert_equal,\\n                           assert_raises,\\n                           assert_warns)\\nfrom dipy.core import geometry as geometry\\nfrom dipy.align import vector_fields as vf\\nfrom dipy.align import imaffine\\nfrom dipy.align.imaffine import AffineInversionError, AffineInvalidValuesError, \\\\\\n    AffineMap, _number_dim_affine_matrix\\nfrom dipy.align.transforms import regtransforms\\nfrom dipy.align.tests.test_parzenhist import setup_random_transform\\nfrom dipy.testing.decorators import set_random_number_generator', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imaffine.py.txt'}),\n",
       " Document(page_content=\"# For each transform type, select a transform factor (indicating how large the\\n# true transform between static and moving images will be), a sampling scheme\\n# (either a positive integer less than or equal to 100, or None) indicating\\n# the percentage (if int) of voxels to be used for estimating the joint PDFs,\\n# or dense sampling (if None), and also specify a starting point (to avoid\\n# starting from the identity)\\nfactors = {('TRANSLATION', 2): (2.0, 0.35, np.array([2.3, 4.5])),\\n           ('ROTATION', 2): (0.1, None, np.array([0.1])),\\n           ('RIGID', 2): (0.1, .50, np.array([0.12, 1.8, 2.7])),\\n           ('SCALING', 2): (0.01, None, np.array([1.05])),\\n           ('AFFINE', 2): (0.1, .50, np.array([0.99, -0.05, 1.3, 0.05, 0.99,\\n                                               2.5])),\\n           ('TRANSLATION', 3): (2.0, None, np.array([2.3, 4.5, 1.7])),\\n           ('ROTATION', 3): (0.1, 1.0, np.array([0.1, 0.15, -0.11])),\\n           ('RIGID', 3): (0.1, None, np.array([0.1, 0.15, -0.11, 2.3, 4.5,\\n                                               1.7])),\\n           ('SCALING', 3): (0.1, .35, np.array([0.95])),\\n           ('AFFINE', 3): (0.1, None, np.array([0.99, -0.05, 0.03, 1.3,\\n                                                0.05, 0.99, -0.10, 2.5,\\n                                                -0.07, 0.10, 0.99, -1.4]))}\\n\\n\\ndef test_transform_centers_of_mass_3d():\\n    shape = (64, 64, 64)\\n    rm = 8\\n    sph = vf.create_sphere(shape[0] // 2, shape[1] // 2, shape[2] // 2, rm)\\n    moving = np.zeros(shape)\\n    # The center of mass will be (16, 16, 16), in image coordinates\\n    moving[:shape[0] // 2, :shape[1] // 2, :shape[2] // 2] = sph[...]\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imaffine.py.txt'}),\n",
       " Document(page_content='def test_transform_centers_of_mass_3d():\\n    shape = (64, 64, 64)\\n    rm = 8\\n    sph = vf.create_sphere(shape[0] // 2, shape[1] // 2, shape[2] // 2, rm)\\n    moving = np.zeros(shape)\\n    # The center of mass will be (16, 16, 16), in image coordinates\\n    moving[:shape[0] // 2, :shape[1] // 2, :shape[2] // 2] = sph[...]\\n\\n    rs = 16\\n    # The center of mass will be (32, 32, 32), in image coordinates\\n    static = vf.create_sphere(shape[0], shape[1], shape[2], rs)\\n\\n    # Create arbitrary image-to-space transforms\\n    axis = np.array([.5, 2.0, 1.5])\\n    t = 0.15  # translation factor\\n    trans = np.array([[1, 0, 0, -t * shape[0]],\\n                      [0, 1, 0, -t * shape[1]],\\n                      [0, 0, 1, -t * shape[2]],\\n                      [0, 0, 0, 1]])\\n    trans_inv = npl.inv(trans)\\n\\n    for rotation_angle in [-1 * np.pi / 6.0, 0.0, np.pi / 5.0]:\\n        for scale_factor in [0.83, 1.3, 2.07]:  # scale\\n            rot = np.zeros(shape=(4, 4))\\n            rot[:3, :3] = geometry.rodrigues_axis_rotation(axis,\\n                                                           rotation_angle)\\n            rot[3, 3] = 1.0\\n            scale = np.array([[1 * scale_factor, 0, 0, 0],\\n                              [0, 1 * scale_factor, 0, 0],\\n                              [0, 0, 1 * scale_factor, 0],\\n                              [0, 0, 0, 1]])\\n\\n            static_grid2world = trans_inv.dot(scale.dot(rot.dot(trans)))\\n            moving_grid2world = npl.inv(static_grid2world)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imaffine.py.txt'}),\n",
       " Document(page_content='static_grid2world = trans_inv.dot(scale.dot(rot.dot(trans)))\\n            moving_grid2world = npl.inv(static_grid2world)\\n\\n            # Expected translation\\n            c_static = static_grid2world.dot((32, 32, 32, 1))[:3]\\n            c_moving = moving_grid2world.dot((16, 16, 16, 1))[:3]\\n            expected = np.eye(4)\\n            expected[:3, 3] = c_moving - c_static\\n\\n            # Implementation under test\\n            actual = imaffine.transform_centers_of_mass(static,\\n                                                        static_grid2world,\\n                                                        moving,\\n                                                        moving_grid2world)\\n            assert_array_almost_equal(actual.affine, expected)\\n\\n\\ndef test_transform_geometric_centers_3d():\\n    # Create arbitrary image-to-space transforms\\n    axis = np.array([.5, 2.0, 1.5])\\n    t = 0.15  # translation factor', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imaffine.py.txt'}),\n",
       " Document(page_content='def test_transform_geometric_centers_3d():\\n    # Create arbitrary image-to-space transforms\\n    axis = np.array([.5, 2.0, 1.5])\\n    t = 0.15  # translation factor\\n\\n    for theta in [-1 * np.pi / 6.0, 0.0, np.pi / 5.0]:  # rotation angle\\n        for s in [0.83, 1.3, 2.07]:  # scale\\n            m_shapes = [(256, 256, 128), (255, 255, 127), (64, 127, 142)]\\n            for shape_moving in m_shapes:\\n                s_shapes = [(256, 256, 128), (255, 255, 127), (64, 127, 142)]\\n                for shape_static in s_shapes:\\n                    moving = np.ndarray(shape=shape_moving)\\n                    static = np.ndarray(shape=shape_static)\\n                    trans = np.array([[1, 0, 0, -t * shape_static[0]],\\n                                      [0, 1, 0, -t * shape_static[1]],\\n                                      [0, 0, 1, -t * shape_static[2]],\\n                                      [0, 0, 0, 1]])\\n                    trans_inv = npl.inv(trans)\\n                    rot = np.zeros(shape=(4, 4))\\n                    rot[:3, :3] = geometry.rodrigues_axis_rotation(axis, theta)\\n                    rot[3, 3] = 1.0\\n                    scale = np.array([[1 * s, 0, 0, 0],\\n                                      [0, 1 * s, 0, 0],\\n                                      [0, 0, 1 * s, 0],\\n                                      [0, 0, 0, 1]])\\n\\n                    static_grid2world = trans_inv.dot(\\n                        scale.dot(rot.dot(trans)))\\n                    moving_grid2world = npl.inv(static_grid2world)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imaffine.py.txt'}),\n",
       " Document(page_content='static_grid2world = trans_inv.dot(\\n                        scale.dot(rot.dot(trans)))\\n                    moving_grid2world = npl.inv(static_grid2world)\\n\\n                    # Expected translation\\n                    c_static = np.array(shape_static, dtype=np.float64) * 0.5\\n                    c_static = tuple(c_static)\\n                    c_static = static_grid2world.dot(c_static + (1,))[:3]\\n                    c_moving = np.array(shape_moving, dtype=np.float64) * 0.5\\n                    c_moving = tuple(c_moving)\\n                    c_moving = moving_grid2world.dot(c_moving + (1,))[:3]\\n                    expected = np.eye(4)\\n                    expected[:3, 3] = c_moving - c_static\\n\\n                    # Implementation under test\\n                    actual = imaffine.transform_geometric_centers(\\n                        static, static_grid2world, moving, moving_grid2world)\\n                    assert_array_almost_equal(actual.affine, expected)\\n\\n\\ndef test_transform_origins_3d():\\n    # Create arbitrary image-to-space transforms\\n    axis = np.array([.5, 2.0, 1.5])\\n    t = 0.15  # translation factor', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imaffine.py.txt'}),\n",
       " Document(page_content='# Implementation under test\\n                    actual = imaffine.transform_geometric_centers(\\n                        static, static_grid2world, moving, moving_grid2world)\\n                    assert_array_almost_equal(actual.affine, expected)\\n\\n\\ndef test_transform_origins_3d():\\n    # Create arbitrary image-to-space transforms\\n    axis = np.array([.5, 2.0, 1.5])\\n    t = 0.15  # translation factor\\n\\n    for theta in [-1 * np.pi / 6.0, 0.0, np.pi / 5.0]:  # rotation angle\\n        for s in [0.83, 1.3, 2.07]:  # scale\\n            m_shapes = [(256, 256, 128), (255, 255, 127), (64, 127, 142)]\\n            for shape_moving in m_shapes:\\n                s_shapes = [(256, 256, 128), (255, 255, 127), (64, 127, 142)]\\n                for shape_static in s_shapes:\\n                    moving = np.ndarray(shape=shape_moving)\\n                    static = np.ndarray(shape=shape_static)\\n                    trans = np.array([[1, 0, 0, -t * shape_static[0]],\\n                                      [0, 1, 0, -t * shape_static[1]],\\n                                      [0, 0, 1, -t * shape_static[2]],\\n                                      [0, 0, 0, 1]])\\n                    trans_inv = npl.inv(trans)\\n                    rot = np.zeros(shape=(4, 4))\\n                    rot[:3, :3] = geometry.rodrigues_axis_rotation(axis, theta)\\n                    rot[3, 3] = 1.0\\n                    scale = np.array([[1 * s, 0, 0, 0],\\n                                      [0, 1 * s, 0, 0],\\n                                      [0, 0, 1 * s, 0],\\n                                      [0, 0, 0, 1]])', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imaffine.py.txt'}),\n",
       " Document(page_content='static_grid2world = trans_inv.dot(\\n                        scale.dot(rot.dot(trans)))\\n                    moving_grid2world = npl.inv(static_grid2world)\\n\\n                    # Expected translation\\n                    c_static = static_grid2world[:3, 3]\\n                    c_moving = moving_grid2world[:3, 3]\\n                    expected = np.eye(4)\\n                    expected[:3, 3] = c_moving - c_static\\n\\n                    # Implementation under test\\n                    actual = imaffine.transform_origins(static,\\n                                                        static_grid2world,\\n                                                        moving,\\n                                                        moving_grid2world)\\n                    assert_array_almost_equal(actual.affine, expected)\\n\\n\\n@set_random_number_generator(202311)\\ndef test_affreg_all_transforms(rng):\\n    # Test affine registration using all transforms with typical settings', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imaffine.py.txt'}),\n",
       " Document(page_content=\"# Make sure dictionary entries are processed in the same order regardless\\n    # of the platform. Otherwise any random numbers drawn within the loop would\\n    # make the test non-deterministic even if we fix the seed before the loop.\\n    # Right now, this test does not draw any samples, but we still sort the\\n    # entries to prevent future related failures.\\n    for ttype in sorted(factors):\\n        dim = ttype[1]\\n        if dim == 2:\\n            nslices = 1\\n        else:\\n            nslices = 45\\n        factor = factors[ttype][0]\\n        sampling_pc = factors[ttype][1]\\n        trans = regtransforms[ttype]\\n        # Shorthand:\\n        srt = setup_random_transform\\n        static, moving, static_g2w, moving_g2w, smask, mmask, T = srt(\\n                                                                      trans,\\n                                                                      factor,\\n                                                                      nslices,\\n                                                                      1.0,\\n                                                                      rng=rng)\\n        # Sum of absolute differences\\n        start_sad = np.abs(static - moving).sum()\\n        metric = imaffine.MutualInformationMetric(32, sampling_pc)\\n        affreg = imaffine.AffineRegistration(metric,\\n                                             [1000, 100, 50],\\n                                             [3, 1, 0],\\n                                             [4, 2, 1],\\n                                             'L-BFGS-B',\\n                                             None,\\n                                             options=None)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imaffine.py.txt'}),\n",
       " Document(page_content=\"metric = imaffine.MutualInformationMetric(32, sampling_pc)\\n        affreg = imaffine.AffineRegistration(metric,\\n                                             [1000, 100, 50],\\n                                             [3, 1, 0],\\n                                             [4, 2, 1],\\n                                             'L-BFGS-B',\\n                                             None,\\n                                             options=None)\\n        x0 = trans.get_identity_parameters()\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imaffine.py.txt'}),\n",
       " Document(page_content='# test warning for using masks (even if all ones) with sparse sampling\\n        if sampling_pc not in [1.0, None]:\\n            affine_map = assert_warns(UserWarning, affreg.optimize,\\n                                      static, moving, trans, x0,\\n                                      static_g2w, moving_g2w,\\n                                      None, None,\\n                                      smask, mmask)\\n        else:\\n            affine_map = affreg.optimize(static, moving, trans, x0,\\n                                         static_g2w, moving_g2w,\\n                                         None, None,\\n                                         smask, mmask)\\n\\n        transformed = affine_map.transform(moving)\\n        # Sum of absolute differences\\n        end_sad = np.abs(static - transformed).sum()\\n        reduction = 1 - end_sad / start_sad\\n        print(\"%s>>%f\" % (ttype, reduction))\\n        assert(reduction > 0.9)\\n\\n    # Verify that exception is raised if level_iters is empty\\n    metric = imaffine.MutualInformationMetric(32)\\n    assert_raises(ValueError, imaffine.AffineRegistration, metric, [])\\n\\n    # Verify that exception is raised if masks are all zeros\\n    affine_map = assert_warns(UserWarning, affreg.optimize,\\n                              static, moving, trans, x0,\\n                              static_g2w, moving_g2w,\\n                              None, None,\\n                              np.zeros_like(smask), np.zeros_like(mmask))', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imaffine.py.txt'}),\n",
       " Document(page_content=\"# Verify that exception is raised if masks are all zeros\\n    affine_map = assert_warns(UserWarning, affreg.optimize,\\n                              static, moving, trans, x0,\\n                              static_g2w, moving_g2w,\\n                              None, None,\\n                              np.zeros_like(smask), np.zeros_like(mmask))\\n\\n\\n@set_random_number_generator(202311)\\ndef test_affreg_defaults(rng):\\n    # Test all default arguments with an arbitrary transform\\n    # Select an arbitrary transform (all of them are already tested\\n    # in test_affreg_all_transforms)\\n    transform_name = 'TRANSLATION'\\n    dim = 2\\n    ttype = (transform_name, dim)\\n    aff_options = ['mass', 'voxel-origin', 'centers', None, np.eye(dim + 1)]\\n\\n    for starting_affine in aff_options:\\n        if dim == 2:\\n            nslices = 1\\n        else:\\n            nslices = 45\\n        factor = factors[ttype][0]\\n        transform = regtransforms[ttype]\\n        static, moving, static_grid2world, moving_grid2world, smask, mmask, T = \\\\\\n            setup_random_transform(transform, factor, nslices, 1.0, rng=rng)\\n        # Sum of absolute differences\\n        start_sad = np.abs(static - moving).sum()\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imaffine.py.txt'}),\n",
       " Document(page_content='for starting_affine in aff_options:\\n        if dim == 2:\\n            nslices = 1\\n        else:\\n            nslices = 45\\n        factor = factors[ttype][0]\\n        transform = regtransforms[ttype]\\n        static, moving, static_grid2world, moving_grid2world, smask, mmask, T = \\\\\\n            setup_random_transform(transform, factor, nslices, 1.0, rng=rng)\\n        # Sum of absolute differences\\n        start_sad = np.abs(static - moving).sum()\\n\\n        metric = None\\n        x0 = None\\n        sigmas = None\\n        scale_factors = None\\n        level_iters = None\\n        static_grid2world = None\\n        moving_grid2world = None\\n        smask = None\\n        mmask = None\\n        for ss_sigma_factor in [1.0, None]:\\n            affreg = imaffine.AffineRegistration(metric,\\n                                                 level_iters,\\n                                                 sigmas,\\n                                                 scale_factors,\\n                                                 \\'L-BFGS-B\\',\\n                                                 ss_sigma_factor,\\n                                                 options=None)\\n            affine_map = affreg.optimize(static, moving, transform, x0,\\n                                         static_grid2world, moving_grid2world,\\n                                         starting_affine, None,\\n                                         smask, mmask)\\n            transformed = affine_map.transform(moving)\\n            # Sum of absolute differences\\n            end_sad = np.abs(static - transformed).sum()\\n            reduction = 1 - end_sad / start_sad\\n            print(\"%s>>%f\" % (ttype, reduction))\\n            assert(reduction > 0.9)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imaffine.py.txt'}),\n",
       " Document(page_content='transformed_inv = affine_map.transform_inverse(static)\\n            # Sum of absolute differences\\n            end_sad = np.abs(moving - transformed_inv).sum()\\n            reduction = 1 - end_sad / start_sad\\n            print(\"%s>>%f\" % (ttype, reduction))\\n            assert(reduction > 0.89)\\n\\n\\n@set_random_number_generator(2022966)\\ndef test_mi_gradient(rng):\\n    # Test the gradient of mutual information\\n    h = 1e-5\\n    # Make sure dictionary entries are processed in the same order regardless\\n    # of the platform. Otherwise any random numbers drawn within the loop would\\n    # make the test non-deterministic even if we fix the seed before the loop:\\n    # in this case the samples are drawn with `np.random.randn` below\\n\\n    for ttype in sorted(factors):\\n        transform = regtransforms[ttype]\\n        dim = ttype[1]\\n        if dim == 2:\\n            nslices = 1\\n        else:\\n            nslices = 45\\n        factor = factors[ttype][0]\\n        sampling_proportion = factors[ttype][1]\\n        theta = factors[ttype][2]\\n        # Start from a small rotation\\n        start = regtransforms[(\\'ROTATION\\', dim)]\\n        nrot = start.get_number_of_parameters()\\n        starting_affine = \\\\\\n            start.param_to_matrix(0.25 * rng.standard_normal(nrot))\\n        # Get data (pair of images related to each other by an known transform)\\n        static, moving, static_g2w, moving_g2w, smask, mmask, M = \\\\\\n            setup_random_transform(transform, factor, nslices, 2.0, rng=rng)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imaffine.py.txt'}),\n",
       " Document(page_content='# Prepare a MutualInformationMetric instance\\n        mi_metric = imaffine.MutualInformationMetric(32, sampling_proportion)\\n        mi_metric.setup(\\n            transform,\\n            static,\\n            moving,\\n            starting_affine=starting_affine)\\n        # Compute the gradient with the implementation under test\\n        actual = mi_metric.gradient(theta)\\n\\n        # Compute the gradient using finite-differences\\n        n = transform.get_number_of_parameters()\\n        expected = np.empty(n, dtype=np.float64)\\n\\n        val0 = mi_metric.distance(theta)\\n        for i in range(n):\\n            dtheta = theta.copy()\\n            dtheta[i] += h\\n            val1 = mi_metric.distance(dtheta)\\n            expected[i] = (val1 - val0) / h\\n\\n        dp = expected.dot(actual)\\n        enorm = npl.norm(expected)\\n        anorm = npl.norm(actual)\\n        nprod = dp / (enorm * anorm)\\n        assert(nprod >= 0.99)\\n\\n\\ndef create_affine_transforms(\\n        dim, translations, rotations, scales, rot_axis=None):\\n    r\"\"\" Creates a list of affine transforms with all combinations of params\\n\\n    This function is intended to be used for testing only. It generates\\n    affine transforms for all combinations of the input parameters in the\\n    following order: let T be a translation, R a rotation and S a scale. The\\n    generated affine will be:\\n\\n    A = T.dot(S).dot(R).dot(T^{-1})\\n\\n    Translation is handled this way because it is convenient to provide\\n    the translation parameters in terms of the center of rotation we wish\\n    to generate.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imaffine.py.txt'}),\n",
       " Document(page_content='This function is intended to be used for testing only. It generates\\n    affine transforms for all combinations of the input parameters in the\\n    following order: let T be a translation, R a rotation and S a scale. The\\n    generated affine will be:\\n\\n    A = T.dot(S).dot(R).dot(T^{-1})\\n\\n    Translation is handled this way because it is convenient to provide\\n    the translation parameters in terms of the center of rotation we wish\\n    to generate.\\n\\n    Parameters\\n    ----------\\n    dim: int (either dim=2 or dim=3)\\n        dimension of the affine transforms\\n    translations: sequence of dim-tuples\\n        each dim-tuple represents a translation parameter\\n    rotations: sequence of floats\\n        each number represents a rotation angle in radians\\n    scales: sequence of floats\\n        each number represents a scale\\n    rot_axis: rotation axis (used for dim=3 only)\\n\\n    Returns\\n    -------\\n    transforms: sequence of (dim + 1)x(dim + 1) matrices\\n        each matrix correspond to an affine transform with a combination\\n        of the input parameters\\n    \"\"\"\\n    transforms = []\\n    for t in translations:\\n        trans_inv = np.eye(dim + 1)\\n        trans_inv[:dim, dim] = -t[:dim]\\n        trans = npl.inv(trans_inv)\\n        for theta in rotations:  # rotation angle\\n            if dim == 2:\\n                ct = np.cos(theta)\\n                st = np.sin(theta)\\n                rot = np.array([[ct, -st, 0],\\n                                [st, ct, 0],\\n                                [0, 0, 1]])\\n            else:\\n                rot = np.eye(dim + 1)\\n                rot[:3, :3] = geometry.rodrigues_axis_rotation(rot_axis, theta)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imaffine.py.txt'}),\n",
       " Document(page_content='for s in scales:  # scale\\n                scale = np.eye(dim + 1) * s\\n                scale[dim, dim] = 1\\n\\n            affine = trans.dot(scale.dot(rot.dot(trans_inv)))\\n            transforms.append(affine)\\n    return transforms\\n\\n\\n@set_random_number_generator(2112927)\\ndef test_affine_map(rng):\\n    dom_shape = np.array([64, 64, 64], dtype=np.int32)\\n    cod_shape = np.array([80, 80, 80], dtype=np.int32)\\n    # Radius of the circle/sphere (testing image)\\n    radius = 16\\n    # Rotation axis (used for 3D transforms only)\\n    rot_axis = np.array([.5, 2.0, 1.5])\\n    # Arbitrary transform parameters\\n    t = 0.15\\n    rotations = [-1 * np.pi / 10.0, 0.0, np.pi / 10.0]\\n    scales = [0.9, 1.0, 1.1]\\n    for dim1 in [2, 3]:\\n        # Setup current dimension\\n        if dim1 == 2:\\n            # Create image of a circle\\n            img = vf.create_circle(cod_shape[0], cod_shape[1], radius)\\n            oracle_linear = vf.transform_2d_affine\\n            oracle_nn = vf.transform_2d_affine_nn\\n        else:\\n            # Create image of a sphere\\n            img = vf.create_sphere(cod_shape[0], cod_shape[1], cod_shape[2],\\n                                   radius)\\n            oracle_linear = vf.transform_3d_affine\\n            oracle_nn = vf.transform_3d_affine_nn\\n        img = np.array(img)\\n        # Translation is the only parameter differing for 2D and 3D\\n        translations = [t * dom_shape[:dim1]]\\n        # Generate affine transforms\\n        gt_affines = create_affine_transforms(dim1, translations, rotations,\\n                                              scales, rot_axis)\\n        # Include the None case\\n        gt_affines.append(None)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imaffine.py.txt'}),\n",
       " Document(page_content=\"# testing str/format/repr\\n        for affine_mat in gt_affines:\\n            aff_map = AffineMap(affine_mat)\\n            assert_equal(str(aff_map), aff_map.__str__())\\n            assert_equal(repr(aff_map), aff_map.__repr__())\\n            for spec in ['f', 'r', 't', '']:\\n                assert_equal(format(aff_map, spec), aff_map.__format__(spec))\\n\\n        for affine in gt_affines:\\n\\n            # make both domain point to the same physical region\\n            # It's ok to use the same transform, we just want to test\\n            # that this information is actually being considered\\n            domain_grid2world = affine\\n            codomain_grid2world = affine\\n            grid2grid_transform = affine\\n\\n            # Evaluate the transform with vector_fields module (already tested)\\n            expected_linear = oracle_linear(img, dom_shape[:dim1],\\n                                            grid2grid_transform)\\n            expected_nn = oracle_nn(img, dom_shape[:dim1], grid2grid_transform)\\n\\n            # Evaluate the transform with the implementation under test\\n            affine_map = imaffine.AffineMap(affine,\\n                                            dom_shape[:dim1],\\n                                            domain_grid2world,\\n                                            cod_shape[:dim1],\\n                                            codomain_grid2world)\\n            actual_linear = affine_map.transform(img, interpolation='linear')\\n            actual_nn = affine_map.transform(img, interpolation='nearest')\\n            assert_array_almost_equal(actual_linear, expected_linear)\\n            assert_array_almost_equal(actual_nn, expected_nn)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imaffine.py.txt'}),\n",
       " Document(page_content='# Test set_affine with valid matrix\\n            affine_map.set_affine(affine)\\n            if affine is None:\\n                assert(affine_map.affine is None)\\n                assert(affine_map.affine_inv is None)\\n            else:\\n                # compatibility with previous versions\\n                assert_array_equal(affine, affine_map.affine)\\n                # new getter\\n                new_copy_affine = affine_map.affine\\n                # value must be the same\\n                assert_array_equal(affine, new_copy_affine)\\n                # but not its reference\\n                assert id(affine) != id(new_copy_affine)\\n                actual = affine_map.affine.dot(affine_map.affine_inv)\\n                assert_array_almost_equal(actual, np.eye(dim1 + 1))\\n\\n            # Evaluate via the inverse transform\\n\\n            # AffineMap will use the inverse of the input matrix when we call\\n            # `transform_inverse`. Since the inverse of the inverse of a matrix\\n            # is not exactly equal to the original matrix (numerical\\n            #  limitations) we need to invert the matrix twice to make sure\\n            # the oracle and the implementation under test apply the same\\n            # transform\\n            aff_inv = None if affine is None else npl.inv(affine)\\n            aff_inv_inv = None if aff_inv is None else npl.inv(aff_inv)\\n            expected_linear = oracle_linear(img, dom_shape[:dim1],\\n                                            aff_inv_inv)\\n            expected_nn = oracle_nn(img, dom_shape[:dim1], aff_inv_inv)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imaffine.py.txt'}),\n",
       " Document(page_content=\"affine_map = imaffine.AffineMap(aff_inv,\\n                                            cod_shape[:dim1],\\n                                            codomain_grid2world,\\n                                            dom_shape[:dim1],\\n                                            domain_grid2world)\\n            actual_linear = affine_map.transform_inverse(\\n                img, interpolation='linear')\\n            actual_nn = affine_map.transform_inverse(img,\\n                                                     interpolation='nearest')\\n            assert_array_almost_equal(actual_linear, expected_linear)\\n            assert_array_almost_equal(actual_nn, expected_nn)\\n\\n        # Verify AffineMap can not be created with non-square matrix\\n        non_square_shapes = [np.zeros((dim1, dim1 + 1), dtype=np.float64),\\n                             np.zeros((dim1 + 1, dim1), dtype=np.float64)]\\n        for nsq in non_square_shapes:\\n            assert_raises(AffineInversionError, AffineMap, nsq)\\n\\n        # Verify incorrect augmentations are caught\\n        for affine_mat in gt_affines:\\n            aff_map = AffineMap(affine_mat)\\n            if affine_mat is None:\\n                continue\\n            bad_aug = aff_map.affine\\n            # no zeros in the first n-1 columns on last row\\n            bad_aug[-1, :] = 1\\n            assert_raises(AffineInvalidValuesError, AffineMap, bad_aug)\\n\\n            bad_aug = aff_map.affine\\n            bad_aug[-1, -1] = 0  # lower right not 1\\n            assert_raises(AffineInvalidValuesError, AffineMap, bad_aug)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imaffine.py.txt'}),\n",
       " Document(page_content=\"bad_aug = aff_map.affine\\n            bad_aug[-1, -1] = 0  # lower right not 1\\n            assert_raises(AffineInvalidValuesError, AffineMap, bad_aug)\\n\\n        # Verify AffineMap cannot be created with a non-invertible matrix\\n        invalid_nan = np.zeros((dim1 + 1, dim1 + 1), dtype=np.float64)\\n        invalid_nan[1, 1] = np.nan\\n        invalid_zeros = np.zeros((dim1 + 1, dim1 + 1), dtype=np.float64)\\n        assert_raises(\\n            imaffine.AffineInvalidValuesError,\\n            imaffine.AffineMap,\\n            invalid_nan)\\n        assert_raises(\\n            AffineInvalidValuesError,\\n            imaffine.AffineMap,\\n            invalid_zeros)\\n\\n        # Test exception is raised when the affine transform matrix is not\\n        # valid\\n        invalid_shape = np.eye(dim1)\\n        affmap_invalid_shape = imaffine.AffineMap(invalid_shape,\\n                                                  dom_shape[:dim1], None,\\n                                                  cod_shape[:dim1], None)\\n        assert_raises(ValueError, affmap_invalid_shape.transform, img)\\n        assert_raises(ValueError, affmap_invalid_shape.transform_inverse, img)\\n\\n        # Verify exception is raised when sampling info is not provided\\n        valid = np.eye(3)\\n        affmap_invalid_shape = imaffine.AffineMap(valid)\\n        assert_raises(ValueError, affmap_invalid_shape.transform, img)\\n        assert_raises(ValueError, affmap_invalid_shape.transform_inverse, img)\\n\\n        # Verify exception is raised when requesting an invalid interpolation\\n        assert_raises(ValueError, affine_map.transform, img, 'invalid')\\n        assert_raises(ValueError, affine_map.transform_inverse, img, 'invalid')\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imaffine.py.txt'}),\n",
       " Document(page_content=\"# Verify exception is raised when requesting an invalid interpolation\\n        assert_raises(ValueError, affine_map.transform, img, 'invalid')\\n        assert_raises(ValueError, affine_map.transform_inverse, img, 'invalid')\\n\\n        # Verify exception is raised when attempting to warp an image of\\n        # invalid dimension\\n        for dim2 in [2, 3]:\\n            affine_map = imaffine.AffineMap(np.eye(dim2),\\n                                            cod_shape[:dim2], None,\\n                                            dom_shape[:dim2], None)\\n            for sh in [(2,), (2, 2, 2, 2)]:\\n                img = np.zeros(sh)\\n                assert_raises(ValueError, affine_map.transform, img)\\n                assert_raises(ValueError, affine_map.transform_inverse, img)\\n            aff_sing = np.zeros((dim2 + 1, dim2 + 1))\\n            aff_nan = np.zeros((dim2 + 1, dim2 + 1))\\n            aff_nan[...] = np.nan\\n            aff_inf = np.zeros((dim2 + 1, dim2 + 1))\\n            aff_inf[...] = np.inf\\n\\n            assert_raises(\\n                AffineInvalidValuesError,\\n                affine_map.set_affine,\\n                aff_sing)\\n            assert_raises(AffineInvalidValuesError, affine_map.set_affine,\\n                          aff_nan)\\n            assert_raises(AffineInvalidValuesError, affine_map.set_affine,\\n                          aff_inf)\\n\\n    # Verify AffineMap can not be created with non-2D matrices : len(shape) != 2\\n    for dim_not_2 in range(10):\\n        if dim_not_2 != _number_dim_affine_matrix:\\n            mat_large_dim = rng.random([2]*dim_not_2)\\n            assert_raises(AffineInversionError, AffineMap, mat_large_dim)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imaffine.py.txt'}),\n",
       " Document(page_content=\"# Verify AffineMap can not be created with non-2D matrices : len(shape) != 2\\n    for dim_not_2 in range(10):\\n        if dim_not_2 != _number_dim_affine_matrix:\\n            mat_large_dim = rng.random([2]*dim_not_2)\\n            assert_raises(AffineInversionError, AffineMap, mat_large_dim)\\n\\n\\n@set_random_number_generator()\\ndef test_MIMetric_invalid_params(rng):\\n    transform = regtransforms[('AFFINE', 3)]\\n    static = rng.random((20, 20, 20))\\n    moving = rng.random((20, 20, 20))\\n    n = transform.get_number_of_parameters()\\n    sampling_proportion = 0.3\\n    theta_sing = np.zeros(n)\\n    theta_nan = np.zeros(n)\\n    theta_nan[...] = np.nan\\n    theta_inf = np.zeros(n)\\n    theta_nan[...] = np.inf\\n\\n    mi_metric = imaffine.MutualInformationMetric(32, sampling_proportion)\\n    mi_metric.setup(transform, static, moving)\\n    for theta in [theta_sing, theta_nan, theta_inf]:\\n        # Test metric value at invalid params\\n        actual_val = mi_metric.distance(theta)\\n        assert(np.isinf(actual_val))\\n\\n        # Test gradient at invalid params\\n        expected_grad = np.zeros(n)\\n        actual_grad = mi_metric.gradient(theta)\\n        assert_equal(actual_grad, expected_grad)\\n\\n        # Test both\\n        actual_val, actual_grad = mi_metric.distance_and_gradient(theta)\\n        assert(np.isinf(actual_val))\\n        assert_equal(actual_grad, expected_grad)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imaffine.py.txt'}),\n",
       " Document(page_content='import numpy as np\\nimport nibabel.eulerangles as eulerangles\\nfrom numpy.testing import (assert_equal,\\n                           assert_array_equal,\\n                           assert_array_almost_equal,\\n                           assert_raises)\\nfrom dipy.core.interpolation import (interpolate_scalar_2d,\\n                                     interpolate_scalar_3d)\\nfrom dipy.data import get_fnames\\nfrom dipy.align import floating\\nfrom dipy.align import imwarp as imwarp\\nfrom dipy.align import metrics as metrics\\nfrom dipy.align import vector_fields as vfu\\nfrom dipy.align import VerbosityLevels\\nfrom dipy.align.imwarp import DiffeomorphicMap\\nfrom dipy.tracking.streamline import deform_streamlines\\nfrom dipy.testing.decorators import set_random_number_generator\\n\\n\\ndef test_mult_aff():\\n    r\"\"\" Test matrix multiplication using None as identity\\n    \"\"\"\\n    A = np.array([[1.0, 2.0], [3.0, 4.0]])\\n    B = np.array([[2.0, 0.0], [0.0, 2.0]])\\n\\n    C = imwarp.mult_aff(A, B)\\n    expected_mult = np.array([[2.0, 4.0], [6.0, 8.0]])\\n    assert_array_almost_equal(C, expected_mult)\\n\\n    C = imwarp.mult_aff(A, None)\\n    assert_array_almost_equal(C, A)\\n\\n    C = imwarp.mult_aff(None, B)\\n    assert_array_almost_equal(C, B)\\n\\n    C = imwarp.mult_aff(None, None)\\n    assert_equal(C, None)\\n\\n\\n@set_random_number_generator(2022966)\\ndef test_diffeomorphic_map_2d(rng):\\n    r\"\"\" Test 2D DiffeomorphicMap', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imwarp.py.txt'}),\n",
       " Document(page_content='C = imwarp.mult_aff(A, B)\\n    expected_mult = np.array([[2.0, 4.0], [6.0, 8.0]])\\n    assert_array_almost_equal(C, expected_mult)\\n\\n    C = imwarp.mult_aff(A, None)\\n    assert_array_almost_equal(C, A)\\n\\n    C = imwarp.mult_aff(None, B)\\n    assert_array_almost_equal(C, B)\\n\\n    C = imwarp.mult_aff(None, None)\\n    assert_equal(C, None)\\n\\n\\n@set_random_number_generator(2022966)\\ndef test_diffeomorphic_map_2d(rng):\\n    r\"\"\" Test 2D DiffeomorphicMap\\n\\n    Creates a random displacement field that exactly maps pixels from an\\n    input image to an output image. First a discrete random assignment\\n    between the images is generated, then each pair of mapped points are\\n    transformed to the physical space by assigning a pair of arbitrary,\\n    fixed affine matrices to input and output images, and finally the\\n    difference between their positions is taken as the displacement vector.\\n    The resulting displacement, although operating in physical space,\\n    maps the points exactly (up to numerical precision).\\n    \"\"\"\\n    domain_shape = (10, 10)\\n    codomain_shape = (10, 10)\\n    # create a simple affine transformation\\n    nr = domain_shape[0]\\n    nc = domain_shape[1]\\n    s = 1.1\\n    t = 0.25\\n    trans = np.array([[1, 0, -t * nr],\\n                      [0, 1, -t * nc],\\n                      [0, 0, 1]])\\n    trans_inv = np.linalg.inv(trans)\\n    scale = np.array([[1 * s, 0, 0],\\n                      [0, 1 * s, 0],\\n                      [0, 0, 1]])\\n    gt_affine = trans_inv.dot(scale.dot(trans))', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imwarp.py.txt'}),\n",
       " Document(page_content=\"# create the random displacement field\\n    domain_grid2world = gt_affine\\n    codomain_grid2world = gt_affine\\n    disp, assign = vfu.create_random_displacement_2d(\\n        np.array(domain_shape, dtype=np.int32),\\n        domain_grid2world, np.array(codomain_shape, dtype=np.int32),\\n        codomain_grid2world)\\n    disp = np.array(disp, dtype=floating)\\n    assign = np.array(assign)\\n    # create a random image (with decimal digits) to warp\\n    moving_image = np.ndarray(codomain_shape, dtype=floating)\\n    ns = np.size(moving_image)\\n    moving_image[...] = rng.integers(0, 10, ns).reshape(codomain_shape)\\n    # set boundary values to zero so we don't test wrong interpolation due\\n    # to floating point precision\\n    moving_image[0, :] = 0\\n    moving_image[-1, :] = 0\\n    moving_image[:, 0] = 0\\n    moving_image[:, -1] = 0\\n\\n    # warp the moving image using the (exact) assignments\\n    expected = moving_image[(assign[..., 0], assign[..., 1])]\\n\\n    # warp using a DiffeomorphicMap instance\\n    diff_map = imwarp.DiffeomorphicMap(2, domain_shape, domain_grid2world,\\n                                       domain_shape, domain_grid2world,\\n                                       codomain_shape, codomain_grid2world,\\n                                       None)\\n    diff_map.forward = disp\\n\\n    # Verify that the transform method accepts different image types (note that\\n    # the actual image contained integer values, we don't want to test\\n    # rounding)\\n    for _type in [floating, np.float64, np.int64, np.int32]:\\n        moving_image = moving_image.astype(_type)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imwarp.py.txt'}),\n",
       " Document(page_content=\"# Verify that the transform method accepts different image types (note that\\n    # the actual image contained integer values, we don't want to test\\n    # rounding)\\n    for _type in [floating, np.float64, np.int64, np.int32]:\\n        moving_image = moving_image.astype(_type)\\n\\n        # warp using linear interpolation\\n        warped = diff_map.transform(moving_image, 'linear')\\n        # compare the images (the linear interpolation may introduce slight\\n        # precision errors)\\n        assert_array_almost_equal(warped, expected, decimal=5)\\n\\n        # Now test the nearest neighbor interpolation\\n        warped = diff_map.transform(moving_image, 'nearest')\\n        # compare the images (now we don't have to worry about precision,\\n        # it is n.n.)\\n        assert_array_almost_equal(warped, expected)\\n\\n        # verify the is_inverse flag\\n        inv = diff_map.inverse()\\n        warped = inv.transform_inverse(moving_image, 'linear')\\n        assert_array_almost_equal(warped, expected, decimal=5)\\n\\n        warped = inv.transform_inverse(moving_image, 'nearest')\\n        assert_array_almost_equal(warped, expected)\\n\\n    # Now test the inverse functionality\\n    diff_map = imwarp.DiffeomorphicMap(2, codomain_shape, codomain_grid2world,\\n                                       codomain_shape, codomain_grid2world,\\n                                       domain_shape, domain_grid2world, None)\\n    diff_map.backward = disp\\n    for _type in [floating, np.float64, np.int64, np.int32]:\\n        moving_image = moving_image.astype(_type)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imwarp.py.txt'}),\n",
       " Document(page_content=\"# Now test the inverse functionality\\n    diff_map = imwarp.DiffeomorphicMap(2, codomain_shape, codomain_grid2world,\\n                                       codomain_shape, codomain_grid2world,\\n                                       domain_shape, domain_grid2world, None)\\n    diff_map.backward = disp\\n    for _type in [floating, np.float64, np.int64, np.int32]:\\n        moving_image = moving_image.astype(_type)\\n\\n        # warp using linear interpolation\\n        warped = diff_map.transform_inverse(moving_image, 'linear')\\n        # compare the images (the linear interpolation may introduce slight\\n        # precision errors)\\n        assert_array_almost_equal(warped, expected, decimal=5)\\n\\n        # Now test the nearest neighbor interpolation\\n        warped = diff_map.transform_inverse(moving_image, 'nearest')\\n        # compare the images (now we don't have to worry about precision,\\n        # it is nearest neighbour)\\n        assert_array_almost_equal(warped, expected)\\n\\n    # Verify that DiffeomorphicMap raises the appropriate exceptions when\\n    # the sampling information is undefined\\n    diff_map = imwarp.DiffeomorphicMap(2, domain_shape, domain_grid2world,\\n                                       domain_shape, domain_grid2world,\\n                                       codomain_shape, codomain_grid2world,\\n                                       None)\\n    diff_map.forward = disp\\n    diff_map.domain_shape = None\\n    # If we don't provide the sampling info, it should try to use the map's\\n    # info, but it's None...\\n    assert_raises(ValueError, diff_map.transform, moving_image, 'linear')\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imwarp.py.txt'}),\n",
       " Document(page_content='# Same test for diff_map.transform_inverse\\n    diff_map = imwarp.DiffeomorphicMap(2, domain_shape, domain_grid2world,\\n                                       domain_shape, domain_grid2world,\\n                                       codomain_shape, codomain_grid2world,\\n                                       None)\\n    diff_map.forward = disp\\n    diff_map.codomain_shape = None\\n    # If we don\\'t provide the sampling info, it should try to use the map\\'s\\n    # info, but it\\'s None...\\n    assert_raises(ValueError, diff_map.transform_inverse,\\n                  moving_image, \\'linear\\')\\n\\n    # We must provide, at least, the reference grid shape\\n    assert_raises(ValueError, imwarp.DiffeomorphicMap, 2, None)\\n\\n    # Verify that matrices are correctly interpreted from string\\n    non_array_obj = diff_map\\n    array_obj = np.ones((3, 3))\\n    assert_raises(ValueError, diff_map.interpret_matrix, \\'a different string\\')\\n    assert_raises(ValueError, diff_map.interpret_matrix, non_array_obj)\\n    assert(diff_map.interpret_matrix(\\'identity\\') is None)\\n    assert(diff_map.interpret_matrix(None) is None)\\n    assert_array_equal(diff_map.interpret_matrix(array_obj), array_obj)\\n\\n\\ndef test_diffeomorphic_map_simplification_2d():\\n    r\"\"\" Test simplification of 2D diffeomorphic maps', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imwarp.py.txt'}),\n",
       " Document(page_content='def test_diffeomorphic_map_simplification_2d():\\n    r\"\"\" Test simplification of 2D diffeomorphic maps\\n\\n    Create an invertible deformation field, and define a DiffeomorphicMap\\n    using different voxel-to-space transforms for domain, codomain, and\\n    reference discretizations, also use a non-identity pre-aligning matrix.\\n    Warp a circle using the diffeomorphic map to obtain the expected warped\\n    circle. Now simplify the DiffeomorphicMap and warp the same circle\\n    using this simplified map. Verify that the two warped circles are equal\\n    up to numerical precision.\\n    \"\"\"\\n    # create a simple affine transformation\\n    dom_shape = (64, 64)\\n    cod_shape = (80, 80)\\n    nr = dom_shape[0]\\n    nc = dom_shape[1]\\n    s = 1.1\\n    t = 0.25\\n    trans = np.array([[1, 0, -t * nr],\\n                      [0, 1, -t * nc],\\n                      [0, 0, 1]])\\n    trans_inv = np.linalg.inv(trans)\\n    scale = np.array([[1 * s, 0, 0],\\n                      [0, 1 * s, 0],\\n                      [0, 0, 1]])\\n    gt_affine = trans_inv.dot(scale.dot(trans))\\n    # Create the invertible displacement fields and the circle\\n    radius = 16\\n    circle = vfu.create_circle(cod_shape[0], cod_shape[1], radius)\\n    d, dinv = vfu.create_harmonic_fields_2d(dom_shape[0],\\n                                            dom_shape[1], 0.3, 6)\\n    # Define different voxel-to-space transforms for domain, codomain and\\n    # reference grid, also, use a non-identity pre-align transform\\n    D = gt_affine\\n    C = imwarp.mult_aff(gt_affine, gt_affine)\\n    R = np.eye(3)\\n    P = gt_affine', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imwarp.py.txt'}),\n",
       " Document(page_content='# Create the original diffeomorphic map\\n    diff_map = imwarp.DiffeomorphicMap(2, dom_shape, R,\\n                                       dom_shape, D,\\n                                       cod_shape, C,\\n                                       P)\\n    diff_map.forward = np.array(d, dtype=floating)\\n    diff_map.backward = np.array(dinv, dtype=floating)\\n    # Warp the circle to obtain the expected image\\n    expected = diff_map.transform(circle, \\'linear\\')\\n\\n    # Simplify\\n    simplified = diff_map.get_simplified_transform()\\n    # warp the circle\\n    warped = simplified.transform(circle, \\'linear\\')\\n    # verify that the simplified map is equivalent to the\\n    # original one\\n    assert_array_almost_equal(warped, expected)\\n    # And of course, it must be simpler...\\n    assert_equal(simplified.domain_grid2world, None)\\n    assert_equal(simplified.codomain_grid2world, None)\\n    assert_equal(simplified.disp_grid2world, None)\\n    assert_equal(simplified.domain_world2grid, None)\\n    assert_equal(simplified.codomain_world2grid, None)\\n    assert_equal(simplified.disp_world2grid, None)\\n\\n\\ndef test_diffeomorphic_map_simplification_3d():\\n    r\"\"\" Test simplification of 3D diffeomorphic maps', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imwarp.py.txt'}),\n",
       " Document(page_content='Create an invertible deformation field, and define a DiffeomorphicMap\\n    using different voxel-to-space transforms for domain, codomain, and\\n    reference discretizations, also use a non-identity pre-aligning matrix.\\n    Warp a sphere using the diffeomorphic map to obtain the expected warped\\n    sphere. Now simplify the DiffeomorphicMap and warp the same sphere\\n    using this simplified map. Verify that the two warped spheres are equal\\n    up to numerical precision.\\n    \"\"\"\\n    # create a simple affine transformation\\n    domain_shape = (64, 64, 64)\\n    codomain_shape = (80, 80, 80)\\n    nr = domain_shape[0]\\n    nc = domain_shape[1]\\n    ns = domain_shape[2]\\n    s = 1.1\\n    t = 0.25\\n    trans = np.array([[1, 0, 0, -t * ns],\\n                      [0, 1, 0, -t * nr],\\n                      [0, 0, 1, -t * nc],\\n                      [0, 0, 0, 1]])\\n    trans_inv = np.linalg.inv(trans)\\n    scale = np.array([[1 * s, 0, 0, 0],\\n                      [0, 1 * s, 0, 0],\\n                      [0, 0, 1 * s, 0],\\n                      [0, 0, 0, 1]])\\n    gt_affine = trans_inv.dot(scale.dot(trans))\\n    # Create the invertible displacement fields and the sphere\\n    radius = 16\\n    sphere = vfu.create_sphere(codomain_shape[0], codomain_shape[1],\\n                               codomain_shape[2], radius)\\n    d, dinv = vfu.create_harmonic_fields_3d(domain_shape[0], domain_shape[1],\\n                                            domain_shape[2], 0.3, 6)\\n    # Define different voxel-to-space transforms for domain, codomain and\\n    # reference grid, also, use a non-identity pre-align transform\\n    D = gt_affine\\n    C = imwarp.mult_aff(gt_affine, gt_affine)\\n    R = np.eye(4)\\n    P = gt_affine', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imwarp.py.txt'}),\n",
       " Document(page_content='# Create the original diffeomorphic map\\n    diff_map = imwarp.DiffeomorphicMap(3, domain_shape, R,\\n                                       domain_shape, D,\\n                                       codomain_shape, C,\\n                                       P)\\n    diff_map.forward = np.array(d, dtype=floating)\\n    diff_map.backward = np.array(dinv, dtype=floating)\\n    # Warp the sphere to obtain the expected image\\n    expected = diff_map.transform(sphere, \\'linear\\')\\n\\n    # Simplify\\n    simplified = diff_map.get_simplified_transform()\\n    # warp the sphere\\n    warped = simplified.transform(sphere, \\'linear\\')\\n    # verify that the simplified map is equivalent to the\\n    # original one\\n    assert_array_almost_equal(warped, expected)\\n    # And of course, it must be simpler...\\n    assert_equal(simplified.domain_grid2world, None)\\n    assert_equal(simplified.codomain_grid2world, None)\\n    assert_equal(simplified.disp_grid2world, None)\\n    assert_equal(simplified.domain_world2grid, None)\\n    assert_equal(simplified.codomain_world2grid, None)\\n    assert_equal(simplified.disp_world2grid, None)\\n\\n\\ndef test_optimizer_exceptions():\\n    r\"\"\" Test exceptions from SyN\\n    \"\"\"\\n    # An arbitrary valid metric\\n    metric = metrics.SSDMetric(2)\\n    # The metric must not be None\\n    assert_raises(ValueError, imwarp.SymmetricDiffeomorphicRegistration,\\n                  None)\\n    # The iterations list must not be empty\\n    assert_raises(ValueError, imwarp.SymmetricDiffeomorphicRegistration,\\n                  metric, [])\\n\\n    optimizer = imwarp.SymmetricDiffeomorphicRegistration(metric, None)\\n    # Verify the default iterations list\\n    assert_array_equal(optimizer.level_iters, [100, 100, 25])', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imwarp.py.txt'}),\n",
       " Document(page_content='optimizer = imwarp.SymmetricDiffeomorphicRegistration(metric, None)\\n    # Verify the default iterations list\\n    assert_array_equal(optimizer.level_iters, [100, 100, 25])\\n\\n    # Verify exception thrown when attempting to fit the energy profile without\\n    # enough data\\n    assert_raises(ValueError, optimizer._get_energy_derivative)\\n    assert_raises(ValueError, optimizer.get_map)\\n\\n\\ndef test_get_direction_and_spacings():\\n    r\"\"\" Test direction and spacings from affine transforms\\n    \"\"\"\\n    xrot = 0.5\\n    yrot = 0.75\\n    zrot = 1.0\\n    direction_gt = eulerangles.euler2mat(zrot, yrot, xrot)\\n    spacings_gt = np.array([1.1, 1.2, 1.3])\\n    scaling_gt = np.diag(spacings_gt)\\n    translation_gt = np.array([1, 2, 3])\\n\\n    affine = np.eye(4)\\n    affine[:3, :3] = direction_gt.dot(scaling_gt)\\n    affine[:3, 3] = translation_gt\\n\\n    direction, spacings = imwarp.get_direction_and_spacings(affine, 3)\\n    assert_array_almost_equal(direction, direction_gt)\\n    assert_array_almost_equal(spacings, spacings_gt)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imwarp.py.txt'}),\n",
       " Document(page_content='affine = np.eye(4)\\n    affine[:3, :3] = direction_gt.dot(scaling_gt)\\n    affine[:3, 3] = translation_gt\\n\\n    direction, spacings = imwarp.get_direction_and_spacings(affine, 3)\\n    assert_array_almost_equal(direction, direction_gt)\\n    assert_array_almost_equal(spacings, spacings_gt)\\n\\n\\ndef simple_callback(sdr, status):\\n    r\"\"\" Verify callback function is called from SyN \"\"\"\\n    if status == imwarp.RegistrationStages.INIT_START:\\n        sdr.INIT_START_CALLED = 1\\n    if status == imwarp.RegistrationStages.INIT_END:\\n        sdr.INIT_END_CALLED = 1\\n    if status == imwarp.RegistrationStages.OPT_START:\\n        sdr.OPT_START_CALLED = 1\\n    if status == imwarp.RegistrationStages.OPT_END:\\n        sdr.OPT_END_CALLED = 1\\n    if status == imwarp.RegistrationStages.SCALE_START:\\n        sdr.SCALE_START_CALLED = 1\\n    if status == imwarp.RegistrationStages.SCALE_END:\\n        sdr.SCALE_END_CALLED = 1\\n    if status == imwarp.RegistrationStages.ITER_START:\\n        sdr.ITER_START_CALLED = 1\\n    if status == imwarp.RegistrationStages.ITER_END:\\n        sdr.ITER_END_CALLED = 1\\n\\n\\ndef test_ssd_2d_demons():\\n    r\"\"\" Test 2D SyN with SSD metric, demons-like optimizer\\n\\n    Classical Circle-To-C experiment for 2D monomodal registration. We\\n    verify that the final registration is of good quality.\\n    \"\"\"\\n    fname_moving = get_fnames(\\'reg_o\\')\\n    fname_static = get_fnames(\\'reg_c\\')', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imwarp.py.txt'}),\n",
       " Document(page_content='def test_ssd_2d_demons():\\n    r\"\"\" Test 2D SyN with SSD metric, demons-like optimizer\\n\\n    Classical Circle-To-C experiment for 2D monomodal registration. We\\n    verify that the final registration is of good quality.\\n    \"\"\"\\n    fname_moving = get_fnames(\\'reg_o\\')\\n    fname_static = get_fnames(\\'reg_c\\')\\n\\n    moving = np.load(fname_moving)\\n    static = np.load(fname_static)\\n    moving = np.array(moving, dtype=floating)\\n    static = np.array(static, dtype=floating)\\n    moving = (moving - moving.min()) / (moving.max() - moving.min())\\n    static = (static - static.min()) / (static.max() - static.min())\\n    # Create the SSD metric\\n    smooth = 4\\n    step_type = \\'demons\\'\\n    similarity_metric = metrics.SSDMetric(\\n        2, smooth=smooth, step_type=step_type)\\n\\n    # Configure and run the Optimizer\\n    level_iters = [200, 100, 50, 25]\\n    step_length = 0.25\\n    opt_tol = 1e-4\\n    inv_iter = 40\\n    inv_tol = 1e-3\\n    ss_sigma_factor = 0.2\\n    optimizer = imwarp.SymmetricDiffeomorphicRegistration(\\n        similarity_metric,\\n        level_iters,\\n        step_length,\\n        ss_sigma_factor,\\n        opt_tol,\\n        inv_iter,\\n        inv_tol)\\n\\n    # test callback being called\\n    optimizer.INIT_START_CALLED = 0\\n    optimizer.INIT_END_CALLED = 0\\n    optimizer.OPT_START_CALLED = 0\\n    optimizer.OPT_END_CALLED = 0\\n    optimizer.SCALE_START_CALLED = 0\\n    optimizer.SCALE_END_CALLED = 0\\n    optimizer.ITER_START_CALLED = 0\\n    optimizer.ITER_END_CALLED = 0\\n\\n    optimizer.callback_counter_test = 0\\n    optimizer.callback = simple_callback\\n\\n    optimizer.verbosity = VerbosityLevels.DEBUG\\n    mapping = optimizer.optimize(static, moving, None)\\n    m = optimizer.get_map()\\n    assert_equal(mapping, m)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imwarp.py.txt'}),\n",
       " Document(page_content='optimizer.callback_counter_test = 0\\n    optimizer.callback = simple_callback\\n\\n    optimizer.verbosity = VerbosityLevels.DEBUG\\n    mapping = optimizer.optimize(static, moving, None)\\n    m = optimizer.get_map()\\n    assert_equal(mapping, m)\\n\\n    warped = mapping.transform(moving)\\n    starting_energy = np.sum((static - moving)**2)\\n    final_energy = np.sum((static - warped)**2)\\n    reduced = 1.0 - final_energy / starting_energy\\n\\n    assert(reduced > 0.9)\\n    assert_equal(optimizer.OPT_START_CALLED, 1)\\n    assert_equal(optimizer.OPT_END_CALLED, 1)\\n    assert_equal(optimizer.SCALE_START_CALLED, 1)\\n    assert_equal(optimizer.SCALE_END_CALLED, 1)\\n    assert_equal(optimizer.ITER_START_CALLED, 1)\\n    assert_equal(optimizer.ITER_END_CALLED, 1)\\n\\n\\ndef test_ssd_2d_gauss_newton():\\n    r\"\"\" Test 2D SyN with SSD metric, Gauss-Newton optimizer\\n\\n    Classical Circle-To-C experiment for 2D monomodal registration. We\\n    verify that the final registration is of good quality.\\n    \"\"\"\\n    fname_moving = get_fnames(\\'reg_o\\')\\n    fname_static = get_fnames(\\'reg_c\\')\\n\\n    moving = np.load(fname_moving)\\n    static = np.load(fname_static)\\n    moving = np.array(moving, dtype=floating)\\n    static = np.array(static, dtype=floating)\\n    moving = (moving - moving.min()) / (moving.max() - moving.min())\\n    static = (static - static.min()) / (static.max() - static.min())\\n    # Create the SSD metric\\n    smooth = 4\\n    inner_iter = 5\\n    step_type = \\'gauss_newton\\'\\n    similarity_metric = metrics.SSDMetric(2, smooth, inner_iter, step_type)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imwarp.py.txt'}),\n",
       " Document(page_content=\"moving = np.load(fname_moving)\\n    static = np.load(fname_static)\\n    moving = np.array(moving, dtype=floating)\\n    static = np.array(static, dtype=floating)\\n    moving = (moving - moving.min()) / (moving.max() - moving.min())\\n    static = (static - static.min()) / (static.max() - static.min())\\n    # Create the SSD metric\\n    smooth = 4\\n    inner_iter = 5\\n    step_type = 'gauss_newton'\\n    similarity_metric = metrics.SSDMetric(2, smooth, inner_iter, step_type)\\n\\n    # Configure and run the Optimizer\\n    level_iters = [200, 100, 50, 25]\\n    step_length = 0.5\\n    opt_tol = 1e-4\\n    inv_iter = 40\\n    inv_tol = 1e-3\\n    ss_sigma_factor = 0.2\\n    optimizer = imwarp.SymmetricDiffeomorphicRegistration(\\n        similarity_metric,\\n        level_iters,\\n        step_length,\\n        ss_sigma_factor,\\n        opt_tol,\\n        inv_iter,\\n        inv_tol)\\n\\n    # test callback not being called\\n    optimizer.INIT_START_CALLED = 0\\n    optimizer.INIT_END_CALLED = 0\\n    optimizer.OPT_START_CALLED = 0\\n    optimizer.OPT_END_CALLED = 0\\n    optimizer.SCALE_START_CALLED = 0\\n    optimizer.SCALE_END_CALLED = 0\\n    optimizer.ITER_START_CALLED = 0\\n    optimizer.ITER_END_CALLED = 0\\n\\n    optimizer.verbosity = VerbosityLevels.DEBUG\\n    transformation = np.eye(3)\\n    mapping = optimizer.optimize(\\n        static, moving, transformation, transformation, transformation)\\n    m = optimizer.get_map()\\n    assert_equal(mapping, m)\\n\\n    warped = mapping.transform(moving)\\n    starting_energy = np.sum((static - moving)**2)\\n    final_energy = np.sum((static - warped)**2)\\n    reduced = 1.0 - final_energy / starting_energy\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imwarp.py.txt'}),\n",
       " Document(page_content='optimizer.verbosity = VerbosityLevels.DEBUG\\n    transformation = np.eye(3)\\n    mapping = optimizer.optimize(\\n        static, moving, transformation, transformation, transformation)\\n    m = optimizer.get_map()\\n    assert_equal(mapping, m)\\n\\n    warped = mapping.transform(moving)\\n    starting_energy = np.sum((static - moving)**2)\\n    final_energy = np.sum((static - warped)**2)\\n    reduced = 1.0 - final_energy / starting_energy\\n\\n    assert(reduced > 0.9)\\n    assert_equal(optimizer.OPT_START_CALLED, 0)\\n    assert_equal(optimizer.OPT_END_CALLED, 0)\\n    assert_equal(optimizer.SCALE_START_CALLED, 0)\\n    assert_equal(optimizer.SCALE_END_CALLED, 0)\\n    assert_equal(optimizer.ITER_START_CALLED, 0)\\n    assert_equal(optimizer.ITER_END_CALLED, 0)\\n\\n\\ndef get_warped_stacked_image(image, nslices, b, m):\\n    r\"\"\" Creates a volume by stacking copies of a deformed image\\n\\n    The image is deformed under an invertible field, and a 3D volume is\\n    generated as follows:\\n    the first and last `nslices`//3 slices are filled with zeros\\n    to simulate background. The remaining middle slices are filled with\\n    copies of the deformed `image` under the action of the invertible\\n    field.\\n\\n    Parameters\\n    ----------\\n    image : 2d array shape(r, c)\\n        the image to be deformed\\n    nslices : int\\n        the number of slices in the final volume\\n    b, m : float\\n        parameters of the harmonic field (as in [1]).\\n\\n    Returns\\n    -------\\n    vol : array shape(r, c) if `nslices`==1 else (r, c, `nslices`)\\n        the volumed generated using the undeformed image\\n    wvol : array shape(r, c) if `nslices`==1 else (r, c, `nslices`)\\n        the volumed generated using the warped image', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imwarp.py.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    vol : array shape(r, c) if `nslices`==1 else (r, c, `nslices`)\\n        the volumed generated using the undeformed image\\n    wvol : array shape(r, c) if `nslices`==1 else (r, c, `nslices`)\\n        the volumed generated using the warped image\\n\\n    References\\n    ----------\\n    [1] Chen, M., Lu, W., Chen, Q., Ruchala, K. J., & Olivera, G. H. (2008).\\n        A simple fixed-point approach to invert a deformation field.\\n        Medical Physics, 35(1), 81. doi:10.1118/1.2816107\\n    \"\"\"\\n    shape = image.shape\\n    # create a synthetic invertible map and warp the circle\\n    d, dinv = vfu.create_harmonic_fields_2d(shape[0], shape[1], b, m)\\n    d = np.asarray(d, dtype=floating)\\n    dinv = np.asarray(dinv, dtype=floating)\\n    mapping = DiffeomorphicMap(2, shape)\\n    mapping.forward, mapping.backward = d, dinv\\n    wimage = mapping.transform(image)\\n\\n    if nslices == 1:\\n        return image, wimage\\n\\n    # normalize and form the 3d by piling slices\\n    image = image.astype(floating)\\n    image = (image - image.min()) / (image.max() - image.min())\\n    zero_slices = nslices // 3\\n    vol = np.zeros(shape=image.shape + (nslices,))\\n    vol[..., zero_slices:(2 * zero_slices)] = image[..., None]\\n    wvol = np.zeros(shape=image.shape + (nslices,))\\n    wvol[..., zero_slices:(2 * zero_slices)] = wimage[..., None]\\n\\n    return vol, wvol\\n\\n\\ndef get_synthetic_warped_circle(nslices):\\n    # get a subsampled circle\\n    fname_cicle = get_fnames(\\'reg_o\\')\\n    circle = np.load(fname_cicle)[::4, ::4].astype(floating)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imwarp.py.txt'}),\n",
       " Document(page_content='return vol, wvol\\n\\n\\ndef get_synthetic_warped_circle(nslices):\\n    # get a subsampled circle\\n    fname_cicle = get_fnames(\\'reg_o\\')\\n    circle = np.load(fname_cicle)[::4, ::4].astype(floating)\\n\\n    # create a synthetic invertible map and warp the circle\\n    d, dinv = vfu.create_harmonic_fields_2d(64, 64, 0.1, 4)\\n    d = np.asarray(d, dtype=floating)\\n    dinv = np.asarray(dinv, dtype=floating)\\n    mapping = DiffeomorphicMap(2, (64, 64))\\n    mapping.forward, mapping.backward = d, dinv\\n    wcircle = mapping.transform(circle)\\n\\n    if nslices == 1:\\n        return circle, wcircle\\n\\n    # normalize and form the 3d by piling slices\\n    circle = (circle - circle.min()) / (circle.max() - circle.min())\\n    circle_3d = np.ndarray(circle.shape + (nslices,), dtype=floating)\\n    circle_3d[...] = circle[..., None]\\n    circle_3d[..., 0] = 0\\n    circle_3d[..., -1] = 0\\n\\n    # do the same with the warped circle\\n    wcircle = (wcircle - wcircle.min()) / (wcircle.max() - wcircle.min())\\n    wcircle_3d = np.ndarray(wcircle.shape + (nslices,), dtype=floating)\\n    wcircle_3d[...] = wcircle[..., None]\\n    wcircle_3d[..., 0] = 0\\n    wcircle_3d[..., -1] = 0\\n\\n    return circle_3d, wcircle_3d\\n\\n\\ndef test_ssd_3d_demons():\\n    r\"\"\" Test 3D SyN with SSD metric, demons-like optimizer\\n\\n    Register a stack of circles (\\'cylinder\\') before and after warping them\\n    with a synthetic diffeomorphism. We verify that the final registration\\n    is of good quality.\\n    \"\"\"\\n    moving, static = get_synthetic_warped_circle(30)\\n    moving[..., :8] = 0\\n    moving[..., -1:-9:-1] = 0\\n    static[..., :8] = 0\\n    static[..., -1:-9:-1] = 0', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imwarp.py.txt'}),\n",
       " Document(page_content='return circle_3d, wcircle_3d\\n\\n\\ndef test_ssd_3d_demons():\\n    r\"\"\" Test 3D SyN with SSD metric, demons-like optimizer\\n\\n    Register a stack of circles (\\'cylinder\\') before and after warping them\\n    with a synthetic diffeomorphism. We verify that the final registration\\n    is of good quality.\\n    \"\"\"\\n    moving, static = get_synthetic_warped_circle(30)\\n    moving[..., :8] = 0\\n    moving[..., -1:-9:-1] = 0\\n    static[..., :8] = 0\\n    static[..., -1:-9:-1] = 0\\n\\n    # Create the SSD metric\\n    smooth = 4\\n    step_type = \\'demons\\'\\n    similarity_metric = metrics.SSDMetric(3, smooth=smooth,\\n                                          step_type=step_type)\\n\\n    # Create the optimizer\\n    level_iters = [10, 10]\\n    step_length = 0.1\\n    opt_tol = 1e-4\\n    inv_iter = 20\\n    inv_tol = 1e-3\\n    ss_sigma_factor = 0.5\\n    optimizer = imwarp.SymmetricDiffeomorphicRegistration(\\n        similarity_metric,\\n        level_iters,\\n        step_length,\\n        ss_sigma_factor,\\n        opt_tol,\\n        inv_iter,\\n        inv_tol)\\n    optimizer.verbosity = VerbosityLevels.DEBUG\\n    mapping = optimizer.optimize(static, moving, None)\\n    m = optimizer.get_map()\\n    assert_equal(mapping, m)\\n\\n    warped = mapping.transform(moving)\\n    starting_energy = np.sum((static - moving)**2)\\n    final_energy = np.sum((static - warped)**2)\\n    reduced = 1.0 - final_energy / starting_energy\\n\\n    assert(reduced > 0.9)\\n\\n\\ndef test_ssd_3d_gauss_newton():\\n    r\"\"\" Test 3D SyN with SSD metric, Gauss-Newton optimizer', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imwarp.py.txt'}),\n",
       " Document(page_content='warped = mapping.transform(moving)\\n    starting_energy = np.sum((static - moving)**2)\\n    final_energy = np.sum((static - warped)**2)\\n    reduced = 1.0 - final_energy / starting_energy\\n\\n    assert(reduced > 0.9)\\n\\n\\ndef test_ssd_3d_gauss_newton():\\n    r\"\"\" Test 3D SyN with SSD metric, Gauss-Newton optimizer\\n\\n    Register a stack of circles (\\'cylinder\\') before and after warping them\\n    with a synthetic diffeomorphism. We verify that the final registration\\n    is of good quality.\\n    \"\"\"\\n    moving, static = get_synthetic_warped_circle(35)\\n    moving[..., :10] = 0\\n    moving[..., -1:-11:-1] = 0\\n    static[..., :10] = 0\\n    static[..., -1:-11:-1] = 0\\n\\n    # Create the SSD metric\\n    smooth = 4\\n    inner_iter = 5\\n    step_type = \\'gauss_newton\\'\\n    similarity_metric = metrics.SSDMetric(3, smooth, inner_iter, step_type)\\n\\n    # Create the optimizer\\n    level_iters = [10, 10]\\n    step_length = 0.1\\n    opt_tol = 1e-4\\n    inv_iter = 20\\n    inv_tol = 1e-3\\n    ss_sigma_factor = 0.5\\n    optimizer = imwarp.SymmetricDiffeomorphicRegistration(\\n        similarity_metric,\\n        level_iters,\\n        step_length,\\n        ss_sigma_factor,\\n        opt_tol,\\n        inv_iter,\\n        inv_tol)\\n    optimizer.verbosity = VerbosityLevels.DEBUG\\n    mapping = optimizer.optimize(static, moving, None)\\n    m = optimizer.get_map()\\n    assert_equal(mapping, m)\\n\\n    warped = mapping.transform(moving)\\n    starting_energy = np.sum((static - moving)**2)\\n    final_energy = np.sum((static - warped)**2)\\n    reduced = 1.0 - final_energy / starting_energy\\n\\n    assert(reduced > 0.9)\\n\\n\\ndef test_cc_2d():\\n    r\"\"\" Test 2D SyN with CC metric', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imwarp.py.txt'}),\n",
       " Document(page_content='warped = mapping.transform(moving)\\n    starting_energy = np.sum((static - moving)**2)\\n    final_energy = np.sum((static - warped)**2)\\n    reduced = 1.0 - final_energy / starting_energy\\n\\n    assert(reduced > 0.9)\\n\\n\\ndef test_cc_2d():\\n    r\"\"\" Test 2D SyN with CC metric\\n\\n    Register a coronal slice from a T1w brain MRI before and after warping\\n    it under a synthetic invertible map. We verify that the final\\n    registration is of good quality.\\n    \"\"\"\\n    fname = get_fnames(\\'t1_coronal_slice\\')\\n    nslices = 1\\n    b = 0.1\\n    m = 4\\n\\n    image = np.load(fname)\\n    moving, static = get_warped_stacked_image(image, nslices, b, m)\\n\\n    # Configure the metric\\n    sigma_diff = 3.0\\n    radius = 4\\n    metric = metrics.CCMetric(2, sigma_diff, radius)\\n\\n    # Configure and run the Optimizer\\n    level_iters = [15, 5]\\n    optimizer = imwarp.SymmetricDiffeomorphicRegistration(metric, level_iters)\\n    optimizer.verbosity = VerbosityLevels.DEBUG\\n    mapping = optimizer.optimize(static, moving, None)\\n    m = optimizer.get_map()\\n    assert_equal(mapping, m)\\n\\n    warped = mapping.transform(moving)\\n    starting_energy = np.sum((static - moving)**2)\\n    final_energy = np.sum((static - warped)**2)\\n    reduced = 1.0 - final_energy / starting_energy\\n\\n    assert(reduced > 0.9)\\n\\n\\ndef test_cc_3d():\\n    r\"\"\" Test 3D SyN with CC metric\\n\\n    Register a volume created by stacking copies of a coronal slice from\\n    a T1w brain MRI before and after warping it under a synthetic\\n    invertible map. We verify that the final registration is of good\\n    quality.\\n    \"\"\"\\n    fname = get_fnames(\\'t1_coronal_slice\\')\\n    nslices = 21\\n    b = 0.1\\n    m = 4', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imwarp.py.txt'}),\n",
       " Document(page_content='assert(reduced > 0.9)\\n\\n\\ndef test_cc_3d():\\n    r\"\"\" Test 3D SyN with CC metric\\n\\n    Register a volume created by stacking copies of a coronal slice from\\n    a T1w brain MRI before and after warping it under a synthetic\\n    invertible map. We verify that the final registration is of good\\n    quality.\\n    \"\"\"\\n    fname = get_fnames(\\'t1_coronal_slice\\')\\n    nslices = 21\\n    b = 0.1\\n    m = 4\\n\\n    image = np.load(fname)\\n    moving, static = get_warped_stacked_image(image, nslices, b, m)\\n\\n    # Create the CC metric\\n    sigma_diff = 2.0\\n    radius = 2\\n    similarity_metric = metrics.CCMetric(3, sigma_diff, radius)\\n\\n    # Create the optimizer\\n    level_iters = [20, 5]\\n    step_length = 0.25\\n    opt_tol = 1e-4\\n    inv_iter = 20\\n    inv_tol = 1e-3\\n    ss_sigma_factor = 0.2\\n    optimizer = imwarp.SymmetricDiffeomorphicRegistration(\\n        similarity_metric,\\n        level_iters,\\n        step_length,\\n        ss_sigma_factor,\\n        opt_tol,\\n        inv_iter,\\n        inv_tol)\\n    optimizer.verbosity = VerbosityLevels.DEBUG\\n\\n    mapping = optimizer.optimize(static, moving, None, None, None)\\n    m = optimizer.get_map()\\n    assert_equal(mapping, m)\\n\\n    warped = mapping.transform(moving)\\n    starting_energy = np.sum((static - moving)**2)\\n    final_energy = np.sum((static - warped)**2)\\n    reduced = 1.0 - final_energy / starting_energy\\n\\n    assert(reduced > 0.9)\\n\\n\\ndef test_em_3d_gauss_newton():\\n    r\"\"\" Test 3D SyN with EM metric, Gauss-Newton optimizer', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imwarp.py.txt'}),\n",
       " Document(page_content='mapping = optimizer.optimize(static, moving, None, None, None)\\n    m = optimizer.get_map()\\n    assert_equal(mapping, m)\\n\\n    warped = mapping.transform(moving)\\n    starting_energy = np.sum((static - moving)**2)\\n    final_energy = np.sum((static - warped)**2)\\n    reduced = 1.0 - final_energy / starting_energy\\n\\n    assert(reduced > 0.9)\\n\\n\\ndef test_em_3d_gauss_newton():\\n    r\"\"\" Test 3D SyN with EM metric, Gauss-Newton optimizer\\n\\n    Register a volume created by stacking copies of a coronal slice from\\n    a T1w brain MRI before and after warping it under a synthetic\\n    invertible map. We verify that the final registration is of good\\n    quality.\\n    \"\"\"\\n    fname = get_fnames(\\'t1_coronal_slice\\')\\n    nslices = 21\\n    b = 0.1\\n    m = 4\\n\\n    image = np.load(fname)\\n    moving, static = get_warped_stacked_image(image, nslices, b, m)\\n\\n    # Create the EM metric\\n    smooth = 2.0\\n    inner_iter = 20\\n    step_length = 0.25\\n    q_levels = 256\\n    double_gradient = True\\n    iter_type = \\'gauss_newton\\'\\n    similarity_metric = metrics.EMMetric(\\n        3, smooth, inner_iter, q_levels, double_gradient, iter_type)\\n\\n    # Create the optimizer\\n    level_iters = [20, 5]\\n    opt_tol = 1e-4\\n    inv_iter = 20\\n    inv_tol = 1e-3\\n    ss_sigma_factor = 1.0\\n    optimizer = imwarp.SymmetricDiffeomorphicRegistration(\\n        similarity_metric,\\n        level_iters,\\n        step_length,\\n        ss_sigma_factor,\\n        opt_tol,\\n        inv_iter,\\n        inv_tol)\\n    optimizer.verbosity = VerbosityLevels.DEBUG\\n    mapping = optimizer.optimize(static, moving, None)\\n    m = optimizer.get_map()\\n    assert_equal(mapping, m)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imwarp.py.txt'}),\n",
       " Document(page_content='warped = mapping.transform(moving)\\n    starting_energy = np.sum((static - moving)**2)\\n    final_energy = np.sum((static - warped)**2)\\n    reduced = 1.0 - final_energy / starting_energy\\n\\n    assert(reduced > 0.9)\\n\\n\\ndef test_em_2d_gauss_newton():\\n    r\"\"\" Test 2D SyN with EM metric, Gauss-Newton optimizer\\n\\n    Register a coronal slice from a T1w brain MRI before and after warping\\n    it under a synthetic invertible map. We verify that the final\\n    registration is of good quality.\\n    \"\"\"\\n\\n    fname = get_fnames(\\'t1_coronal_slice\\')\\n    nslices = 1\\n    b = 0.1\\n    m = 4\\n\\n    image = np.load(fname)\\n    moving, static = get_warped_stacked_image(image, nslices, b, m)\\n\\n    # Configure the metric\\n    smooth = 5.0\\n    inner_iter = 20\\n    q_levels = 256\\n    double_gradient = False\\n    iter_type = \\'gauss_newton\\'\\n    metric = metrics.EMMetric(\\n        2, smooth, inner_iter, q_levels, double_gradient, iter_type)\\n\\n    # Configure and run the Optimizer\\n    level_iters = [40, 20, 10]\\n    optimizer = imwarp.SymmetricDiffeomorphicRegistration(metric, level_iters)\\n    optimizer.verbosity = VerbosityLevels.DEBUG\\n    mapping = optimizer.optimize(static, moving, None)\\n    m = optimizer.get_map()\\n    assert_equal(mapping, m)\\n\\n    warped = mapping.transform(moving)\\n    starting_energy = np.sum((static - moving)**2)\\n    final_energy = np.sum((static - warped)**2)\\n    reduced = 1.0 - final_energy / starting_energy\\n\\n    assert(reduced > 0.9)\\n\\n\\ndef test_em_3d_demons():\\n    r\"\"\" Test 3D SyN with EM metric, demons-like optimizer', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imwarp.py.txt'}),\n",
       " Document(page_content='warped = mapping.transform(moving)\\n    starting_energy = np.sum((static - moving)**2)\\n    final_energy = np.sum((static - warped)**2)\\n    reduced = 1.0 - final_energy / starting_energy\\n\\n    assert(reduced > 0.9)\\n\\n\\ndef test_em_3d_demons():\\n    r\"\"\" Test 3D SyN with EM metric, demons-like optimizer\\n\\n    Register a volume created by stacking copies of a coronal slice from\\n    a T1w brain MRI before and after warping it under a synthetic\\n    invertible map. We verify that the final registration is of good\\n    quality.\\n    \"\"\"\\n    fname = get_fnames(\\'t1_coronal_slice\\')\\n    nslices = 21\\n    b = 0.1\\n    m = 4\\n\\n    image = np.load(fname)\\n    moving, static = get_warped_stacked_image(image, nslices, b, m)\\n\\n    # Create the EM metric\\n    smooth = 2.0\\n    inner_iter = 20\\n    step_length = 0.25\\n    q_levels = 256\\n    double_gradient = True\\n    iter_type = \\'demons\\'\\n    similarity_metric = metrics.EMMetric(\\n        3, smooth, inner_iter, q_levels, double_gradient, iter_type)\\n\\n    # Create the optimizer\\n    level_iters = [20, 5]\\n    opt_tol = 1e-4\\n    inv_iter = 20\\n    inv_tol = 1e-3\\n    ss_sigma_factor = 1.0\\n    optimizer = imwarp.SymmetricDiffeomorphicRegistration(\\n        similarity_metric,\\n        level_iters,\\n        step_length,\\n        ss_sigma_factor,\\n        opt_tol,\\n        inv_iter,\\n        inv_tol)\\n    optimizer.verbosity = VerbosityLevels.DEBUG\\n    mapping = optimizer.optimize(static, moving, None)\\n    m = optimizer.get_map()\\n    assert_equal(mapping, m)\\n\\n    warped = mapping.transform(moving)\\n    starting_energy = np.sum((static - moving)**2)\\n    final_energy = np.sum((static - warped)**2)\\n    reduced = 1.0 - final_energy / starting_energy\\n\\n    assert(reduced > 0.9)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imwarp.py.txt'}),\n",
       " Document(page_content='warped = mapping.transform(moving)\\n    starting_energy = np.sum((static - moving)**2)\\n    final_energy = np.sum((static - warped)**2)\\n    reduced = 1.0 - final_energy / starting_energy\\n\\n    assert(reduced > 0.9)\\n\\n\\ndef test_em_2d_demons():\\n    r\"\"\" Test 2D SyN with EM metric, demons-like optimizer\\n\\n    Register a coronal slice from a T1w brain MRI before and after warping\\n    it under a synthetic invertible map. We verify that the final\\n    registration is of good quality.\\n    \"\"\"\\n    fname = get_fnames(\\'t1_coronal_slice\\')\\n    nslices = 1\\n    b = 0.1\\n    m = 4\\n\\n    image = np.load(fname)\\n    moving, static = get_warped_stacked_image(image, nslices, b, m)\\n\\n    # Configure the metric\\n    smooth = 2.0\\n    inner_iter = 20\\n    q_levels = 256\\n    double_gradient = False\\n    iter_type = \\'demons\\'\\n    metric = metrics.EMMetric(\\n        2, smooth, inner_iter, q_levels, double_gradient, iter_type)\\n\\n    # Configure and run the Optimizer\\n    level_iters = [40, 20, 10]\\n    optimizer = imwarp.SymmetricDiffeomorphicRegistration(metric, level_iters)\\n    optimizer.verbosity = VerbosityLevels.DEBUG\\n    mapping = optimizer.optimize(static, moving, None)\\n    m = optimizer.get_map()\\n    assert_equal(mapping, m)\\n\\n    warped = mapping.transform(moving)\\n    starting_energy = np.sum((static - moving)**2)\\n    final_energy = np.sum((static - warped)**2)\\n    reduced = 1.0 - final_energy / starting_energy\\n\\n    assert(reduced > 0.9)\\n\\n\\n@set_random_number_generator(1741332)\\ndef test_coordinate_mapping(rng):\\n    r\"\"\"Test coordinate mapping with DiffeomorphicMap', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imwarp.py.txt'}),\n",
       " Document(page_content='1. Create a random displacement field and a small affine transform to map\\n       grid to world coordinates.\\n    2. Create a DiffeomorphicMap with the previously created field and affine\\n       transform.\\n    3. Create a random input image.\\n    4. Select a few non-boundary voxels from the domain grid.\\n    5. Warp the input image with the DiffeomorphicMap and interpolate the\\n       **warped image** at the selected locations. The result is the\\n       `expected` array.\\n    6. Map only the selected points using the DiffeomorphicMap and\\n       interpolate the **input image** at the warped points. The result is the\\n       `actual` array, which should be almost equal to the `expected` array.\\n    \"\"\"\\n    for dim in range(2, 4):\\n        npoints = 100\\n        points = np.empty((npoints, dim), dtype=np.float64)\\n        if dim == 2:\\n            domain_shape = (10, 10)\\n            codomain_shape = (15, 15)\\n            nr = domain_shape[0]\\n            nc = domain_shape[1]\\n            s = 1.1\\n            t = 0.25\\n            trans = np.array([[1, 0, -t*nr],\\n                              [0, 1, -t*nc],\\n                              [0, 0, 1]])\\n            trans_inv = np.linalg.inv(trans)\\n            scale = np.array([[1*s, 0, 0],\\n                              [0, 1*s, 0],\\n                              [0, 0, 1]])\\n            gt_affine = trans_inv.dot(scale.dot(trans))\\n            n = codomain_shape[0] * codomain_shape[1]\\n            moving_image = rng.integers(0, 10, n).reshape(codomain_shape)\\n            moving_image = moving_image.astype(np.float64)\\n            # Select a few grid coordinates not at the boundary of the domain\\n            points[:, 0] = rng.integers(1, nr-1, npoints)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imwarp.py.txt'}),\n",
       " Document(page_content='[0, 1*s, 0],\\n                              [0, 0, 1]])\\n            gt_affine = trans_inv.dot(scale.dot(trans))\\n            n = codomain_shape[0] * codomain_shape[1]\\n            moving_image = rng.integers(0, 10, n).reshape(codomain_shape)\\n            moving_image = moving_image.astype(np.float64)\\n            # Select a few grid coordinates not at the boundary of the domain\\n            points[:, 0] = rng.integers(1, nr-1, npoints)\\n            points[:, 1] = rng.integers(1, nc-1, npoints)\\n            random_df = vfu.create_random_displacement_2d\\n            interpolate_f = interpolate_scalar_2d\\n        else:\\n            domain_shape = (10, 10, 10)\\n            codomain_shape = (15, 15, 15)\\n            nr = domain_shape[0]\\n            nc = domain_shape[1]\\n            ns = domain_shape[2]\\n            s = 1.1\\n            t = 0.25\\n            trans = np.array([[1, 0, 0, -t*ns],\\n                              [0, 1, 0, -t*nr],\\n                              [0, 0, 1, -t*nc],\\n                              [0, 0, 0, 1]])\\n            trans_inv = np.linalg.inv(trans)\\n            scale = np.array([[1*s, 0, 0, 0],\\n                              [0, 1*s, 0, 0],\\n                              [0, 0, 1*s, 0],\\n                              [0, 0, 0, 1]])\\n            gt_affine = trans_inv.dot(scale.dot(trans))\\n            n = codomain_shape[0] * codomain_shape[1] * codomain_shape[2]\\n            moving_image = rng.integers(0, 10, n).reshape(codomain_shape)\\n            moving_image = moving_image.astype(np.float64)\\n            # Select a few grid coordinates not at the boundary of the domain\\n            points[:, 0] = rng.integers(1, nr-1, npoints)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imwarp.py.txt'}),\n",
       " Document(page_content='[0, 0, 1*s, 0],\\n                              [0, 0, 0, 1]])\\n            gt_affine = trans_inv.dot(scale.dot(trans))\\n            n = codomain_shape[0] * codomain_shape[1] * codomain_shape[2]\\n            moving_image = rng.integers(0, 10, n).reshape(codomain_shape)\\n            moving_image = moving_image.astype(np.float64)\\n            # Select a few grid coordinates not at the boundary of the domain\\n            points[:, 0] = rng.integers(1, nr-1, npoints)\\n            points[:, 1] = rng.integers(1, nc-1, npoints)\\n            points[:, 2] = rng.integers(1, ns-1, npoints)\\n            random_df = vfu.create_random_displacement_3d\\n            interpolate_f = interpolate_scalar_3d', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imwarp.py.txt'}),\n",
       " Document(page_content=\"# create the random displacement field\\n        domain_grid2world = gt_affine\\n        codomain_grid2world = gt_affine\\n        disp, assign = random_df(np.array(domain_shape, dtype=np.int32),\\n                                 domain_grid2world,\\n                                 np.array(codomain_shape, dtype=np.int32),\\n                                 codomain_grid2world)\\n        disp = disp.astype(floating)\\n        # Create a DiffeomorphicMap instance\\n        diff_map = imwarp.DiffeomorphicMap(dim, domain_shape,\\n                                           domain_grid2world, domain_shape,\\n                                           domain_grid2world, codomain_shape,\\n                                           codomain_grid2world, None)\\n        diff_map.forward = disp\\n\\n        # Here, expected is obtained after two interpolation steps, therefore\\n        # we need to increase the tolerance when comparing against the result\\n        # using only one interpolation step (we set decimal=5 below)\\n        warped = diff_map.transform(moving_image, 'linear')\\n        expected, inside = interpolate_f(warped, points)\\n\\n        # Now map the points with the implementation under test\\n        # Specify how to map the given array to world coordinates\\n        in2world = diff_map.domain_grid2world\\n        # Request mapping back from world to grid coordinates\\n        world2out = diff_map.domain_world2grid\\n        # Execute warping\\n        wpoints = diff_map.transform_points(points, in2world, world2out)\\n        # Interpolate at warped points and verify it's equal to direct warping\\n        actual, inside = interpolate_f(moving_image, wpoints)\\n        assert_array_almost_equal(actual, expected, decimal=5)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imwarp.py.txt'}),\n",
       " Document(page_content='if dim in [3, 4]:\\n            wpoints_2 = deform_streamlines([points, ], disp, np.eye(4),\\n                                           domain_grid2world, np.eye(4),\\n                                           codomain_grid2world)\\n\\n            assert_array_almost_equal(wpoints, wpoints_2[0])', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_imwarp.py.txt'}),\n",
       " Document(page_content=\"import itertools\\nimport numpy as np\\nfrom scipy import ndimage\\nfrom dipy.align import floating\\nfrom dipy.align.metrics import SSDMetric, CCMetric, EMMetric\\nfrom numpy.testing import (assert_array_equal,\\n                           assert_array_almost_equal,\\n                           assert_raises)\\nfrom dipy.testing.decorators import set_random_number_generator\\n\\n\\ndef test_exceptions():\\n    for invalid_dim in [-1, 0, 1, 4, 5]:\\n        assert_raises(ValueError, CCMetric, invalid_dim)\\n        assert_raises(ValueError, EMMetric, invalid_dim)\\n        assert_raises(ValueError, SSDMetric, invalid_dim)\\n    assert_raises(ValueError, SSDMetric, 3, step_type='unknown_metric_name')\\n    assert_raises(ValueError, EMMetric, 3, step_type='unknown_metric_name')\\n\\n    def init_metric(shape, radius):\\n        dim = len(shape)\\n        metric = CCMetric(dim, radius=radius)\\n        metric.set_static_image(np.arange(np.prod(shape),\\n                                          dtype=float).reshape(shape),\\n                                np.eye(4), np.ones(dim), np.eye(3))\\n        metric.set_moving_image(np.arange(np.prod(shape),\\n                                dtype=float).reshape(shape),\\n                                np.eye(4), np.ones(dim), np.eye(3))\\n        return metric\\n\\n    # Generate many shape combinations\\n    shapes_2d = itertools.product((5, 8), (8, 5))\\n    shapes_3d = itertools.product((5, 8), (8, 5), (30, 50))\\n    all_shapes = itertools.chain(shapes_2d, shapes_3d)\\n    # expected to fail for any dimension < 2*radius + 1.\\n    for shape in all_shapes:\\n        metric = init_metric(shape, 4)\\n        assert_raises(ValueError, metric.initialize_iteration)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_metrics.py.txt'}),\n",
       " Document(page_content='# Generate many shape combinations\\n    shapes_2d = itertools.product((5, 8), (8, 5))\\n    shapes_3d = itertools.product((5, 8), (8, 5), (30, 50))\\n    all_shapes = itertools.chain(shapes_2d, shapes_3d)\\n    # expected to fail for any dimension < 2*radius + 1.\\n    for shape in all_shapes:\\n        metric = init_metric(shape, 4)\\n        assert_raises(ValueError, metric.initialize_iteration)\\n\\n    # expected to pass for any dimension == 2*radius + 1.\\n    metric = init_metric((9, 9), 4)\\n    metric.initialize_iteration()\\n\\n\\n@set_random_number_generator(7181309)\\ndef test_EMMetric_image_dynamics(rng):\\n    metric = EMMetric(2)\\n\\n    target_shape = (10, 10)\\n    # create a random image\\n    image = np.ndarray(target_shape, dtype=floating)\\n    image[...] = rng.integers(\\n        0, 10, np.size(image)).reshape(tuple(target_shape))\\n    # compute the expected binary mask\\n    expected = (image > 0).astype(np.int32)\\n\\n    metric.use_static_image_dynamics(image, None)\\n    assert_array_equal(expected, metric.static_image_mask)\\n\\n    metric.use_moving_image_dynamics(image, None)\\n    assert_array_equal(expected, metric.moving_image_mask)\\n\\n\\ndef test_em_demons_step_2d():\\n    r\"\"\"\\n    Compares the output of the demons step in 2d against an analytical\\n    step. The fixed image is given by $F(x) = \\\\frac{1}{2}||x - c_f||^2$, the\\n    moving image is given by $G(x) = \\\\frac{1}{2}||x - c_g||^2$,\\n    $x, c_f, c_g \\\\in R^{2}$', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_metrics.py.txt'}),\n",
       " Document(page_content='metric.use_moving_image_dynamics(image, None)\\n    assert_array_equal(expected, metric.moving_image_mask)\\n\\n\\ndef test_em_demons_step_2d():\\n    r\"\"\"\\n    Compares the output of the demons step in 2d against an analytical\\n    step. The fixed image is given by $F(x) = \\\\frac{1}{2}||x - c_f||^2$, the\\n    moving image is given by $G(x) = \\\\frac{1}{2}||x - c_g||^2$,\\n    $x, c_f, c_g \\\\in R^{2}$\\n\\n    References\\n    ----------\\n    [Vercauteren09] Vercauteren, T., Pennec, X., Perchant, A., & Ayache, N.\\n                    (2009). Diffeomorphic demons: efficient non-parametric\\n                    image registration. NeuroImage, 45(1 Suppl), S61-72.\\n                    doi:10.1016/j.neuroimage.2008.10.040\\n    \"\"\"\\n    # Select arbitrary images\\' shape (same shape for both images)\\n    sh = (20, 10)\\n\\n    # Select arbitrary centers\\n    c_f = np.asarray(sh) / 2\\n    c_g = c_f + 0.5\\n\\n    # Compute the identity vector field I(x) = x in R^2\\n    x_0 = np.asarray(range(sh[0]))\\n    x_1 = np.asarray(range(sh[1]))\\n    X = np.ndarray(sh + (2,), dtype=np.float64)\\n    O = np.ones(sh)\\n    X[..., 0] = x_0[:, None] * O\\n    X[..., 1] = x_1[None, :] * O\\n\\n    # Compute the gradient fields of F and G\\n    grad_F = X - c_f\\n    grad_G = X - c_g\\n\\n    # The squared norm of grad_G to be used later\\n    sq_norm_grad_F = np.sum(grad_F**2, -1)\\n    sq_norm_grad_G = np.sum(grad_G**2, -1)\\n\\n    # Compute F and G\\n    F = 0.5 * sq_norm_grad_F\\n    G = 0.5 * sq_norm_grad_G', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_metrics.py.txt'}),\n",
       " Document(page_content=\"# Compute the gradient fields of F and G\\n    grad_F = X - c_f\\n    grad_G = X - c_g\\n\\n    # The squared norm of grad_G to be used later\\n    sq_norm_grad_F = np.sum(grad_F**2, -1)\\n    sq_norm_grad_G = np.sum(grad_G**2, -1)\\n\\n    # Compute F and G\\n    F = 0.5 * sq_norm_grad_F\\n    G = 0.5 * sq_norm_grad_G\\n\\n    # Create an instance of EMMetric\\n    metric = EMMetric(2)\\n    metric.static_spacing = np.array([1.2, 1.2])\\n    # The $\\\\sigma_x$ (eq. 4 in [Vercauteren09]) parameter is computed in ANTS\\n    # based on the image's spacing\\n    sigma_x_sq = np.sum(metric.static_spacing**2) / metric.dim\\n    # Set arbitrary values for $\\\\sigma_i$ (eq. 4 in [Vercauteren09])\\n    # The original Demons algorithm used simply |F(x) - G(x)| as an\\n    # estimator, so let's use it as well\\n    sigma_i_sq = (F - G)**2\\n    # Set the properties relevant to the demons methods\\n    metric.smooth = 3.0\\n    metric.gradient_static = np.array(grad_F, dtype=floating)\\n    metric.gradient_moving = np.array(grad_G, dtype=floating)\\n    metric.static_image = np.array(F, dtype=floating)\\n    metric.moving_image = np.array(G, dtype=floating)\\n    metric.staticq_means_field = np.array(F, dtype=floating)\\n    metric.staticq_sigma_sq_field = np.array(sigma_i_sq, dtype=floating)\\n    metric.movingq_means_field = np.array(G, dtype=floating)\\n    metric.movingq_sigma_sq_field = np.array(sigma_i_sq, dtype=floating)\\n\\n    # compute the step using the implementation under test\\n    actual_forward = metric.compute_demons_step(True)\\n    actual_backward = metric.compute_demons_step(False)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_metrics.py.txt'}),\n",
       " Document(page_content='# compute the step using the implementation under test\\n    actual_forward = metric.compute_demons_step(True)\\n    actual_backward = metric.compute_demons_step(False)\\n\\n    # Now directly compute the demons steps according to eq 4 in\\n    # [Vercauteren09]\\n    num_fwd = sigma_x_sq * (G - F)\\n    den_fwd = sigma_x_sq * sq_norm_grad_F + sigma_i_sq\\n    # This is $J^{P}$ in eq. 4 [Vercauteren09]\\n    expected_fwd = -1 * np.array(grad_F)\\n    expected_fwd[..., 0] *= num_fwd / den_fwd\\n    expected_fwd[..., 1] *= num_fwd / den_fwd\\n    # apply Gaussian smoothing\\n    expected_fwd[..., 0] = ndimage.gaussian_filter(expected_fwd[..., 0], 3.0)\\n    expected_fwd[..., 1] = ndimage.gaussian_filter(expected_fwd[..., 1], 3.0)\\n\\n    num_bwd = sigma_x_sq * (F - G)\\n    den_bwd = sigma_x_sq * sq_norm_grad_G + sigma_i_sq\\n    # This is $J^{P}$ in eq. 4 [Vercauteren09]\\n    expected_bwd = -1 * np.array(grad_G)\\n    expected_bwd[..., 0] *= num_bwd / den_bwd\\n    expected_bwd[..., 1] *= num_bwd / den_bwd\\n    # apply Gaussian smoothing\\n    expected_bwd[..., 0] = ndimage.gaussian_filter(expected_bwd[..., 0], 3.0)\\n    expected_bwd[..., 1] = ndimage.gaussian_filter(expected_bwd[..., 1], 3.0)\\n\\n    assert_array_almost_equal(actual_forward, expected_fwd)\\n    assert_array_almost_equal(actual_backward, expected_bwd)\\n\\n\\ndef test_em_demons_step_3d():\\n    r\"\"\"\\n    Compares the output of the demons step in 3d against an analytical\\n    step. The fixed image is given by $F(x) = \\\\frac{1}{2}||x - c_f||^2$, the\\n    moving image is given by $G(x) = \\\\frac{1}{2}||x - c_g||^2$,\\n    $x, c_f, c_g \\\\in R^{3}$', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_metrics.py.txt'}),\n",
       " Document(page_content='assert_array_almost_equal(actual_forward, expected_fwd)\\n    assert_array_almost_equal(actual_backward, expected_bwd)\\n\\n\\ndef test_em_demons_step_3d():\\n    r\"\"\"\\n    Compares the output of the demons step in 3d against an analytical\\n    step. The fixed image is given by $F(x) = \\\\frac{1}{2}||x - c_f||^2$, the\\n    moving image is given by $G(x) = \\\\frac{1}{2}||x - c_g||^2$,\\n    $x, c_f, c_g \\\\in R^{3}$\\n\\n    References\\n    ----------\\n    [Vercauteren09] Vercauteren, T., Pennec, X., Perchant, A., & Ayache, N.\\n                    (2009). Diffeomorphic demons: efficient non-parametric\\n                    image registration. NeuroImage, 45(1 Suppl), S61-72.\\n                    doi:10.1016/j.neuroimage.2008.10.040\\n    \"\"\"\\n    # Select arbitrary images\\' shape (same shape for both images)\\n    sh = (20, 15, 10)\\n\\n    # Select arbitrary centers\\n    c_f = np.asarray(sh) / 2\\n    c_g = c_f + 0.5\\n\\n    # Compute the identity vector field I(x) = x in R^2\\n    x_0 = np.asarray(range(sh[0]))\\n    x_1 = np.asarray(range(sh[1]))\\n    x_2 = np.asarray(range(sh[2]))\\n    X = np.ndarray(sh + (3,), dtype=np.float64)\\n    O = np.ones(sh)\\n    X[..., 0] = x_0[:, None, None] * O\\n    X[..., 1] = x_1[None, :, None] * O\\n    X[..., 2] = x_2[None, None, :] * O\\n\\n    # Compute the gradient fields of F and G\\n    grad_F = X - c_f\\n    grad_G = X - c_g\\n\\n    # The squared norm of grad_G to be used later\\n    sq_norm_grad_F = np.sum(grad_F**2, -1)\\n    sq_norm_grad_G = np.sum(grad_G**2, -1)\\n\\n    # Compute F and G\\n    F = 0.5 * sq_norm_grad_F\\n    G = 0.5 * sq_norm_grad_G', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_metrics.py.txt'}),\n",
       " Document(page_content=\"# Compute the gradient fields of F and G\\n    grad_F = X - c_f\\n    grad_G = X - c_g\\n\\n    # The squared norm of grad_G to be used later\\n    sq_norm_grad_F = np.sum(grad_F**2, -1)\\n    sq_norm_grad_G = np.sum(grad_G**2, -1)\\n\\n    # Compute F and G\\n    F = 0.5 * sq_norm_grad_F\\n    G = 0.5 * sq_norm_grad_G\\n\\n    # Create an instance of EMMetric\\n    metric = EMMetric(3)\\n    metric.static_spacing = np.array([1.2, 1.2, 1.2])\\n    # The $\\\\sigma_x$ (eq. 4 in [Vercauteren09]) parameter is computed in ANTS\\n    # based on the image's spacing\\n    sigma_x_sq = np.sum(metric.static_spacing**2) / metric.dim\\n    # Set arbitrary values for $\\\\sigma_i$ (eq. 4 in [Vercauteren09])\\n    # The original Demons algorithm used simply |F(x) - G(x)| as an\\n    # estimator, so let's use it as well\\n    sigma_i_sq = (F - G)**2\\n    # Set the properties relevant to the demons methods\\n    metric.smooth = 3.0\\n    metric.gradient_static = np.array(grad_F, dtype=floating)\\n    metric.gradient_moving = np.array(grad_G, dtype=floating)\\n    metric.static_image = np.array(F, dtype=floating)\\n    metric.moving_image = np.array(G, dtype=floating)\\n    metric.staticq_means_field = np.array(F, dtype=floating)\\n    metric.staticq_sigma_sq_field = np.array(sigma_i_sq, dtype=floating)\\n    metric.movingq_means_field = np.array(G, dtype=floating)\\n    metric.movingq_sigma_sq_field = np.array(sigma_i_sq, dtype=floating)\\n\\n    # compute the step using the implementation under test\\n    actual_forward = metric.compute_demons_step(True)\\n    actual_backward = metric.compute_demons_step(False)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_metrics.py.txt'}),\n",
       " Document(page_content='# compute the step using the implementation under test\\n    actual_forward = metric.compute_demons_step(True)\\n    actual_backward = metric.compute_demons_step(False)\\n\\n    # Now directly compute the demons steps according to eq 4 in\\n    # [Vercauteren09]\\n    num_fwd = sigma_x_sq * (G - F)\\n    den_fwd = sigma_x_sq * sq_norm_grad_F + sigma_i_sq\\n    expected_fwd = -1 * np.array(grad_F)\\n    expected_fwd[..., 0] *= num_fwd / den_fwd\\n    expected_fwd[..., 1] *= num_fwd / den_fwd\\n    expected_fwd[..., 2] *= num_fwd / den_fwd\\n    # apply Gaussian smoothing\\n    expected_fwd[..., 0] = ndimage.gaussian_filter(expected_fwd[..., 0], 3.0)\\n    expected_fwd[..., 1] = ndimage.gaussian_filter(expected_fwd[..., 1], 3.0)\\n    expected_fwd[..., 2] = ndimage.gaussian_filter(expected_fwd[..., 2], 3.0)\\n\\n    num_bwd = sigma_x_sq * (F - G)\\n    den_bwd = sigma_x_sq * sq_norm_grad_G + sigma_i_sq\\n    expected_bwd = -1 * np.array(grad_G)\\n    expected_bwd[..., 0] *= num_bwd / den_bwd\\n    expected_bwd[..., 1] *= num_bwd / den_bwd\\n    expected_bwd[..., 2] *= num_bwd / den_bwd\\n    # apply Gaussian smoothing\\n    expected_bwd[..., 0] = ndimage.gaussian_filter(expected_bwd[..., 0], 3.0)\\n    expected_bwd[..., 1] = ndimage.gaussian_filter(expected_bwd[..., 1], 3.0)\\n    expected_bwd[..., 2] = ndimage.gaussian_filter(expected_bwd[..., 2], 3.0)\\n\\n    assert_array_almost_equal(actual_forward, expected_fwd)\\n    assert_array_almost_equal(actual_backward, expected_bwd)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_metrics.py.txt'}),\n",
       " Document(page_content='import numpy as np\\nimport scipy as sp\\nfrom functools import reduce\\nfrom operator import mul\\nfrom dipy.core.ndindex import ndindex\\nfrom dipy.core.interpolation import (interpolate_scalar_2d,\\n                                     interpolate_scalar_3d)\\nfrom dipy.data import get_fnames\\nfrom dipy.align import vector_fields as vf\\nfrom dipy.align.transforms import regtransforms\\nfrom dipy.align.parzenhist import (ParzenJointHistogram,\\n                                   cubic_spline,\\n                                   cubic_spline_derivative,\\n                                   sample_domain_regular)\\nfrom numpy.testing import (assert_array_equal,\\n                           assert_array_almost_equal,\\n                           assert_almost_equal,\\n                           assert_equal,\\n                           assert_raises)\\nfrom dipy.testing.decorators import set_random_number_generator\\n\\nfactors = {(\\'TRANSLATION\\', 2): 2.0,\\n           (\\'ROTATION\\', 2): 0.1,\\n           (\\'RIGID\\', 2): 0.1,\\n           (\\'SCALING\\', 2): 0.01,\\n           (\\'AFFINE\\', 2): 0.1,\\n           (\\'TRANSLATION\\', 3): 2.0,\\n           (\\'ROTATION\\', 3): 0.1,\\n           (\\'RIGID\\', 3): 0.1,\\n           (\\'SCALING\\', 3): 0.1,\\n           (\\'AFFINE\\', 3): 0.1}\\n\\n\\ndef create_random_image_pair(sh, nvals, rng=None):\\n    r\"\"\" Create a pair of images with an arbitrary, non-uniform joint PDF', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_parzenhist.py.txt'}),\n",
       " Document(page_content='factors = {(\\'TRANSLATION\\', 2): 2.0,\\n           (\\'ROTATION\\', 2): 0.1,\\n           (\\'RIGID\\', 2): 0.1,\\n           (\\'SCALING\\', 2): 0.01,\\n           (\\'AFFINE\\', 2): 0.1,\\n           (\\'TRANSLATION\\', 3): 2.0,\\n           (\\'ROTATION\\', 3): 0.1,\\n           (\\'RIGID\\', 3): 0.1,\\n           (\\'SCALING\\', 3): 0.1,\\n           (\\'AFFINE\\', 3): 0.1}\\n\\n\\ndef create_random_image_pair(sh, nvals, rng=None):\\n    r\"\"\" Create a pair of images with an arbitrary, non-uniform joint PDF\\n\\n    Parameters\\n    ----------\\n    sh : array, shape (dim,)\\n        the shape of the images to be created\\n    nvals : int\\n        maximum number of different values in the generated 2D images.\\n        The voxel intensities of the returned images will be in\\n        {0, 1, ..., nvals-1}\\n    rng : numpy.random.generator\\n        numpy\\'s random generator. If None, it is set with a random seed.\\n        Default is None\\n\\n    Returns\\n    -------\\n    static : array, shape=sh\\n        first image in the image pair\\n    moving : array, shape=sh\\n        second image in the image pair\\n    \"\"\"\\n    if rng is None:\\n        rng = np.random.default_rng()\\n    sz = reduce(mul, sh, 1)\\n    sh = tuple(sh)\\n    static = rng.integers(0, nvals, sz).reshape(sh)\\n\\n    # This is just a simple way of making  the distribution non-uniform\\n    moving = static.copy()\\n    moving += rng.integers(0, nvals // 2, sz).reshape(sh) - nvals // 4\\n\\n    # This is just a simple way of making  the distribution non-uniform\\n    static = moving.copy()\\n    static += rng.integers(0, nvals // 2, sz).reshape(sh) - nvals // 4\\n\\n    return static.astype(np.float64), moving.astype(np.float64)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_parzenhist.py.txt'}),\n",
       " Document(page_content='# This is just a simple way of making  the distribution non-uniform\\n    moving = static.copy()\\n    moving += rng.integers(0, nvals // 2, sz).reshape(sh) - nvals // 4\\n\\n    # This is just a simple way of making  the distribution non-uniform\\n    static = moving.copy()\\n    static += rng.integers(0, nvals // 2, sz).reshape(sh) - nvals // 4\\n\\n    return static.astype(np.float64), moving.astype(np.float64)\\n\\n\\ndef test_cubic_spline():\\n    # Cubic spline as defined in [Mattes03] eq. (3)\\n    #\\n    # [Mattes03] Mattes, D., Haynor, D. R., Vesselle, H., Lewellen, T. K.,\\n    #            & Eubank, W. PET-CT image registration in the chest using\\n    #            free-form deformations. IEEE Transactions on Medical Imaging,\\n    #            22(1), 120-8, 2003.\\n    in_list = []\\n    expected = []\\n    for epsilon in [-1e-9, 0.0, 1e-9]:\\n        for t in [-2.0, -1.0, 0.0, 1.0, 2.0]:\\n            x = t + epsilon\\n            in_list.append(x)\\n            absx = np.abs(x)\\n            sqrx = x * x\\n            if absx < 1:\\n                expected.append((4.0 - 6 * sqrx + 3.0 * (absx ** 3)) / 6.0)\\n            elif absx < 2:\\n                expected.append(((2 - absx) ** 3) / 6.0)\\n            else:\\n                expected.append(0.0)\\n    actual = cubic_spline(np.array(in_list, dtype=np.float64))\\n    assert_array_almost_equal(actual, np.array(expected, dtype=np.float64))', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_parzenhist.py.txt'}),\n",
       " Document(page_content='def test_cubic_spline_derivative():\\n    # Test derivative of the cubic spline, as defined in [Mattes03] eq. (3) by\\n    # comparing the analytical and numerical derivatives\\n    #\\n    # [Mattes03] Mattes, D., Haynor, D. R., Vesselle, H., Lewellen, T. K.,\\n    #            & Eubank, W. PET-CT image registration in the chest using\\n    #            free-form deformations. IEEE Transactions on Medical Imaging,\\n    #            22(1), 120-8, 2003.\\n    in_list = []\\n    for epsilon in [-1e-9, 0.0, 1e-9]:\\n        for t in [-2.0, -1.0, 0.0, 1.0, 2.0]:\\n            x = t + epsilon\\n            in_list.append(x)\\n    h = 1e-6\\n    in_list = np.array(in_list)\\n    input_h = in_list + h\\n    s = np.array(cubic_spline(in_list))\\n    s_h = np.array(cubic_spline(input_h))\\n    expected = (s_h - s) / h\\n    actual = cubic_spline_derivative(in_list)\\n    assert_array_almost_equal(actual, expected)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_parzenhist.py.txt'}),\n",
       " Document(page_content='def test_parzen_joint_histogram():\\n    # Test the simple functionality of ParzenJointHistogram,\\n    # the gradients and computation of the joint intensity distribution\\n    # will be tested independently\\n    for nbins in [15, 30, 50]:\\n        for min_int in [-10.0, 0.0, 10.0]:\\n            for intensity_range in [0.1, 1.0, 10.0]:\\n                fact = 1\\n                max_int = min_int + intensity_range\\n                P = ParzenJointHistogram(nbins)\\n                # Make a pair of 4-pixel images, introduce +/- 1 values\\n                # that will be excluded using a mask\\n                static = np.array([min_int - 1.0, min_int,\\n                                   max_int, max_int + 1.0])\\n                # Multiply by an arbitrary value (make the ranges different)\\n                moving = fact * np.array([min_int, min_int - 1.0,\\n                                          max_int + 1.0, max_int])\\n                # Create a mask to exclude the invalid values (beyond min and\\n                # max computed above)\\n                static_mask = np.array([0, 1, 1, 0])\\n                moving_mask = np.array([1, 0, 0, 1])\\n\\n                P.setup(static, moving, static_mask, moving_mask)\\n\\n                # Test bin_normalize_static at the boundary\\n                normalized = P.bin_normalize_static(min_int)\\n                assert_almost_equal(normalized, P.padding)\\n                index = P.bin_index(normalized)\\n                assert_equal(index, P.padding)\\n                normalized = P.bin_normalize_static(max_int)\\n                assert_almost_equal(normalized, nbins - P.padding)\\n                index = P.bin_index(normalized)\\n                assert_equal(index, nbins - 1 - P.padding)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_parzenhist.py.txt'}),\n",
       " Document(page_content='# Test bin_normalize_moving at the boundary\\n                normalized = P.bin_normalize_moving(fact * min_int)\\n                assert_almost_equal(normalized, P.padding)\\n                index = P.bin_index(normalized)\\n                assert_equal(index, P.padding)\\n                normalized = P.bin_normalize_moving(fact * max_int)\\n                assert_almost_equal(normalized, nbins - P.padding)\\n                index = P.bin_index(normalized)\\n                assert_equal(index, nbins - 1 - P.padding)\\n\\n                # Test bin_index not at the boundary\\n                delta_s = (max_int - min_int) / (nbins - 2 * P.padding)\\n                delta_m = fact * (max_int - min_int) / (nbins - 2 * P.padding)\\n                for i in range(nbins - 2 * P.padding):\\n                    normalized = P.bin_normalize_static(min_int +\\n                                                        (i + 0.5) * delta_s)\\n                    index = P.bin_index(normalized)\\n                    assert_equal(index, P.padding + i)\\n\\n                    normalized = P.bin_normalize_moving(fact * min_int +\\n                                                        (i + 0.5) * delta_m)\\n                    index = P.bin_index(normalized)\\n                    assert_equal(index, P.padding + i)\\n\\n\\n@set_random_number_generator(1246592)\\ndef test_parzen_densities(rng):\\n    # Test the computation of the joint intensity distribution\\n    # using a dense and a sparse set of values\\n    nbins = 32\\n    nr = 30\\n    nc = 35\\n    ns = 20\\n    nvals = 50', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_parzenhist.py.txt'}),\n",
       " Document(page_content='@set_random_number_generator(1246592)\\ndef test_parzen_densities(rng):\\n    # Test the computation of the joint intensity distribution\\n    # using a dense and a sparse set of values\\n    nbins = 32\\n    nr = 30\\n    nc = 35\\n    ns = 20\\n    nvals = 50\\n\\n    for dim in [2, 3]:\\n        if dim == 2:\\n            shape = (nr, nc)\\n            static, moving = create_random_image_pair(shape, nvals, rng)\\n        else:\\n            shape = (ns, nr, nc)\\n            static, moving = create_random_image_pair(shape, nvals, rng)\\n\\n        # Initialize\\n        parzen_hist = ParzenJointHistogram(nbins)\\n        parzen_hist.setup(static, moving)\\n        # Get distributions computed by dense sampling\\n        parzen_hist.update_pdfs_dense(static, moving)\\n        actual_joint_dense = parzen_hist.joint\\n        actual_mmarginal_dense = parzen_hist.mmarginal\\n        actual_smarginal_dense = parzen_hist.smarginal\\n\\n        # Get distributions computed by sparse sampling\\n        sval = static.reshape(-1)\\n        mval = moving.reshape(-1)\\n        parzen_hist.update_pdfs_sparse(sval, mval)\\n        actual_joint_sparse = parzen_hist.joint\\n        actual_mmarginal_sparse = parzen_hist.mmarginal\\n        actual_smarginal_sparse = parzen_hist.smarginal', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_parzenhist.py.txt'}),\n",
       " Document(page_content='# Get distributions computed by sparse sampling\\n        sval = static.reshape(-1)\\n        mval = moving.reshape(-1)\\n        parzen_hist.update_pdfs_sparse(sval, mval)\\n        actual_joint_sparse = parzen_hist.joint\\n        actual_mmarginal_sparse = parzen_hist.mmarginal\\n        actual_smarginal_sparse = parzen_hist.smarginal\\n\\n        # Compute the expected joint distribution with dense sampling\\n        expected_joint_dense = np.zeros(shape=(nbins, nbins))\\n        for index in ndindex(shape):\\n            sv = parzen_hist.bin_normalize_static(static[index])\\n            mv = parzen_hist.bin_normalize_moving(moving[index])\\n            sbin = parzen_hist.bin_index(sv)\\n            # The spline is centered at mv, will evaluate for all row\\n            spline_arg = np.array([i - mv for i in range(nbins)])\\n            contribution = cubic_spline(spline_arg)\\n            expected_joint_dense[sbin, :] += contribution\\n\\n        # Compute the expected joint distribution with sparse sampling\\n        expected_joint_sparse = np.zeros(shape=(nbins, nbins))\\n        for index in range(sval.shape[0]):\\n            sv = parzen_hist.bin_normalize_static(sval[index])\\n            mv = parzen_hist.bin_normalize_moving(mval[index])\\n            sbin = parzen_hist.bin_index(sv)\\n            # The spline is centered at mv, will evaluate for all row\\n            spline_arg = np.array([i - mv for i in range(nbins)])\\n            contribution = cubic_spline(spline_arg)\\n            expected_joint_sparse[sbin, :] += contribution', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_parzenhist.py.txt'}),\n",
       " Document(page_content='# Verify joint distributions\\n        expected_joint_dense /= expected_joint_dense.sum()\\n        expected_joint_sparse /= expected_joint_sparse.sum()\\n        assert_array_almost_equal(actual_joint_dense, expected_joint_dense)\\n        assert_array_almost_equal(actual_joint_sparse, expected_joint_sparse)\\n\\n        # Verify moving marginals\\n        expected_mmarginal_dense = expected_joint_dense.sum(0)\\n        expected_mmarginal_dense /= expected_mmarginal_dense.sum()\\n        expected_mmarginal_sparse = expected_joint_sparse.sum(0)\\n        expected_mmarginal_sparse /= expected_mmarginal_sparse.sum()\\n        assert_array_almost_equal(actual_mmarginal_dense,\\n                                  expected_mmarginal_dense)\\n        assert_array_almost_equal(actual_mmarginal_sparse,\\n                                  expected_mmarginal_sparse)\\n\\n        # Verify static marginals\\n        expected_smarginal_dense = expected_joint_dense.sum(1)\\n        expected_smarginal_dense /= expected_smarginal_dense.sum()\\n        expected_smarginal_sparse = expected_joint_sparse.sum(1)\\n        expected_smarginal_sparse /= expected_smarginal_sparse.sum()\\n        assert_array_almost_equal(actual_smarginal_dense,\\n                                  expected_smarginal_dense)\\n        assert_array_almost_equal(actual_smarginal_sparse,\\n                                  expected_smarginal_sparse)\\n\\n\\ndef setup_random_transform(transform, rfactor, nslices=45, sigma=1, rng=None):\\n    r\"\"\" Creates a pair of images related to each other by an affine transform', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_parzenhist.py.txt'}),\n",
       " Document(page_content='def setup_random_transform(transform, rfactor, nslices=45, sigma=1, rng=None):\\n    r\"\"\" Creates a pair of images related to each other by an affine transform\\n\\n    We transform the static image with a random transform so that the\\n    returned ground-truth transform will produce the static image\\n    when applied to the moving image. This will simply stack some copies of\\n    a T1 coronal slice image and add some zero slices up and down to\\n    reduce boundary artefacts when interpolating.\\n\\n    Parameters\\n    ----------\\n    transform: instance of Transform\\n        defines the type of random transformation that will be created\\n    rfactor: float\\n        the factor to multiply the uniform(0,1) random noise that will be\\n        added to the identity parameters to create the random transform\\n    nslices: int\\n        number of slices to be stacked to form the volumes\\n    sigma: float\\n        standard deviation of the gaussian filter\\n    rng: np.random.Generator\\n        numpy\\'s random generator. If None, it is set with a random seed.\\n        Default is None\\n    \"\"\"\\n    if rng is None:\\n        rng = np.random.default_rng()\\n    dim = 2 if nslices == 1 else 3\\n    if transform.get_dim() != dim:\\n        raise ValueError(\"Transform and requested volume have different dims.\")\\n    zero_slices = nslices // 3\\n\\n    fname = get_fnames(\\'t1_coronal_slice\\')\\n    moving_slice = np.load(fname)\\n    moving_slice = moving_slice[40:180, 50:210]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_parzenhist.py.txt'}),\n",
       " Document(page_content=\"fname = get_fnames('t1_coronal_slice')\\n    moving_slice = np.load(fname)\\n    moving_slice = moving_slice[40:180, 50:210]\\n\\n    if nslices == 1:\\n        dim = 2\\n        moving = moving_slice\\n        transform_method = vf.transform_2d_affine\\n    else:\\n        dim = 3\\n        transform_method = vf.transform_3d_affine\\n        moving = np.zeros(shape=moving_slice.shape + (nslices,))\\n        moving[..., zero_slices:(2 * zero_slices)] = moving_slice[..., None]\\n\\n    moving = sp.ndimage.gaussian_filter(moving, sigma)\\n\\n    moving_g2w = np.eye(dim + 1)\\n    mmask = np.ones_like(moving, dtype=np.int32)\\n\\n    # Create a transform by slightly perturbing the identity parameters\\n    theta = transform.get_identity_parameters()\\n    n = transform.get_number_of_parameters()\\n    theta += rng.random(n) * rfactor\\n\\n    M = transform.param_to_matrix(theta)\\n    shape = np.array(moving.shape, dtype=np.int32)\\n    static = np.array(transform_method(moving.astype(np.float32), shape, M))\\n    static = static.astype(np.float64)\\n    static_g2w = np.eye(dim + 1)\\n    smask = np.ones_like(static, dtype=np.int32)\\n\\n    return static, moving, static_g2w, moving_g2w, smask, mmask, M\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_parzenhist.py.txt'}),\n",
       " Document(page_content='M = transform.param_to_matrix(theta)\\n    shape = np.array(moving.shape, dtype=np.int32)\\n    static = np.array(transform_method(moving.astype(np.float32), shape, M))\\n    static = static.astype(np.float64)\\n    static_g2w = np.eye(dim + 1)\\n    smask = np.ones_like(static, dtype=np.int32)\\n\\n    return static, moving, static_g2w, moving_g2w, smask, mmask, M\\n\\n\\n@set_random_number_generator(3147702)\\ndef test_joint_pdf_gradients_dense(rng):\\n    # Compare the analytical and numerical (finite differences) gradient of\\n    # the joint distribution (i.e. derivatives of each histogram cell) w.r.t.\\n    # the transform parameters. Since the histograms are discrete partitions\\n    # of the image intensities, the finite difference approximation is\\n    # normally not very close to the analytical derivatives. Other sources of\\n    # error are the interpolation used when transforming the images and the\\n    # boundary intensities introduced when interpolating outside of the image\\n    # (i.e. some \"zeros\" are introduced at the boundary which affect the\\n    # numerical derivatives but is not taken into account by the analytical\\n    # derivatives). Thus, we need to relax the verification. Instead of\\n    # looking for the analytical and numerical gradients to be very close to\\n    # each other, we will verify that they approximately point in the same\\n    # direction by testing if the angle they form is close to zero.\\n    h = 1e-4', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_parzenhist.py.txt'}),\n",
       " Document(page_content='# Make sure dictionary entries are processed in the same order regardless\\n    # of the platform. Otherwise any random numbers drawn within the loop\\n    # would make the test non-deterministic even if we fix the seed before\\n    # the loop. Right now, this test does not draw any samples, but we still\\n    # sort the entries to prevent future related failures.\\n    for ttype in sorted(factors):\\n        dim = ttype[1]\\n        if dim == 2:\\n            nslices = 1\\n            transform_method = vf.transform_2d_affine\\n        else:\\n            nslices = 45\\n            transform_method = vf.transform_3d_affine\\n\\n        transform = regtransforms[ttype]\\n        factor = factors[ttype]\\n        theta = transform.get_identity_parameters()\\n\\n        static, moving, static_g2w, moving_g2w, smask, mmask, M = \\\\\\n            setup_random_transform(transform, factor, nslices, 5.0,\\n                                   rng=rng)\\n        parzen_hist = ParzenJointHistogram(32)\\n        parzen_hist.setup(static, moving, smask, mmask)\\n\\n        # Compute the gradient at theta with the implementation under test\\n        M = transform.param_to_matrix(theta)\\n        shape = np.array(static.shape, dtype=np.int32)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_parzenhist.py.txt'}),\n",
       " Document(page_content='static, moving, static_g2w, moving_g2w, smask, mmask, M = \\\\\\n            setup_random_transform(transform, factor, nslices, 5.0,\\n                                   rng=rng)\\n        parzen_hist = ParzenJointHistogram(32)\\n        parzen_hist.setup(static, moving, smask, mmask)\\n\\n        # Compute the gradient at theta with the implementation under test\\n        M = transform.param_to_matrix(theta)\\n        shape = np.array(static.shape, dtype=np.int32)\\n\\n        moved = transform_method(moving.astype(np.float32), shape, M)\\n        moved = np.array(moved)\\n        parzen_hist.update_pdfs_dense(static.astype(np.float64),\\n                                      moved.astype(np.float64))\\n        # Get the joint distribution evaluated at theta\\n        J0 = np.copy(parzen_hist.joint)\\n        grid_to_space = np.eye(dim + 1)\\n        spacing = np.ones(dim, dtype=np.float64)\\n        mgrad, inside = vf.gradient(moving.astype(np.float32), moving_g2w,\\n                                    spacing, shape, grid_to_space)\\n        params = transform.get_identity_parameters()\\n        parzen_hist.update_gradient_dense(\\n            params, transform, static.astype(np.float64),\\n            moved.astype(np.float64), grid_to_space,\\n            mgrad, smask, mmask)\\n        actual = np.copy(parzen_hist.joint_grad)\\n        # Now we have the gradient of the joint distribution w.r.t. the\\n        # transform parameters', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_parzenhist.py.txt'}),\n",
       " Document(page_content='# Compute the gradient using finite differences\\n        n = transform.get_number_of_parameters()\\n        expected = np.empty_like(actual)\\n        for i in range(n):\\n            dtheta = theta.copy()\\n            dtheta[i] += h\\n            # Update the joint distribution with the transformed moving image\\n            M = transform.param_to_matrix(dtheta)\\n            shape = np.array(static.shape, dtype=np.int32)\\n            moved = transform_method(moving.astype(np.float32), shape, M)\\n            moved = np.array(moved)\\n            parzen_hist.update_pdfs_dense(static.astype(np.float64),\\n                                          moved.astype(np.float64))\\n            J1 = np.copy(parzen_hist.joint)\\n            expected[..., i] = (J1 - J0) / h\\n\\n        # Dot product and norms of gradients of each joint histogram cell\\n        # i.e. the derivatives of each cell w.r.t. all parameters\\n        P = (expected * actual).sum(2)\\n        enorms = np.sqrt((expected ** 2).sum(2))\\n        anorms = np.sqrt((actual ** 2).sum(2))\\n        prodnorms = enorms * anorms\\n        # Cosine of angle between the expected and actual gradients.\\n        # Exclude very small gradients\\n        P[prodnorms > 1e-6] /= (prodnorms[prodnorms > 1e-6])\\n        P[prodnorms <= 1e-6] = 0\\n        # Verify that a large proportion of the gradients point almost in\\n        # the same direction. Disregard very small gradients\\n        mean_cosine = P[P != 0].mean()\\n        std_cosine = P[P != 0].std()\\n        assert(mean_cosine > 0.9)\\n        assert(std_cosine < 0.25)\\n\\n\\n@set_random_number_generator(3147702)\\ndef test_joint_pdf_gradients_sparse(rng):\\n    h = 1e-4', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_parzenhist.py.txt'}),\n",
       " Document(page_content='@set_random_number_generator(3147702)\\ndef test_joint_pdf_gradients_sparse(rng):\\n    h = 1e-4\\n\\n    # Make sure dictionary entries are processed in the same order regardless\\n    # of the platform. Otherwise any random numbers drawn within the loop\\n    # would make the test non-deterministic even if we fix the seed before\\n    # the loop.Right now, this test does not draw any samples, but we still\\n    # sort the entries to prevent future related failures.\\n\\n    for ttype in sorted(factors):\\n        dim = ttype[1]\\n        if dim == 2:\\n            nslices = 1\\n            interp_method = interpolate_scalar_2d\\n        else:\\n            nslices = 45\\n            interp_method = interpolate_scalar_3d\\n\\n        transform = regtransforms[ttype]\\n        factor = factors[ttype]\\n        theta = transform.get_identity_parameters()\\n\\n        static, moving, static_g2w, moving_g2w, smask, mmask, M = \\\\\\n            setup_random_transform(transform, factor, nslices, 5.0,\\n                                   rng=rng)\\n        parzen_hist = ParzenJointHistogram(32)\\n        parzen_hist.setup(static, moving, smask, mmask)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_parzenhist.py.txt'}),\n",
       " Document(page_content='transform = regtransforms[ttype]\\n        factor = factors[ttype]\\n        theta = transform.get_identity_parameters()\\n\\n        static, moving, static_g2w, moving_g2w, smask, mmask, M = \\\\\\n            setup_random_transform(transform, factor, nslices, 5.0,\\n                                   rng=rng)\\n        parzen_hist = ParzenJointHistogram(32)\\n        parzen_hist.setup(static, moving, smask, mmask)\\n\\n        # Sample the fixed-image domain\\n        k = 3\\n        sigma = 0.25\\n        shape = np.array(static.shape, dtype=np.int32)\\n        samples = sample_domain_regular(k, shape, static_g2w, sigma, rng)\\n        samples = np.array(samples)\\n        samples = np.hstack((samples, np.ones(samples.shape[0])[:, None]))\\n        sp_to_static = np.linalg.inv(static_g2w)\\n        samples_static_grid = sp_to_static.dot(samples.T).T[..., :dim]\\n        intensities_static, inside = interp_method(static.astype(np.float32),\\n                                                   samples_static_grid)\\n        # The routines in vector_fields operate, mostly, with float32 because\\n        # they were thought to be used for non-linear registration. We may need\\n        # to write some float64 counterparts for affine registration, where\\n        # memory is not so big issue\\n        intensities_static = np.array(intensities_static, dtype=np.float64)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_parzenhist.py.txt'}),\n",
       " Document(page_content='# Compute the gradient at theta with the implementation under test\\n        M = transform.param_to_matrix(theta)\\n        sp_to_moving = np.linalg.inv(moving_g2w).dot(M)\\n        samples_moving_grid = sp_to_moving.dot(samples.T).T[..., :dim]\\n        intensities_moving, inside = interp_method(moving.astype(np.float32),\\n                                                   samples_moving_grid)\\n        intensities_moving = np.array(intensities_moving, dtype=np.float64)\\n        parzen_hist.update_pdfs_sparse(intensities_static, intensities_moving)\\n        # Get the joint distribution evaluated at theta\\n        J0 = np.copy(parzen_hist.joint)\\n\\n        spacing = np.ones(dim + 1, dtype=np.float64)\\n        mgrad, inside = vf.sparse_gradient(moving.astype(np.float32),\\n                                           sp_to_moving, spacing, samples)\\n        parzen_hist.update_gradient_sparse(\\n            theta, transform, intensities_static,\\n            intensities_moving, samples[..., :dim],\\n            mgrad)\\n        # Get the gradient of the joint distribution w.r.t. the transform\\n        # parameters\\n        actual = np.copy(parzen_hist.joint_grad)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_parzenhist.py.txt'}),\n",
       " Document(page_content='# Compute the gradient using finite differences\\n        n = transform.get_number_of_parameters()\\n        expected = np.empty_like(actual)\\n        for i in range(n):\\n            dtheta = theta.copy()\\n            dtheta[i] += h\\n            # Update the joint distribution with the transformed moving image\\n            M = transform.param_to_matrix(dtheta)\\n            sp_to_moving = np.linalg.inv(moving_g2w).dot(M)\\n            samples_moving_grid = sp_to_moving.dot(samples.T).T\\n            intensities_moving, inside = \\\\\\n                interp_method(moving.astype(np.float32), samples_moving_grid)\\n            intensities_moving = np.array(intensities_moving, dtype=np.float64)\\n            parzen_hist.update_pdfs_sparse(\\n                intensities_static, intensities_moving)\\n            J1 = np.copy(parzen_hist.joint)\\n            expected[..., i] = (J1 - J0) / h\\n\\n        # Dot product and norms of gradients of each joint histogram cell\\n        # i.e. the derivatives of each cell w.r.t. all parameters\\n        P = (expected * actual).sum(2)\\n        enorms = np.sqrt((expected ** 2).sum(2))\\n        anorms = np.sqrt((actual ** 2).sum(2))\\n        prodnorms = enorms * anorms\\n        # Cosine of angle between the expected and actual gradients.\\n        # Exclude very small gradients\\n        P[prodnorms > 1e-6] /= (prodnorms[prodnorms > 1e-6])\\n        P[prodnorms <= 1e-6] = 0\\n        # Verify that a large proportion of the gradients point almost in\\n        # the same direction. Disregard very small gradients\\n        mean_cosine = P[P != 0].mean()\\n        std_cosine = P[P != 0].std()\\n        assert(mean_cosine > 0.98)\\n        assert(std_cosine < 0.16)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_parzenhist.py.txt'}),\n",
       " Document(page_content='def test_sample_domain_regular():\\n    # Test 2D sampling\\n    shape = np.array((10, 10), dtype=np.int32)\\n    affine = np.eye(3)\\n    invalid_affine = np.eye(2)\\n    sigma = 0\\n    dim = len(shape)\\n    n = shape[0] * shape[1]\\n    k = 2\\n    # Verify exception is raised with invalid affine\\n    assert_raises(ValueError, sample_domain_regular, k, shape,\\n                  invalid_affine, sigma)\\n    samples = sample_domain_regular(k, shape, affine, sigma)\\n    isamples = np.array(samples, dtype=np.int32)\\n    indices = (isamples[:, 0] * shape[1] + isamples[:, 1])\\n    # Verify correct number of points sampled\\n    assert_array_equal(samples.shape, [n // k, dim])\\n    # Verify all sampled points are different\\n    assert_equal(len(set(indices)), len(indices))\\n    # Verify the sampling was regular at rate k\\n    assert_equal((indices % k).sum(), 0)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_parzenhist.py.txt'}),\n",
       " Document(page_content='# Test 3D sampling\\n    shape = np.array((5, 10, 10), dtype=np.int32)\\n    affine = np.eye(4)\\n    invalid_affine = np.eye(3)\\n    sigma = 0\\n    dim = len(shape)\\n    n = shape[0] * shape[1] * shape[2]\\n    k = 10\\n    # Verify exception is raised with invalid affine\\n    assert_raises(ValueError, sample_domain_regular, k, shape,\\n                  invalid_affine, sigma)\\n    samples = sample_domain_regular(k, shape, affine, sigma)\\n    isamples = np.array(samples, dtype=np.int32)\\n    indices = (isamples[:, 0] * shape[1] * shape[2] +\\n               isamples[:, 1] * shape[2] +\\n               isamples[:, 2])\\n    # Verify correct number of points sampled\\n    assert_array_equal(samples.shape, [n // k, dim])\\n    # Verify all sampled points are different\\n    assert_equal(len(set(indices)), len(indices))\\n    # Verify the sampling was regular at rate k\\n    assert_equal((indices % k).sum(), 0)\\n\\n\\ndef test_exceptions():\\n    H = ParzenJointHistogram(32)\\n    valid = np.empty((2, 2, 2), dtype=np.float64)\\n    invalid = np.empty((2, 2, 2, 2), dtype=np.float64)\\n\\n    # Test exception from `ParzenJointHistogram.update_pdfs_dense`\\n    assert_raises(ValueError, H.update_pdfs_dense, valid, invalid)\\n    assert_raises(ValueError, H.update_pdfs_dense, invalid, valid)\\n    assert_raises(ValueError, H.update_pdfs_dense, invalid, invalid)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_parzenhist.py.txt'}),\n",
       " Document(page_content=\"def test_exceptions():\\n    H = ParzenJointHistogram(32)\\n    valid = np.empty((2, 2, 2), dtype=np.float64)\\n    invalid = np.empty((2, 2, 2, 2), dtype=np.float64)\\n\\n    # Test exception from `ParzenJointHistogram.update_pdfs_dense`\\n    assert_raises(ValueError, H.update_pdfs_dense, valid, invalid)\\n    assert_raises(ValueError, H.update_pdfs_dense, invalid, valid)\\n    assert_raises(ValueError, H.update_pdfs_dense, invalid, invalid)\\n\\n    # Test exception from `ParzenJointHistogram.update_gradient_dense`\\n    for shape in [(5, 5), (5, 5, 5)]:\\n        dim = len(shape)\\n        grid2world = np.eye(dim + 1)\\n        transform = regtransforms[('ROTATION', dim)]\\n        theta = transform.get_identity_parameters()\\n        valid_img = np.empty(shape, dtype=np.float64)\\n        valid_grad = np.empty(shape + (dim,), dtype=np.float64)\\n\\n        invalid_img = np.empty((2, 2, 2, 2), dtype=np.float64)\\n        invalid_grad_type = np.empty_like(valid_grad, dtype=np.int32)\\n        invalid_grad_dim = np.empty(shape + (dim + 1,), dtype=np.float64)\\n\\n        for s, m, g in [(valid_img, valid_img, invalid_grad_type),\\n                        (valid_img, valid_img, invalid_grad_dim),\\n                        (invalid_img, valid_img, valid_grad),\\n                        (invalid_img, invalid_img, invalid_grad_type),\\n                        (invalid_img, invalid_img, invalid_grad_dim)]:\\n            assert_raises(ValueError, H.update_gradient_dense,\\n                          theta, transform, s, m, grid2world, g)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_parzenhist.py.txt'}),\n",
       " Document(page_content=\"for s, m, g in [(valid_img, valid_img, invalid_grad_type),\\n                        (valid_img, valid_img, invalid_grad_dim),\\n                        (invalid_img, valid_img, valid_grad),\\n                        (invalid_img, invalid_img, invalid_grad_type),\\n                        (invalid_img, invalid_img, invalid_grad_dim)]:\\n            assert_raises(ValueError, H.update_gradient_dense,\\n                          theta, transform, s, m, grid2world, g)\\n\\n    # Test exception from `ParzenJointHistogram.update_gradient_dense`\\n    nsamples = 2\\n    for dim in [2, 3]:\\n        transform = regtransforms[('ROTATION', dim)]\\n        theta = transform.get_identity_parameters()\\n        valid_vals = np.empty((nsamples,), dtype=np.float64)\\n        valid_grad = np.empty((nsamples, dim), dtype=np.float64)\\n        valid_points = np.empty((nsamples, dim), dtype=np.float64)\\n\\n        invalid_grad_type = np.empty((nsamples, dim), dtype=np.int32)\\n        invalid_grad_dim = np.empty((nsamples, dim + 2), dtype=np.float64)\\n        invalid_grad_len = np.empty((nsamples + 1, dim), dtype=np.float64)\\n        invalid_vals = np.empty((nsamples + 1), dtype=np.float64)\\n        invalid_points_dim = np.empty((nsamples, dim + 2), dtype=np.float64)\\n        invalid_points_len = np.empty((nsamples + 1, dim), dtype=np.float64)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_parzenhist.py.txt'}),\n",
       " Document(page_content='invalid_grad_type = np.empty((nsamples, dim), dtype=np.int32)\\n        invalid_grad_dim = np.empty((nsamples, dim + 2), dtype=np.float64)\\n        invalid_grad_len = np.empty((nsamples + 1, dim), dtype=np.float64)\\n        invalid_vals = np.empty((nsamples + 1), dtype=np.float64)\\n        invalid_points_dim = np.empty((nsamples, dim + 2), dtype=np.float64)\\n        invalid_points_len = np.empty((nsamples + 1, dim), dtype=np.float64)\\n\\n        C = [(invalid_vals, valid_vals, valid_points, valid_grad),\\n             (valid_vals, invalid_vals, valid_points, valid_grad),\\n             (valid_vals, valid_vals, invalid_points_dim, valid_grad),\\n             (valid_vals, valid_vals, invalid_points_dim, invalid_grad_dim),\\n             (valid_vals, valid_vals, invalid_points_len, valid_grad),\\n             (valid_vals, valid_vals, valid_points, invalid_grad_type),\\n             (valid_vals, valid_vals, valid_points, invalid_grad_dim),\\n             (valid_vals, valid_vals, valid_points, invalid_grad_len)]\\n\\n        for s, m, p, g in C:\\n            assert_raises(ValueError, H.update_gradient_sparse,\\n                          theta, transform, s, m, p, g)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_parzenhist.py.txt'}),\n",
       " Document(page_content='import numpy as np\\nimport nibabel as nib\\nfrom numpy.testing import (assert_,\\n                           assert_equal,\\n                           assert_almost_equal,\\n                           assert_raises)\\nfrom dipy.io.image import load_nifti\\nfrom dipy.data import get_fnames\\nfrom dipy.align.reslice import reslice\\nfrom dipy.denoise.noise_estimate import estimate_sigma\\n\\n\\ndef test_resample():\\n    fimg, _, _ = get_fnames(\"small_25\")\\n    data, affine, zooms = load_nifti(fimg, return_voxsize=True)\\n\\n    # test that new zooms are correctly from the affine (check with 3D volume)\\n    new_zooms = (1, 1.2, 2.1)\\n    data2, affine2 = reslice(data[..., 0], affine, zooms, new_zooms, order=1,\\n                             mode=\\'constant\\')\\n    img2 = nib.Nifti1Image(data2, affine2)\\n    new_zooms_confirmed = img2.header.get_zooms()[:3]\\n    assert_almost_equal(new_zooms, new_zooms_confirmed)\\n\\n    # test that shape changes correctly for the first 3 dimensions (check 4D)\\n    new_zooms = (1, 1, 1.)\\n    data2, affine2 = reslice(data, affine, zooms, new_zooms, order=0,\\n                             mode=\\'reflect\\')\\n    assert_equal(2 * np.array(data.shape[:3]), data2.shape[:3])\\n    assert_equal(data2.shape[-1], data.shape[-1])\\n\\n    # same with different interpolation order\\n    new_zooms = (1, 1, 1.)\\n    data3, affine2 = reslice(data, affine, zooms, new_zooms, order=5,\\n                             mode=\\'reflect\\')\\n    assert_equal(2 * np.array(data.shape[:3]), data3.shape[:3])\\n    assert_equal(data3.shape[-1], data.shape[-1])\\n\\n    # test that the sigma will be reduced with interpolation\\n    sigmas = estimate_sigma(data)\\n    sigmas2 = estimate_sigma(data2)\\n    sigmas3 = estimate_sigma(data3)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_reslice.py.txt'}),\n",
       " Document(page_content=\"# same with different interpolation order\\n    new_zooms = (1, 1, 1.)\\n    data3, affine2 = reslice(data, affine, zooms, new_zooms, order=5,\\n                             mode='reflect')\\n    assert_equal(2 * np.array(data.shape[:3]), data3.shape[:3])\\n    assert_equal(data3.shape[-1], data.shape[-1])\\n\\n    # test that the sigma will be reduced with interpolation\\n    sigmas = estimate_sigma(data)\\n    sigmas2 = estimate_sigma(data2)\\n    sigmas3 = estimate_sigma(data3)\\n\\n    assert_(np.all(sigmas > sigmas2))\\n    assert_(np.all(sigmas2 > sigmas3))\\n\\n    # check that 4D resampling matches 3D resampling\\n    data2, affine2 = reslice(data, affine, zooms, new_zooms)\\n    for i in range(data.shape[-1]):\\n        _data, _affine = reslice(data[..., i], affine, zooms, new_zooms)\\n        assert_almost_equal(data2[..., i], _data)\\n        assert_almost_equal(affine2, _affine)\\n\\n    # check use of multiprocessing pool of specified size\\n    data3, affine3 = reslice(data, affine, zooms, new_zooms, num_processes=4)\\n    assert_almost_equal(data2, data3)\\n    assert_almost_equal(affine2, affine3)\\n\\n    # check use of multiprocessing pool of autoconfigured size\\n    data3, affine3 = reslice(data, affine, zooms, new_zooms, num_processes=-1)\\n    assert_almost_equal(data2, data3)\\n    assert_almost_equal(affine2, affine3)\\n\\n    # test invalid values of num_threads\\n    assert_raises(ValueError, reslice, data, affine, zooms, new_zooms,\\n                  num_processes=0)\\n\\n    # test invalid volume dimension\\n    assert_raises(ValueError, reslice, np.zeros((4, 4, 4, 4, 1)), affine,\\n                  zooms, new_zooms)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_reslice.py.txt'}),\n",
       " Document(page_content='import numpy as np\\nimport scipy as sp\\nfrom numpy.testing import (assert_array_equal,\\n                           assert_array_almost_equal,\\n                           assert_raises)\\nfrom dipy.align import floating\\nfrom dipy.align.imwarp import get_direction_and_spacings\\nfrom dipy.align.scalespace import ScaleSpace, IsotropicScaleSpace\\nfrom dipy.align.tests.test_imwarp import get_synthetic_warped_circle\\nfrom dipy.testing.decorators import set_random_number_generator\\n\\n\\ndef test_scale_space():\\n    num_levels = 3\\n    for test_class in [ScaleSpace, IsotropicScaleSpace]:\\n        for dim in [2, 3]:\\n            print(dim, test_class)\\n            if dim == 2:\\n                moving, static = get_synthetic_warped_circle(1)\\n            else:\\n                moving, static = get_synthetic_warped_circle(30)\\n            input_spacing = np.array([1.1, 1.2, 1.5])[:dim]\\n            grid2world = np.diag(tuple(input_spacing) + (1.0,))', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_scalespace.py.txt'}),\n",
       " Document(page_content='def test_scale_space():\\n    num_levels = 3\\n    for test_class in [ScaleSpace, IsotropicScaleSpace]:\\n        for dim in [2, 3]:\\n            print(dim, test_class)\\n            if dim == 2:\\n                moving, static = get_synthetic_warped_circle(1)\\n            else:\\n                moving, static = get_synthetic_warped_circle(30)\\n            input_spacing = np.array([1.1, 1.2, 1.5])[:dim]\\n            grid2world = np.diag(tuple(input_spacing) + (1.0,))\\n\\n            original = moving\\n            if test_class is ScaleSpace:\\n                ss = test_class(\\n                    original,\\n                    num_levels,\\n                    grid2world,\\n                    input_spacing)\\n            elif test_class is IsotropicScaleSpace:\\n                factors = [4, 2, 1]\\n                sigmas = [3.0, 1.0, 0.0]\\n                ss = test_class(\\n                    original,\\n                    factors,\\n                    sigmas,\\n                    grid2world,\\n                    input_spacing)\\n            for level in range(num_levels):\\n                # Verify sigmas and images are consistent\\n                sigmas = ss.get_sigmas(level)\\n                expected = sp.ndimage.gaussian_filter(original, sigmas)\\n                expected = ((expected - expected.min()) /\\n                            (expected.max() - expected.min()))\\n                actual = ss.get_image(level)\\n                assert_array_almost_equal(actual, expected)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_scalespace.py.txt'}),\n",
       " Document(page_content='# Verify scalings and spacings are consistent\\n                spacings = ss.get_spacing(level)\\n                scalings = ss.get_scaling(level)\\n                expected = ss.get_spacing(0) * scalings\\n                actual = ss.get_spacing(level)\\n                assert_array_almost_equal(actual, expected)\\n\\n                # Verify affine and affine_inv are consistent\\n                affine = ss.get_affine(level)\\n                affine_inv = ss.get_affine_inv(level)\\n                expected = np.eye(1 + dim)\\n                actual = affine.dot(affine_inv)\\n                assert_array_almost_equal(actual, expected)\\n\\n                # Verify affine consistent with spacings\\n                exp_dir, expected_sp = get_direction_and_spacings(affine, dim)\\n                actual_sp = spacings\\n                assert_array_almost_equal(actual_sp, expected_sp)\\n\\n\\n@set_random_number_generator(2022966)\\ndef test_scale_space_exceptions(rng):\\n    target_shape = (32, 32)\\n    # create a random image\\n    image = np.ndarray(target_shape, dtype=floating)\\n    ns = np.size(image)\\n    image[...] = rng.integers(0, 10, ns).reshape(tuple(target_shape))\\n    zeros = (image == 0).astype(np.int32)\\n\\n    ss = ScaleSpace(image, 3)\\n    for invalid_level in [-1, 3, 4]:\\n        assert_raises(ValueError, ss.get_image, invalid_level)\\n\\n    # Verify that the mask is correctly applied, when requested\\n    ss = ScaleSpace(image, 3, mask0=True)\\n    for level in range(3):\\n        img = ss.get_image(level)\\n        z = (img == 0).astype(np.int32)\\n        assert_array_equal(zeros, z)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_scalespace.py.txt'}),\n",
       " Document(page_content='import numpy as np\\nfrom numpy.testing import (assert_,\\n                           assert_equal,\\n                           assert_almost_equal,\\n                           assert_array_almost_equal,\\n                           assert_raises)\\nfrom dipy.align.streamlinear import (compose_matrix44,\\n                                     decompose_matrix44,\\n                                     BundleSumDistanceMatrixMetric,\\n                                     BundleMinDistanceMatrixMetric,\\n                                     BundleMinDistanceMetric,\\n                                     StreamlineLinearRegistration,\\n                                     StreamlineDistanceMetric,\\n                                     groupwise_slr,\\n                                     get_unique_pairs)\\n\\nfrom dipy.tracking.streamline import (center_streamlines,\\n                                      unlist_streamlines,\\n                                      relist_streamlines,\\n                                      transform_streamlines,\\n                                      set_number_of_points,\\n                                      Streamlines)\\nfrom dipy.io.streamline import load_tractogram\\nfrom dipy.core.geometry import compose_matrix\\n\\nfrom dipy.data import get_fnames, two_cingulum_bundles, read_five_af_bundles\\nfrom dipy.align.bundlemin import (_bundle_minimum_distance_matrix,\\n                                  _bundle_minimum_distance,\\n                                  distance_matrix_mdf)\\nfrom dipy.testing.decorators import set_random_number_generator', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_streamlinear.py.txt'}),\n",
       " Document(page_content=\"from dipy.data import get_fnames, two_cingulum_bundles, read_five_af_bundles\\nfrom dipy.align.bundlemin import (_bundle_minimum_distance_matrix,\\n                                  _bundle_minimum_distance,\\n                                  distance_matrix_mdf)\\nfrom dipy.testing.decorators import set_random_number_generator\\n\\n\\ndef simulated_bundle(no_streamlines=10, waves=False, no_pts=12):\\n    t = np.linspace(-10, 10, 200)\\n    # parallel waves or parallel lines\\n    bundle = []\\n    for i in np.linspace(-5, 5, no_streamlines):\\n        if waves:\\n            pts = np.vstack((np.cos(t), t, i * np.ones(t.shape))).T\\n        else:\\n            pts = np.vstack((np.zeros(t.shape), t, i * np.ones(t.shape))).T\\n        pts = set_number_of_points(pts, no_pts)\\n        bundle.append(pts)\\n\\n    return bundle\\n\\n\\ndef fornix_streamlines(no_pts=12):\\n    fname = get_fnames('fornix')\\n\\n    fornix = load_tractogram(fname, 'same',\\n                             bbox_valid_check=False).streamlines\\n\\n    fornix_streamlines = Streamlines(fornix)\\n    streamlines = set_number_of_points(fornix_streamlines, no_pts)\\n    return streamlines\\n\\n\\ndef evaluate_convergence(bundle, new_bundle2):\\n    pts_static = np.concatenate(bundle, axis=0)\\n    pts_moved = np.concatenate(new_bundle2, axis=0)\\n    assert_array_almost_equal(pts_static, pts_moved, 3)\\n\\n\\ndef test_rigid_parallel_lines():\\n\\n    bundle_initial = simulated_bundle()\\n    bundle, shift = center_streamlines(bundle_initial)\\n    mat = compose_matrix44([20, 0, 10, 0, 40, 0])\\n\\n    bundle2 = transform_streamlines(bundle, mat)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_streamlinear.py.txt'}),\n",
       " Document(page_content=\"def evaluate_convergence(bundle, new_bundle2):\\n    pts_static = np.concatenate(bundle, axis=0)\\n    pts_moved = np.concatenate(new_bundle2, axis=0)\\n    assert_array_almost_equal(pts_static, pts_moved, 3)\\n\\n\\ndef test_rigid_parallel_lines():\\n\\n    bundle_initial = simulated_bundle()\\n    bundle, shift = center_streamlines(bundle_initial)\\n    mat = compose_matrix44([20, 0, 10, 0, 40, 0])\\n\\n    bundle2 = transform_streamlines(bundle, mat)\\n\\n    bundle_sum_distance = BundleSumDistanceMatrixMetric()\\n    options = {'maxcor': 100, 'ftol': 1e-9, 'gtol': 1e-16, 'eps': 1e-3}\\n    srr = StreamlineLinearRegistration(metric=bundle_sum_distance,\\n                                       x0=np.zeros(6),\\n                                       method='L-BFGS-B',\\n                                       bounds=None,\\n                                       options=options)\\n\\n    new_bundle2 = srr.optimize(bundle, bundle2).transform(bundle2)\\n    evaluate_convergence(bundle, new_bundle2)\\n\\n\\ndef test_rigid_real_bundles():\\n\\n    bundle_initial = fornix_streamlines()[:20]\\n    bundle, shift = center_streamlines(bundle_initial)\\n\\n    mat = compose_matrix44([0, 0, 20, 45., 0, 0])\\n\\n    bundle2 = transform_streamlines(bundle, mat)\\n\\n    bundle_sum_distance = BundleSumDistanceMatrixMetric()\\n    srr = StreamlineLinearRegistration(bundle_sum_distance,\\n                                       x0=np.zeros(6),\\n                                       method='Powell')\\n    new_bundle2 = srr.optimize(bundle, bundle2).transform(bundle2)\\n\\n    evaluate_convergence(bundle, new_bundle2)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_streamlinear.py.txt'}),\n",
       " Document(page_content=\"mat = compose_matrix44([0, 0, 20, 45., 0, 0])\\n\\n    bundle2 = transform_streamlines(bundle, mat)\\n\\n    bundle_sum_distance = BundleSumDistanceMatrixMetric()\\n    srr = StreamlineLinearRegistration(bundle_sum_distance,\\n                                       x0=np.zeros(6),\\n                                       method='Powell')\\n    new_bundle2 = srr.optimize(bundle, bundle2).transform(bundle2)\\n\\n    evaluate_convergence(bundle, new_bundle2)\\n\\n    bundle_min_distance = BundleMinDistanceMatrixMetric()\\n    srr = StreamlineLinearRegistration(bundle_min_distance,\\n                                       x0=np.zeros(6),\\n                                       method='Powell')\\n    new_bundle2 = srr.optimize(bundle, bundle2).transform(bundle2)\\n\\n    evaluate_convergence(bundle, new_bundle2)\\n\\n    assert_raises(ValueError, StreamlineLinearRegistration, method='Whatever')\\n\\n\\ndef test_rigid_partial_real_bundles():\\n\\n    static = fornix_streamlines()[:20]\\n    moving = fornix_streamlines()[20:40]\\n    static_center, shift = center_streamlines(static)\\n    moving_center, shift2 = center_streamlines(moving)\\n\\n    print(shift2)\\n    mat = compose_matrix(translate=np.array([0, 0, 0.]),\\n                         angles=np.deg2rad([40, 0, 0.]))\\n    moved = transform_streamlines(moving_center, mat)\\n\\n    srr = StreamlineLinearRegistration()\\n\\n    srm = srr.optimize(static_center, moved)\\n    print(srm.fopt)\\n    print(srm.iterations)\\n    print(srm.funcs)\\n\\n    moving_back = srm.transform(moved)\\n    print(srm.matrix)\\n\\n    static_center = set_number_of_points(static_center, 100)\\n    moving_center = set_number_of_points(moving_back, 100)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_streamlinear.py.txt'}),\n",
       " Document(page_content='srr = StreamlineLinearRegistration()\\n\\n    srm = srr.optimize(static_center, moved)\\n    print(srm.fopt)\\n    print(srm.iterations)\\n    print(srm.funcs)\\n\\n    moving_back = srm.transform(moved)\\n    print(srm.matrix)\\n\\n    static_center = set_number_of_points(static_center, 100)\\n    moving_center = set_number_of_points(moving_back, 100)\\n\\n    vol = np.zeros((100, 100, 100))\\n    spts = np.concatenate(static_center, axis=0)\\n    spts = np.round(spts).astype(int) + np.array([50, 50, 50])\\n\\n    mpts = np.concatenate(moving_center, axis=0)\\n    mpts = np.round(mpts).astype(int) + np.array([50, 50, 50])\\n\\n    for index in spts:\\n        i, j, k = index\\n        vol[i, j, k] = 1\\n\\n    vol2 = np.zeros((100, 100, 100))\\n    for index in mpts:\\n        i, j, k = index\\n        vol2[i, j, k] = 1\\n\\n    overlap = np.sum(np.logical_and(vol, vol2)) / float(np.sum(vol2))\\n\\n    assert_equal(overlap * 100 > 40, True)\\n\\n\\ndef test_stream_rigid():\\n\\n    static = fornix_streamlines()[:20]\\n    moving = fornix_streamlines()[20:40]\\n    center_streamlines(static)\\n\\n    mat = compose_matrix44([0, 0, 0, 0, 40, 0])\\n    moving = transform_streamlines(moving, mat)\\n\\n    srr = StreamlineLinearRegistration()\\n    sr_params = srr.optimize(static, moving)\\n    moved = transform_streamlines(moving, sr_params.matrix)\\n\\n    srr = StreamlineLinearRegistration(verbose=True)\\n    srm = srr.optimize(static, moving)\\n    moved2 = transform_streamlines(moving, srm.matrix)\\n    moved3 = srm.transform(moving)\\n\\n    assert_array_almost_equal(moved[0], moved2[0], decimal=3)\\n    assert_array_almost_equal(moved2[0], moved3[0], decimal=3)\\n\\n\\ndef test_min_vs_min_fast_precision():\\n\\n    static = fornix_streamlines()[:20]\\n    moving = fornix_streamlines()[:20]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_streamlinear.py.txt'}),\n",
       " Document(page_content=\"srr = StreamlineLinearRegistration(verbose=True)\\n    srm = srr.optimize(static, moving)\\n    moved2 = transform_streamlines(moving, srm.matrix)\\n    moved3 = srm.transform(moving)\\n\\n    assert_array_almost_equal(moved[0], moved2[0], decimal=3)\\n    assert_array_almost_equal(moved2[0], moved3[0], decimal=3)\\n\\n\\ndef test_min_vs_min_fast_precision():\\n\\n    static = fornix_streamlines()[:20]\\n    moving = fornix_streamlines()[:20]\\n\\n    static = [s.astype('f8') for s in static]\\n    moving = [m.astype('f8') for m in moving]\\n\\n    bmd = BundleMinDistanceMatrixMetric()\\n    bmd.setup(static, moving)\\n\\n    bmdf = BundleMinDistanceMetric()\\n    bmdf.setup(static, moving)\\n\\n    x_test = [0.01, 0, 0, 0, 0, 0]\\n\\n    print(bmd.distance(x_test))\\n    print(bmdf.distance(x_test))\\n    assert_equal(bmd.distance(x_test), bmdf.distance(x_test))\\n\\n\\n@set_random_number_generator()\\ndef test_same_number_of_points(rng):\\n    A = [rng.random((10, 3)), rng.random((20, 3))]\\n    B = [rng.random((21, 3)), rng.random((30, 3))]\\n    C = [rng.random((10, 3)), rng.random((10, 3))]\\n    D = [rng.random((20, 3)), rng.random((20, 3))]\\n\\n    slr = StreamlineLinearRegistration()\\n    assert_raises(ValueError, slr.optimize, A, B)\\n    assert_raises(ValueError, slr.optimize, C, D)\\n    assert_raises(ValueError, slr.optimize, C, B)\\n\\n\\ndef test_efficient_bmd():\\n\\n    a = np.array([[1, 1, 1],\\n                  [2, 2, 2],\\n                  [3, 3, 3]])\\n\\n    streamlines = [a, a + 2, a + 4]\\n\\n    points, offsets = unlist_streamlines(streamlines)\\n    points = points.astype(np.double)\\n    points2 = points.copy()\\n\\n    D = np.zeros((len(offsets), len(offsets)), dtype='f8')\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_streamlinear.py.txt'}),\n",
       " Document(page_content=\"def test_efficient_bmd():\\n\\n    a = np.array([[1, 1, 1],\\n                  [2, 2, 2],\\n                  [3, 3, 3]])\\n\\n    streamlines = [a, a + 2, a + 4]\\n\\n    points, offsets = unlist_streamlines(streamlines)\\n    points = points.astype(np.double)\\n    points2 = points.copy()\\n\\n    D = np.zeros((len(offsets), len(offsets)), dtype='f8')\\n\\n    _bundle_minimum_distance_matrix(points, points2,\\n                                    len(offsets), len(offsets),\\n                                    a.shape[0], D)\\n\\n    assert_equal(np.sum(np.diag(D)), 0)\\n\\n    points2 += 2\\n\\n    _bundle_minimum_distance_matrix(points, points2,\\n                                    len(offsets), len(offsets),\\n                                    a.shape[0], D)\\n\\n    streamlines2 = relist_streamlines(points2, offsets)\\n    D2 = distance_matrix_mdf(streamlines, streamlines2)\\n\\n    assert_array_almost_equal(D, D2)\\n\\n    cols = D2.shape[1]\\n    rows = D2.shape[0]\\n\\n    dist = 0.25 * (np.sum(np.min(D2, axis=0)) / float(cols) +\\n                   np.sum(np.min(D2, axis=1)) / float(rows)) ** 2\\n\\n    dist2 = _bundle_minimum_distance(points, points2,\\n                                     len(offsets), len(offsets),\\n                                     a.shape[0])\\n    assert_almost_equal(dist, dist2)\\n\\n\\n@set_random_number_generator()\\ndef test_openmp_locks(rng):\\n    static = []\\n    moving = []\\n    pts = 20\\n\\n    for i in range(1000):\\n        s = rng.random((pts, 3))\\n        static.append(s)\\n        moving.append(s + 2)\\n\\n    moving = moving[2:]\\n\\n    points, offsets = unlist_streamlines(static)\\n    points2, offsets2 = unlist_streamlines(moving)\\n\\n    D = np.zeros((len(offsets), len(offsets2)), dtype='f8')\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_streamlinear.py.txt'}),\n",
       " Document(page_content=\"@set_random_number_generator()\\ndef test_openmp_locks(rng):\\n    static = []\\n    moving = []\\n    pts = 20\\n\\n    for i in range(1000):\\n        s = rng.random((pts, 3))\\n        static.append(s)\\n        moving.append(s + 2)\\n\\n    moving = moving[2:]\\n\\n    points, offsets = unlist_streamlines(static)\\n    points2, offsets2 = unlist_streamlines(moving)\\n\\n    D = np.zeros((len(offsets), len(offsets2)), dtype='f8')\\n\\n    _bundle_minimum_distance_matrix(points, points2,\\n                                    len(offsets), len(offsets2),\\n                                    pts, D)\\n\\n    dist1 = 0.25 * (np.sum(np.min(D, axis=0)) / float(D.shape[1]) +\\n                    np.sum(np.min(D, axis=1)) / float(D.shape[0])) ** 2\\n\\n    dist2 = _bundle_minimum_distance(points, points2,\\n                                     len(offsets), len(offsets2),\\n                                     pts)\\n\\n    assert_almost_equal(dist1, dist2, 6)\\n\\n\\ndef test_from_to_rigid():\\n\\n    t = np.array([10, 2, 3, 0.1, 20., 30.])\\n    mat = compose_matrix44(t)\\n    vec = decompose_matrix44(mat, 6)\\n\\n    assert_array_almost_equal(t, vec)\\n\\n    t = np.array([0, 0, 0, 180, 0., 0.])\\n\\n    mat = np.eye(4)\\n    mat[0, 0] = -1\\n\\n    vec = decompose_matrix44(mat, 6)\\n\\n    assert_array_almost_equal(-t, vec)\\n\\n\\ndef test_matrix44():\\n\\n    assert_raises(ValueError, compose_matrix44, np.ones(5))\\n    assert_raises(ValueError, compose_matrix44, np.ones(13))\\n    assert_raises(ValueError, compose_matrix44, np.ones(16))\\n\\n\\ndef test_abstract_metric_class():\\n\\n    class DummyStreamlineMetric(StreamlineDistanceMetric):\\n        def test(self):\\n            pass\\n    assert_raises(TypeError, DummyStreamlineMetric)\\n\\n\\ndef test_evolution_of_previous_iterations():\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_streamlinear.py.txt'}),\n",
       " Document(page_content=\"assert_array_almost_equal(-t, vec)\\n\\n\\ndef test_matrix44():\\n\\n    assert_raises(ValueError, compose_matrix44, np.ones(5))\\n    assert_raises(ValueError, compose_matrix44, np.ones(13))\\n    assert_raises(ValueError, compose_matrix44, np.ones(16))\\n\\n\\ndef test_abstract_metric_class():\\n\\n    class DummyStreamlineMetric(StreamlineDistanceMetric):\\n        def test(self):\\n            pass\\n    assert_raises(TypeError, DummyStreamlineMetric)\\n\\n\\ndef test_evolution_of_previous_iterations():\\n\\n    static = fornix_streamlines()[:20]\\n    moving = fornix_streamlines()[:20]\\n\\n    moving = [m + np.array([10., 0., 0.]) for m in moving]\\n\\n    slr = StreamlineLinearRegistration(evolution=True)\\n\\n    slm = slr.optimize(static, moving)\\n\\n    assert_equal(len(slm.matrix_history), slm.iterations)\\n\\n\\ndef test_similarity_real_bundles():\\n\\n    bundle_initial = fornix_streamlines()\\n    bundle_initial, shift = center_streamlines(bundle_initial)\\n    bundle = bundle_initial[:20]\\n    xgold = [0, 0, 10, 0, 0, 0, 1.5]\\n    mat = compose_matrix44(xgold)\\n    bundle2 = transform_streamlines(bundle_initial[:20], mat)\\n\\n    metric = BundleMinDistanceMatrixMetric()\\n    x0 = np.array([0, 0, 0, 0, 0, 0, 1], 'f8')\\n\\n    slr = StreamlineLinearRegistration(metric=metric,\\n                                       x0=x0,\\n                                       method='Powell',\\n                                       bounds=None,\\n                                       verbose=False)\\n\\n    slm = slr.optimize(bundle, bundle2)\\n    new_bundle2 = slm.transform(bundle2)\\n    evaluate_convergence(bundle, new_bundle2)\\n\\n\\ndef test_affine_real_bundles():\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_streamlinear.py.txt'}),\n",
       " Document(page_content=\"slr = StreamlineLinearRegistration(metric=metric,\\n                                       x0=x0,\\n                                       method='Powell',\\n                                       bounds=None,\\n                                       verbose=False)\\n\\n    slm = slr.optimize(bundle, bundle2)\\n    new_bundle2 = slm.transform(bundle2)\\n    evaluate_convergence(bundle, new_bundle2)\\n\\n\\ndef test_affine_real_bundles():\\n\\n    bundle_initial = fornix_streamlines()\\n    bundle_initial, shift = center_streamlines(bundle_initial)\\n    bundle = bundle_initial[:20]\\n    xgold = [0, 4, 2, 0, 10, 10, 1.2, 1.1, 1., 0., 0.2, 0.]\\n    mat = compose_matrix44(xgold)\\n    bundle2 = transform_streamlines(bundle_initial[:20], mat)\\n\\n    x0 = np.array([0, 0, 0, 0, 0, 0, 1., 1., 1., 0, 0, 0])\\n\\n    x = 25\\n\\n    bounds = [(-x, x), (-x, x), (-x, x),\\n              (-x, x), (-x, x), (-x, x),\\n              (0.1, 1.5), (0.1, 1.5), (0.1, 1.5),\\n              (-1, 1), (-1, 1), (-1, 1)]\\n\\n    options = {'maxcor': 10, 'ftol': 1e-7, 'gtol': 1e-5, 'eps': 1e-8}\\n\\n    metric = BundleMinDistanceMatrixMetric()\\n\\n    slr = StreamlineLinearRegistration(metric=metric,\\n                                       x0=x0,\\n                                       method='L-BFGS-B',\\n                                       bounds=bounds,\\n                                       verbose=True,\\n                                       options=options)\\n    slm = slr.optimize(bundle, bundle2)\\n\\n    new_bundle2 = slm.transform(bundle2)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_streamlinear.py.txt'}),\n",
       " Document(page_content='metric = BundleMinDistanceMatrixMetric()\\n\\n    slr = StreamlineLinearRegistration(metric=metric,\\n                                       x0=x0,\\n                                       method=\\'L-BFGS-B\\',\\n                                       bounds=bounds,\\n                                       verbose=True,\\n                                       options=options)\\n    slm = slr.optimize(bundle, bundle2)\\n\\n    new_bundle2 = slm.transform(bundle2)\\n\\n    slr2 = StreamlineLinearRegistration(metric=metric,\\n                                        x0=x0,\\n                                        method=\\'Powell\\',\\n                                        bounds=None,\\n                                        verbose=True,\\n                                        options=None)\\n\\n    slm2 = slr2.optimize(bundle, new_bundle2)\\n\\n    new_bundle2 = slm2.transform(new_bundle2)\\n\\n    evaluate_convergence(bundle, new_bundle2)\\n\\n\\ndef test_vectorize_streamlines():\\n\\n    cingulum_bundles = two_cingulum_bundles()\\n\\n    cb_subj1 = cingulum_bundles[0]\\n    cb_subj1 = set_number_of_points(cb_subj1, 10)\\n    cb_subj1_pts_no = np.array([s.shape[0] for s in cb_subj1])\\n\\n    assert_equal(np.all(cb_subj1_pts_no == 10), True)\\n\\n\\n@set_random_number_generator()\\ndef test_x0_input(rng):\\n    for x0 in [6, 7, 12, \"Rigid\", \\'rigid\\', \"similarity\", \"Affine\"]:\\n        StreamlineLinearRegistration(x0=x0)\\n\\n    for x0 in [rng.random(6), rng.random(7), rng.random(12)]:\\n        StreamlineLinearRegistration(x0=x0)\\n\\n    for x0 in [8, 20, \"Whatever\", rng.random(20), rng.random((20, 3))]:\\n        assert_raises(ValueError, StreamlineLinearRegistration, x0=x0)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_streamlinear.py.txt'}),\n",
       " Document(page_content='assert_equal(np.all(cb_subj1_pts_no == 10), True)\\n\\n\\n@set_random_number_generator()\\ndef test_x0_input(rng):\\n    for x0 in [6, 7, 12, \"Rigid\", \\'rigid\\', \"similarity\", \"Affine\"]:\\n        StreamlineLinearRegistration(x0=x0)\\n\\n    for x0 in [rng.random(6), rng.random(7), rng.random(12)]:\\n        StreamlineLinearRegistration(x0=x0)\\n\\n    for x0 in [8, 20, \"Whatever\", rng.random(20), rng.random((20, 3))]:\\n        assert_raises(ValueError, StreamlineLinearRegistration, x0=x0)\\n\\n    x0 = rng.random((4, 3))\\n    assert_raises(ValueError, StreamlineLinearRegistration, x0=x0)\\n\\n    x0_6 = np.zeros(6)\\n    x0_7 = np.array([0, 0, 0, 0, 0, 0, 1.])\\n    x0_12 = np.array([0, 0, 0, 0, 0, 0, 1., 1., 1., 0, 0, 0])\\n\\n    x0_s = [x0_6, x0_7, x0_12, x0_6, x0_7, x0_12]\\n\\n    for i, x0 in enumerate([6, 7, 12, \"Rigid\", \"similarity\", \"Affine\"]):\\n        slr = StreamlineLinearRegistration(x0=x0)\\n        assert_equal(slr.x0, x0_s[i])\\n\\n\\n@set_random_number_generator()\\ndef test_compose_decompose_matrix44(rng):\\n    for i in range(20):\\n        x0 = rng.random(12)\\n        mat = compose_matrix44(x0[:6])\\n        assert_array_almost_equal(x0[:6], decompose_matrix44(mat, size=6))\\n        mat = compose_matrix44(x0[:7])\\n        assert_array_almost_equal(x0[:7], decompose_matrix44(mat, size=7))\\n        mat = compose_matrix44(x0[:12])\\n        assert_array_almost_equal(x0[:12], decompose_matrix44(mat, size=12))\\n\\n    assert_raises(ValueError, decompose_matrix44, mat, 20)\\n\\n\\ndef test_cascade_of_optimizations_and_threading():\\n\\n    cingulum_bundles = two_cingulum_bundles()\\n\\n    cb1 = cingulum_bundles[0]\\n    cb1 = set_number_of_points(cb1, 20)\\n\\n    test_x0 = np.array([10, 4, 3, 0, 20, 10, 1.5, 1.5, 1.5, 0., 0.2, 0])', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_streamlinear.py.txt'}),\n",
       " Document(page_content=\"assert_raises(ValueError, decompose_matrix44, mat, 20)\\n\\n\\ndef test_cascade_of_optimizations_and_threading():\\n\\n    cingulum_bundles = two_cingulum_bundles()\\n\\n    cb1 = cingulum_bundles[0]\\n    cb1 = set_number_of_points(cb1, 20)\\n\\n    test_x0 = np.array([10, 4, 3, 0, 20, 10, 1.5, 1.5, 1.5, 0., 0.2, 0])\\n\\n    cb2 = transform_streamlines(cingulum_bundles[0],\\n                                compose_matrix44(test_x0))\\n    cb2 = set_number_of_points(cb2, 20)\\n\\n    print('first rigid')\\n    slr = StreamlineLinearRegistration(x0=6, num_threads=1)\\n    slm = slr.optimize(cb1, cb2)\\n\\n    print('then similarity')\\n    slr2 = StreamlineLinearRegistration(x0=7, num_threads=2)\\n    slm2 = slr2.optimize(cb1, cb2, slm.matrix)\\n\\n    print('then affine')\\n    slr3 = StreamlineLinearRegistration(x0=12, options={'maxiter': 50},\\n                                        num_threads=None)\\n    slm3 = slr3.optimize(cb1, cb2, slm2.matrix)\\n\\n    assert_(slm2.fopt < slm.fopt)\\n    assert_(slm3.fopt < slm2.fopt)\\n\\n\\n@set_random_number_generator()\\ndef test_wrong_num_threads(rng):\\n    A = [rng.random((10, 3)), rng.random((10, 3))]\\n    B = [rng.random((10, 3)), rng.random((10, 3))]\\n\\n    slr = StreamlineLinearRegistration(num_threads=0)\\n    assert_raises(ValueError, slr.optimize, A, B)\\n\\n\\ndef test_get_unique_pairs():\\n\\n    # Regular case\\n    pairs, exclude = get_unique_pairs(6)\\n    assert_equal(len(np.unique(pairs)), 6)\\n    assert_equal(exclude, None)\\n\\n    # Odd case\\n    pairs, exclude = get_unique_pairs(5)\\n    assert_equal(len(np.unique(pairs)), 4)\\n    assert_equal(isinstance(exclude, (int, np.int64, np.int32)), True)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_streamlinear.py.txt'}),\n",
       " Document(page_content='slr = StreamlineLinearRegistration(num_threads=0)\\n    assert_raises(ValueError, slr.optimize, A, B)\\n\\n\\ndef test_get_unique_pairs():\\n\\n    # Regular case\\n    pairs, exclude = get_unique_pairs(6)\\n    assert_equal(len(np.unique(pairs)), 6)\\n    assert_equal(exclude, None)\\n\\n    # Odd case\\n    pairs, exclude = get_unique_pairs(5)\\n    assert_equal(len(np.unique(pairs)), 4)\\n    assert_equal(isinstance(exclude, (int, np.int64, np.int32)), True)\\n\\n    # Iterative case\\n    new_pairs, new_exclude = get_unique_pairs(5, pairs)\\n    assert_equal(len(np.unique(pairs)), 4)\\n    assert_equal(exclude != new_exclude, True)\\n\\n    # Check errors\\n    assert_raises(TypeError, get_unique_pairs, 2.7)\\n    assert_raises(ValueError, get_unique_pairs, 1)\\n\\n\\ndef test_groupwise_slr():\\n\\n    bundles = read_five_af_bundles()\\n\\n    # Test regular use case with convergence\\n    new_bundles, T, d = groupwise_slr(bundles, verbose=True)\\n\\n    assert_equal(len(new_bundles), len(bundles))\\n    assert_equal(type(new_bundles), list)\\n    assert_equal(len(T), len(bundles))\\n    assert_equal(type(T), list)\\n\\n    # Test regular use case without convergence (few iterations)\\n    new_bundles, T, d = groupwise_slr(bundles, max_iter=3, tol=-10,\\n                                      verbose=True)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_streamlinear.py.txt'}),\n",
       " Document(page_content='from numpy.testing import assert_equal\\nfrom dipy.align.streamwarp import (bundlewarp,\\n                                   bundlewarp_shape_analysis,\\n                                   bundlewarp_vector_filed)\\n\\nfrom dipy.data import two_cingulum_bundles\\nfrom dipy.tracking.streamline import set_number_of_points, Streamlines\\n\\n\\ndef test_bundlewarp():\\n\\n    cingulum_bundles = two_cingulum_bundles()\\n\\n    cb1 = Streamlines(cingulum_bundles[0])\\n    cb1 = set_number_of_points(cb1, 20)\\n\\n    cb2 = Streamlines(cingulum_bundles[1])\\n    cb2 = set_number_of_points(cb2, 20)\\n\\n    deformed_bundle, affine_bundle, dists, mp, warp = bundlewarp(cb1, cb2)\\n\\n    assert_equal(len(affine_bundle), len(cb2))\\n    assert_equal(len(deformed_bundle), len(cb2))\\n    assert_equal(len(deformed_bundle), len(affine_bundle))\\n\\n    assert_equal(dists.shape, (len(cb2), len(cb1)))\\n\\n    assert_equal(len(cb2), len(mp))\\n\\n    assert_equal(len(cb2), len(warp))\\n\\n\\ndef test_bundlewarp_vector_filed():\\n\\n    cingulum_bundles = two_cingulum_bundles()\\n\\n    cb1 = Streamlines(cingulum_bundles[0])\\n    cb1 = set_number_of_points(cb1, 20)\\n\\n    cb2 = Streamlines(cingulum_bundles[1])\\n    cb2 = set_number_of_points(cb2, 20)\\n\\n    deformed_bundle, affine_bundle, dists, mp, warp = bundlewarp(cb1, cb2)\\n\\n    offsets, directions, colors = bundlewarp_vector_filed(affine_bundle,\\n                                                          deformed_bundle)\\n\\n    assert_equal(len(offsets), len(cb2.get_data()))\\n    assert_equal(len(directions), len(cb2.get_data()))\\n    assert_equal(len(colors), len(cb2.get_data()))', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_streamwarp.py.txt'}),\n",
       " Document(page_content='cb2 = Streamlines(cingulum_bundles[1])\\n    cb2 = set_number_of_points(cb2, 20)\\n\\n    deformed_bundle, affine_bundle, dists, mp, warp = bundlewarp(cb1, cb2)\\n\\n    offsets, directions, colors = bundlewarp_vector_filed(affine_bundle,\\n                                                          deformed_bundle)\\n\\n    assert_equal(len(offsets), len(cb2.get_data()))\\n    assert_equal(len(directions), len(cb2.get_data()))\\n    assert_equal(len(colors), len(cb2.get_data()))\\n\\n    assert_equal(len(offsets), len(deformed_bundle.get_data()))\\n    assert_equal(len(directions), len(deformed_bundle.get_data()))\\n    assert_equal(len(colors), len(deformed_bundle.get_data()))\\n\\n\\ndef test_bundle_shape_profile():\\n\\n    cingulum_bundles = two_cingulum_bundles()\\n\\n    cb1 = Streamlines(cingulum_bundles[0])\\n    cb1 = set_number_of_points(cb1, 20)\\n\\n    cb2 = Streamlines(cingulum_bundles[1])\\n    cb2 = set_number_of_points(cb2, 20)\\n\\n    deformed_bundle, affine_bundle, dists, mp, warp = bundlewarp(cb1, cb2)\\n\\n    n = 10\\n    shape_profile, stdv = bundlewarp_shape_analysis(affine_bundle,\\n                                                    deformed_bundle,\\n                                                    no_disks=n)\\n\\n    assert_equal(len(shape_profile), n)\\n    assert_equal(len(stdv), n)\\n\\n    n = 100\\n    shape_profile, stdv = bundlewarp_shape_analysis(affine_bundle,\\n                                                    deformed_bundle,\\n                                                    no_disks=n)\\n\\n    assert_equal(len(shape_profile), n)\\n    assert_equal(len(stdv), n)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_streamwarp.py.txt'}),\n",
       " Document(page_content='import numpy as np\\nfrom dipy.align import floating\\nfrom dipy.align import sumsqdiff as ssd\\nfrom numpy.testing import (assert_equal,\\n                           assert_almost_equal,\\n                           assert_array_almost_equal,\\n                           assert_allclose)\\nfrom dipy.testing.decorators import set_random_number_generator\\n\\n\\ndef iterate_residual_field_ssd_2d(delta_field, sigmasq_field, grad, target,\\n                                  lambda_param, dfield):\\n    r\"\"\"\\n    This implementation is for testing purposes only. The problem\\n    with Gauss-Seidel iterations is that it depends on the order\\n    in which we iterate over the variables, so it is necessary to\\n    replicate the implementation under test.\\n    \"\"\"\\n    nrows, ncols = delta_field.shape\\n    if target is None:\\n        b = np.zeros_like(grad)\\n        b[..., 0] = delta_field * grad[..., 0]\\n        b[..., 1] = delta_field * grad[..., 1]\\n    else:\\n        b = target\\n\\n    y = np.zeros(2)\\n    for r in range(nrows):\\n        for c in range(ncols):\\n            sigmasq = sigmasq_field[r, c] if sigmasq_field is not None else 1\\n            # This has to be done inside the nested loops because\\n            # some d[...] may have been previously modified\\n            nn = 0\\n            y[:] = 0\\n            for (dRow, dCol) in [(-1, 0), (0, 1), (1, 0), (0, -1)]:\\n                dr = r + dRow\\n                if dr < 0 or dr >= nrows:\\n                    continue\\n                dc = c + dCol\\n                if dc < 0 or dc >= ncols:\\n                    continue\\n                nn += 1\\n                y += dfield[dr, dc]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_sumsqdiff.py.txt'}),\n",
       " Document(page_content='if np.isinf(sigmasq):\\n                dfield[r, c] = y / nn\\n            else:\\n                tau = sigmasq * lambda_param * nn\\n                A = np.outer(grad[r, c], grad[r, c]) + tau * np.eye(2)\\n                det = np.linalg.det(A)\\n                if det < 1e-9:\\n                    nrm2 = np.sum(grad[r, c]**2)\\n                    if nrm2 < 1e-9:\\n                        dfield[r, c, :] = 0\\n                    else:\\n                        dfield[r, c] = b[r, c] / nrm2\\n                else:\\n                    y = b[r, c] + sigmasq * lambda_param * y\\n                    dfield[r, c] = np.linalg.solve(A, y)\\n\\n\\ndef iterate_residual_field_ssd_3d(delta_field, sigmasq_field, grad, target,\\n                                  lambda_param, dfield):\\n    r\"\"\"\\n    This implementation is for testing purposes only. The problem\\n    with Gauss-Seidel iterations is that it depends on the order\\n    in which we iterate over the variables, so it is necessary to\\n    replicate the implementation under test.\\n    \"\"\"\\n    nslices, nrows, ncols = delta_field.shape\\n    if target is None:\\n        b = np.zeros_like(grad)\\n        for i in range(3):\\n            b[..., i] = delta_field * grad[..., i]\\n    else:\\n        b = target', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_sumsqdiff.py.txt'}),\n",
       " Document(page_content='y = np.ndarray((3,))\\n    for s in range(nslices):\\n        for r in range(nrows):\\n            for c in range(ncols):\\n                g = grad[s, r, c]\\n                # delta = delta_field[s, r, c]\\n                sigmasq = sigmasq_field[\\n                    s, r, c] if sigmasq_field is not None else 1\\n                nn = 0\\n                y[:] = 0\\n                for dSlice, dRow, dCol in [(-1, 0, 0), (0, -1, 0), (0, 0, 1),\\n                                           (0, 1, 0), (0, 0, -1), (1, 0, 0)]:\\n                    ds = s + dSlice\\n                    if ds < 0 or ds >= nslices:\\n                        continue\\n                    dr = r + dRow\\n                    if dr < 0 or dr >= nrows:\\n                        continue\\n                    dc = c + dCol\\n                    if dc < 0 or dc >= ncols:\\n                        continue\\n                    nn += 1\\n                    y += dfield[ds, dr, dc]\\n                if np.isinf(sigmasq):\\n                    dfield[s, r, c] = y / nn\\n                elif sigmasq < 1e-9:\\n                    nrm2 = np.sum(g**2)\\n                    if nrm2 < 1e-9:\\n                        dfield[s, r, c, :] = 0\\n                    else:\\n                        dfield[s, r, c, :] = b[s, r, c] / nrm2\\n                else:\\n                    tau = sigmasq * lambda_param * nn\\n                    y = b[s, r, c] + sigmasq * lambda_param * y\\n                    G = np.outer(g, g) + tau * np.eye(3)\\n                    try:\\n                        dfield[s, r, c] = np.linalg.solve(G, y)\\n                    except np.linalg.linalg.LinAlgError:\\n                        nrm2 = np.sum(g**2)\\n                        if nrm2 < 1e-9:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_sumsqdiff.py.txt'}),\n",
       " Document(page_content='dfield[s, r, c, :] = b[s, r, c] / nrm2\\n                else:\\n                    tau = sigmasq * lambda_param * nn\\n                    y = b[s, r, c] + sigmasq * lambda_param * y\\n                    G = np.outer(g, g) + tau * np.eye(3)\\n                    try:\\n                        dfield[s, r, c] = np.linalg.solve(G, y)\\n                    except np.linalg.linalg.LinAlgError:\\n                        nrm2 = np.sum(g**2)\\n                        if nrm2 < 1e-9:\\n                            dfield[s, r, c, :] = 0\\n                        else:\\n                            dfield[s, r, c] = b[s, r, c] / nrm2', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_sumsqdiff.py.txt'}),\n",
       " Document(page_content=\"@set_random_number_generator(5512751)\\ndef test_compute_residual_displacement_field_ssd_2d(rng):\\n    # Select arbitrary images' shape (same shape for both images)\\n    sh = (20, 10)\\n\\n    # Select arbitrary centers\\n    c_f = np.asarray(sh) / 2\\n    c_g = c_f + 0.5\\n\\n    # Compute the identity vector field I(x) = x in R^2\\n    x_0 = np.asarray(range(sh[0]))\\n    x_1 = np.asarray(range(sh[1]))\\n    X = np.ndarray(sh + (2,), dtype=np.float64)\\n    O = np.ones(sh)\\n    X[..., 0] = x_0[:, None] * O\\n    X[..., 1] = x_1[None, :] * O\\n\\n    # Compute the gradient fields of F and G\\n    grad_F = X - c_f\\n    grad_G = X - c_g\\n\\n    Fnoise = rng.random(\\n        np.size(grad_F)).reshape(\\n        grad_F.shape) * grad_F.max() * 0.1\\n    Fnoise = Fnoise.astype(floating)\\n    grad_F += Fnoise\\n\\n    Gnoise = rng.random(\\n        np.size(grad_G)).reshape(\\n        grad_G.shape) * grad_G.max() * 0.1\\n    Gnoise = Gnoise.astype(floating)\\n    grad_G += Gnoise\\n\\n    # The squared norm of grad_G\\n    sq_norm_grad_G = np.sum(grad_G**2, -1)\\n\\n    # Compute F and G\\n    F = 0.5 * np.sum(grad_F**2, -1)\\n    G = 0.5 * sq_norm_grad_G\\n\\n    Fnoise = rng.random(np.size(F)).reshape(F.shape) * F.max() * 0.1\\n    Fnoise = Fnoise.astype(floating)\\n    F += Fnoise\\n\\n    Gnoise = rng.random(np.size(G)).reshape(G.shape) * G.max() * 0.1\\n    Gnoise = Gnoise.astype(floating)\\n    G += Gnoise\\n\\n    delta_field = np.array(F - G, dtype=floating)\\n\\n    sigma_field = \\\\\\n        rng.standard_normal(delta_field.size).reshape(delta_field.shape)\\n    sigma_field = sigma_field.astype(floating)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_sumsqdiff.py.txt'}),\n",
       " Document(page_content='Fnoise = rng.random(np.size(F)).reshape(F.shape) * F.max() * 0.1\\n    Fnoise = Fnoise.astype(floating)\\n    F += Fnoise\\n\\n    Gnoise = rng.random(np.size(G)).reshape(G.shape) * G.max() * 0.1\\n    Gnoise = Gnoise.astype(floating)\\n    G += Gnoise\\n\\n    delta_field = np.array(F - G, dtype=floating)\\n\\n    sigma_field = \\\\\\n        rng.standard_normal(delta_field.size).reshape(delta_field.shape)\\n    sigma_field = sigma_field.astype(floating)\\n\\n    # Select some pixels to force sigma_field = infinite\\n    inf_sigma = rng.integers(0, 2, sh[0] * sh[1])\\n    inf_sigma = inf_sigma.reshape(sh)\\n    sigma_field[inf_sigma == 1] = np.inf\\n\\n    # Select an initial displacement field\\n    d = rng.standard_normal(grad_G.size).reshape(grad_G.shape).astype(floating)\\n    lambda_param = 1.5\\n\\n    # Implementation under test\\n    iut = ssd.compute_residual_displacement_field_ssd_2d\\n\\n    # In the first iteration we test the case target=None\\n    # In the second iteration, target is not None\\n    target = None\\n    rtol = 1e-9\\n    atol = 1e-4\\n    for it in range(2):\\n        # Sum of differences with the neighbors\\n        s = np.zeros_like(d, dtype=np.float64)\\n        s[:, :-1] += d[:, :-1] - d[:, 1:]  # right\\n        s[:, 1:] += d[:, 1:] - d[:, :-1]  # left\\n        s[:-1, :] += d[:-1, :] - d[1:, :]  # down\\n        s[1:, :] += d[1:, :] - d[:-1, :]  # up\\n        s *= lambda_param\\n\\n        # Dot product of displacement and gradient\\n        dp = d[..., 0] * grad_G[..., 0] + \\\\\\n            d[..., 1] * grad_G[..., 1]\\n        dp = dp.astype(np.float64)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_sumsqdiff.py.txt'}),\n",
       " Document(page_content='# Dot product of displacement and gradient\\n        dp = d[..., 0] * grad_G[..., 0] + \\\\\\n            d[..., 1] * grad_G[..., 1]\\n        dp = dp.astype(np.float64)\\n\\n        # Compute expected residual\\n        if target is None:\\n            expected = np.zeros_like(grad_G)\\n            expected[..., 0] = delta_field * grad_G[..., 0]\\n            expected[..., 1] = delta_field * grad_G[..., 1]\\n        else:\\n            expected = target.copy().astype(np.float64)\\n\\n        # Expected residuals when sigma != infinity\\n        expected[inf_sigma == 0, 0] -= grad_G[inf_sigma == 0, 0] * \\\\\\n            dp[inf_sigma == 0] + sigma_field[inf_sigma == 0] * s[inf_sigma == 0, 0]\\n        expected[inf_sigma == 0, 1] -= grad_G[inf_sigma == 0, 1] * \\\\\\n            dp[inf_sigma == 0] + sigma_field[inf_sigma == 0] * s[inf_sigma == 0, 1]\\n        # Expected residuals when sigma == infinity\\n        expected[inf_sigma == 1] = -1.0 * s[inf_sigma == 1]\\n\\n        # Test residual field computation starting with residual = None\\n        actual = iut(delta_field, sigma_field, grad_G.astype(floating),\\n                     target, lambda_param, d, None)\\n        assert_allclose(actual, expected, rtol=rtol, atol=atol)\\n        # destroy previous result\\n        actual = np.ndarray(actual.shape, dtype=floating)\\n\\n        # Test residual field computation starting with residual is not None\\n        iut(delta_field, sigma_field, grad_G.astype(floating),\\n            target, lambda_param, d, actual)\\n        assert_allclose(actual, expected, rtol=rtol, atol=atol)\\n\\n        # Set target for next iteration\\n        target = actual', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_sumsqdiff.py.txt'}),\n",
       " Document(page_content=\"# Test residual field computation starting with residual is not None\\n        iut(delta_field, sigma_field, grad_G.astype(floating),\\n            target, lambda_param, d, actual)\\n        assert_allclose(actual, expected, rtol=rtol, atol=atol)\\n\\n        # Set target for next iteration\\n        target = actual\\n\\n        # Test Gauss-Seidel step with residual=None and residual=target\\n        for residual in [None, target]:\\n            expected = d.copy()\\n            iterate_residual_field_ssd_2d(\\n                delta_field,\\n                sigma_field,\\n                grad_G.astype(floating),\\n                residual,\\n                lambda_param,\\n                expected)\\n\\n            actual = d.copy()\\n            ssd.iterate_residual_displacement_field_ssd_2d(\\n                delta_field,\\n                sigma_field,\\n                grad_G.astype(floating),\\n                residual,\\n                lambda_param,\\n                actual)\\n\\n            assert_allclose(actual, expected, rtol=rtol, atol=atol)\\n\\n\\n@set_random_number_generator(5512751)\\ndef test_compute_residual_displacement_field_ssd_3d(rng):\\n    # Select arbitrary images' shape (same shape for both images)\\n    sh = (20, 15, 10)\\n\\n    # Select arbitrary centers\\n    c_f = np.asarray(sh) / 2\\n    c_g = c_f + 0.5\\n\\n    # Compute the identity vector field I(x) = x in R^2\\n    x_0 = np.asarray(range(sh[0]))\\n    x_1 = np.asarray(range(sh[1]))\\n    x_2 = np.asarray(range(sh[2]))\\n    X = np.ndarray(sh + (3,), dtype=np.float64)\\n    O = np.ones(sh)\\n    X[..., 0] = x_0[:, None, None] * O\\n    X[..., 1] = x_1[None, :, None] * O\\n    X[..., 2] = x_2[None, None, :] * O\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_sumsqdiff.py.txt'}),\n",
       " Document(page_content='# Select arbitrary centers\\n    c_f = np.asarray(sh) / 2\\n    c_g = c_f + 0.5\\n\\n    # Compute the identity vector field I(x) = x in R^2\\n    x_0 = np.asarray(range(sh[0]))\\n    x_1 = np.asarray(range(sh[1]))\\n    x_2 = np.asarray(range(sh[2]))\\n    X = np.ndarray(sh + (3,), dtype=np.float64)\\n    O = np.ones(sh)\\n    X[..., 0] = x_0[:, None, None] * O\\n    X[..., 1] = x_1[None, :, None] * O\\n    X[..., 2] = x_2[None, None, :] * O\\n\\n    # Compute the gradient fields of F and G\\n    grad_F = X - c_f\\n    grad_G = X - c_g\\n\\n    Fnoise = rng.random(\\n        np.size(grad_F)).reshape(\\n        grad_F.shape) * grad_F.max() * 0.1\\n    Fnoise = Fnoise.astype(floating)\\n    grad_F += Fnoise\\n\\n    Gnoise = rng.random(\\n        np.size(grad_G)).reshape(\\n        grad_G.shape) * grad_G.max() * 0.1\\n    Gnoise = Gnoise.astype(floating)\\n    grad_G += Gnoise\\n\\n    # The squared norm of grad_G\\n    sq_norm_grad_G = np.sum(grad_G**2, -1)\\n\\n    # Compute F and G\\n    F = 0.5 * np.sum(grad_F**2, -1)\\n    G = 0.5 * sq_norm_grad_G\\n\\n    Fnoise = rng.random(np.size(F)).reshape(F.shape) * F.max() * 0.1\\n    Fnoise = Fnoise.astype(floating)\\n    F += Fnoise\\n\\n    Gnoise = rng.random(np.size(G)).reshape(G.shape) * G.max() * 0.1\\n    Gnoise = Gnoise.astype(floating)\\n    G += Gnoise\\n\\n    delta_field = np.array(F - G, dtype=floating)\\n\\n    sigma_field = rng.random(delta_field.size).reshape(delta_field.shape)\\n    sigma_field = sigma_field.astype(floating)\\n\\n    # Select some pixels to force sigma_field = infinite\\n    inf_sigma = rng.integers(0, 2, sh[0] * sh[1] * sh[2])\\n    inf_sigma = inf_sigma.reshape(sh)\\n    sigma_field[inf_sigma == 1] = np.inf', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_sumsqdiff.py.txt'}),\n",
       " Document(page_content='Gnoise = rng.random(np.size(G)).reshape(G.shape) * G.max() * 0.1\\n    Gnoise = Gnoise.astype(floating)\\n    G += Gnoise\\n\\n    delta_field = np.array(F - G, dtype=floating)\\n\\n    sigma_field = rng.random(delta_field.size).reshape(delta_field.shape)\\n    sigma_field = sigma_field.astype(floating)\\n\\n    # Select some pixels to force sigma_field = infinite\\n    inf_sigma = rng.integers(0, 2, sh[0] * sh[1] * sh[2])\\n    inf_sigma = inf_sigma.reshape(sh)\\n    sigma_field[inf_sigma == 1] = np.inf\\n\\n    # Select an initial displacement field\\n    d = rng.random(grad_G.size).reshape(grad_G.shape).astype(floating)\\n    lambda_param = 1.5\\n\\n    # Implementation under test\\n    iut = ssd.compute_residual_displacement_field_ssd_3d\\n\\n    # In the first iteration we test the case target=None\\n    # In the second iteration, target is not None\\n    target = None\\n    rtol = 1e-9\\n    atol = 1e-4\\n    for it in range(2):\\n        # Sum of differences with the neighbors\\n        s = np.zeros_like(d, dtype=np.float64)\\n        s[:, :, :-1] += d[:, :, :-1] - d[:, :, 1:]  # right\\n        s[:, :, 1:] += d[:, :, 1:] - d[:, :, :-1]  # left\\n        s[:, :-1, :] += d[:, :-1, :] - d[:, 1:, :]  # down\\n        s[:, 1:, :] += d[:, 1:, :] - d[:, :-1, :]  # up\\n        s[:-1, :, :] += d[:-1, :, :] - d[1:, :, :]  # below\\n        s[1:, :, :] += d[1:, :, :] - d[:-1, :, :]  # above\\n        s *= lambda_param\\n\\n        # Dot product of displacement and gradient\\n        dp = d[..., 0] * grad_G[..., 0] + \\\\\\n            d[..., 1] * grad_G[..., 1] + \\\\\\n            d[..., 2] * grad_G[..., 2]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_sumsqdiff.py.txt'}),\n",
       " Document(page_content='# Dot product of displacement and gradient\\n        dp = d[..., 0] * grad_G[..., 0] + \\\\\\n            d[..., 1] * grad_G[..., 1] + \\\\\\n            d[..., 2] * grad_G[..., 2]\\n\\n        # Compute expected residual\\n        if target is None:\\n            expected = np.zeros_like(grad_G)\\n            for i in range(3):\\n                expected[..., i] = delta_field * grad_G[..., i]\\n        else:\\n            expected = target.copy().astype(np.float64)\\n\\n        # Expected residuals when sigma != infinity\\n        for i in range(3):\\n            expected[inf_sigma == 0, i] -= grad_G[inf_sigma == 0, i] * \\\\\\n                dp[inf_sigma == 0] + sigma_field[inf_sigma == 0] * s[inf_sigma == 0, i]\\n        # Expected residuals when sigma == infinity\\n        expected[inf_sigma == 1] = -1.0 * s[inf_sigma == 1]\\n\\n        # Test residual field computation starting with residual = None\\n        actual = iut(delta_field, sigma_field, grad_G.astype(floating),\\n                     target, lambda_param, d, None)\\n        assert_allclose(actual, expected, rtol=rtol, atol=atol)\\n        # destroy previous result\\n        actual = np.ndarray(actual.shape, dtype=floating)\\n\\n        # Test residual field computation starting with residual is not None\\n        iut(delta_field, sigma_field, grad_G.astype(floating),\\n            target, lambda_param, d, actual)\\n        assert_allclose(actual, expected, rtol=rtol, atol=atol)\\n\\n        # Set target for next iteration\\n        target = actual', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_sumsqdiff.py.txt'}),\n",
       " Document(page_content='# Test residual field computation starting with residual is not None\\n        iut(delta_field, sigma_field, grad_G.astype(floating),\\n            target, lambda_param, d, actual)\\n        assert_allclose(actual, expected, rtol=rtol, atol=atol)\\n\\n        # Set target for next iteration\\n        target = actual\\n\\n        # Test Gauss-Seidel step with residual=None and residual=target\\n        for residual in [None, target]:\\n            expected = d.copy()\\n            iterate_residual_field_ssd_3d(\\n                delta_field,\\n                sigma_field,\\n                grad_G.astype(floating),\\n                residual,\\n                lambda_param,\\n                expected)\\n\\n            actual = d.copy()\\n            ssd.iterate_residual_displacement_field_ssd_3d(\\n                delta_field,\\n                sigma_field,\\n                grad_G.astype(floating),\\n                residual,\\n                lambda_param,\\n                actual)\\n\\n            # the numpy linear solver may differ from our custom implementation\\n            # we need to increase the tolerance a bit\\n            assert_allclose(actual, expected, rtol=rtol, atol=atol * 10)\\n\\n\\ndef test_solve_2d_symmetric_positive_definite():\\n    # Select some arbitrary right-hand sides\\n    bs = [np.array([1.1, 2.2]),\\n          np.array([1e-2, 3e-3]),\\n          np.array([1e2, 1e3]),\\n          np.array([1e-5, 1e5])]\\n\\n    # Select arbitrary symmetric positive-definite matrices\\n    As = []\\n\\n    # Identity\\n    identity = np.array([1.0, 0.0, 1.0])\\n    As.append(identity)\\n\\n    # Small determinant\\n    small_det = np.array([1e-3, 1e-4, 1e-3])\\n    As.append(small_det)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_sumsqdiff.py.txt'}),\n",
       " Document(page_content='def test_solve_2d_symmetric_positive_definite():\\n    # Select some arbitrary right-hand sides\\n    bs = [np.array([1.1, 2.2]),\\n          np.array([1e-2, 3e-3]),\\n          np.array([1e2, 1e3]),\\n          np.array([1e-5, 1e5])]\\n\\n    # Select arbitrary symmetric positive-definite matrices\\n    As = []\\n\\n    # Identity\\n    identity = np.array([1.0, 0.0, 1.0])\\n    As.append(identity)\\n\\n    # Small determinant\\n    small_det = np.array([1e-3, 1e-4, 1e-3])\\n    As.append(small_det)\\n\\n    # Large determinant\\n    large_det = np.array([1e6, 1e4, 1e6])\\n    As.append(large_det)\\n\\n    for A in As:\\n        AA = np.array([[A[0], A[1]], [A[1], A[2]]])\\n        det = np.linalg.det(AA)\\n        for b in bs:\\n            expected = np.linalg.solve(AA, b)\\n            actual = ssd.solve_2d_symmetric_positive_definite(A, b, det)\\n            assert_allclose(expected, actual, rtol=1e-9, atol=1e-9)\\n\\n\\ndef test_solve_3d_symmetric_positive_definite():\\n    # Select some arbitrary right-hand sides\\n    bs = [np.array([1.1, 2.2, 3.3]),\\n          np.array([1e-2, 3e-3, 2e-2]),\\n          np.array([1e2, 1e3, 5e-2]),\\n          np.array([1e-5, 1e5, 1.0])]\\n\\n    # Select arbitrary taus\\n    taus = [0.0, 1.0, 1e-4, 1e5]\\n\\n    # Select arbitrary matrices\\n    gs = []\\n\\n    # diagonal\\n    diag = np.array([0.0, 0.0, 0.0])\\n    gs.append(diag)\\n\\n    # canonical basis\\n    gs.append(np.array([1.0, 0.0, 0.0]))\\n    gs.append(np.array([0.0, 1.0, 0.0]))\\n    gs.append(np.array([0.0, 0.0, 1.0]))\\n\\n    # other\\n    gs.append(np.array([1.0, 0.5, 0.0]))\\n    gs.append(np.array([0.0, 0.2, 0.1]))\\n    gs.append(np.array([0.3, 0.0, 0.9]))', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_sumsqdiff.py.txt'}),\n",
       " Document(page_content='# Select arbitrary taus\\n    taus = [0.0, 1.0, 1e-4, 1e5]\\n\\n    # Select arbitrary matrices\\n    gs = []\\n\\n    # diagonal\\n    diag = np.array([0.0, 0.0, 0.0])\\n    gs.append(diag)\\n\\n    # canonical basis\\n    gs.append(np.array([1.0, 0.0, 0.0]))\\n    gs.append(np.array([0.0, 1.0, 0.0]))\\n    gs.append(np.array([0.0, 0.0, 1.0]))\\n\\n    # other\\n    gs.append(np.array([1.0, 0.5, 0.0]))\\n    gs.append(np.array([0.0, 0.2, 0.1]))\\n    gs.append(np.array([0.3, 0.0, 0.9]))\\n\\n    for g in gs:\\n        A = g[:, None] * g[None, :]\\n        for tau in taus:\\n            AA = A + tau * np.eye(3)\\n            for b in bs:\\n                actual, is_singular = ssd.solve_3d_symmetric_positive_definite(\\n                    g, b, tau)\\n                if tau == 0.0:\\n                    assert_equal(is_singular, 1)\\n                else:\\n                    expected = np.linalg.solve(AA, b)\\n                    assert_allclose(expected, actual, rtol=1e-9, atol=1e-9)\\n\\n\\ndef test_compute_energy_ssd_2d():\\n    sh = (32, 32)\\n\\n    # Select arbitrary centers\\n    c_f = np.asarray(sh) / 2\\n    c_g = c_f + 0.5\\n\\n    # Compute the identity vector field I(x) = x in R^2\\n    x_0 = np.asarray(range(sh[0]))\\n    x_1 = np.asarray(range(sh[1]))\\n    X = np.ndarray(sh + (2,), dtype=np.float64)\\n    O = np.ones(sh)\\n    X[..., 0] = x_0[:, None] * O\\n    X[..., 1] = x_1[None, :] * O\\n\\n    # Compute the gradient fields of F and G\\n    grad_F = X - c_f\\n    grad_G = X - c_g\\n\\n    # Compute F and G\\n    F = 0.5 * np.sum(grad_F**2, -1)\\n    G = 0.5 * np.sum(grad_G**2, -1)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_sumsqdiff.py.txt'}),\n",
       " Document(page_content='# Compute the identity vector field I(x) = x in R^2\\n    x_0 = np.asarray(range(sh[0]))\\n    x_1 = np.asarray(range(sh[1]))\\n    X = np.ndarray(sh + (2,), dtype=np.float64)\\n    O = np.ones(sh)\\n    X[..., 0] = x_0[:, None] * O\\n    X[..., 1] = x_1[None, :] * O\\n\\n    # Compute the gradient fields of F and G\\n    grad_F = X - c_f\\n    grad_G = X - c_g\\n\\n    # Compute F and G\\n    F = 0.5 * np.sum(grad_F**2, -1)\\n    G = 0.5 * np.sum(grad_G**2, -1)\\n\\n    # Note: this should include the energy corresponding to the\\n    # regularization term, but it is discarded in ANTS (they just\\n    # consider the data term, which is not the objective function\\n    # being optimized). This test case should be updated after\\n    # further investigation\\n    expected = ((F - G)**2).sum()\\n    actual = ssd.compute_energy_ssd_2d(np.array(F - G, dtype=floating))\\n    assert_almost_equal(expected, actual)\\n\\n\\ndef test_compute_energy_ssd_3d():\\n    sh = (32, 32, 32)\\n\\n    # Select arbitrary centers\\n    c_f = np.asarray(sh) / 2\\n    c_g = c_f + 0.5\\n\\n    # Compute the identity vector field I(x) = x in R^2\\n    x_0 = np.asarray(range(sh[0]))\\n    x_1 = np.asarray(range(sh[1]))\\n    x_2 = np.asarray(range(sh[2]))\\n    X = np.ndarray(sh + (3,), dtype=np.float64)\\n    O = np.ones(sh)\\n    X[..., 0] = x_0[:, None, None] * O\\n    X[..., 1] = x_1[None, :, None] * O\\n    X[..., 2] = x_2[None, None, :] * O\\n\\n    # Compute the gradient fields of F and G\\n    grad_F = X - c_f\\n    grad_G = X - c_g\\n\\n    # Compute F and G\\n    F = 0.5 * np.sum(grad_F**2, -1)\\n    G = 0.5 * np.sum(grad_G**2, -1)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_sumsqdiff.py.txt'}),\n",
       " Document(page_content='# Compute the gradient fields of F and G\\n    grad_F = X - c_f\\n    grad_G = X - c_g\\n\\n    # Compute F and G\\n    F = 0.5 * np.sum(grad_F**2, -1)\\n    G = 0.5 * np.sum(grad_G**2, -1)\\n\\n    # Note: this should include the energy corresponding to the\\n    # regularization term, but it is discarded in ANTS (they just\\n    # consider the data term, which is not the objective function\\n    # being optimized). This test case should be updated after\\n    # further investigating\\n    expected = ((F - G)**2).sum()\\n    actual = ssd.compute_energy_ssd_3d(np.array(F - G, dtype=floating))\\n    assert_almost_equal(expected, actual)\\n\\n\\n@set_random_number_generator(1137271)\\ndef test_compute_ssd_demons_step_2d(rng):\\n    r\"\"\"\\n    Compares the output of the demons step in 2d against an analytical\\n    step. The fixed image is given by $F(x) = \\\\frac{1}{2}||x - c_f||^2$, the\\n    moving image is given by $G(x) = \\\\frac{1}{2}||x - c_g||^2$,\\n    $x, c_f, c_g \\\\in R^{2}$\\n\\n    References\\n    ----------\\n    [Vercauteren09] Vercauteren, T., Pennec, X., Perchant, A., & Ayache, N.\\n                    (2009). Diffeomorphic demons: efficient non-parametric\\n                    image registration. NeuroImage, 45(1 Suppl), S61-72.\\n                    doi:10.1016/j.neuroimage.2008.10.040\\n    \"\"\"\\n    # Select arbitrary images\\' shape (same shape for both images)\\n    sh = (20, 10)\\n\\n    # Select arbitrary centers\\n    c_f = np.asarray(sh) / 2\\n    c_g = c_f + 0.5\\n\\n    # Compute the identity vector field I(x) = x in R^2\\n    x_0 = np.asarray(range(sh[0]))\\n    x_1 = np.asarray(range(sh[1]))\\n    X = np.ndarray(sh + (2,), dtype=np.float64)\\n    O = np.ones(sh)\\n    X[..., 0] = x_0[:, None] * O\\n    X[..., 1] = x_1[None, :] * O', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_sumsqdiff.py.txt'}),\n",
       " Document(page_content='# Select arbitrary centers\\n    c_f = np.asarray(sh) / 2\\n    c_g = c_f + 0.5\\n\\n    # Compute the identity vector field I(x) = x in R^2\\n    x_0 = np.asarray(range(sh[0]))\\n    x_1 = np.asarray(range(sh[1]))\\n    X = np.ndarray(sh + (2,), dtype=np.float64)\\n    O = np.ones(sh)\\n    X[..., 0] = x_0[:, None] * O\\n    X[..., 1] = x_1[None, :] * O\\n\\n    # Compute the gradient fields of F and G\\n    grad_F = X - c_f\\n    grad_G = X - c_g\\n\\n    Fnoise = rng.random(\\n        np.size(grad_F)).reshape(\\n        grad_F.shape) * grad_F.max() * 0.1\\n    Fnoise = Fnoise.astype(floating)\\n    grad_F += Fnoise\\n\\n    Gnoise = rng.random(\\n        np.size(grad_G)).reshape(\\n        grad_G.shape) * grad_G.max() * 0.1\\n    Gnoise = Gnoise.astype(floating)\\n    grad_G += Gnoise\\n\\n    # The squared norm of grad_G to be used later\\n    sq_norm_grad_G = np.sum(grad_G**2, -1)\\n\\n    # Compute F and G\\n    F = 0.5 * np.sum(grad_F**2, -1)\\n    G = 0.5 * sq_norm_grad_G\\n\\n    Fnoise = rng.random(np.size(F)).reshape(F.shape) * F.max() * 0.1\\n    Fnoise = Fnoise.astype(floating)\\n    F += Fnoise\\n\\n    Gnoise = rng.random(np.size(G)).reshape(G.shape) * G.max() * 0.1\\n    Gnoise = Gnoise.astype(floating)\\n    G += Gnoise\\n\\n    delta_field = np.array(G - F, dtype=floating)\\n\\n    # Select some pixels to force gradient = 0 and F=G\\n    random_labels = rng.integers(0, 2, sh[0] * sh[1])\\n    random_labels = random_labels.reshape(sh)\\n\\n    F[random_labels == 0] = G[random_labels == 0]\\n    delta_field[random_labels == 0] = 0\\n    grad_G[random_labels == 0, ...] = 0\\n    sq_norm_grad_G[random_labels == 0, ...] = 0', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_sumsqdiff.py.txt'}),\n",
       " Document(page_content=\"delta_field = np.array(G - F, dtype=floating)\\n\\n    # Select some pixels to force gradient = 0 and F=G\\n    random_labels = rng.integers(0, 2, sh[0] * sh[1])\\n    random_labels = random_labels.reshape(sh)\\n\\n    F[random_labels == 0] = G[random_labels == 0]\\n    delta_field[random_labels == 0] = 0\\n    grad_G[random_labels == 0, ...] = 0\\n    sq_norm_grad_G[random_labels == 0, ...] = 0\\n\\n    # Set arbitrary values for $\\\\sigma_i$ (eq. 4 in [Vercauteren09])\\n    # The original Demons algorithm used simply |F(x) - G(x)| as an\\n    # estimator, so let's use it as well\\n    sigma_i_sq = (F - G)**2\\n\\n    # Now select arbitrary parameters for $\\\\sigma_x$ (eq 4 in [Vercauteren09])\\n    for sigma_x_sq in [0.01, 1.5, 4.2]:\\n        # Directly compute the demons step according to eq. 4 in\\n        # [Vercauteren09]\\n        num = (sigma_x_sq * (F - G))[random_labels == 1]\\n        den = (sigma_x_sq * sq_norm_grad_G + sigma_i_sq)[random_labels == 1]\\n        # This is $J^{P}$ in eq. 4 [Vercauteren09]\\n        expected = (-1 * np.array(grad_G))\\n        expected[random_labels == 1, 0] *= num / den\\n        expected[random_labels == 1, 1] *= num / den\\n        expected[random_labels == 0, ...] = 0\\n\\n        # Now compute it using the implementation under test\\n        actual = np.empty_like(expected, dtype=floating)\\n\\n        ssd.compute_ssd_demons_step_2d(delta_field,\\n                                       np.array(grad_G, dtype=floating),\\n                                       sigma_x_sq,\\n                                       actual)\\n\\n        assert_array_almost_equal(actual, expected)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_sumsqdiff.py.txt'}),\n",
       " Document(page_content='# Now compute it using the implementation under test\\n        actual = np.empty_like(expected, dtype=floating)\\n\\n        ssd.compute_ssd_demons_step_2d(delta_field,\\n                                       np.array(grad_G, dtype=floating),\\n                                       sigma_x_sq,\\n                                       actual)\\n\\n        assert_array_almost_equal(actual, expected)\\n\\n\\n@set_random_number_generator(1137271)\\ndef test_compute_ssd_demons_step_3d(rng):\\n    r\"\"\"\\n    Compares the output of the demons step in 3d against an analytical\\n    step. The fixed image is given by $F(x) = \\\\frac{1}{2}||x - c_f||^2$, the\\n    moving image is given by $G(x) = \\\\frac{1}{2}||x - c_g||^2$,\\n    $x, c_f, c_g \\\\in R^{3}$\\n\\n    References\\n    ----------\\n    [Vercauteren09] Vercauteren, T., Pennec, X., Perchant, A., & Ayache, N.\\n                    (2009). Diffeomorphic demons: efficient non-parametric\\n                    image registration. NeuroImage, 45(1 Suppl), S61-72.\\n                    doi:10.1016/j.neuroimage.2008.10.040\\n    \"\"\"\\n\\n    # Select arbitrary images\\' shape (same shape for both images)\\n    sh = (20, 15, 10)\\n\\n    # Select arbitrary centers\\n    c_f = np.asarray(sh) / 2\\n    c_g = c_f + 0.5\\n\\n    # Compute the identity vector field I(x) = x in R^2\\n    x_0 = np.asarray(range(sh[0]))\\n    x_1 = np.asarray(range(sh[1]))\\n    x_2 = np.asarray(range(sh[2]))\\n    X = np.ndarray(sh + (3,), dtype=np.float64)\\n    O = np.ones(sh)\\n    X[..., 0] = x_0[:, None, None] * O\\n    X[..., 1] = x_1[None, :, None] * O\\n    X[..., 2] = x_2[None, None, :] * O\\n\\n    # Compute the gradient fields of F and G\\n    grad_F = X - c_f\\n    grad_G = X - c_g', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_sumsqdiff.py.txt'}),\n",
       " Document(page_content='# Compute the identity vector field I(x) = x in R^2\\n    x_0 = np.asarray(range(sh[0]))\\n    x_1 = np.asarray(range(sh[1]))\\n    x_2 = np.asarray(range(sh[2]))\\n    X = np.ndarray(sh + (3,), dtype=np.float64)\\n    O = np.ones(sh)\\n    X[..., 0] = x_0[:, None, None] * O\\n    X[..., 1] = x_1[None, :, None] * O\\n    X[..., 2] = x_2[None, None, :] * O\\n\\n    # Compute the gradient fields of F and G\\n    grad_F = X - c_f\\n    grad_G = X - c_g\\n\\n    Fnoise = rng.random(\\n        np.size(grad_F)).reshape(\\n        grad_F.shape) * grad_F.max() * 0.1\\n    Fnoise = Fnoise.astype(floating)\\n    grad_F += Fnoise\\n\\n    Gnoise = rng.random(\\n        np.size(grad_G)).reshape(\\n        grad_G.shape) * grad_G.max() * 0.1\\n    Gnoise = Gnoise.astype(floating)\\n    grad_G += Gnoise\\n\\n    # The squared norm of grad_G to be used later\\n    sq_norm_grad_G = np.sum(grad_G**2, -1)\\n\\n    # Compute F and G\\n    F = 0.5 * np.sum(grad_F**2, -1)\\n    G = 0.5 * sq_norm_grad_G\\n\\n    Fnoise = rng.random(np.size(F)).reshape(F.shape) * F.max() * 0.1\\n    Fnoise = Fnoise.astype(floating)\\n    F += Fnoise\\n\\n    Gnoise = rng.random(np.size(G)).reshape(G.shape) * G.max() * 0.1\\n    Gnoise = Gnoise.astype(floating)\\n    G += Gnoise\\n\\n    delta_field = np.array(G - F, dtype=floating)\\n\\n    # Select some pixels to force gradient = 0 and F=G\\n    random_labels = rng.integers(0, 2, sh[0] * sh[1] * sh[2])\\n    random_labels = random_labels.reshape(sh)\\n\\n    F[random_labels == 0] = G[random_labels == 0]\\n    delta_field[random_labels == 0] = 0\\n    grad_G[random_labels == 0, ...] = 0\\n    sq_norm_grad_G[random_labels == 0, ...] = 0', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_sumsqdiff.py.txt'}),\n",
       " Document(page_content=\"delta_field = np.array(G - F, dtype=floating)\\n\\n    # Select some pixels to force gradient = 0 and F=G\\n    random_labels = rng.integers(0, 2, sh[0] * sh[1] * sh[2])\\n    random_labels = random_labels.reshape(sh)\\n\\n    F[random_labels == 0] = G[random_labels == 0]\\n    delta_field[random_labels == 0] = 0\\n    grad_G[random_labels == 0, ...] = 0\\n    sq_norm_grad_G[random_labels == 0, ...] = 0\\n\\n    # Set arbitrary values for $\\\\sigma_i$ (eq. 4 in [Vercauteren09])\\n    # The original Demons algorithm used simply |F(x) - G(x)| as an\\n    # estimator, so let's use it as well\\n    sigma_i_sq = (F - G)**2\\n\\n    # Now select arbitrary parameters for $\\\\sigma_x$ (eq 4 in [Vercauteren09])\\n    for sigma_x_sq in [0.01, 1.5, 4.2]:\\n        # Directly compute the demons step according to eq. 4 in\\n        # [Vercauteren09]\\n        num = (sigma_x_sq * (F - G))[random_labels == 1]\\n        den = (sigma_x_sq * sq_norm_grad_G + sigma_i_sq)[random_labels == 1]\\n        # This is $J^{P}$ in eq. 4 [Vercauteren09]\\n        expected = (-1 * np.array(grad_G))\\n        expected[random_labels == 1, 0] *= num / den\\n        expected[random_labels == 1, 1] *= num / den\\n        expected[random_labels == 1, 2] *= num / den\\n        expected[random_labels == 0, ...] = 0\\n\\n        # Now compute it using the implementation under test\\n        actual = np.empty_like(expected, dtype=floating)\\n\\n        ssd.compute_ssd_demons_step_3d(delta_field,\\n                                       np.array(grad_G, dtype=floating),\\n                                       sigma_x_sq,\\n                                       actual)\\n\\n        assert_array_almost_equal(actual, expected)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_sumsqdiff.py.txt'}),\n",
       " Document(page_content=\"import numpy as np\\nfrom numpy.testing import (assert_array_equal,\\n                           assert_array_almost_equal,\\n                           assert_equal,\\n                           assert_raises)\\n\\nfrom dipy.align.transforms import regtransforms, Transform\\nfrom dipy.testing.decorators import set_random_number_generator\\n\\n\\ndef test_number_of_parameters():\\n    expected_params = {('TRANSLATION', 2): 2,\\n                       ('TRANSLATION', 3): 3,\\n                       ('ROTATION', 2): 1,\\n                       ('ROTATION', 3): 3,\\n                       ('RIGID', 2): 3,\\n                       ('RIGID', 3): 6,\\n                       ('SCALING', 2): 1,\\n                       ('SCALING', 3): 1,\\n                       ('AFFINE', 2): 6,\\n                       ('AFFINE', 3): 12,\\n                       ('RIGIDSCALING', 2): 5,\\n                       ('RIGIDSCALING', 3): 9,\\n                       ('RIGIDISOSCALING', 2): 4,\\n                       ('RIGIDISOSCALING', 3): 7}\\n\\n    for ttype, transform in regtransforms.items():\\n        assert_equal(\\n            transform.get_number_of_parameters(),\\n            expected_params[ttype])\\n\\n\\n@set_random_number_generator()\\ndef test_param_to_matrix_2d(rng):\\n    # Test translation matrix 2D\\n    transform = regtransforms[('TRANSLATION', 2)]\\n    dx, dy = rng.uniform(size=(2,))\\n    theta = np.array([dx, dy])\\n    expected = np.array([[1, 0, dx], [0, 1, dy], [0, 0, 1]])\\n    actual = transform.param_to_matrix(theta)\\n    assert_array_equal(actual, expected)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_transforms.py.txt'}),\n",
       " Document(page_content=\"@set_random_number_generator()\\ndef test_param_to_matrix_2d(rng):\\n    # Test translation matrix 2D\\n    transform = regtransforms[('TRANSLATION', 2)]\\n    dx, dy = rng.uniform(size=(2,))\\n    theta = np.array([dx, dy])\\n    expected = np.array([[1, 0, dx], [0, 1, dy], [0, 0, 1]])\\n    actual = transform.param_to_matrix(theta)\\n    assert_array_equal(actual, expected)\\n\\n    # Test rotation matrix 2D\\n    transform = regtransforms[('ROTATION', 2)]\\n    angle = rng.uniform()\\n    theta = np.array([angle])\\n    ct = np.cos(angle)\\n    st = np.sin(angle)\\n    expected = np.array([[ct, -st, 0], [st, ct, 0], [0, 0, 1]])\\n    actual = transform.param_to_matrix(theta)\\n    assert_array_almost_equal(actual, expected)\\n\\n    # Test rigid matrix 2D\\n    transform = regtransforms[('RIGID', 2)]\\n    angle, dx, dy = rng.uniform(size=(3,))\\n    theta = np.array([angle, dx, dy])\\n    ct = np.cos(angle)\\n    st = np.sin(angle)\\n    expected = np.array([[ct, -st, dx], [st, ct, dy], [0, 0, 1]])\\n    actual = transform.param_to_matrix(theta)\\n    assert_array_almost_equal(actual, expected)\\n\\n    # Test scaling matrix 2D\\n    transform = regtransforms[('SCALING', 2)]\\n    factor = rng.uniform()\\n    theta = np.array([factor])\\n    expected = np.array([[factor, 0, 0], [0, factor, 0], [0, 0, 1]])\\n    actual = transform.param_to_matrix(theta)\\n    assert_array_almost_equal(actual, expected)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_transforms.py.txt'}),\n",
       " Document(page_content=\"# Test scaling matrix 2D\\n    transform = regtransforms[('SCALING', 2)]\\n    factor = rng.uniform()\\n    theta = np.array([factor])\\n    expected = np.array([[factor, 0, 0], [0, factor, 0], [0, 0, 1]])\\n    actual = transform.param_to_matrix(theta)\\n    assert_array_almost_equal(actual, expected)\\n\\n    # Test rigid isoscaling matrix 2D\\n    transform = regtransforms[('RIGIDISOSCALING', 2)]\\n    angle, dx, dy, factor = rng.uniform(size=(4,))\\n    theta = np.array([angle, dx, dy, factor])\\n    ct = np.cos(angle)\\n    st = np.sin(angle)\\n    expected = np.array([[ct*factor, -st*factor, dx],\\n                         [st*factor, ct*factor, dy],\\n                         [0, 0, 1]])\\n    actual = transform.param_to_matrix(theta)\\n    assert_array_almost_equal(actual, expected)\\n\\n    # Test rigid scaling matrix 2D\\n    transform = regtransforms[('RIGIDSCALING', 2)]\\n    angle, dx, dy, sx, sy = rng.uniform(size=(5,))\\n    theta = np.array([angle, dx, dy, sx, sy])\\n    ct = np.cos(angle)\\n    st = np.sin(angle)\\n    expected = np.array([[ct*sx, -st*sx, dx],\\n                         [st*sy, ct*sy, dy],\\n                         [0, 0, 1]])\\n    actual = transform.param_to_matrix(theta)\\n    assert_array_almost_equal(actual, expected)\\n\\n    # Test affine 2D\\n    transform = regtransforms[('AFFINE', 2)]\\n    theta = rng.uniform(size=(6,))\\n    expected = np.eye(3)\\n    expected[0, :] = theta[:3]\\n    expected[1, :] = theta[3:6]\\n    actual = transform.param_to_matrix(theta)\\n    assert_array_almost_equal(actual, expected)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_transforms.py.txt'}),\n",
       " Document(page_content=\"# Test affine 2D\\n    transform = regtransforms[('AFFINE', 2)]\\n    theta = rng.uniform(size=(6,))\\n    expected = np.eye(3)\\n    expected[0, :] = theta[:3]\\n    expected[1, :] = theta[3:6]\\n    actual = transform.param_to_matrix(theta)\\n    assert_array_almost_equal(actual, expected)\\n\\n    # Verify that ValueError is raised if incorrect number of parameters\\n    for transform in regtransforms.values():\\n        n = transform.get_number_of_parameters()\\n        # Set incorrect number of parameters\\n        theta = np.zeros(n + 1, dtype=np.float64)\\n        assert_raises(ValueError, transform.param_to_matrix, theta)\\n\\n\\n@set_random_number_generator()\\ndef test_param_to_matrix_3d(rng):\\n    # Test translation matrix 3D\\n    transform = regtransforms[('TRANSLATION', 3)]\\n    dx, dy, dz = rng.uniform(size=(3,))\\n    theta = np.array([dx, dy, dz])\\n    expected = np.array([[1, 0, 0, dx],\\n                         [0, 1, 0, dy],\\n                         [0, 0, 1, dz],\\n                         [0, 0, 0, 1]])\\n    actual = transform.param_to_matrix(theta)\\n    assert_array_equal(actual, expected)\\n\\n    # Create helpful rotation matrix function\\n    def get_rotation_matrix(alpha, beta, gamma):\\n        ca = np.cos(alpha)\\n        sa = np.sin(alpha)\\n        cb = np.cos(beta)\\n        sb = np.sin(beta)\\n        cc = np.cos(gamma)\\n        sc = np.sin(gamma)\\n\\n        X = np.array([[1, 0, 0],\\n                      [0, ca, -sa],\\n                      [0, sa, ca]])\\n        Y = np.array([[cb, 0, sb],\\n                      [0, 1, 0],\\n                      [-sb, 0, cb]])\\n        Z = np.array([[cc, -sc, 0],\\n                      [sc, cc, 0],\\n                      [0, 0, 1]])\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_transforms.py.txt'}),\n",
       " Document(page_content=\"X = np.array([[1, 0, 0],\\n                      [0, ca, -sa],\\n                      [0, sa, ca]])\\n        Y = np.array([[cb, 0, sb],\\n                      [0, 1, 0],\\n                      [-sb, 0, cb]])\\n        Z = np.array([[cc, -sc, 0],\\n                      [sc, cc, 0],\\n                      [0, 0, 1]])\\n\\n        return Z.dot(X.dot(Y))  # Apply in order: Y, X, Z (Y goes to the right)\\n\\n    # Test rotation matrix 3D\\n    transform = regtransforms[('ROTATION', 3)]\\n    theta = rng.uniform(size=(3,))\\n    R = get_rotation_matrix(theta[0], theta[1], theta[2])\\n    expected = np.eye(4)\\n    expected[:3, :3] = R[:3, :3]\\n    actual = transform.param_to_matrix(theta)\\n    assert_array_almost_equal(actual, expected)\\n\\n    # Test rigid matrix 3D\\n    transform = regtransforms[('RIGID', 3)]\\n    theta = rng.uniform(size=(6,))\\n    R = get_rotation_matrix(theta[0], theta[1], theta[2])\\n    expected = np.eye(4)\\n    expected[:3, :3] = R[:3, :3]\\n    expected[:3, 3] = theta[3:6]\\n    actual = transform.param_to_matrix(theta)\\n    assert_array_almost_equal(actual, expected)\\n\\n    # Test rigid isoscaling matrix 3D\\n    transform = regtransforms[('RIGIDISOSCALING', 3)]\\n    theta = rng.uniform(size=(7,))\\n    R = get_rotation_matrix(theta[0], theta[1], theta[2])\\n    expected = np.eye(4)\\n    expected[:3, :3] = R[:3, :3] * theta[6]\\n    expected[:3, 3] = theta[3:6]\\n    actual = transform.param_to_matrix(theta)\\n    assert_array_almost_equal(actual, expected)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_transforms.py.txt'}),\n",
       " Document(page_content=\"# Test rigid isoscaling matrix 3D\\n    transform = regtransforms[('RIGIDISOSCALING', 3)]\\n    theta = rng.uniform(size=(7,))\\n    R = get_rotation_matrix(theta[0], theta[1], theta[2])\\n    expected = np.eye(4)\\n    expected[:3, :3] = R[:3, :3] * theta[6]\\n    expected[:3, 3] = theta[3:6]\\n    actual = transform.param_to_matrix(theta)\\n    assert_array_almost_equal(actual, expected)\\n\\n    # Test rigid scaling matrix 3D\\n    transform = regtransforms[('RIGIDSCALING', 3)]\\n    theta = rng.uniform(size=(9,))\\n    R = get_rotation_matrix(theta[0], theta[1], theta[2])\\n    expected = np.eye(4)\\n    R[0, :3] *= theta[6]\\n    R[1, :3] *= theta[7]\\n    R[2, :3] *= theta[8]\\n    expected[:3, :3] = R[:3, :3]\\n    expected[:3, 3] = theta[3:6]\\n    actual = transform.param_to_matrix(theta)\\n    assert_array_almost_equal(actual, expected)\\n\\n    # Test scaling matrix 3D\\n    transform = regtransforms[('SCALING', 3)]\\n    factor = rng.uniform()\\n    theta = np.array([factor])\\n    expected = np.array([[factor, 0, 0, 0],\\n                         [0, factor, 0, 0],\\n                         [0, 0, factor, 0],\\n                         [0, 0, 0, 1]])\\n    actual = transform.param_to_matrix(theta)\\n    assert_array_almost_equal(actual, expected)\\n\\n    # Test affine 3D\\n    transform = regtransforms[('AFFINE', 3)]\\n    theta = rng.uniform(size=(12,))\\n    expected = np.eye(4)\\n    expected[0, :] = theta[:4]\\n    expected[1, :] = theta[4:8]\\n    expected[2, :] = theta[8:12]\\n    actual = transform.param_to_matrix(theta)\\n    assert_array_almost_equal(actual, expected)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_transforms.py.txt'}),\n",
       " Document(page_content=\"# Test affine 3D\\n    transform = regtransforms[('AFFINE', 3)]\\n    theta = rng.uniform(size=(12,))\\n    expected = np.eye(4)\\n    expected[0, :] = theta[:4]\\n    expected[1, :] = theta[4:8]\\n    expected[2, :] = theta[8:12]\\n    actual = transform.param_to_matrix(theta)\\n    assert_array_almost_equal(actual, expected)\\n\\n    # Verify that ValueError is raised if incorrect number of parameters\\n    for transform in regtransforms.values():\\n        n = transform.get_number_of_parameters()\\n        # Set incorrect number of parameters\\n        theta = np.zeros(n + 1, dtype=np.float64)\\n        assert_raises(ValueError, transform.param_to_matrix, theta)\\n\\n\\ndef test_identity_parameters():\\n    for transform in regtransforms.values():\\n        dim = transform.get_dim()\\n        theta = transform.get_identity_parameters()\\n\\n        expected = np.eye(dim + 1)\\n        actual = transform.param_to_matrix(theta)\\n        assert_array_almost_equal(actual, expected)\\n\\n\\n@set_random_number_generator()\\ndef test_jacobian_functions(rng):\\n    # Compare the analytical Jacobians with their numerical approximations\\n    h = 1e-8\\n    nsamples = 50\\n\\n    for transform in regtransforms.values():\\n        n = transform.get_number_of_parameters()\\n        dim = transform.get_dim()\\n\\n        expected = np.empty((dim, n))\\n        theta = rng.uniform(size=(n,))\\n        T = transform.param_to_matrix(theta)\\n\\n        for j in range(nsamples):\\n            x = 255 * (rng.uniform(size=(dim,)) - 0.5)\\n            actual = transform.jacobian(theta, x)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_transforms.py.txt'}),\n",
       " Document(page_content=\"for transform in regtransforms.values():\\n        n = transform.get_number_of_parameters()\\n        dim = transform.get_dim()\\n\\n        expected = np.empty((dim, n))\\n        theta = rng.uniform(size=(n,))\\n        T = transform.param_to_matrix(theta)\\n\\n        for j in range(nsamples):\\n            x = 255 * (rng.uniform(size=(dim,)) - 0.5)\\n            actual = transform.jacobian(theta, x)\\n\\n            # Approximate with finite differences\\n            x_hom = np.ones(dim + 1)\\n            x_hom[:dim] = x[:]\\n            for i in range(n):\\n                dtheta = theta.copy()\\n                dtheta[i] += h\\n                dT = np.array(transform.param_to_matrix(dtheta))\\n                g = (dT - T).dot(x_hom) / h\\n                expected[:, i] = g[:dim]\\n\\n            assert_array_almost_equal(actual, expected, decimal=5)\\n\\n    # Test ValueError is raised when theta parameter doesn't have the right\\n    # length\\n    for transform in regtransforms.values():\\n        n = transform.get_number_of_parameters()\\n        # Wrong number of parameters\\n        theta = np.zeros(n + 1)\\n        x = np.zeros(dim)\\n        assert_raises(ValueError, transform.jacobian, theta, x)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_transforms.py.txt'}),\n",
       " Document(page_content=\"assert_array_almost_equal(actual, expected, decimal=5)\\n\\n    # Test ValueError is raised when theta parameter doesn't have the right\\n    # length\\n    for transform in regtransforms.values():\\n        n = transform.get_number_of_parameters()\\n        # Wrong number of parameters\\n        theta = np.zeros(n + 1)\\n        x = np.zeros(dim)\\n        assert_raises(ValueError, transform.jacobian, theta, x)\\n\\n\\ndef test_invalid_transform():\\n    # Note: users should not attempt to use the base class Transform:\\n    # they should get an instance of one of its derived classes from the\\n    # regtransforms dictionary (the base class is not contained there)\\n    # If for some reason the user instantiates it and attempts to use it,\\n    # however, it will raise exceptions when attempting to retrieve its\\n    # Jacobian, identity parameters or its matrix representation. It will\\n    # return -1 if queried about its dimension or number of parameters\\n    transform = Transform()\\n    theta = np.ndarray(3)\\n    x = np.ndarray(3)\\n    assert_raises(ValueError, transform.jacobian, theta, x)\\n    assert_raises(ValueError, transform.get_identity_parameters)\\n    assert_raises(ValueError, transform.param_to_matrix, theta)\\n\\n    expected = -1\\n    actual = transform.get_number_of_parameters()\\n    assert_equal(actual, expected)\\n\\n    actual = transform.get_dim()\\n    assert_equal(actual, expected)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_transforms.py.txt'}),\n",
       " Document(page_content='import numpy as np\\nfrom numpy.testing import (assert_array_equal,\\n                           assert_array_almost_equal,\\n                           assert_almost_equal,\\n                           assert_equal,\\n                           assert_raises)\\nfrom scipy.ndimage import map_coordinates\\nfrom nibabel.affines import apply_affine, from_matvec\\nfrom dipy.core import geometry\\nfrom dipy.align import floating\\nfrom dipy.align import imwarp\\nfrom dipy.align import vector_fields as vfu\\nfrom dipy.align.transforms import regtransforms\\nfrom dipy.align.parzenhist import sample_domain_regular\\nfrom dipy.testing.decorators import set_random_number_generator\\n\\n\\n@set_random_number_generator(3921116)\\ndef test_random_displacement_field_2d(rng):\\n    from_shape = (25, 32)\\n    to_shape = (33, 29)\\n\\n    # Create grid coordinates\\n    x_0 = np.asarray(range(from_shape[0]))\\n    x_1 = np.asarray(range(from_shape[1]))\\n    X = np.empty((3,) + from_shape, dtype=np.float64)\\n    O = np.ones(from_shape)\\n    X[0, ...] = x_0[:, None] * O\\n    X[1, ...] = x_1[None, :] * O\\n    X[2, ...] = 1\\n\\n    # Create an arbitrary image-to-space transform\\n    t = 0.15  # translation factor\\n\\n    trans = np.array([[1, 0, -t * from_shape[0]],\\n                      [0, 1, -t * from_shape[1]],\\n                      [0, 0, 1]])\\n    trans_inv = np.linalg.inv(trans)\\n\\n    for theta in [-1 * np.pi / 6.0, 0.0, np.pi / 5.0]:  # rotation angle\\n        for s in [0.83, 1.3, 2.07]:  # scale\\n            ct = np.cos(theta)\\n            st = np.sin(theta)\\n\\n            rot = np.array([[ct, -st, 0],\\n                            [st, ct, 0],\\n                            [0, 0, 1]])', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='trans = np.array([[1, 0, -t * from_shape[0]],\\n                      [0, 1, -t * from_shape[1]],\\n                      [0, 0, 1]])\\n    trans_inv = np.linalg.inv(trans)\\n\\n    for theta in [-1 * np.pi / 6.0, 0.0, np.pi / 5.0]:  # rotation angle\\n        for s in [0.83, 1.3, 2.07]:  # scale\\n            ct = np.cos(theta)\\n            st = np.sin(theta)\\n\\n            rot = np.array([[ct, -st, 0],\\n                            [st, ct, 0],\\n                            [0, 0, 1]])\\n\\n            scale = np.array([[1 * s, 0, 0],\\n                              [0, 1 * s, 0],\\n                              [0, 0, 1]])\\n\\n            from_grid2world = trans_inv.dot(scale.dot(rot.dot(trans)))\\n            to_grid2world = from_grid2world.dot(scale)\\n            to_world2grid = np.linalg.inv(to_grid2world)\\n\\n            field, assignment = vfu.create_random_displacement_2d(\\n                np.array(from_shape, dtype=np.int32), from_grid2world,\\n                np.array(to_shape, dtype=np.int32), to_grid2world, rng=rng)\\n            field = np.array(field, dtype=floating)\\n            assignment = np.array(assignment)\\n            # Verify the assignments are inside the requested region\\n            assert_equal(0, (assignment < 0).sum())\\n            for i in range(2):\\n                assert_equal(0, (assignment[..., i] >= to_shape[i]).sum())\\n\\n            # Compute the warping coordinates (see warp_2d documentation)\\n            Y = np.apply_along_axis(from_grid2world.dot, 0, X)[0:2, ...]\\n            Z = np.zeros_like(X)\\n            Z[0, ...] = Y[0, ...] + field[..., 0]\\n            Z[1, ...] = Y[1, ...] + field[..., 1]\\n            Z[2, ...] = 1\\n            W = np.apply_along_axis(to_world2grid.dot, 0, Z)[0:2, ...]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='# Compute the warping coordinates (see warp_2d documentation)\\n            Y = np.apply_along_axis(from_grid2world.dot, 0, X)[0:2, ...]\\n            Z = np.zeros_like(X)\\n            Z[0, ...] = Y[0, ...] + field[..., 0]\\n            Z[1, ...] = Y[1, ...] + field[..., 1]\\n            Z[2, ...] = 1\\n            W = np.apply_along_axis(to_world2grid.dot, 0, Z)[0:2, ...]\\n\\n            # Verify the claimed assignments are correct\\n            assert_array_almost_equal(W[0, ...], assignment[..., 0], 5)\\n            assert_array_almost_equal(W[1, ...], assignment[..., 1], 5)\\n\\n    # Test exception is raised when the affine transform matrix is not valid\\n    valid = np.zeros((2, 3), dtype=np.float64)\\n    invalid = np.zeros((2, 2), dtype=np.float64)\\n    shape = np.array(from_shape, dtype=np.int32)\\n    assert_raises(ValueError, vfu.create_random_displacement_2d,\\n                  shape, invalid, shape, valid, rng=rng)\\n    assert_raises(ValueError, vfu.create_random_displacement_2d,\\n                  shape, valid, shape, invalid, rng=rng)\\n\\n\\n@set_random_number_generator(7127562)\\ndef test_random_displacement_field_3d(rng):\\n    from_shape = (25, 32, 31)\\n    to_shape = (33, 29, 35)\\n\\n    # Create grid coordinates\\n    x_0 = np.asarray(range(from_shape[0]))\\n    x_1 = np.asarray(range(from_shape[1]))\\n    x_2 = np.asarray(range(from_shape[2]))\\n    X = np.empty((4,) + from_shape, dtype=np.float64)\\n    O = np.ones(from_shape)\\n    X[0, ...] = x_0[:, None, None] * O\\n    X[1, ...] = x_1[None, :, None] * O\\n    X[2, ...] = x_2[None, None, :] * O\\n    X[3, ...] = 1\\n\\n    # Select an arbitrary rotation axis\\n    axis = np.array([.5, 2.0, 1.5])', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='# Create grid coordinates\\n    x_0 = np.asarray(range(from_shape[0]))\\n    x_1 = np.asarray(range(from_shape[1]))\\n    x_2 = np.asarray(range(from_shape[2]))\\n    X = np.empty((4,) + from_shape, dtype=np.float64)\\n    O = np.ones(from_shape)\\n    X[0, ...] = x_0[:, None, None] * O\\n    X[1, ...] = x_1[None, :, None] * O\\n    X[2, ...] = x_2[None, None, :] * O\\n    X[3, ...] = 1\\n\\n    # Select an arbitrary rotation axis\\n    axis = np.array([.5, 2.0, 1.5])\\n\\n    # Create an arbitrary image-to-space transform\\n    t = 0.15  # translation factor\\n\\n    trans = np.array([[1, 0, 0, -t * from_shape[0]],\\n                      [0, 1, 0, -t * from_shape[1]],\\n                      [0, 0, 1, -t * from_shape[2]],\\n                      [0, 0, 0, 1]])\\n    trans_inv = np.linalg.inv(trans)\\n\\n    for theta in [-1 * np.pi / 6.0, 0.0, np.pi / 5.0]:  # rotation angle\\n        for s in [0.83, 1.3, 2.07]:  # scale\\n            rot = np.zeros(shape=(4, 4))\\n            rot[:3, :3] = geometry.rodrigues_axis_rotation(axis, theta)\\n            rot[3, 3] = 1.0\\n\\n            scale = np.array([[1 * s, 0, 0, 0],\\n                              [0, 1 * s, 0, 0],\\n                              [0, 0, 1 * s, 0],\\n                              [0, 0, 0, 1]])\\n\\n            from_grid2world = trans_inv.dot(scale.dot(rot.dot(trans)))\\n            to_grid2world = from_grid2world.dot(scale)\\n            to_world2grid = np.linalg.inv(to_grid2world)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='scale = np.array([[1 * s, 0, 0, 0],\\n                              [0, 1 * s, 0, 0],\\n                              [0, 0, 1 * s, 0],\\n                              [0, 0, 0, 1]])\\n\\n            from_grid2world = trans_inv.dot(scale.dot(rot.dot(trans)))\\n            to_grid2world = from_grid2world.dot(scale)\\n            to_world2grid = np.linalg.inv(to_grid2world)\\n\\n            field, assignment = vfu.create_random_displacement_3d(\\n                np.array(from_shape, dtype=np.int32), from_grid2world,\\n                np.array(to_shape, dtype=np.int32), to_grid2world, rng=rng)\\n            field = np.array(field, dtype=floating)\\n            assignment = np.array(assignment)\\n            # Verify the assignments are inside the requested region\\n            assert_equal(0, (assignment < 0).sum())\\n            for i in range(3):\\n                assert_equal(0, (assignment[..., i] >= to_shape[i]).sum())\\n\\n            # Compute the warping coordinates (see warp_2d documentation)\\n            Y = np.apply_along_axis(from_grid2world.dot, 0, X)[0:3, ...]\\n            Z = np.zeros_like(X)\\n            Z[0, ...] = Y[0, ...] + field[..., 0]\\n            Z[1, ...] = Y[1, ...] + field[..., 1]\\n            Z[2, ...] = Y[2, ...] + field[..., 2]\\n            Z[3, ...] = 1\\n            W = np.apply_along_axis(to_world2grid.dot, 0, Z)[0:3, ...]\\n\\n            # Verify the claimed assignments are correct\\n            assert_array_almost_equal(W[0, ...], assignment[..., 0], 5)\\n            assert_array_almost_equal(W[1, ...], assignment[..., 1], 5)\\n            assert_array_almost_equal(W[2, ...], assignment[..., 2], 5)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='# Verify the claimed assignments are correct\\n            assert_array_almost_equal(W[0, ...], assignment[..., 0], 5)\\n            assert_array_almost_equal(W[1, ...], assignment[..., 1], 5)\\n            assert_array_almost_equal(W[2, ...], assignment[..., 2], 5)\\n\\n    # Test exception is raised when the affine transform matrix is not valid\\n    valid = np.zeros((3, 4), dtype=np.float64)\\n    invalid = np.zeros((3, 3), dtype=np.float64)\\n    shape = np.array(from_shape, dtype=np.int32)\\n    assert_raises(ValueError, vfu.create_random_displacement_2d,\\n                  shape, invalid, shape, valid, rng=rng)\\n    assert_raises(ValueError, vfu.create_random_displacement_2d,\\n                  shape, valid, shape, invalid, rng=rng)\\n\\n\\ndef test_harmonic_fields_2d():\\n    nrows = 64\\n    ncols = 67\\n    mid_row = nrows // 2\\n    mid_col = ncols // 2\\n    expected_d = np.empty(shape=(nrows, ncols, 2))\\n    expected_d_inv = np.empty(shape=(nrows, ncols, 2))\\n    for b in [0.1, 0.3, 0.7]:\\n        for m in [2, 4, 7]:\\n            for i in range(nrows):\\n                for j in range(ncols):\\n                    ii = i - mid_row\\n                    jj = j - mid_col\\n                    theta = np.arctan2(ii, jj)\\n                    expected_d[i, j, 0] =\\\\\\n                        ii * (1.0 / (1 + b * np.cos(m * theta)) - 1.0)\\n                    expected_d[i, j, 1] =\\\\\\n                        jj * (1.0 / (1 + b * np.cos(m * theta)) - 1.0)\\n                    expected_d_inv[i, j, 0] = b * np.cos(m * theta) * ii\\n                    expected_d_inv[i, j, 1] = b * np.cos(m * theta) * jj', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='actual_d, actual_d_inv =\\\\\\n                vfu.create_harmonic_fields_2d(nrows, ncols, b, m)\\n            assert_array_almost_equal(expected_d, actual_d)\\n            assert_array_almost_equal(expected_d_inv, expected_d_inv)\\n\\n\\ndef test_harmonic_fields_3d():\\n    nslices = 25\\n    nrows = 34\\n    ncols = 37\\n    mid_slice = nslices // 2\\n    mid_row = nrows // 2\\n    mid_col = ncols // 2\\n    expected_d = np.empty(shape=(nslices, nrows, ncols, 3))\\n    expected_d_inv = np.empty(shape=(nslices, nrows, ncols, 3))\\n    for b in [0.3, 0.7]:\\n        for m in [2, 5]:\\n            for k in range(nslices):\\n                for i in range(nrows):\\n                    for j in range(ncols):\\n                        kk = k - mid_slice\\n                        ii = i - mid_row\\n                        jj = j - mid_col\\n                        theta = np.arctan2(ii, jj)\\n                        expected_d[k, i, j, 0] =\\\\\\n                            kk * (1.0 / (1 + b * np.cos(m * theta)) - 1.0)\\n                        expected_d[k, i, j, 1] =\\\\\\n                            ii * (1.0 / (1 + b * np.cos(m * theta)) - 1.0)\\n                        expected_d[k, i, j, 2] =\\\\\\n                            jj * (1.0 / (1 + b * np.cos(m * theta)) - 1.0)\\n                        expected_d_inv[k, i, j, 0] = b * np.cos(m * theta) * kk\\n                        expected_d_inv[k, i, j, 1] = b * np.cos(m * theta) * ii\\n                        expected_d_inv[k, i, j, 2] = b * np.cos(m * theta) * jj', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='actual_d, actual_d_inv =\\\\\\n                vfu.create_harmonic_fields_3d(nslices, nrows, ncols, b, m)\\n            assert_array_almost_equal(expected_d, actual_d)\\n            assert_array_almost_equal(expected_d_inv, expected_d_inv)\\n\\n\\ndef test_circle():\\n    sh = (64, 61)\\n    cr = sh[0] // 2\\n    cc = sh[1] // 2\\n    x_0 = np.asarray(range(sh[0]))\\n    x_1 = np.asarray(range(sh[1]))\\n    X = np.empty((2,) + sh, dtype=np.float64)\\n    O = np.ones(sh)\\n    X[0, ...] = x_0[:, None] * O - cr\\n    X[1, ...] = x_1[None, :] * O - cc\\n    nrm = np.sqrt(np.sum(X ** 2, axis=0))\\n    for radius in [0, 7, 17, 32]:\\n        expected = nrm <= radius\\n        actual = vfu.create_circle(sh[0], sh[1], radius)\\n        assert_array_almost_equal(actual, expected)\\n\\n\\ndef test_sphere():\\n    sh = (64, 61, 57)\\n    cs = sh[0] // 2\\n    cr = sh[1] // 2\\n    cc = sh[2] // 2\\n    x_0 = np.asarray(range(sh[0]))\\n    x_1 = np.asarray(range(sh[1]))\\n    x_2 = np.asarray(range(sh[2]))\\n    X = np.empty((3,) + sh, dtype=np.float64)\\n    O = np.ones(sh)\\n    X[0, ...] = x_0[:, None, None] * O - cs\\n    X[1, ...] = x_1[None, :, None] * O - cr\\n    X[2, ...] = x_2[None, None, :] * O - cc\\n    nrm = np.sqrt(np.sum(X ** 2, axis=0))\\n    for radius in [0, 7, 17, 32]:\\n        expected = nrm <= radius\\n        actual = vfu.create_sphere(sh[0], sh[1], sh[2], radius)\\n        assert_array_almost_equal(actual, expected)\\n\\ndef test_warping_2d():\\n    r\"\"\"\\n    Tests the cython implementation of the 2d warpings against scipy\\n    \"\"\"\\n    sh = (64, 64)\\n    nr = sh[0]\\n    nc = sh[1]\\n\\n    # Create an image of a circle\\n    radius = 24\\n    circle = vfu.create_circle(nr, nc, radius)\\n    circle = np.array(circle, dtype=floating)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='def test_warping_2d():\\n    r\"\"\"\\n    Tests the cython implementation of the 2d warpings against scipy\\n    \"\"\"\\n    sh = (64, 64)\\n    nr = sh[0]\\n    nc = sh[1]\\n\\n    # Create an image of a circle\\n    radius = 24\\n    circle = vfu.create_circle(nr, nc, radius)\\n    circle = np.array(circle, dtype=floating)\\n\\n    # Create a displacement field for warping\\n    d, dinv = vfu.create_harmonic_fields_2d(nr, nc, 0.2, 8)\\n    d = np.asarray(d).astype(floating)\\n\\n    # Create grid coordinates\\n    x_0 = np.asarray(range(sh[0]))\\n    x_1 = np.asarray(range(sh[1]))\\n    X = np.empty((3,) + sh, dtype=np.float64)\\n    O = np.ones(sh)\\n    X[0, ...] = x_0[:, None] * O\\n    X[1, ...] = x_1[None, :] * O\\n    X[2, ...] = 1\\n\\n    # Select an arbitrary translation matrix\\n    t = 0.1\\n    trans = np.array([[1, 0, -t * nr],\\n                      [0, 1, -t * nc],\\n                      [0, 0, 1]])\\n    trans_inv = np.linalg.inv(trans)\\n\\n    # Select arbitrary rotation and scaling matrices\\n    for theta in [-1 * np.pi / 6.0, 0.0, np.pi / 6.0]:  # rotation angle\\n        for s in [0.42, 1.3, 2.15]:  # scale\\n            ct = np.cos(theta)\\n            st = np.sin(theta)\\n\\n            rot = np.array([[ct, -st, 0],\\n                            [st, ct, 0],\\n                            [0, 0, 1]])\\n\\n            scale = np.array([[1 * s, 0, 0],\\n                              [0, 1 * s, 0],\\n                              [0, 0, 1]])\\n\\n            aff = trans_inv.dot(scale.dot(rot.dot(trans)))', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='rot = np.array([[ct, -st, 0],\\n                            [st, ct, 0],\\n                            [0, 0, 1]])\\n\\n            scale = np.array([[1 * s, 0, 0],\\n                              [0, 1 * s, 0],\\n                              [0, 0, 1]])\\n\\n            aff = trans_inv.dot(scale.dot(rot.dot(trans)))\\n\\n            # Select arbitrary (but different) grid-to-space transforms\\n            sampling_grid2world = scale\\n            field_grid2world = aff\\n            field_world2grid = np.linalg.inv(field_grid2world)\\n            image_grid2world = aff.dot(scale)\\n            image_world2grid = np.linalg.inv(image_grid2world)\\n\\n            A = field_world2grid.dot(sampling_grid2world)\\n            B = image_world2grid.dot(sampling_grid2world)\\n            C = image_world2grid\\n\\n            # Reorient the displacement field according to its grid-to-space\\n            # transform\\n            dcopy = np.copy(d)\\n            vfu.reorient_vector_field_2d(dcopy, field_grid2world)\\n            extended_dcopy = np.zeros((nr + 2, nc + 2, 2), dtype=floating)\\n            extended_dcopy[1:nr + 1, 1:nc + 1, :] = dcopy\\n\\n            # Compute the warping coordinates (see warp_2d documentation)\\n            Y = np.apply_along_axis(A.dot, 0, X)[0:2, ...]\\n            Z = np.zeros_like(X)\\n            Z[0, ...] = map_coordinates(extended_dcopy[..., 0], Y + 1, order=1)\\n            Z[1, ...] = map_coordinates(extended_dcopy[..., 1], Y + 1, order=1)\\n            Z[2, ...] = 0\\n            Z = np.apply_along_axis(C.dot, 0, Z)[0:2, ...]\\n            T = np.apply_along_axis(B.dot, 0, X)[0:2, ...]\\n            W = T + Z', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='# Compute the warping coordinates (see warp_2d documentation)\\n            Y = np.apply_along_axis(A.dot, 0, X)[0:2, ...]\\n            Z = np.zeros_like(X)\\n            Z[0, ...] = map_coordinates(extended_dcopy[..., 0], Y + 1, order=1)\\n            Z[1, ...] = map_coordinates(extended_dcopy[..., 1], Y + 1, order=1)\\n            Z[2, ...] = 0\\n            Z = np.apply_along_axis(C.dot, 0, Z)[0:2, ...]\\n            T = np.apply_along_axis(B.dot, 0, X)[0:2, ...]\\n            W = T + Z\\n\\n            # Test bilinear interpolation\\n            expected = map_coordinates(circle, W, order=1)\\n            warped = vfu.warp_2d(circle, dcopy, A, B, C,\\n                                 np.array(sh, dtype=np.int32))\\n            assert_array_almost_equal(warped, expected)\\n\\n            # Test nearest neighbor interpolation\\n            expected = map_coordinates(circle, W, order=0)\\n            warped = vfu.warp_2d_nn(circle, dcopy, A, B, C,\\n                                    np.array(sh, dtype=np.int32))\\n            assert_array_almost_equal(warped, expected)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='# Test nearest neighbor interpolation\\n            expected = map_coordinates(circle, W, order=0)\\n            warped = vfu.warp_2d_nn(circle, dcopy, A, B, C,\\n                                    np.array(sh, dtype=np.int32))\\n            assert_array_almost_equal(warped, expected)\\n\\n    # Test exception is raised when the affine transform matrix is not valid\\n    val = np.zeros((2, 3), dtype=np.float64)\\n    inval = np.zeros((2, 2), dtype=np.float64)\\n    sh = np.array(sh, dtype=np.int32)\\n    # Exceptions from warp_2d\\n    assert_raises(ValueError, vfu.warp_2d, circle, d, inval, val, val, sh)\\n    assert_raises(ValueError, vfu.warp_2d, circle, d, val, inval, val, sh)\\n    assert_raises(ValueError, vfu.warp_2d, circle, d, val, val, inval, sh)\\n    # Exceptions from warp_2d_nn\\n    assert_raises(ValueError, vfu.warp_2d_nn, circle, d, inval, val, val, sh)\\n    assert_raises(ValueError, vfu.warp_2d_nn, circle, d, val, inval, val, sh)\\n    assert_raises(ValueError, vfu.warp_2d_nn, circle, d, val, val, inval, sh)\\n\\n\\ndef test_warping_3d():\\n    r\"\"\"\\n    Tests the cython implementation of the 2d warpings against scipy\\n    \"\"\"\\n    sh = (64, 64, 64)\\n    ns = sh[0]\\n    nr = sh[1]\\n    nc = sh[2]\\n\\n    # Create an image of a sphere\\n    radius = 24\\n    sphere = vfu.create_sphere(ns, nr, nc, radius)\\n    sphere = np.array(sphere, dtype=floating)\\n\\n    # Create a displacement field for warping\\n    d, dinv = vfu.create_harmonic_fields_3d(ns, nr, nc, 0.2, 8)\\n    d = np.asarray(d).astype(floating)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='def test_warping_3d():\\n    r\"\"\"\\n    Tests the cython implementation of the 2d warpings against scipy\\n    \"\"\"\\n    sh = (64, 64, 64)\\n    ns = sh[0]\\n    nr = sh[1]\\n    nc = sh[2]\\n\\n    # Create an image of a sphere\\n    radius = 24\\n    sphere = vfu.create_sphere(ns, nr, nc, radius)\\n    sphere = np.array(sphere, dtype=floating)\\n\\n    # Create a displacement field for warping\\n    d, dinv = vfu.create_harmonic_fields_3d(ns, nr, nc, 0.2, 8)\\n    d = np.asarray(d).astype(floating)\\n\\n    # Create grid coordinates\\n    x_0 = np.asarray(range(sh[0]))\\n    x_1 = np.asarray(range(sh[1]))\\n    x_2 = np.asarray(range(sh[2]))\\n    X = np.empty((4,) + sh, dtype=np.float64)\\n    O = np.ones(sh)\\n    X[0, ...] = x_0[:, None, None] * O\\n    X[1, ...] = x_1[None, :, None] * O\\n    X[2, ...] = x_2[None, None, :] * O\\n    X[3, ...] = 1\\n\\n    # Select an arbitrary rotation axis\\n    axis = np.array([.5, 2.0, 1.5])\\n    # Select an arbitrary translation matrix\\n    t = 0.1\\n    trans = np.array([[1, 0, 0, -t * ns],\\n                      [0, 1, 0, -t * nr],\\n                      [0, 0, 1, -t * nc],\\n                      [0, 0, 0, 1]])\\n    trans_inv = np.linalg.inv(trans)\\n\\n    # Select arbitrary rotation and scaling matrices\\n    for theta in [-1 * np.pi / 5.0, 0.0, np.pi / 5.0]:  # rotation angle\\n        for s in [0.45, 1.1, 2.0]:  # scale\\n            rot = np.zeros(shape=(4, 4))\\n            rot[:3, :3] = geometry.rodrigues_axis_rotation(axis, theta)\\n            rot[3, 3] = 1.0\\n\\n            scale = np.array([[1 * s, 0, 0, 0],\\n                              [0, 1 * s, 0, 0],\\n                              [0, 0, 1 * s, 0],\\n                              [0, 0, 0, 1]])\\n\\n            aff = trans_inv.dot(scale.dot(rot.dot(trans)))', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='scale = np.array([[1 * s, 0, 0, 0],\\n                              [0, 1 * s, 0, 0],\\n                              [0, 0, 1 * s, 0],\\n                              [0, 0, 0, 1]])\\n\\n            aff = trans_inv.dot(scale.dot(rot.dot(trans)))\\n\\n            # Select arbitrary (but different) grid-to-space transforms\\n            sampling_grid2world = scale\\n            field_grid2world = aff\\n            field_world2grid = np.linalg.inv(field_grid2world)\\n            image_grid2world = aff.dot(scale)\\n            image_world2grid = np.linalg.inv(image_grid2world)\\n\\n            A = field_world2grid.dot(sampling_grid2world)\\n            B = image_world2grid.dot(sampling_grid2world)\\n            C = image_world2grid\\n\\n            # Reorient the displacement field according to its grid-to-space\\n            # transform\\n            dcopy = np.copy(d)\\n            vfu.reorient_vector_field_3d(dcopy, field_grid2world)\\n\\n            extended_dcopy = np.zeros(\\n                (ns + 2, nr + 2, nc + 2, 3), dtype=floating)\\n            extended_dcopy[1:ns + 1, 1:nr + 1, 1:nc + 1, :] = dcopy\\n\\n            # Compute the warping coordinates (see warp_2d documentation)\\n            Y = np.apply_along_axis(A.dot, 0, X)[0:3, ...]\\n            Z = np.zeros_like(X)\\n            Z[0, ...] = map_coordinates(extended_dcopy[..., 0], Y + 1, order=1)\\n            Z[1, ...] = map_coordinates(extended_dcopy[..., 1], Y + 1, order=1)\\n            Z[2, ...] = map_coordinates(extended_dcopy[..., 2], Y + 1, order=1)\\n            Z[3, ...] = 0\\n            Z = np.apply_along_axis(C.dot, 0, Z)[0:3, ...]\\n            T = np.apply_along_axis(B.dot, 0, X)[0:3, ...]\\n            W = T + Z', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='# Test bilinear interpolation\\n            expected = map_coordinates(sphere, W, order=1)\\n            warped = vfu.warp_3d(sphere, dcopy, A, B, C,\\n                                 np.array(sh, dtype=np.int32))\\n            assert_array_almost_equal(warped, expected, decimal=5)\\n\\n            # Test nearest neighbor interpolation\\n            expected = map_coordinates(sphere, W, order=0)\\n            warped = vfu.warp_3d_nn(sphere, dcopy, A, B, C,\\n                                    np.array(sh, dtype=np.int32))\\n            assert_array_almost_equal(warped, expected, decimal=5)\\n\\n    # Test exception is raised when the affine transform matrix is not valid\\n    val = np.zeros((3, 4), dtype=np.float64)\\n    inval = np.zeros((3, 3), dtype=np.float64)\\n    sh = np.array(sh, dtype=np.int32)\\n    # Exceptions from warp_3d\\n    assert_raises(ValueError, vfu.warp_3d, sphere, d, inval, val, val, sh)\\n    assert_raises(ValueError, vfu.warp_3d, sphere, d, val, inval, val, sh)\\n    assert_raises(ValueError, vfu.warp_3d, sphere, d, val, val, inval, sh)\\n    # Exceptions from warp_3d_nn\\n    assert_raises(ValueError, vfu.warp_3d_nn, sphere, d, inval, val, val, sh)\\n    assert_raises(ValueError, vfu.warp_3d_nn, sphere, d, val, inval, val, sh)\\n    assert_raises(ValueError, vfu.warp_3d_nn, sphere, d, val, val, inval, sh)\\n\\n\\ndef test_affine_transforms_2d():\\n    r\"\"\"\\n    Tests 2D affine transform functions against scipy implementation\\n    \"\"\"\\n    # Create a simple invertible affine transform\\n    d_shape = (64, 64)\\n    codomain_shape = (80, 80)\\n    nr = d_shape[0]\\n    nc = d_shape[1]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='def test_affine_transforms_2d():\\n    r\"\"\"\\n    Tests 2D affine transform functions against scipy implementation\\n    \"\"\"\\n    # Create a simple invertible affine transform\\n    d_shape = (64, 64)\\n    codomain_shape = (80, 80)\\n    nr = d_shape[0]\\n    nc = d_shape[1]\\n\\n    # Create an image of a circle\\n    radius = 16\\n    circle = vfu.create_circle(codomain_shape[0], codomain_shape[1], radius)\\n    circle = np.array(circle, dtype=floating)\\n\\n    # Create grid coordinates\\n    x_0 = np.asarray(range(d_shape[0]))\\n    x_1 = np.asarray(range(d_shape[1]))\\n    X = np.empty((3,) + d_shape, dtype=np.float64)\\n    O = np.ones(d_shape)\\n    X[0, ...] = x_0[:, None] * O\\n    X[1, ...] = x_1[None, :] * O\\n    X[2, ...] = 1\\n\\n    # Generate affine transforms\\n    t = 0.3\\n    trans = np.array([[1, 0, -t * nr],\\n                      [0, 1, -t * nc],\\n                      [0, 0, 1]])\\n    trans_inv = np.linalg.inv(trans)\\n    for theta in [-1 * np.pi / 5.0, 0.0, np.pi / 5.0]:  # rotation angle\\n        for s in [0.5, 1.0, 2.0]:  # scale\\n            ct = np.cos(theta)\\n            st = np.sin(theta)\\n\\n            rot = np.array([[ct, -st, 0],\\n                            [st, ct, 0],\\n                            [0, 0, 1]])\\n\\n            scale = np.array([[1 * s, 0, 0],\\n                              [0, 1 * s, 0],\\n                              [0, 0, 1]])\\n\\n            gt_affine = trans_inv.dot(scale.dot(rot.dot(trans)))\\n\\n            # Apply the affine transform to the grid coordinates\\n            Y = np.apply_along_axis(gt_affine.dot, 0, X)[0:2, ...]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='rot = np.array([[ct, -st, 0],\\n                            [st, ct, 0],\\n                            [0, 0, 1]])\\n\\n            scale = np.array([[1 * s, 0, 0],\\n                              [0, 1 * s, 0],\\n                              [0, 0, 1]])\\n\\n            gt_affine = trans_inv.dot(scale.dot(rot.dot(trans)))\\n\\n            # Apply the affine transform to the grid coordinates\\n            Y = np.apply_along_axis(gt_affine.dot, 0, X)[0:2, ...]\\n\\n            expected = map_coordinates(circle, Y, order=1)\\n            warped = vfu.transform_2d_affine(\\n                circle, np.array(\\n                    d_shape, dtype=np.int32), gt_affine)\\n            assert_array_almost_equal(warped, expected)\\n\\n            # Test affine warping with nearest-neighbor interpolation\\n            expected = map_coordinates(circle, Y, order=0)\\n            warped = vfu.transform_2d_affine_nn(\\n                circle, np.array(d_shape, dtype=np.int32), gt_affine)\\n            assert_array_almost_equal(warped, expected)\\n\\n    # Test the affine = None case\\n    warped = vfu.transform_2d_affine(\\n        circle, np.array(\\n            codomain_shape, dtype=np.int32), None)\\n    assert_array_equal(warped, circle)\\n\\n    warped = vfu.transform_2d_affine_nn(\\n        circle, np.array(\\n            codomain_shape, dtype=np.int32), None)\\n    assert_array_equal(warped, circle)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='# Test the affine = None case\\n    warped = vfu.transform_2d_affine(\\n        circle, np.array(\\n            codomain_shape, dtype=np.int32), None)\\n    assert_array_equal(warped, circle)\\n\\n    warped = vfu.transform_2d_affine_nn(\\n        circle, np.array(\\n            codomain_shape, dtype=np.int32), None)\\n    assert_array_equal(warped, circle)\\n\\n    # Test exception is raised when the affine transform matrix is not valid\\n    invalid = np.zeros((2, 2), dtype=np.float64)\\n    invalid_nan = np.zeros((3, 3), dtype=np.float64)\\n    invalid_nan[1, 1] = np.nan\\n    shape = np.array(codomain_shape, dtype=np.int32)\\n    # Exceptions from transform_2d\\n    assert_raises(ValueError, vfu.transform_2d_affine, circle, shape, invalid)\\n    assert_raises(\\n        ValueError,\\n        vfu.transform_2d_affine,\\n        circle,\\n        shape,\\n        invalid_nan)\\n    # Exceptions from transform_2d_nn\\n    assert_raises(\\n        ValueError,\\n        vfu.transform_2d_affine_nn,\\n        circle,\\n        shape,\\n        invalid)\\n    assert_raises(\\n        ValueError,\\n        vfu.transform_2d_affine_nn,\\n        circle,\\n        shape,\\n        invalid_nan)\\n\\n\\ndef test_affine_transforms_3d():\\n    r\"\"\"\\n    Tests 3D affine transform functions against scipy implementation\\n    \"\"\"\\n    # Create a simple invertible affine transform\\n    d_shape = (64, 64, 64)\\n    codomain_shape = (80, 80, 80)\\n    ns = d_shape[0]\\n    nr = d_shape[1]\\n    nc = d_shape[2]\\n\\n    # Create an image of a sphere\\n    radius = 16\\n    sphere = vfu.create_sphere(codomain_shape[0], codomain_shape[1],\\n                               codomain_shape[2], radius)\\n    sphere = np.array(sphere, dtype=floating)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='# Create an image of a sphere\\n    radius = 16\\n    sphere = vfu.create_sphere(codomain_shape[0], codomain_shape[1],\\n                               codomain_shape[2], radius)\\n    sphere = np.array(sphere, dtype=floating)\\n\\n    # Create grid coordinates\\n    x_0 = np.asarray(range(d_shape[0]))\\n    x_1 = np.asarray(range(d_shape[1]))\\n    x_2 = np.asarray(range(d_shape[2]))\\n    X = np.empty((4,) + d_shape, dtype=np.float64)\\n    O = np.ones(d_shape)\\n    X[0, ...] = x_0[:, None, None] * O\\n    X[1, ...] = x_1[None, :, None] * O\\n    X[2, ...] = x_2[None, None, :] * O\\n    X[3, ...] = 1\\n\\n    # Generate affine transforms\\n    # Select an arbitrary rotation axis\\n    axis = np.array([.5, 2.0, 1.5])\\n    t = 0.3\\n    trans = np.array([[1, 0, 0, -t * ns],\\n                      [0, 1, 0, -t * nr],\\n                      [0, 0, 1, -t * nc],\\n                      [0, 0, 0, 1]])\\n    trans_inv = np.linalg.inv(trans)\\n    for theta in [-1 * np.pi / 5.0, 0.0, np.pi / 5.0]:  # rotation angle\\n        for s in [0.45, 1.1, 2.3]:  # scale\\n            rot = np.zeros(shape=(4, 4))\\n            rot[:3, :3] = geometry.rodrigues_axis_rotation(axis, theta)\\n            rot[3, 3] = 1.0\\n\\n            scale = np.array([[1 * s, 0, 0, 0],\\n                              [0, 1 * s, 0, 0],\\n                              [0, 0, 1 * s, 0],\\n                              [0, 0, 0, 1]])\\n\\n            gt_affine = trans_inv.dot(scale.dot(rot.dot(trans)))\\n\\n            # Apply the affine transform to the grid coordinates\\n            Y = np.apply_along_axis(gt_affine.dot, 0, X)[0:3, ...]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='scale = np.array([[1 * s, 0, 0, 0],\\n                              [0, 1 * s, 0, 0],\\n                              [0, 0, 1 * s, 0],\\n                              [0, 0, 0, 1]])\\n\\n            gt_affine = trans_inv.dot(scale.dot(rot.dot(trans)))\\n\\n            # Apply the affine transform to the grid coordinates\\n            Y = np.apply_along_axis(gt_affine.dot, 0, X)[0:3, ...]\\n\\n            expected = map_coordinates(sphere, Y, order=1)\\n            transformed = vfu.transform_3d_affine(\\n                sphere, np.array(d_shape, dtype=np.int32), gt_affine)\\n            assert_array_almost_equal(transformed, expected)\\n\\n            # Test affine transform with nearest-neighbor interpolation\\n            expected = map_coordinates(sphere, Y, order=0)\\n            transformed = vfu.transform_3d_affine_nn(\\n                sphere, np.array(d_shape, dtype=np.int32), gt_affine)\\n            assert_array_almost_equal(transformed, expected)\\n\\n    # Test the affine = None case\\n    transformed = vfu.transform_3d_affine(\\n        sphere, np.array(codomain_shape, dtype=np.int32), None)\\n    assert_array_equal(transformed, sphere)\\n\\n    transformed = vfu.transform_3d_affine_nn(\\n        sphere, np.array(codomain_shape, dtype=np.int32), None)\\n    assert_array_equal(transformed, sphere)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='# Test the affine = None case\\n    transformed = vfu.transform_3d_affine(\\n        sphere, np.array(codomain_shape, dtype=np.int32), None)\\n    assert_array_equal(transformed, sphere)\\n\\n    transformed = vfu.transform_3d_affine_nn(\\n        sphere, np.array(codomain_shape, dtype=np.int32), None)\\n    assert_array_equal(transformed, sphere)\\n\\n    # Test exception is raised when the affine transform matrix is not valid\\n    invalid = np.zeros((3, 3), dtype=np.float64)\\n    invalid_nan = np.zeros((4, 4), dtype=np.float64)\\n    invalid_nan[1, 1] = np.nan\\n    shape = np.array(codomain_shape, dtype=np.int32)\\n    # Exceptions from transform_3d_affine\\n    assert_raises(ValueError, vfu.transform_3d_affine, sphere, shape, invalid)\\n    assert_raises(\\n        ValueError,\\n        vfu.transform_3d_affine,\\n        sphere,\\n        shape,\\n        invalid_nan)\\n    # Exceptions from transform_3d_affine_nn\\n    assert_raises(\\n        ValueError,\\n        vfu.transform_3d_affine_nn,\\n        sphere,\\n        shape,\\n        invalid)\\n    assert_raises(\\n        ValueError,\\n        vfu.transform_3d_affine_nn,\\n        sphere,\\n        shape,\\n        invalid_nan)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='@set_random_number_generator(8315759)\\ndef test_compose_vector_fields_2d(rng):\\n    r\"\"\"\\n    Creates two random displacement field that exactly map pixels from an input\\n    image to an output image. The resulting displacements and their\\n    composition, although operating in physical space, map the points exactly\\n    (up to numerical precision).\\n    \"\"\"\\n    input_shape = (10, 10)\\n    tgt_sh = (10, 10)\\n    # create a simple affine transformation\\n    nr = input_shape[0]\\n    nc = input_shape[1]\\n    s = 1.5\\n    t = 2.5\\n    trans = np.array([[1, 0, -t * nr],\\n                      [0, 1, -t * nc],\\n                      [0, 0, 1]])\\n    trans_inv = np.linalg.inv(trans)\\n    scale = np.array([[1 * s, 0, 0],\\n                      [0, 1 * s, 0],\\n                      [0, 0, 1]])\\n    gt_affine = trans_inv.dot(scale.dot(trans))\\n\\n    # create two random displacement fields\\n    input_grid2world = gt_affine\\n    target_grid2world = gt_affine\\n\\n    disp1, assign1 = vfu.create_random_displacement_2d(\\n        np.array(input_shape, dtype=np.int32),\\n        input_grid2world,\\n        np.array(tgt_sh, dtype=np.int32),\\n        target_grid2world, rng=rng)\\n    disp1 = np.array(disp1, dtype=floating)\\n    assign1 = np.array(assign1)\\n\\n    disp2, assign2 = vfu.create_random_displacement_2d(\\n        np.array(input_shape, dtype=np.int32),\\n        input_grid2world,\\n        np.array(tgt_sh, dtype=np.int32),\\n        target_grid2world, rng=rng)\\n    disp2 = np.array(disp2, dtype=floating)\\n    assign2 = np.array(assign2)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content=\"disp2, assign2 = vfu.create_random_displacement_2d(\\n        np.array(input_shape, dtype=np.int32),\\n        input_grid2world,\\n        np.array(tgt_sh, dtype=np.int32),\\n        target_grid2world, rng=rng)\\n    disp2 = np.array(disp2, dtype=floating)\\n    assign2 = np.array(assign2)\\n\\n    # create a random image (with decimal digits) to warp\\n    moving_image = np.empty(tgt_sh, dtype=floating)\\n    moving_image[...] =\\\\\\n        rng.integers(0, 10, np.size(moving_image)).reshape(tuple(tgt_sh))\\n    # set boundary values to zero so we don't test wrong interpolation due to\\n    # floating point precision\\n    moving_image[0, :] = 0\\n    moving_image[-1, :] = 0\\n    moving_image[:, 0] = 0\\n    moving_image[:, -1] = 0\\n\\n    # evaluate the composed warping using the exact assignments\\n    # (first 1 then 2)\\n    warp1 = moving_image[(assign2[..., 0], assign2[..., 1])]\\n    expected = warp1[(assign1[..., 0], assign1[..., 1])]\\n\\n    # compose the displacement fields\\n    target_world2grid = np.linalg.inv(target_grid2world)\\n    premult_index = target_world2grid.dot(input_grid2world)\\n    premult_disp = target_world2grid\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='# evaluate the composed warping using the exact assignments\\n    # (first 1 then 2)\\n    warp1 = moving_image[(assign2[..., 0], assign2[..., 1])]\\n    expected = warp1[(assign1[..., 0], assign1[..., 1])]\\n\\n    # compose the displacement fields\\n    target_world2grid = np.linalg.inv(target_grid2world)\\n    premult_index = target_world2grid.dot(input_grid2world)\\n    premult_disp = target_world2grid\\n\\n    for time_scaling in [0.25, 0.5, 1.0, 2.0, 4.0]:\\n        composition, stats = vfu.compose_vector_fields_2d(disp1,\\n                                                          disp2 / time_scaling,\\n                                                          premult_index,\\n                                                          premult_disp,\\n                                                          time_scaling, None)\\n        # apply the implementation under test\\n        warped = np.array(vfu.warp_2d(moving_image, composition, None,\\n                                      premult_index, premult_disp))\\n        assert_array_almost_equal(warped, expected)\\n\\n        # test also using nearest neighbor interpolation\\n        warped = np.array(vfu.warp_2d_nn(moving_image, composition, None,\\n                                         premult_index, premult_disp))\\n        assert_array_almost_equal(warped, expected)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='# test also using nearest neighbor interpolation\\n        warped = np.array(vfu.warp_2d_nn(moving_image, composition, None,\\n                                         premult_index, premult_disp))\\n        assert_array_almost_equal(warped, expected)\\n\\n        # test updating the displacement field instead of creating a new one\\n        composition = disp1.copy()\\n        vfu.compose_vector_fields_2d(composition, disp2 / time_scaling,\\n                                     premult_index, premult_disp, time_scaling,\\n                                     composition)\\n        # apply the implementation under test\\n        warped = np.array(vfu.warp_2d(moving_image, composition, None,\\n                                      premult_index, premult_disp))\\n        assert_array_almost_equal(warped, expected)\\n\\n        # test also using nearest neighbor interpolation\\n        warped = np.array(vfu.warp_2d_nn(moving_image, composition, None,\\n                                         premult_index, premult_disp))\\n        assert_array_almost_equal(warped, expected)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='# test also using nearest neighbor interpolation\\n        warped = np.array(vfu.warp_2d_nn(moving_image, composition, None,\\n                                         premult_index, premult_disp))\\n        assert_array_almost_equal(warped, expected)\\n\\n    # Test non-overlapping case\\n    x_0 = np.asarray(range(input_shape[0]))\\n    x_1 = np.asarray(range(input_shape[1]))\\n    X = np.empty(input_shape + (2,), dtype=np.float64)\\n    O = np.ones(input_shape)\\n    X[..., 0] = x_0[:, None] * O\\n    X[..., 1] = x_1[None, :] * O\\n    random_labels = rng.integers(\\n        0, 2, input_shape[0] * input_shape[1] * 2)\\n    random_labels = random_labels.reshape(input_shape + (2,))\\n    values = np.array([-1, tgt_sh[0]])\\n    disp1 = (values[random_labels] - X).astype(floating)\\n    composition, stats = vfu.compose_vector_fields_2d(disp1,\\n                                                      disp2,\\n                                                      None,\\n                                                      None,\\n                                                      1.0, None)\\n    assert_array_almost_equal(composition, np.zeros_like(composition))\\n\\n    # test updating the displacement field instead of creating a new one\\n    composition = disp1.copy()\\n    vfu.compose_vector_fields_2d(composition, disp2, None, None, 1.0,\\n                                 composition)\\n    assert_array_almost_equal(composition, np.zeros_like(composition))', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='# test updating the displacement field instead of creating a new one\\n    composition = disp1.copy()\\n    vfu.compose_vector_fields_2d(composition, disp2, None, None, 1.0,\\n                                 composition)\\n    assert_array_almost_equal(composition, np.zeros_like(composition))\\n\\n    # Test exception is raised when the affine transform matrix is not valid\\n    valid = np.zeros((2, 3), dtype=np.float64)\\n    invalid = np.zeros((2, 2), dtype=np.float64)\\n    assert_raises(ValueError, vfu.compose_vector_fields_2d, disp1, disp2,\\n                  invalid, valid, 1.0, None)\\n    assert_raises(ValueError, vfu.compose_vector_fields_2d, disp1, disp2,\\n                  valid, invalid, 1.0, None)\\n\\n\\n@set_random_number_generator(8315759)\\ndef test_compose_vector_fields_3d(rng):\\n    r\"\"\"\\n    Creates two random displacement field that exactly map pixels from an input\\n    image to an output image. The resulting displacements and their\\n    composition, although operating in physical space, map the points exactly\\n    (up to numerical precision).\\n    \"\"\"\\n    input_shape = (10, 10, 10)\\n    tgt_sh = (10, 10, 10)\\n    # create a simple affine transformation\\n    ns = input_shape[0]\\n    nr = input_shape[1]\\n    nc = input_shape[2]\\n    s = 1.5\\n    t = 2.5\\n    trans = np.array([[1, 0, 0, -t * ns],\\n                      [0, 1, 0, -t * nr],\\n                      [0, 0, 1, -t * nc],\\n                      [0, 0, 0, 1]])\\n    trans_inv = np.linalg.inv(trans)\\n    scale = np.array([[1 * s, 0, 0, 0],\\n                      [0, 1 * s, 0, 0],\\n                      [0, 0, 1 * s, 0],\\n                      [0, 0, 0, 1]])\\n    gt_affine = trans_inv.dot(scale.dot(trans))', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content=\"# create two random displacement fields\\n    input_grid2world = gt_affine\\n    target_grid2world = gt_affine\\n\\n    disp1, assign1 = vfu.create_random_displacement_3d(\\n        np.array(input_shape, dtype=np.int32),\\n        input_grid2world, np.array(tgt_sh, dtype=np.int32),\\n        target_grid2world, rng=rng)\\n    disp1 = np.array(disp1, dtype=floating)\\n    assign1 = np.array(assign1)\\n\\n    disp2, assign2 = vfu.create_random_displacement_3d(\\n        np.array(\\n            input_shape, dtype=np.int32), input_grid2world, np.array(\\n            tgt_sh, dtype=np.int32), target_grid2world, rng=rng)\\n    disp2 = np.array(disp2, dtype=floating)\\n    assign2 = np.array(assign2)\\n\\n    # create a random image (with decimal digits) to warp\\n    moving_image = np.empty(tgt_sh, dtype=floating)\\n    moving_image[...] =\\\\\\n        rng.integers(0, 10, np.size(moving_image)).reshape(tuple(tgt_sh))\\n    # set boundary values to zero so we don't test wrong interpolation due to\\n    # floating point precision\\n    moving_image[0, :, :] = 0\\n    moving_image[-1, :, :] = 0\\n    moving_image[:, 0, :] = 0\\n    moving_image[:, -1, :] = 0\\n    moving_image[:, :, 0] = 0\\n    moving_image[:, :, -1] = 0\\n\\n    # evaluate the composed warping using the exact assignments\\n    # (first 1 then 2)\\n\\n    warp1 = moving_image[(assign2[..., 0], assign2[..., 1], assign2[..., 2])]\\n    expected = warp1[(assign1[..., 0], assign1[..., 1], assign1[..., 2])]\\n\\n    # compose the displacement fields\\n    target_world2grid = np.linalg.inv(target_grid2world)\\n    premult_index = target_world2grid.dot(input_grid2world)\\n    premult_disp = target_world2grid\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='# evaluate the composed warping using the exact assignments\\n    # (first 1 then 2)\\n\\n    warp1 = moving_image[(assign2[..., 0], assign2[..., 1], assign2[..., 2])]\\n    expected = warp1[(assign1[..., 0], assign1[..., 1], assign1[..., 2])]\\n\\n    # compose the displacement fields\\n    target_world2grid = np.linalg.inv(target_grid2world)\\n    premult_index = target_world2grid.dot(input_grid2world)\\n    premult_disp = target_world2grid\\n\\n    for time_scaling in [0.25, 0.5, 1.0, 2.0, 4.0]:\\n        composition, stats = vfu.compose_vector_fields_3d(disp1,\\n                                                          disp2 / time_scaling,\\n                                                          premult_index,\\n                                                          premult_disp,\\n                                                          time_scaling, None)\\n        # apply the implementation under test\\n        warped = np.array(vfu.warp_3d(moving_image, composition, None,\\n                                      premult_index, premult_disp))\\n        assert_array_almost_equal(warped, expected)\\n\\n        # test also using nearest neighbor interpolation\\n        warped = np.array(vfu.warp_3d_nn(moving_image, composition, None,\\n                                         premult_index, premult_disp))\\n        assert_array_almost_equal(warped, expected)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='# test also using nearest neighbor interpolation\\n        warped = np.array(vfu.warp_3d_nn(moving_image, composition, None,\\n                                         premult_index, premult_disp))\\n        assert_array_almost_equal(warped, expected)\\n\\n        # test updating the displacement field instead of creating a new one\\n        composition = disp1.copy()\\n        vfu.compose_vector_fields_3d(composition, disp2 / time_scaling,\\n                                     premult_index, premult_disp,\\n                                     time_scaling, composition)\\n        # apply the implementation under test\\n        warped = np.array(vfu.warp_3d(moving_image, composition, None,\\n                                      premult_index, premult_disp))\\n        assert_array_almost_equal(warped, expected)\\n\\n        # test also using nearest neighbor interpolation\\n        warped = np.array(vfu.warp_3d_nn(moving_image, composition, None,\\n                                         premult_index, premult_disp))\\n        assert_array_almost_equal(warped, expected)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='# test also using nearest neighbor interpolation\\n        warped = np.array(vfu.warp_3d_nn(moving_image, composition, None,\\n                                         premult_index, premult_disp))\\n        assert_array_almost_equal(warped, expected)\\n\\n    # Test non-overlapping case\\n    x_0 = np.asarray(range(input_shape[0]))\\n    x_1 = np.asarray(range(input_shape[1]))\\n    x_2 = np.asarray(range(input_shape[2]))\\n    X = np.empty(input_shape + (3,), dtype=np.float64)\\n    O = np.ones(input_shape)\\n    X[..., 0] = x_0[:, None, None] * O\\n    X[..., 1] = x_1[None, :, None] * O\\n    X[..., 2] = x_2[None, None, :] * O\\n    sz = input_shape[0] * input_shape[1] * input_shape[2] * 3\\n    random_labels = rng.integers(0, 2, sz)\\n    random_labels = random_labels.reshape(input_shape + (3,))\\n    values = np.array([-1, tgt_sh[0]])\\n    disp1 = (values[random_labels] - X).astype(floating)\\n    composition, stats = vfu.compose_vector_fields_3d(disp1,\\n                                                      disp2,\\n                                                      None,\\n                                                      None,\\n                                                      1.0, None)\\n    assert_array_almost_equal(composition, np.zeros_like(composition))\\n\\n    # test updating the displacement field instead of creating a new one\\n    composition = disp1.copy()\\n    vfu.compose_vector_fields_3d(composition, disp2, None, None, 1.0,\\n                                 composition)\\n    assert_array_almost_equal(composition, np.zeros_like(composition))', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='# test updating the displacement field instead of creating a new one\\n    composition = disp1.copy()\\n    vfu.compose_vector_fields_3d(composition, disp2, None, None, 1.0,\\n                                 composition)\\n    assert_array_almost_equal(composition, np.zeros_like(composition))\\n\\n    # Test exception is raised when the affine transform matrix is not valid\\n    valid = np.zeros((3, 4), dtype=np.float64)\\n    invalid = np.zeros((3, 3), dtype=np.float64)\\n    assert_raises(ValueError, vfu.compose_vector_fields_3d, disp1, disp2,\\n                  invalid, valid, 1.0, None)\\n    assert_raises(ValueError, vfu.compose_vector_fields_3d, disp1, disp2,\\n                  valid, invalid, 1.0, None)\\n\\n\\ndef test_invert_vector_field_2d():\\n    r\"\"\"\\n    Inverts a synthetic, analytically invertible, displacement field\\n    \"\"\"\\n    shape = (64, 64)\\n    nr = shape[0]\\n    nc = shape[1]\\n    # Create an arbitrary image-to-space transform\\n    t = 2.5  # translation factor\\n\\n    trans = np.array([[1, 0, -t * nr],\\n                      [0, 1, -t * nc],\\n                      [0, 0, 1]])\\n    trans_inv = np.linalg.inv(trans)\\n\\n    d, _ = vfu.create_harmonic_fields_2d(nr, nc, 0.2, 8)\\n    d = np.asarray(d).astype(floating)\\n\\n    for theta in [-1 * np.pi / 5.0, 0.0, np.pi / 5.0]:  # rotation angle\\n        for s in [0.5, 1.0, 2.0]:  # scale\\n            ct = np.cos(theta)\\n            st = np.sin(theta)\\n\\n            rot = np.array([[ct, -st, 0],\\n                            [st, ct, 0],\\n                            [0, 0, 1]])\\n\\n            scale = np.array([[1 * s, 0, 0],\\n                              [0, 1 * s, 0],\\n                              [0, 0, 1]])', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='for theta in [-1 * np.pi / 5.0, 0.0, np.pi / 5.0]:  # rotation angle\\n        for s in [0.5, 1.0, 2.0]:  # scale\\n            ct = np.cos(theta)\\n            st = np.sin(theta)\\n\\n            rot = np.array([[ct, -st, 0],\\n                            [st, ct, 0],\\n                            [0, 0, 1]])\\n\\n            scale = np.array([[1 * s, 0, 0],\\n                              [0, 1 * s, 0],\\n                              [0, 0, 1]])\\n\\n            gt_affine = trans_inv.dot(scale.dot(rot.dot(trans)))\\n            gt_affine_inv = np.linalg.inv(gt_affine)\\n            dcopy = np.copy(d)\\n\\n            # make sure the field remains invertible after the re-mapping\\n            vfu.reorient_vector_field_2d(dcopy, gt_affine)\\n\\n            inv_approx =\\\\\\n                vfu.invert_vector_field_fixed_point_2d(dcopy, gt_affine_inv,\\n                                                       np.array([s, s]),\\n                                                       40, 1e-7)\\n\\n            mapping = imwarp.DiffeomorphicMap(2, (nr, nc), gt_affine)\\n            mapping.forward = dcopy\\n            mapping.backward = inv_approx\\n            residual, stats = mapping.compute_inversion_error()\\n            assert_almost_equal(stats[1], 0, decimal=4)\\n            assert_almost_equal(stats[2], 0, decimal=4)\\n\\n    # Test exception is raised when the affine transform matrix is not valid\\n    invalid = np.zeros((2, 2), dtype=np.float64)\\n    spacing = np.array([1.0, 1.0])\\n    assert_raises(ValueError, vfu.invert_vector_field_fixed_point_2d,\\n                  d, invalid, spacing, 40, 1e-7, None)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='# Test exception is raised when the affine transform matrix is not valid\\n    invalid = np.zeros((2, 2), dtype=np.float64)\\n    spacing = np.array([1.0, 1.0])\\n    assert_raises(ValueError, vfu.invert_vector_field_fixed_point_2d,\\n                  d, invalid, spacing, 40, 1e-7, None)\\n\\n\\ndef test_invert_vector_field_3d():\\n    r\"\"\"\\n    Inverts a synthetic, analytically invertible, displacement field\\n    \"\"\"\\n    shape = (64, 64, 64)\\n    ns = shape[0]\\n    nr = shape[1]\\n    nc = shape[2]\\n\\n    # Create an arbitrary image-to-space transform\\n\\n    # Select an arbitrary rotation axis\\n    axis = np.array([2.0, 0.5, 1.0])\\n    t = 2.5  # translation factor\\n\\n    trans = np.array([[1, 0, 0, -t * ns],\\n                      [0, 1, 0, -t * nr],\\n                      [0, 0, 1, -t * nc],\\n                      [0, 0, 0, 1]])\\n    trans_inv = np.linalg.inv(trans)\\n\\n    d, _ = vfu.create_harmonic_fields_3d(ns, nr, nc, 0.2, 8)\\n    d = np.asarray(d).astype(floating)\\n\\n    for theta in [-1 * np.pi / 5.0, 0.0, np.pi / 5.0]:  # rotation angle\\n        for s in [0.5, 1.0, 2.0]:  # scale\\n            rot = np.zeros(shape=(4, 4))\\n            rot[:3, :3] = geometry.rodrigues_axis_rotation(axis, theta)\\n            rot[3, 3] = 1.0\\n            scale = np.array([[1 * s, 0, 0, 0],\\n                              [0, 1 * s, 0, 0],\\n                              [0, 0, 1 * s, 0],\\n                              [0, 0, 0, 1]])\\n\\n            gt_affine = trans_inv.dot(scale.dot(rot.dot(trans)))\\n            gt_affine_inv = np.linalg.inv(gt_affine)\\n            dcopy = np.copy(d)\\n\\n            # make sure the field remains invertible after the re-mapping\\n            vfu.reorient_vector_field_3d(dcopy, gt_affine)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content=\"gt_affine = trans_inv.dot(scale.dot(rot.dot(trans)))\\n            gt_affine_inv = np.linalg.inv(gt_affine)\\n            dcopy = np.copy(d)\\n\\n            # make sure the field remains invertible after the re-mapping\\n            vfu.reorient_vector_field_3d(dcopy, gt_affine)\\n\\n            # Note: the spacings are used just to check convergence, so they\\n            # don't need to be very accurate. Here we are passing (0.5 * s) to\\n            # force the algorithm to make more iterations: in ANTS, there is a\\n            # hard-coded bound on the maximum residual, that's why we cannot\\n            # force more iteration by changing the parameters.\\n            # We will investigate this issue with more detail in the future.\\n\\n            inv_approx = vfu.invert_vector_field_fixed_point_3d(\\n                dcopy, gt_affine_inv, np.array([s, s, s]) * 0.5, 40, 1e-7)\\n\\n            mapping = imwarp.DiffeomorphicMap(3, (nr, nc), gt_affine)\\n            mapping.forward = dcopy\\n            mapping.backward = inv_approx\\n            residual, stats = mapping.compute_inversion_error()\\n            assert_almost_equal(stats[1], 0, decimal=3)\\n            assert_almost_equal(stats[2], 0, decimal=3)\\n\\n    # Test exception is raised when the affine transform matrix is not valid\\n    invalid = np.zeros((3, 3), dtype=np.float64)\\n    spacing = np.array([1.0, 1.0, 1.0])\\n    assert_raises(ValueError, vfu.invert_vector_field_fixed_point_3d,\\n                  d, invalid, spacing, 40, 1e-7, None)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='# Test exception is raised when the affine transform matrix is not valid\\n    invalid = np.zeros((3, 3), dtype=np.float64)\\n    spacing = np.array([1.0, 1.0, 1.0])\\n    assert_raises(ValueError, vfu.invert_vector_field_fixed_point_3d,\\n                  d, invalid, spacing, 40, 1e-7, None)\\n\\n\\ndef test_resample_vector_field_2d():\\n    r\"\"\"\\n    Expand a vector field by 2, then subsample by 2, the resulting\\n    field should be the original one\\n    \"\"\"\\n    domain_shape = np.array((64, 64), dtype=np.int32)\\n    reduced_shape = np.array((32, 32), dtype=np.int32)\\n    factors = np.array([0.5, 0.5])\\n    d, dinv = vfu.create_harmonic_fields_2d(reduced_shape[0], reduced_shape[1],\\n                                            0.3, 6)\\n    d = np.array(d, dtype=floating)\\n\\n    expanded = vfu.resample_displacement_field_2d(d, factors, domain_shape)\\n    subsampled = expanded[::2, ::2, :]\\n\\n    assert_array_almost_equal(d, subsampled)\\n\\n\\ndef test_resample_vector_field_3d():\\n    r\"\"\"\\n    Expand a vector field by 2, then subsample by 2, the resulting\\n    field should be the original one\\n    \"\"\"\\n    domain_shape = np.array((64, 64, 64), dtype=np.int32)\\n    reduced_shape = np.array((32, 32, 32), dtype=np.int32)\\n    factors = np.array([0.5, 0.5, 0.5])\\n    d, dinv = vfu.create_harmonic_fields_3d(reduced_shape[0], reduced_shape[1],\\n                                            reduced_shape[2], 0.3, 6)\\n    d = np.array(d, dtype=floating)\\n\\n    expanded = vfu.resample_displacement_field_3d(d, factors, domain_shape)\\n    subsampled = expanded[::2, ::2, ::2, :]\\n\\n    assert_array_almost_equal(d, subsampled)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='expanded = vfu.resample_displacement_field_3d(d, factors, domain_shape)\\n    subsampled = expanded[::2, ::2, ::2, :]\\n\\n    assert_array_almost_equal(d, subsampled)\\n\\n\\n@set_random_number_generator(8315759)\\ndef test_downsample_scalar_field_2d(rng):\\n    size = 32\\n    sh = (size, size)\\n    for reduce_r in [True, False]:\\n        nr = size - 1 if reduce_r else size\\n        for reduce_c in [True, False]:\\n            nc = size - 1 if reduce_c else size\\n            image = np.empty((size, size), dtype=floating)\\n            image[...] = rng.integers(0, 10, np.size(image)).reshape(sh)\\n\\n            if reduce_r:\\n                image[-1, :] = 0\\n            if reduce_c:\\n                image[:, -1] = 0\\n\\n            a = image[::2, ::2]\\n            b = image[1::2, ::2]\\n            c = image[::2, 1::2]\\n            d = image[1::2, 1::2]\\n\\n            expected = 0.25 * (a + b + c + d)\\n\\n            if reduce_r:\\n                expected[-1, :] *= 2\\n            if reduce_c:\\n                expected[:, -1] *= 2\\n\\n            actual = np.array(vfu.downsample_scalar_field_2d(image[:nr, :nc]))\\n            assert_array_almost_equal(expected, actual)\\n\\n\\n@set_random_number_generator(2115556)\\ndef test_downsample_displacement_field_2d(rng):\\n    size = 32\\n    sh = (size, size, 2)\\n    for reduce_r in [True, False]:\\n        nr = size - 1 if reduce_r else size\\n        for reduce_c in [True, False]:\\n            nc = size - 1 if reduce_c else size\\n            field = np.empty((size, size, 2), dtype=floating)\\n            field[...] = rng.integers(0, 10, np.size(field)).reshape(sh)\\n\\n            if reduce_r:\\n                field[-1, :, :] = 0\\n            if reduce_c:\\n                field[:, -1, :] = 0', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='if reduce_r:\\n                field[-1, :, :] = 0\\n            if reduce_c:\\n                field[:, -1, :] = 0\\n\\n            a = field[::2, ::2, :]\\n            b = field[1::2, ::2, :]\\n            c = field[::2, 1::2, :]\\n            d = field[1::2, 1::2, :]\\n\\n            expected = 0.25 * (a + b + c + d)\\n\\n            if reduce_r:\\n                expected[-1, :, :] *= 2\\n            if reduce_c:\\n                expected[:, -1, :] *= 2\\n\\n            actual = vfu.downsample_displacement_field_2d(field[:nr, :nc, :])\\n            assert_array_almost_equal(expected, actual)\\n\\n\\n@set_random_number_generator(8315759)\\ndef test_downsample_scalar_field_3d(rng):\\n    size = 32\\n    sh = (size, size, size)\\n    for reduce_s in [True, False]:\\n        ns = size - 1 if reduce_s else size\\n        for reduce_r in [True, False]:\\n            nr = size - 1 if reduce_r else size\\n            for reduce_c in [True, False]:\\n                nc = size - 1 if reduce_c else size\\n                image = np.empty((size, size, size), dtype=floating)\\n                image[...] =\\\\\\n                    rng.integers(0, 10, np.size(image)).reshape(sh)\\n\\n                if reduce_s:\\n                    image[-1, :, :] = 0\\n                if reduce_r:\\n                    image[:, -1, :] = 0\\n                if reduce_c:\\n                    image[:, :, -1] = 0\\n\\n                a = image[::2, ::2, ::2]\\n                b = image[1::2, ::2, ::2]\\n                c = image[::2, 1::2, ::2]\\n                d = image[1::2, 1::2, ::2]\\n                aa = image[::2, ::2, 1::2]\\n                bb = image[1::2, ::2, 1::2]\\n                cc = image[::2, 1::2, 1::2]\\n                dd = image[1::2, 1::2, 1::2]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='a = image[::2, ::2, ::2]\\n                b = image[1::2, ::2, ::2]\\n                c = image[::2, 1::2, ::2]\\n                d = image[1::2, 1::2, ::2]\\n                aa = image[::2, ::2, 1::2]\\n                bb = image[1::2, ::2, 1::2]\\n                cc = image[::2, 1::2, 1::2]\\n                dd = image[1::2, 1::2, 1::2]\\n\\n                expected = 0.125 * (a + b + c + d + aa + bb + cc + dd)\\n\\n                if reduce_s:\\n                    expected[-1, :, :] *= 2\\n                if reduce_r:\\n                    expected[:, -1, :] *= 2\\n                if reduce_c:\\n                    expected[:, :, -1] *= 2\\n\\n                actual = vfu.downsample_scalar_field_3d(image[:ns, :nr, :nc])\\n                assert_array_almost_equal(expected, actual)\\n\\n\\n@set_random_number_generator(8315759)\\ndef test_downsample_displacement_field_3d(rng):\\n    size = 32\\n    sh = (size, size, size, 3)\\n    for reduce_s in [True, False]:\\n        ns = size - 1 if reduce_s else size\\n        for reduce_r in [True, False]:\\n            nr = size - 1 if reduce_r else size\\n            for reduce_c in [True, False]:\\n                nc = size - 1 if reduce_c else size\\n                field = np.empty((size, size, size, 3), dtype=floating)\\n                field[...] =\\\\\\n                    rng.integers(0, 10, np.size(field)).reshape(sh)\\n\\n                if reduce_s:\\n                    field[-1, :, :] = 0\\n                if reduce_r:\\n                    field[:, -1, :] = 0\\n                if reduce_c:\\n                    field[:, :, -1] = 0', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='if reduce_s:\\n                    field[-1, :, :] = 0\\n                if reduce_r:\\n                    field[:, -1, :] = 0\\n                if reduce_c:\\n                    field[:, :, -1] = 0\\n\\n                a = field[::2, ::2, ::2, :]\\n                b = field[1::2, ::2, ::2, :]\\n                c = field[::2, 1::2, ::2, :]\\n                d = field[1::2, 1::2, ::2, :]\\n                aa = field[::2, ::2, 1::2, :]\\n                bb = field[1::2, ::2, 1::2, :]\\n                cc = field[::2, 1::2, 1::2, :]\\n                dd = field[1::2, 1::2, 1::2, :]\\n\\n                expected = 0.125 * (a + b + c + d + aa + bb + cc + dd)\\n\\n                if reduce_s:\\n                    expected[-1, :, :, :] *= 2\\n                if reduce_r:\\n                    expected[:, -1, :, :] *= 2\\n                if reduce_c:\\n                    expected[:, :, -1, :] *= 2\\n\\n                actual =\\\\\\n                    vfu.downsample_displacement_field_3d(field[:ns, :nr, :nc])\\n                assert_array_almost_equal(expected, actual)\\n\\n\\ndef test_reorient_vector_field_2d():\\n    shape = (16, 16)\\n    d, dinv = vfu.create_harmonic_fields_2d(shape[0], shape[1], 0.2, 4)\\n    d = np.array(d, dtype=floating)\\n\\n    # the vector field rotated 90 degrees\\n    expected = np.empty(shape=shape + (2,), dtype=floating)\\n    expected[..., 0] = -1 * d[..., 1]\\n    expected[..., 1] = d[..., 0]\\n\\n    # rotate 45 degrees twice\\n    c = np.sqrt(0.5)\\n    affine = np.array([[c, -c, 0.0], [c, c, 0.0]])\\n    vfu.reorient_vector_field_2d(d, affine)\\n    vfu.reorient_vector_field_2d(d, affine)\\n\\n    # verify almost equal\\n    assert_array_almost_equal(d, expected)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='# the vector field rotated 90 degrees\\n    expected = np.empty(shape=shape + (2,), dtype=floating)\\n    expected[..., 0] = -1 * d[..., 1]\\n    expected[..., 1] = d[..., 0]\\n\\n    # rotate 45 degrees twice\\n    c = np.sqrt(0.5)\\n    affine = np.array([[c, -c, 0.0], [c, c, 0.0]])\\n    vfu.reorient_vector_field_2d(d, affine)\\n    vfu.reorient_vector_field_2d(d, affine)\\n\\n    # verify almost equal\\n    assert_array_almost_equal(d, expected)\\n\\n    # Test exception is raised when the affine transform matrix is not valid\\n    invalid = np.zeros((2, 2), dtype=np.float64)\\n    assert_raises(ValueError, vfu.reorient_vector_field_2d, d, invalid)\\n\\n\\ndef test_reorient_vector_field_3d():\\n    sh = (16, 16, 16)\\n    d, dinv = vfu.create_harmonic_fields_3d(sh[0], sh[1], sh[2], 0.2, 4)\\n    d = np.array(d, dtype=floating)\\n    dinv = np.array(dinv, dtype=floating)\\n\\n    # the vector field rotated 90 degrees around the last axis\\n    expected = np.empty(shape=sh + (3,), dtype=floating)\\n    expected[..., 0] = -1 * d[..., 1]\\n    expected[..., 1] = d[..., 0]\\n    expected[..., 2] = d[..., 2]\\n\\n    # rotate 45 degrees twice around the last axis\\n    c = np.sqrt(0.5)\\n    affine = np.array([[c, -c, 0, 0], [c, c, 0, 0], [0, 0, 1, 0]])\\n    vfu.reorient_vector_field_3d(d, affine)\\n    vfu.reorient_vector_field_3d(d, affine)\\n\\n    # verify almost equal\\n    assert_array_almost_equal(d, expected)\\n\\n    # the vector field rotated 90 degrees around the first axis\\n    expected[..., 0] = dinv[..., 0]\\n    expected[..., 1] = -1 * dinv[..., 2]\\n    expected[..., 2] = dinv[..., 1]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='# rotate 45 degrees twice around the last axis\\n    c = np.sqrt(0.5)\\n    affine = np.array([[c, -c, 0, 0], [c, c, 0, 0], [0, 0, 1, 0]])\\n    vfu.reorient_vector_field_3d(d, affine)\\n    vfu.reorient_vector_field_3d(d, affine)\\n\\n    # verify almost equal\\n    assert_array_almost_equal(d, expected)\\n\\n    # the vector field rotated 90 degrees around the first axis\\n    expected[..., 0] = dinv[..., 0]\\n    expected[..., 1] = -1 * dinv[..., 2]\\n    expected[..., 2] = dinv[..., 1]\\n\\n    # rotate 45 degrees twice around the first axis\\n    affine = np.array([[1, 0, 0, 0], [0, c, -c, 0], [0, c, c, 0]])\\n    vfu.reorient_vector_field_3d(dinv, affine)\\n    vfu.reorient_vector_field_3d(dinv, affine)\\n\\n    # verify almost equal\\n    assert_array_almost_equal(dinv, expected)\\n\\n    # Test exception is raised when the affine transform matrix is not valid\\n    invalid = np.zeros((3, 3), dtype=np.float64)\\n    assert_raises(ValueError, vfu.reorient_vector_field_3d, d, invalid)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='# rotate 45 degrees twice around the first axis\\n    affine = np.array([[1, 0, 0, 0], [0, c, -c, 0], [0, c, c, 0]])\\n    vfu.reorient_vector_field_3d(dinv, affine)\\n    vfu.reorient_vector_field_3d(dinv, affine)\\n\\n    # verify almost equal\\n    assert_array_almost_equal(dinv, expected)\\n\\n    # Test exception is raised when the affine transform matrix is not valid\\n    invalid = np.zeros((3, 3), dtype=np.float64)\\n    assert_raises(ValueError, vfu.reorient_vector_field_3d, d, invalid)\\n\\n\\n@set_random_number_generator(1134781)\\ndef test_reorient_random_vector_fields(rng):\\n    # Test reorienting vector field\\n    for n_dims, func in ((2, vfu.reorient_vector_field_2d),\\n                         (3, vfu.reorient_vector_field_3d)):\\n        size = [20, 30, 40][:n_dims] + [n_dims]\\n        arr = rng.normal(size=size)\\n        arr_32 = arr.astype(floating)\\n        affine = from_matvec(rng.normal(size=(n_dims, n_dims)),\\n                             np.zeros(n_dims))\\n        func(arr_32, affine)\\n        assert_almost_equal(arr_32, apply_affine(affine, arr), 6)\\n        # Reorient reorients without translation\\n        trans = np.arange(n_dims) + 2\\n        affine[:-1, -1] = trans\\n        arr_32 = arr.astype(floating)\\n        func(arr_32, affine)\\n        assert_almost_equal(arr_32, apply_affine(affine, arr) - trans, 6)\\n\\n        # Test exception is raised when the affine transform is not valid\\n        invalid = np.eye(n_dims)\\n        assert_raises(ValueError, func, arr_32, invalid)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content=\"# Test exception is raised when the affine transform is not valid\\n        invalid = np.eye(n_dims)\\n        assert_raises(ValueError, func, arr_32, invalid)\\n\\n\\n@set_random_number_generator(3921116)\\ndef test_gradient_2d(rng):\\n    sh = (25, 32)\\n    # Create grid coordinates\\n    x_0 = np.asarray(range(sh[0]))\\n    x_1 = np.asarray(range(sh[1]))\\n    X = np.empty(sh + (3,), dtype=np.float64)\\n    O = np.ones(sh)\\n    X[..., 0] = x_0[:, None] * O\\n    X[..., 1] = x_1[None, :] * O\\n    X[..., 2] = 1\\n\\n    transform = regtransforms[('RIGID', 2)]\\n    theta = np.array([0.1, 5.0, 2.5])\\n    T = transform.param_to_matrix(theta)\\n    TX = X.dot(T.T)\\n    # Eval an arbitrary (known) function at TX\\n    # f(x, y) = ax^2 + bxy + cy^{2}\\n    # df/dx = 2ax + by\\n    # df/dy = 2cy + bx\\n    a = 2e-3\\n    b = 5e-3\\n    c = 7e-3\\n    img = a * TX[..., 0] ** 2 +\\\\\\n        b * TX[..., 0] * TX[..., 1] +\\\\\\n        c * TX[..., 1] ** 2\\n    img = img.astype(floating)\\n    # img is an image sampled at X with grid-to-space transform T\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='# Test sparse gradient: choose some sample points (in space)\\n    sample = sample_domain_regular(20, np.array(sh, dtype=np.int32),\\n                                   T, rng=rng)\\n    sample = np.array(sample)\\n    # Compute the analytical gradient at all points\\n    expected = np.empty((sample.shape[0], 2), dtype=floating)\\n    expected[..., 0] = 2 * a * sample[:, 0] + b * sample[:, 1]\\n    expected[..., 1] = 2 * c * sample[:, 1] + b * sample[:, 0]\\n    # Get the numerical gradient with the implementation under test\\n    sp_to_grid = np.linalg.inv(T)\\n    img_spacing = np.ones(2)\\n    actual, inside = vfu.sparse_gradient(img, sp_to_grid, img_spacing, sample)\\n    diff = np.abs(expected - actual).mean(1) * inside\\n    # The finite differences are really not accurate, especially with float32\\n    assert_equal(diff.max() < 1e-3, True)\\n    # Verify exception is raised when passing invalid affine or spacings\\n    invalid_affine = np.eye(2)\\n    invalid_spacings = np.ones(1)\\n    assert_raises(ValueError, vfu.sparse_gradient, img, invalid_affine,\\n                  img_spacing, sample)\\n    assert_raises(ValueError, vfu.sparse_gradient, img, sp_to_grid,\\n                  invalid_spacings, sample)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content=\"# Test dense gradient\\n    # Compute the analytical gradient at all points\\n    expected = np.empty(sh + (2,), dtype=floating)\\n    expected[..., 0] = 2 * a * TX[..., 0] + b * TX[..., 1]\\n    expected[..., 1] = 2 * c * TX[..., 1] + b * TX[..., 0]\\n    # Get the numerical gradient with the implementation under test\\n    sp_to_grid = np.linalg.inv(T)\\n    img_spacing = np.ones(2)\\n    actual, inside = vfu.gradient(img, sp_to_grid, img_spacing, sh, T)\\n    diff = np.abs(expected - actual).mean(2) * inside\\n    # In the dense case, we are evaluating at the exact points (sample points\\n    # are not slightly moved like in the sparse case) so we have more precision\\n    assert_equal(diff.max() < 1e-5, True)\\n    # Verify exception is raised when passing invalid affine or spacings\\n    assert_raises(ValueError, vfu.gradient, img, invalid_affine, img_spacing,\\n                  sh, T)\\n    assert_raises(ValueError, vfu.gradient, img, sp_to_grid, img_spacing,\\n                  sh, invalid_affine)\\n    assert_raises(ValueError, vfu.gradient, img, sp_to_grid, invalid_spacings,\\n                  sh, T)\\n\\n\\n@set_random_number_generator(3921116)\\ndef test_gradient_3d(rng):\\n    shape = (25, 32, 15)\\n    # Create grid coordinates\\n    x_0 = np.asarray(range(shape[0]))\\n    x_1 = np.asarray(range(shape[1]))\\n    x_2 = np.asarray(range(shape[2]))\\n    X = np.zeros(shape + (4,), dtype=np.float64)\\n    O = np.ones(shape)\\n    X[..., 0] = x_0[:, None, None] * O\\n    X[..., 1] = x_1[None, :, None] * O\\n    X[..., 2] = x_2[None, None, :] * O\\n    X[..., 3] = 1\\n\\n    transform = regtransforms[('RIGID', 3)]\\n    theta = np.array([0.1, 0.05, 0.12, -12.0, -15.5, -7.2])\\n    T = transform.param_to_matrix(theta)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content=\"transform = regtransforms[('RIGID', 3)]\\n    theta = np.array([0.1, 0.05, 0.12, -12.0, -15.5, -7.2])\\n    T = transform.param_to_matrix(theta)\\n\\n    TX = X.dot(T.T)\\n    # Eval an arbitrary (known) function at TX\\n    # f(x, y, z) = ax^2 + by^2 + cz^2 + dxy + exz + fyz\\n    # df/dx = 2ax + dy + ez\\n    # df/dy = 2by + dx + fz\\n    # df/dz = 2cz + ex + fy\\n    a, b, c = 2e-3, 3e-3, 1e-3\\n    d, e, f = 1e-3, 2e-3, 3e-3\\n    img = a * TX[..., 0] ** 2 + b * TX[..., 1] ** 2 +\\\\\\n        c * TX[..., 2] ** 2 + d * TX[..., 0] * TX[..., 1] +\\\\\\n        e * TX[..., 0] * TX[..., 2] + f * TX[..., 1] * TX[..., 2]\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='img = img.astype(floating)\\n    # Test sparse gradient: choose some sample points (in space)\\n    sample =\\\\\\n        sample_domain_regular(100, np.array(shape, dtype=np.int32),\\n                              T, rng=rng)\\n    sample = np.array(sample)\\n    # Compute the analytical gradient at all points\\n    expected = np.empty((sample.shape[0], 3), dtype=floating)\\n    expected[..., 0] =\\\\\\n        2 * a * sample[:, 0] + d * sample[:, 1] + e * sample[:, 2]\\n    expected[..., 1] =\\\\\\n        2 * b * sample[:, 1] + d * sample[:, 0] + f * sample[:, 2]\\n    expected[..., 2] =\\\\\\n        2 * c * sample[:, 2] + e * sample[:, 0] + f * sample[:, 1]\\n    # Get the numerical gradient with the implementation under test\\n    sp_to_grid = np.linalg.inv(T)\\n    img_spacing = np.ones(3)\\n    actual, inside = vfu.sparse_gradient(img, sp_to_grid, img_spacing, sample)\\n    # Discard points outside the image domain\\n    diff = np.abs(expected - actual).mean(1) * inside\\n    # The finite differences are really not accurate, especially with float32\\n    assert_equal(diff.max() < 1e-3, True)\\n    # Verify exception is raised when passing invalid affine or spacings\\n    invalid_affine = np.eye(3)\\n    invalid_spacings = np.ones(2)\\n    assert_raises(ValueError, vfu.sparse_gradient, img, invalid_affine,\\n                  img_spacing, sample)\\n    assert_raises(ValueError, vfu.sparse_gradient, img, sp_to_grid,\\n                  invalid_spacings, sample)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='# Test dense gradient\\n    # Compute the analytical gradient at all points\\n    expected = np.empty(shape + (3,), dtype=floating)\\n    expected[..., 0] = 2 * a * TX[..., 0] + d * TX[..., 1] + e * TX[..., 2]\\n    expected[..., 1] = 2 * b * TX[..., 1] + d * TX[..., 0] + f * TX[..., 2]\\n    expected[..., 2] = 2 * c * TX[..., 2] + e * TX[..., 0] + f * TX[..., 1]\\n    # Get the numerical gradient with the implementation under test\\n    sp_to_grid = np.linalg.inv(T)\\n    img_spacing = np.ones(3)\\n    actual, inside = vfu.gradient(img, sp_to_grid, img_spacing, shape, T)\\n    diff = np.abs(expected - actual).mean(3) * inside\\n    # In the dense case, we are evaluating at the exact points (sample points\\n    # are not slightly moved like in the sparse case) so we have more precision\\n    assert_equal(diff.max() < 1e-5, True)\\n    # Verify exception is raised when passing invalid affine or spacings\\n    assert_raises(ValueError, vfu.gradient, img, invalid_affine, img_spacing,\\n                  shape, T)\\n    assert_raises(ValueError, vfu.gradient, img, sp_to_grid, img_spacing,\\n                  shape, invalid_affine)\\n    assert_raises(ValueError, vfu.gradient, img, sp_to_grid, invalid_spacings,\\n                  shape, T)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_vector_fields.py.txt'}),\n",
       " Document(page_content='import numpy as np\\nfrom numpy.testing import (assert_equal, assert_array_almost_equal,\\n                           assert_raises)\\n\\nfrom dipy.align.streamlinear import (compose_matrix44, decompose_matrix44,\\n                                     transform_streamlines, whole_brain_slr,\\n                                     slr_with_qbx)\\nfrom dipy.io.streamline import load_tractogram\\nfrom dipy.data import get_fnames\\nfrom dipy.tracking.streamline import Streamlines\\nfrom dipy.tracking.distances import bundles_distances_mam\\n\\n\\ndef test_whole_brain_slr():\\n    fname = get_fnames(\\'fornix\\')\\n\\n    fornix = load_tractogram(fname, \\'same\\',\\n                             bbox_valid_check=False).streamlines\\n\\n    f = Streamlines(fornix)\\n    f1 = f.copy()\\n    f2 = f.copy()\\n\\n    # check translation\\n    f2._data += np.array([50, 0, 0])\\n\\n    moved, transform, qb_centroids1, qb_centroids2 = whole_brain_slr(\\n            f1, f2, x0=\\'affine\\', verbose=True, rm_small_clusters=2,\\n            greater_than=0, less_than=np.inf,\\n            qbx_thr=[5, 2, 1], progressive=False)\\n\\n    # we can check the quality of registration by comparing the matrices\\n    # MAM streamline distances before and after SLR\\n    D12 = bundles_distances_mam(f1, f2)\\n    D1M = bundles_distances_mam(f1, moved)\\n\\n    d12_minsum = np.sum(np.min(D12, axis=0))\\n    d1m_minsum = np.sum(np.min(D1M, axis=0))\\n\\n    print(\"distances= \", d12_minsum, \" \", d1m_minsum)\\n\\n    assert_equal(d1m_minsum < d12_minsum, True)\\n\\n    assert_array_almost_equal(transform[:3, 3], [-50, -0, -0], 2)\\n\\n    # check rotation\\n\\n    mat = compose_matrix44([0, 0, 0, 15, 0, 0])\\n\\n    f3 = f.copy()\\n    f3 = transform_streamlines(f3, mat)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_whole_brain_slr.py.txt'}),\n",
       " Document(page_content='d12_minsum = np.sum(np.min(D12, axis=0))\\n    d1m_minsum = np.sum(np.min(D1M, axis=0))\\n\\n    print(\"distances= \", d12_minsum, \" \", d1m_minsum)\\n\\n    assert_equal(d1m_minsum < d12_minsum, True)\\n\\n    assert_array_almost_equal(transform[:3, 3], [-50, -0, -0], 2)\\n\\n    # check rotation\\n\\n    mat = compose_matrix44([0, 0, 0, 15, 0, 0])\\n\\n    f3 = f.copy()\\n    f3 = transform_streamlines(f3, mat)\\n\\n    moved, transform, qb_centroids1, qb_centroids2 = slr_with_qbx(\\n            f1, f3, verbose=False, rm_small_clusters=1, greater_than=20,\\n            less_than=np.inf, qbx_thr=[2],\\n            progressive=True)\\n\\n    # we can also check the quality by looking at the decomposed transform\\n\\n    assert_array_almost_equal(decompose_matrix44(transform)[3], -15, 2)\\n\\n    moved, transform, qb_centroids1, qb_centroids2 = slr_with_qbx(\\n            f1, f3, verbose=False, rm_small_clusters=1, select_random=400,\\n            greater_than=20, less_than=np.inf, qbx_thr=[2],\\n            progressive=True)\\n\\n    # we can also check the quality by looking at the decomposed transform\\n\\n    assert_array_almost_equal(decompose_matrix44(transform)[3], -15, 2)\\n\\n\\ndef test_slr_one_streamline():\\n    fname = get_fnames(\\'fornix\\')\\n\\n    fornix = load_tractogram(fname, \\'same\\',\\n                             bbox_valid_check=False).streamlines\\n\\n    f = Streamlines(fornix)\\n    f1_one = Streamlines([f[0]])\\n    f2 = f.copy()\\n    f2._data += np.array([50, 0, 0])\\n\\n    assert_raises(ValueError, slr_with_qbx, f1_one, f2, verbose=False,\\n                  rm_small_clusters=50, greater_than=20,\\n                  less_than=np.inf, qbx_thr=[2], progressive=True)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_whole_brain_slr.py.txt'}),\n",
       " Document(page_content=\"def test_slr_one_streamline():\\n    fname = get_fnames('fornix')\\n\\n    fornix = load_tractogram(fname, 'same',\\n                             bbox_valid_check=False).streamlines\\n\\n    f = Streamlines(fornix)\\n    f1_one = Streamlines([f[0]])\\n    f2 = f.copy()\\n    f2._data += np.array([50, 0, 0])\\n\\n    assert_raises(ValueError, slr_with_qbx, f1_one, f2, verbose=False,\\n                  rm_small_clusters=50, greater_than=20,\\n                  less_than=np.inf, qbx_thr=[2], progressive=True)\\n\\n    assert_raises(ValueError, slr_with_qbx, f2, f1_one, verbose=False,\\n                  rm_small_clusters=50, greater_than=20,\\n                  less_than=np.inf, qbx_thr=[2], progressive=True)\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\align\\\\tests\\\\test_whole_brain_slr.py.txt'}),\n",
       " Document(page_content='\"\"\" Utility functions for algebra etc \"\"\"\\n\\nimport itertools\\nimport math\\nimport numpy as np\\nimport numpy.linalg as npl\\n\\n# epsilon for testing whether a number is close to zero\\n_EPS = np.finfo(float).eps * 4.0\\n\\n# axis sequences for Euler angles\\n_NEXT_AXIS = [1, 2, 0, 1]\\n\\n# map axes strings to/from tuples of inner axis, parity, repetition, frame\\n_AXES2TUPLE = {\\n    \\'sxyz\\': (0, 0, 0, 0), \\'sxyx\\': (0, 0, 1, 0), \\'sxzy\\': (0, 1, 0, 0),\\n    \\'sxzx\\': (0, 1, 1, 0), \\'syzx\\': (1, 0, 0, 0), \\'syzy\\': (1, 0, 1, 0),\\n    \\'syxz\\': (1, 1, 0, 0), \\'syxy\\': (1, 1, 1, 0), \\'szxy\\': (2, 0, 0, 0),\\n    \\'szxz\\': (2, 0, 1, 0), \\'szyx\\': (2, 1, 0, 0), \\'szyz\\': (2, 1, 1, 0),\\n    \\'rzyx\\': (0, 0, 0, 1), \\'rxyx\\': (0, 0, 1, 1), \\'ryzx\\': (0, 1, 0, 1),\\n    \\'rxzx\\': (0, 1, 1, 1), \\'rxzy\\': (1, 0, 0, 1), \\'ryzy\\': (1, 0, 1, 1),\\n    \\'rzxy\\': (1, 1, 0, 1), \\'ryxy\\': (1, 1, 1, 1), \\'ryxz\\': (2, 0, 0, 1),\\n    \\'rzxz\\': (2, 0, 1, 1), \\'rxyz\\': (2, 1, 0, 1), \\'rzyz\\': (2, 1, 1, 1)}\\n\\n_TUPLE2AXES = dict((v, k) for k, v in _AXES2TUPLE.items())\\n\\n\\ndef sphere2cart(r, theta, phi):\\n    \"\"\" Spherical to Cartesian coordinates\\n\\n    This is the standard physics convention where `theta` is the\\n    inclination (polar) angle, and `phi` is the azimuth angle.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\geometry.py.txt'}),\n",
       " Document(page_content='_TUPLE2AXES = dict((v, k) for k, v in _AXES2TUPLE.items())\\n\\n\\ndef sphere2cart(r, theta, phi):\\n    \"\"\" Spherical to Cartesian coordinates\\n\\n    This is the standard physics convention where `theta` is the\\n    inclination (polar) angle, and `phi` is the azimuth angle.\\n\\n    Imagine a sphere with center (0,0,0).  Orient it with the z axis\\n    running south-north, the y axis running west-east and the x axis\\n    from posterior to anterior.  `theta` (the inclination angle) is the\\n    angle to rotate from the z-axis (the zenith) around the y-axis,\\n    towards the x axis.  Thus the rotation is counter-clockwise from the\\n    point of view of positive y.  `phi` (azimuth) gives the angle of\\n    rotation around the z-axis towards the y axis.  The rotation is\\n    counter-clockwise from the point of view of positive z.\\n\\n    Equivalently, given a point P on the sphere, with coordinates x, y,\\n    z, `theta` is the angle between P and the z-axis, and `phi` is\\n    the angle between the projection of P onto the XY plane, and the X\\n    axis.\\n\\n    Geographical nomenclature designates theta as \\'co-latitude\\', and phi\\n    as \\'longitude\\'\\n\\n    Parameters\\n    ----------\\n    r : array_like\\n       radius\\n    theta : array_like\\n       inclination or polar angle\\n    phi : array_like\\n       azimuth angle\\n\\n    Returns\\n    -------\\n    x : array\\n       x coordinate(s) in Cartesian space\\n    y : array\\n       y coordinate(s) in Cartesian space\\n    z : array\\n       z coordinate\\n\\n    Notes\\n    -----\\n    See these pages:\\n\\n    * https://en.wikipedia.org/wiki/Spherical_coordinate_system\\n    * https://mathworld.wolfram.com/SphericalCoordinates.html', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\geometry.py.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    x : array\\n       x coordinate(s) in Cartesian space\\n    y : array\\n       y coordinate(s) in Cartesian space\\n    z : array\\n       z coordinate\\n\\n    Notes\\n    -----\\n    See these pages:\\n\\n    * https://en.wikipedia.org/wiki/Spherical_coordinate_system\\n    * https://mathworld.wolfram.com/SphericalCoordinates.html\\n\\n    for excellent discussion of the many different conventions\\n    possible.  Here we use the physics conventions, used in the\\n    wikipedia page.\\n\\n    Derivations of the formulae are simple. Consider a vector x, y, z of\\n    length r (norm of x, y, z).  The inclination angle (theta) can be\\n    found from: cos(theta) == z / r -> z == r * cos(theta).  This gives\\n    the hypotenuse of the projection onto the XY plane, which we will\\n    call Q. Q == r*sin(theta). Now x / Q == cos(phi) -> x == r *\\n    sin(theta) * cos(phi) and so on.\\n\\n    We have deliberately named this function ``sphere2cart`` rather than\\n    ``sph2cart`` to distinguish it from the Matlab function of that\\n    name, because the Matlab function uses an unusual convention for the\\n    angles that we did not want to replicate.  The Matlab function is\\n    trivial to implement with the formulae given in the Matlab help.\\n\\n    \"\"\"\\n    sin_theta = np.sin(theta)\\n    x = r * np.cos(phi) * sin_theta\\n    y = r * np.sin(phi) * sin_theta\\n    z = r * np.cos(theta)\\n    x, y, z = np.broadcast_arrays(x, y, z)\\n    return x, y, z\\n\\n\\ndef cart2sphere(x, y, z):\\n    r\"\"\" Return angles for Cartesian 3D coordinates `x`, `y`, and `z`\\n\\n    See doc for ``sphere2cart`` for angle conventions and derivation\\n    of the formulae.\\n\\n    $0\\\\le\\\\theta\\\\mathrm{(theta)}\\\\le\\\\pi$ and $-\\\\pi\\\\le\\\\phi\\\\mathrm{(phi)}\\\\le\\\\pi$', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\geometry.py.txt'}),\n",
       " Document(page_content='\"\"\"\\n    sin_theta = np.sin(theta)\\n    x = r * np.cos(phi) * sin_theta\\n    y = r * np.sin(phi) * sin_theta\\n    z = r * np.cos(theta)\\n    x, y, z = np.broadcast_arrays(x, y, z)\\n    return x, y, z\\n\\n\\ndef cart2sphere(x, y, z):\\n    r\"\"\" Return angles for Cartesian 3D coordinates `x`, `y`, and `z`\\n\\n    See doc for ``sphere2cart`` for angle conventions and derivation\\n    of the formulae.\\n\\n    $0\\\\le\\\\theta\\\\mathrm{(theta)}\\\\le\\\\pi$ and $-\\\\pi\\\\le\\\\phi\\\\mathrm{(phi)}\\\\le\\\\pi$\\n\\n    Parameters\\n    ----------\\n    x : array_like\\n       x coordinate in Cartesian space\\n    y : array_like\\n       y coordinate in Cartesian space\\n    z : array_like\\n       z coordinate\\n\\n    Returns\\n    -------\\n    r : array\\n       radius\\n    theta : array\\n       inclination (polar) angle\\n    phi : array\\n       azimuth angle\\n\\n    \"\"\"\\n    r = np.sqrt(x * x + y * y + z * z)\\n    theta = np.arccos(np.divide(z, r, where=r > 0))\\n    theta = np.where(r > 0, theta, 0.)\\n    phi = np.arctan2(y, x)\\n    r, theta, phi = np.broadcast_arrays(r, theta, phi)\\n    return r, theta, phi\\n\\n\\ndef sph2latlon(theta, phi):\\n    \"\"\"Convert spherical coordinates to latitude and longitude.\\n\\n    Returns\\n    -------\\n    lat, lon : ndarray\\n        Latitude and longitude.\\n\\n    \"\"\"\\n    return np.rad2deg(theta - np.pi / 2), np.rad2deg(phi - np.pi)\\n\\n\\ndef normalized_vector(vec, axis=-1):\\n    \"\"\" Return vector divided by its Euclidean (L2) norm\\n\\n    See :term:`unit vector` and :term:`Euclidean norm`\\n\\n    Parameters\\n    ----------\\n    vec : array_like shape (3,)\\n\\n    Returns\\n    -------\\n    nvec : array shape (3,)\\n       vector divided by L2 norm', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\geometry.py.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    lat, lon : ndarray\\n        Latitude and longitude.\\n\\n    \"\"\"\\n    return np.rad2deg(theta - np.pi / 2), np.rad2deg(phi - np.pi)\\n\\n\\ndef normalized_vector(vec, axis=-1):\\n    \"\"\" Return vector divided by its Euclidean (L2) norm\\n\\n    See :term:`unit vector` and :term:`Euclidean norm`\\n\\n    Parameters\\n    ----------\\n    vec : array_like shape (3,)\\n\\n    Returns\\n    -------\\n    nvec : array shape (3,)\\n       vector divided by L2 norm\\n\\n    Examples\\n    --------\\n    >>> vec = [1, 2, 3]\\n    >>> l2n = np.sqrt(np.dot(vec, vec))\\n    >>> nvec = normalized_vector(vec)\\n    >>> np.allclose(np.array(vec) / l2n, nvec)\\n    True\\n    >>> vec = np.array([[1, 2, 3]])\\n    >>> vec.shape == (1, 3)\\n    True\\n    >>> normalized_vector(vec).shape == (1, 3)\\n    True\\n\\n    \"\"\"\\n    return vec / vector_norm(vec, axis, keepdims=True)\\n\\n\\ndef vector_norm(vec, axis=-1, keepdims=False):\\n    \"\"\" Return vector Euclidean (L2) norm\\n\\n    See :term:`unit vector` and :term:`Euclidean norm`\\n\\n    Parameters\\n    ----------\\n    vec : array_like\\n        Vectors to norm.\\n    axis : int\\n        Axis over which to norm. By default norm over last axis. If `axis` is\\n        None, `vec` is flattened then normed.\\n    keepdims : bool\\n        If True, the output will have the same number of dimensions as `vec`,\\n        with shape 1 on `axis`.\\n\\n    Returns\\n    -------\\n    norm : array\\n        Euclidean norms of vectors.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> vec = [[8, 15, 0], [0, 36, 77]]\\n    >>> vector_norm(vec)\\n    array([ 17.,  85.])\\n    >>> vector_norm(vec, keepdims=True)\\n    array([[ 17.],\\n           [ 85.]])\\n    >>> vector_norm(vec, axis=0)\\n    array([  8.,  39.,  77.])', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\geometry.py.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    norm : array\\n        Euclidean norms of vectors.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> vec = [[8, 15, 0], [0, 36, 77]]\\n    >>> vector_norm(vec)\\n    array([ 17.,  85.])\\n    >>> vector_norm(vec, keepdims=True)\\n    array([[ 17.],\\n           [ 85.]])\\n    >>> vector_norm(vec, axis=0)\\n    array([  8.,  39.,  77.])\\n\\n    \"\"\"\\n    vec = np.asarray(vec)\\n    vec_norm = np.sqrt((vec * vec).sum(axis))\\n    if keepdims:\\n        if axis is None:\\n            shape = [1] * vec.ndim\\n        else:\\n            shape = list(vec.shape)\\n            shape[axis] = 1\\n        vec_norm = vec_norm.reshape(shape)\\n    return vec_norm\\n\\n\\ndef rodrigues_axis_rotation(r, theta):\\n    \"\"\" Rodrigues formula\\n\\n    Rotation matrix for rotation around axis r for angle theta.\\n\\n    The rotation matrix is given by the Rodrigues formula:\\n\\n    R = Id + sin(theta)*Sn + (1-cos(theta))*Sn^2\\n\\n    with::\\n\\n             0  -nz  ny\\n      Sn =   nz   0 -nx\\n            -ny  nx   0\\n\\n    where n = r / ||r||\\n\\n    In case the angle ||r|| is very small, the above formula may lead\\n    to numerical instabilities. We instead use a Taylor expansion\\n    around theta=0:\\n\\n    R = I + sin(theta)/tetha Sr + (1-cos(theta))/teta2 Sr^2\\n\\n    leading to:\\n\\n    R = I + (1-theta2/6)*Sr + (1/2-theta2/24)*Sr^2\\n\\n    Parameters\\n    ----------\\n    r :  array_like shape (3,), axis\\n    theta : float, angle in degrees\\n\\n    Returns\\n    -------\\n    R : array, shape (3,3), rotation matrix', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\geometry.py.txt'}),\n",
       " Document(page_content='where n = r / ||r||\\n\\n    In case the angle ||r|| is very small, the above formula may lead\\n    to numerical instabilities. We instead use a Taylor expansion\\n    around theta=0:\\n\\n    R = I + sin(theta)/tetha Sr + (1-cos(theta))/teta2 Sr^2\\n\\n    leading to:\\n\\n    R = I + (1-theta2/6)*Sr + (1/2-theta2/24)*Sr^2\\n\\n    Parameters\\n    ----------\\n    r :  array_like shape (3,), axis\\n    theta : float, angle in degrees\\n\\n    Returns\\n    -------\\n    R : array, shape (3,3), rotation matrix\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from dipy.core.geometry import rodrigues_axis_rotation\\n    >>> v=np.array([0,0,1])\\n    >>> u=np.array([1,0,0])\\n    >>> R=rodrigues_axis_rotation(v,40)\\n    >>> ur=np.dot(R,u)\\n    >>> np.round(np.rad2deg(np.arccos(np.dot(ur,u))))\\n    40.0\\n\\n    \"\"\"\\n    theta = np.deg2rad(theta)\\n    if theta > 1e-30:\\n        n = r / np.linalg.norm(r)\\n        Sn = np.array([[0, -n[2], n[1]], [n[2], 0, -n[0]], [-n[1], n[0], 0]])\\n        R = np.eye(3) + np.sin(theta) * Sn + \\\\\\n            (1 - np.cos(theta)) * np.dot(Sn, Sn)\\n    else:\\n        Sr = np.array([[0, -r[2], r[1]], [r[2], 0, -r[0]], [-r[1], r[0], 0]])\\n        theta2 = theta * theta\\n        R = np.eye(3) + (1 - theta2 / 6.) * \\\\\\n            Sr + (.5 - theta2 / 24.) * np.dot(Sr, Sr)\\n    return R\\n\\n\\ndef nearest_pos_semi_def(B):\\n    \"\"\" Least squares positive semi-definite tensor estimation\\n\\n    Parameters\\n    ----------\\n    B : (3,3) array_like\\n       B matrix - symmetric. We do not check the symmetry.\\n\\n    Returns\\n    -------\\n    npds : (3,3) array\\n       Estimated nearest positive semi-definite array to matrix `B`.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\geometry.py.txt'}),\n",
       " Document(page_content='def nearest_pos_semi_def(B):\\n    \"\"\" Least squares positive semi-definite tensor estimation\\n\\n    Parameters\\n    ----------\\n    B : (3,3) array_like\\n       B matrix - symmetric. We do not check the symmetry.\\n\\n    Returns\\n    -------\\n    npds : (3,3) array\\n       Estimated nearest positive semi-definite array to matrix `B`.\\n\\n    Examples\\n    --------\\n    >>> B = np.diag([1, 1, -1])\\n    >>> nearest_pos_semi_def(B)\\n    array([[ 0.75,  0.  ,  0.  ],\\n           [ 0.  ,  0.75,  0.  ],\\n           [ 0.  ,  0.  ,  0.  ]])\\n\\n    References\\n    ----------\\n    .. [1] Niethammer M, San Jose Estepar R, Bouix S, Shenton M, Westin CF.\\n           On diffusion tensor estimation. Conf Proc IEEE Eng Med Biol Soc.\\n           2006;1:2622-5. PubMed PMID: 17946125; PubMed Central PMCID:\\n           PMC2791793.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\geometry.py.txt'}),\n",
       " Document(page_content='Examples\\n    --------\\n    >>> B = np.diag([1, 1, -1])\\n    >>> nearest_pos_semi_def(B)\\n    array([[ 0.75,  0.  ,  0.  ],\\n           [ 0.  ,  0.75,  0.  ],\\n           [ 0.  ,  0.  ,  0.  ]])\\n\\n    References\\n    ----------\\n    .. [1] Niethammer M, San Jose Estepar R, Bouix S, Shenton M, Westin CF.\\n           On diffusion tensor estimation. Conf Proc IEEE Eng Med Biol Soc.\\n           2006;1:2622-5. PubMed PMID: 17946125; PubMed Central PMCID:\\n           PMC2791793.\\n\\n    \"\"\"\\n    B = np.asarray(B)\\n    vals, vecs = npl.eigh(B)\\n    # indices of eigenvalues in descending order\\n    inds = np.argsort(vals)[::-1]\\n    vals = vals[inds]\\n    cardneg = np.sum(vals < 0)\\n    if cardneg == 0:\\n        return B\\n    if cardneg == 3:\\n        return np.zeros((3, 3))\\n    lam1a, lam2a, lam3a = vals\\n    scalers = np.zeros((3,))\\n    if cardneg == 2:\\n        b112 = np.max([0, lam1a + (lam2a + lam3a) / 3.])\\n        scalers[0] = b112\\n    elif cardneg == 1:\\n        lam1b = lam1a + 0.25 * lam3a\\n        lam2b = lam2a + 0.25 * lam3a\\n        if lam1b >= 0 and lam2b >= 0:\\n            scalers[:2] = lam1b, lam2b\\n        else:  # one of the lam1b, lam2b is < 0\\n            if lam2b < 0:\\n                b111 = np.max([0, lam1a + (lam2a + lam3a) / 3.])\\n                scalers[0] = b111\\n            if lam1b < 0:\\n                b221 = np.max([0, lam2a + (lam1a + lam3a) / 3.])\\n                scalers[1] = b221\\n    # resort the scalers to match the original vecs\\n    scalers = scalers[np.argsort(inds)]\\n    return np.dot(vecs, np.dot(np.diag(scalers), vecs.T))\\n\\n\\ndef sphere_distance(pts1, pts2, radius=None, check_radius=True):\\n    \"\"\" Distance across sphere surface between `pts1` and `pts2`', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\geometry.py.txt'}),\n",
       " Document(page_content='def sphere_distance(pts1, pts2, radius=None, check_radius=True):\\n    \"\"\" Distance across sphere surface between `pts1` and `pts2`\\n\\n    Parameters\\n    ----------\\n    pts1 : (N,R) or (R,) array_like\\n       where N is the number of points and R is the number of\\n       coordinates defining a point (``R==3`` for 3D)\\n    pts2 : (N,R) or (R,) array_like\\n       where N is the number of points and R is the number of\\n       coordinates defining a point (``R==3`` for 3D).  It should be\\n       possible to broadcast `pts1` against `pts2`\\n    radius : None or float, optional\\n       Radius of sphere.  Default is to work out radius from mean of the\\n       length of each point vector\\n    check_radius : bool, optional\\n       If True, check if the points are on the sphere surface - i.e\\n       check if the vector lengths in `pts1` and `pts2` are close to\\n       `radius`.  Default is True.\\n\\n    Returns\\n    -------\\n    d : (N,) or (0,) array\\n       Distances between corresponding points in `pts1` and `pts2`\\n       across the spherical surface, i.e. the great circle distance\\n\\n    See Also\\n    --------\\n    cart_distance : cartesian distance between points\\n    vector_cosine : cosine of angle between vectors', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\geometry.py.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    d : (N,) or (0,) array\\n       Distances between corresponding points in `pts1` and `pts2`\\n       across the spherical surface, i.e. the great circle distance\\n\\n    See Also\\n    --------\\n    cart_distance : cartesian distance between points\\n    vector_cosine : cosine of angle between vectors\\n\\n    Examples\\n    --------\\n    >>> print(\\'%.4f\\' % sphere_distance([0,1],[1,0]))\\n    1.5708\\n    >>> print(\\'%.4f\\' % sphere_distance([0,3],[3,0]))\\n    4.7124\\n    \"\"\"\\n    pts1 = np.asarray(pts1)\\n    pts2 = np.asarray(pts2)\\n    lens1 = np.sqrt(np.sum(pts1 ** 2, axis=-1))\\n    lens2 = np.sqrt(np.sum(pts2 ** 2, axis=-1))\\n    if radius is None:\\n        radius = (np.mean(lens1) + np.mean(lens2)) / 2.0\\n    if check_radius:\\n        if not (np.allclose(radius, lens1) and\\n                np.allclose(radius, lens2)):\\n            raise ValueError(\\'Radii do not match sphere surface\\')\\n    # Get angle with vector cosine\\n    dots = np.inner(pts1, pts2)\\n    lens = lens1 * lens2\\n    angle_cos = np.arccos(dots / lens)\\n    return angle_cos * radius\\n\\n\\ndef cart_distance(pts1, pts2):\\n    \"\"\" Cartesian distance between `pts1` and `pts2`\\n\\n    If either of `pts1` or `pts2` is 2D, then we take the first\\n    dimension to index points, and the second indexes coordinate.  More\\n    generally, we take the last dimension to be the coordinate\\n    dimension.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\geometry.py.txt'}),\n",
       " Document(page_content='def cart_distance(pts1, pts2):\\n    \"\"\" Cartesian distance between `pts1` and `pts2`\\n\\n    If either of `pts1` or `pts2` is 2D, then we take the first\\n    dimension to index points, and the second indexes coordinate.  More\\n    generally, we take the last dimension to be the coordinate\\n    dimension.\\n\\n    Parameters\\n    ----------\\n    pts1 : (N,R) or (R,) array_like\\n       where N is the number of points and R is the number of\\n       coordinates defining a point (``R==3`` for 3D)\\n    pts2 : (N,R) or (R,) array_like\\n       where N is the number of points and R is the number of\\n       coordinates defining a point (``R==3`` for 3D).  It should be\\n       possible to broadcast `pts1` against `pts2`\\n\\n    Returns\\n    -------\\n    d : (N,) or (0,) array\\n       Cartesian distances between corresponding points in `pts1` and\\n       `pts2`\\n\\n    See Also\\n    --------\\n    sphere_distance : distance between points on sphere surface\\n\\n    Examples\\n    --------\\n    >>> cart_distance([0,0,0], [0,0,3])\\n    3.0\\n    \"\"\"\\n    sqs = np.subtract(pts1, pts2) ** 2\\n    return np.sqrt(np.sum(sqs, axis=-1))\\n\\n\\ndef vector_cosine(vecs1, vecs2):\\n    \"\"\" Cosine of angle between two (sets of) vectors\\n\\n    The cosine of the angle between two vectors ``v1`` and ``v2`` is\\n    given by the inner product of ``v1`` and ``v2`` divided by the\\n    product of the vector lengths::\\n\\n       v_cos = np.inner(v1, v2) / (np.sqrt(np.sum(v1**2)) *\\n                                   np.sqrt(np.sum(v2**2)))', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\geometry.py.txt'}),\n",
       " Document(page_content='def vector_cosine(vecs1, vecs2):\\n    \"\"\" Cosine of angle between two (sets of) vectors\\n\\n    The cosine of the angle between two vectors ``v1`` and ``v2`` is\\n    given by the inner product of ``v1`` and ``v2`` divided by the\\n    product of the vector lengths::\\n\\n       v_cos = np.inner(v1, v2) / (np.sqrt(np.sum(v1**2)) *\\n                                   np.sqrt(np.sum(v2**2)))\\n\\n    Parameters\\n    ----------\\n    vecs1 : (N, R) or (R,) array_like\\n       N vectors (as rows) or single vector.  Vectors have R elements.\\n    vecs1 : (N, R) or (R,) array_like\\n       N vectors (as rows) or single vector.  Vectors have R elements.\\n       It should be possible to broadcast `vecs1` against `vecs2`\\n\\n    Returns\\n    -------\\n    vcos : (N,) or (0,) array\\n       Vector cosines.  To get the angles you will need ``np.arccos``\\n\\n    Notes\\n    -----\\n    The vector cosine will be the same as the correlation only if all\\n    the input vectors have zero mean.\\n\\n    \"\"\"\\n    vecs1 = np.asarray(vecs1)\\n    vecs2 = np.asarray(vecs2)\\n    lens1 = np.sqrt(np.sum(vecs1 ** 2, axis=-1))\\n    lens2 = np.sqrt(np.sum(vecs2 ** 2, axis=-1))\\n    dots = np.inner(vecs1, vecs2)\\n    lens = lens1 * lens2\\n    return dots / lens\\n\\n\\ndef lambert_equal_area_projection_polar(theta, phi):\\n    r\"\"\" Lambert Equal Area Projection from polar sphere to plane\\n\\n    Return positions in (y1,y2) plane corresponding to the points\\n    with polar coordinates (theta, phi) on the unit sphere, under the\\n    Lambert Equal Area Projection mapping (see Mardia and Jupp (2000),\\n    Directional Statistics, p. 161).\\n\\n    See doc for ``sphere2cart`` for angle conventions\\n\\n    - $0 \\\\le \\\\theta \\\\le \\\\pi$ and $0 \\\\le \\\\phi \\\\le 2 \\\\pi$\\n    - $|(y_1,y_2)| \\\\le 2$', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\geometry.py.txt'}),\n",
       " Document(page_content='Return positions in (y1,y2) plane corresponding to the points\\n    with polar coordinates (theta, phi) on the unit sphere, under the\\n    Lambert Equal Area Projection mapping (see Mardia and Jupp (2000),\\n    Directional Statistics, p. 161).\\n\\n    See doc for ``sphere2cart`` for angle conventions\\n\\n    - $0 \\\\le \\\\theta \\\\le \\\\pi$ and $0 \\\\le \\\\phi \\\\le 2 \\\\pi$\\n    - $|(y_1,y_2)| \\\\le 2$\\n\\n    The Lambert EAP maps the upper hemisphere to the planar disc of radius 1\\n    and the lower hemisphere to the planar annulus between radii 1 and 2,\\n    and *vice versa*.\\n\\n    Parameters\\n    ----------\\n    theta : array_like\\n       theta spherical coordinates\\n    phi : array_like\\n       phi spherical coordinates\\n\\n    Returns\\n    -------\\n    y : (N,2) array\\n       planar coordinates of points following mapping by Lambert\\'s EAP.\\n    \"\"\"\\n\\n    return 2 * np.repeat(np.sin(theta / 2), 2).reshape((theta.shape[0], 2)) * \\\\\\n        np.column_stack((np.cos(phi), np.sin(phi)))\\n\\n\\ndef lambert_equal_area_projection_cart(x, y, z):\\n    r\"\"\" Lambert Equal Area Projection from cartesian vector to plane\\n\\n    Return positions in $(y_1,y_2)$ plane corresponding to the\\n    directions of the vectors with cartesian coordinates xyz under the\\n    Lambert Equal Area Projection mapping (see Mardia and Jupp (2000),\\n    Directional Statistics, p. 161).\\n\\n    The Lambert EAP maps the upper hemisphere to the planar disc of radius 1\\n    and the lower hemisphere to the planar annulus between radii 1 and 2,\\n    The Lambert EAP maps the upper hemisphere to the planar disc of radius 1\\n    and the lower hemisphere to the planar annulus between radii 1 and 2.\\n    and *vice versa*.\\n\\n    See doc for ``sphere2cart`` for angle conventions', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\geometry.py.txt'}),\n",
       " Document(page_content='The Lambert EAP maps the upper hemisphere to the planar disc of radius 1\\n    and the lower hemisphere to the planar annulus between radii 1 and 2,\\n    The Lambert EAP maps the upper hemisphere to the planar disc of radius 1\\n    and the lower hemisphere to the planar annulus between radii 1 and 2.\\n    and *vice versa*.\\n\\n    See doc for ``sphere2cart`` for angle conventions\\n\\n    Parameters\\n    ----------\\n    x : array_like\\n       x coordinate in Cartesian space\\n    y : array_like\\n       y coordinate in Cartesian space\\n    z : array_like\\n       z coordinate\\n\\n    Returns\\n    -------\\n    y : (N,2) array\\n       planar coordinates of points following mapping by Lambert\\'s EAP.\\n\\n    \"\"\"\\n    (r, theta, phi) = cart2sphere(x, y, z)\\n    return lambert_equal_area_projection_polar(theta, phi)\\n\\n\\ndef euler_matrix(ai, aj, ak, axes=\\'sxyz\\'):\\n    \"\"\"Return homogeneous rotation matrix from Euler angles and axis sequence.\\n\\n    Code modified from the work of Christoph Gohlke, link provided here\\n    https://github.com/cgohlke/transformations/blob/master/transformations/transformations.py\\n\\n    Parameters\\n    ----------\\n    ai, aj, ak : Euler\\'s roll, pitch and yaw angles\\n    axes : One of 24 axis sequences as string or encoded tuple\\n\\n    Returns\\n    -------\\n    matrix : ndarray (4, 4)\\n\\n    Code modified from the work of Christoph Gohlke, link provided here\\n    https://github.com/cgohlke/transformations/blob/master/transformations/transformations.py', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\geometry.py.txt'}),\n",
       " Document(page_content='Parameters\\n    ----------\\n    ai, aj, ak : Euler\\'s roll, pitch and yaw angles\\n    axes : One of 24 axis sequences as string or encoded tuple\\n\\n    Returns\\n    -------\\n    matrix : ndarray (4, 4)\\n\\n    Code modified from the work of Christoph Gohlke, link provided here\\n    https://github.com/cgohlke/transformations/blob/master/transformations/transformations.py\\n\\n    Examples\\n    --------\\n    >>> import numpy\\n    >>> R = euler_matrix(1, 2, 3, \\'syxz\\')\\n    >>> numpy.allclose(numpy.sum(R[0]), -1.34786452)\\n    True\\n    >>> R = euler_matrix(1, 2, 3, (0, 1, 0, 1))\\n    >>> numpy.allclose(numpy.sum(R[0]), -0.383436184)\\n    True\\n    >>> ai, aj, ak = (4.0*math.pi) * (numpy.random.random(3) - 0.5)\\n    >>> for axes in _AXES2TUPLE.keys():\\n    ...    _ = euler_matrix(ai, aj, ak, axes)\\n    >>> for axes in _TUPLE2AXES.keys():\\n    ...    _ = euler_matrix(ai, aj, ak, axes)\\n\\n    \"\"\"\\n    try:\\n        firstaxis, parity, repetition, frame = _AXES2TUPLE[axes]\\n    except (AttributeError, KeyError):\\n        firstaxis, parity, repetition, frame = axes\\n\\n    i = firstaxis\\n    j = _NEXT_AXIS[i + parity]\\n    k = _NEXT_AXIS[i - parity + 1]\\n\\n    if frame:\\n        ai, ak = ak, ai\\n    if parity:\\n        ai, aj, ak = -ai, -aj, -ak\\n\\n    si, sj, sk = math.sin(ai), math.sin(aj), math.sin(ak)\\n    ci, cj, ck = math.cos(ai), math.cos(aj), math.cos(ak)\\n    cc, cs = ci * ck, ci * sk\\n    sc, ss = si * ck, si * sk', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\geometry.py.txt'}),\n",
       " Document(page_content='i = firstaxis\\n    j = _NEXT_AXIS[i + parity]\\n    k = _NEXT_AXIS[i - parity + 1]\\n\\n    if frame:\\n        ai, ak = ak, ai\\n    if parity:\\n        ai, aj, ak = -ai, -aj, -ak\\n\\n    si, sj, sk = math.sin(ai), math.sin(aj), math.sin(ak)\\n    ci, cj, ck = math.cos(ai), math.cos(aj), math.cos(ak)\\n    cc, cs = ci * ck, ci * sk\\n    sc, ss = si * ck, si * sk\\n\\n    M = np.identity(4)\\n    if repetition:\\n        M[i, i] = cj\\n        M[i, j] = sj * si\\n        M[i, k] = sj * ci\\n        M[j, i] = sj * sk\\n        M[j, j] = -cj * ss + cc\\n        M[j, k] = -cj * cs - sc\\n        M[k, i] = -sj * ck\\n        M[k, j] = cj * sc + cs\\n        M[k, k] = cj * cc - ss\\n    else:\\n        M[i, i] = cj * ck\\n        M[i, j] = sj * sc - cs\\n        M[i, k] = sj * cc + ss\\n        M[j, i] = cj * sk\\n        M[j, j] = sj * ss + cc\\n        M[j, k] = sj * cs - sc\\n        M[k, i] = -sj\\n        M[k, j] = cj * si\\n        M[k, k] = cj * ci\\n    return M\\n\\n\\ndef compose_matrix(scale=None, shear=None, angles=None, translate=None,\\n                   perspective=None):\\n    \"\"\"Return 4x4 transformation matrix from sequence of\\n    transformations.\\n\\n    Code modified from the work of Christoph Gohlke, link provided here\\n    https://github.com/cgohlke/transformations/blob/master/transformations/transformations.py\\n\\n    This is the inverse of the ``decompose_matrix`` function.\\n\\n    Parameters\\n    ----------\\n    scale : (3,) array_like\\n        Scaling factors.\\n    shear : array_like\\n        Shear factors for x-y, x-z, y-z axes.\\n    angles : array_like\\n        Euler angles about static x, y, z axes.\\n    translate : array_like\\n        Translation vector along x, y, z axes.\\n    perspective : array_like\\n        Perspective partition of matrix.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\geometry.py.txt'}),\n",
       " Document(page_content='This is the inverse of the ``decompose_matrix`` function.\\n\\n    Parameters\\n    ----------\\n    scale : (3,) array_like\\n        Scaling factors.\\n    shear : array_like\\n        Shear factors for x-y, x-z, y-z axes.\\n    angles : array_like\\n        Euler angles about static x, y, z axes.\\n    translate : array_like\\n        Translation vector along x, y, z axes.\\n    perspective : array_like\\n        Perspective partition of matrix.\\n\\n    Returns\\n    -------\\n    matrix : 4x4 array\\n\\n\\n    Examples\\n    --------\\n    >>> import math\\n    >>> import numpy as np\\n    >>> import dipy.core.geometry as gm\\n    >>> scale = np.random.random(3) - 0.5\\n    >>> shear = np.random.random(3) - 0.5\\n    >>> angles = (np.random.random(3) - 0.5) * (2*math.pi)\\n    >>> trans = np.random.random(3) - 0.5\\n    >>> persp = np.random.random(4) - 0.5\\n    >>> M0 = gm.compose_matrix(scale, shear, angles, trans, persp)\\n\\n    \"\"\"\\n    M = np.identity(4)\\n    if perspective is not None:\\n        P = np.identity(4)\\n        P[3, :] = perspective[:4]\\n        M = np.dot(M, P)\\n    if translate is not None:\\n        T = np.identity(4)\\n        T[:3, 3] = translate[:3]\\n        M = np.dot(M, T)\\n    if angles is not None:\\n        R = euler_matrix(angles[0], angles[1], angles[2], \\'sxyz\\')\\n        M = np.dot(M, R)\\n    if shear is not None:\\n        Z = np.identity(4)\\n        Z[1, 2] = shear[2]\\n        Z[0, 2] = shear[1]\\n        Z[0, 1] = shear[0]\\n        M = np.dot(M, Z)\\n    if scale is not None:\\n        S = np.identity(4)\\n        S[0, 0] = scale[0]\\n        S[1, 1] = scale[1]\\n        S[2, 2] = scale[2]\\n        M = np.dot(M, S)\\n    M /= M[3, 3]\\n    return M', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\geometry.py.txt'}),\n",
       " Document(page_content='def decompose_matrix(matrix):\\n    \"\"\"Return sequence of transformations from transformation matrix.\\n\\n    Code modified from the excellent work of Christoph Gohlke, link\\n    provided here:\\n    https://github.com/cgohlke/transformations/blob/master/transformations/transformations.py\\n\\n    Parameters\\n    ----------\\n    matrix : array_like\\n        Non-degenerate homogeneous transformation matrix\\n\\n    Returns\\n    -------\\n    scale : (3,) ndarray\\n        Three scaling factors.\\n    shear : (3,) ndarray\\n        Shear factors for x-y, x-z, y-z axes.\\n    angles : (3,) ndarray\\n        Euler angles about static x, y, z axes.\\n    translate : (3,) ndarray\\n        Translation vector along x, y, z axes.\\n    perspective : ndarray\\n        Perspective partition of matrix.\\n\\n    Raises\\n    ------\\n    ValueError\\n        If matrix is of wrong type or degenerate.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> T0=np.diag([2,1,1,1])\\n    >>> scale, shear, angles, trans, persp = decompose_matrix(T0)\\n\\n    \"\"\"\\n    M = np.array(matrix, dtype=np.float64, copy=True).T\\n    if abs(M[3, 3]) < _EPS:\\n        raise ValueError(\"M[3, 3] is zero\")\\n    M /= M[3, 3]\\n    P = M.copy()\\n    P[:, 3] = 0, 0, 0, 1\\n    if not np.linalg.det(P):\\n        raise ValueError(\"matrix is singular\")\\n\\n    scale = np.zeros((3, ), dtype=np.float64)\\n    shear = [0, 0, 0]\\n    angles = [0, 0, 0]\\n\\n    if any(abs(M[:3, 3]) > _EPS):\\n        perspective = np.dot(M[:, 3], np.linalg.inv(P.T))\\n        M[:, 3] = 0, 0, 0, 1\\n    else:\\n        perspective = np.array((0, 0, 0, 1), dtype=np.float64)\\n\\n    translate = M[3, :3].copy()\\n    M[3, :3] = 0', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\geometry.py.txt'}),\n",
       " Document(page_content='scale = np.zeros((3, ), dtype=np.float64)\\n    shear = [0, 0, 0]\\n    angles = [0, 0, 0]\\n\\n    if any(abs(M[:3, 3]) > _EPS):\\n        perspective = np.dot(M[:, 3], np.linalg.inv(P.T))\\n        M[:, 3] = 0, 0, 0, 1\\n    else:\\n        perspective = np.array((0, 0, 0, 1), dtype=np.float64)\\n\\n    translate = M[3, :3].copy()\\n    M[3, :3] = 0\\n\\n    row = M[:3, :3].copy()\\n    scale[0] = vector_norm(row[0])\\n    row[0] /= scale[0]\\n    shear[0] = np.dot(row[0], row[1])\\n    row[1] -= row[0] * shear[0]\\n    scale[1] = vector_norm(row[1])\\n    row[1] /= scale[1]\\n    shear[0] /= scale[1]\\n    shear[1] = np.dot(row[0], row[2])\\n    row[2] -= row[0] * shear[1]\\n    shear[2] = np.dot(row[1], row[2])\\n    row[2] -= row[1] * shear[2]\\n    scale[2] = vector_norm(row[2])\\n    row[2] /= scale[2]\\n    shear[1:] /= scale[2]\\n\\n    if np.dot(row[0], np.cross(row[1], row[2])) < 0:\\n        scale *= -1\\n        row *= -1\\n\\n    angles[1] = math.asin(-row[0, 2])\\n    if math.cos(angles[1]):\\n        angles[0] = math.atan2(row[1, 2], row[2, 2])\\n        angles[2] = math.atan2(row[0, 1], row[0, 0])\\n    else:\\n        # angles[0] = math.atan2(row[1, 0], row[1, 1])\\n        angles[0] = math.atan2(-row[2, 1], row[1, 1])\\n        angles[2] = 0.0\\n\\n    return scale, shear, angles, translate, perspective\\n\\n\\ndef circumradius(a, b, c):\\n    \"\"\" a, b and c are 3-dimensional vectors which are the vertices of a\\n    triangle. The function returns the circumradius of the triangle, i.e\\n    the radius of the smallest circle that can contain the triangle. In\\n    the degenerate case when the 3 points are collinear it returns\\n    half the distance between the furthest apart points.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\geometry.py.txt'}),\n",
       " Document(page_content='return scale, shear, angles, translate, perspective\\n\\n\\ndef circumradius(a, b, c):\\n    \"\"\" a, b and c are 3-dimensional vectors which are the vertices of a\\n    triangle. The function returns the circumradius of the triangle, i.e\\n    the radius of the smallest circle that can contain the triangle. In\\n    the degenerate case when the 3 points are collinear it returns\\n    half the distance between the furthest apart points.\\n\\n    Parameters\\n    ----------\\n    a, b, c : (3,) array_like\\n       the three vertices of the triangle\\n\\n    Returns\\n    -------\\n    circumradius : float\\n        the desired circumradius\\n    \"\"\"\\n    x = a - c\\n    xx = np.linalg.norm(x) ** 2\\n    y = b - c\\n    yy = np.linalg.norm(y) ** 2\\n    z = np.cross(x, y)\\n    # test for collinearity\\n    if np.linalg.norm(z) == 0:\\n        return np.sqrt(np.max(np.dot(x, x), np.dot(y, y),\\n                              np.dot(a - b, a - b))) / 2.\\n    else:\\n        m = np.vstack((x, y, z))\\n        w = np.dot(np.linalg.inv(m.T), np.array([xx / 2., yy / 2., 0]))\\n        return np.linalg.norm(w) / 2.\\n\\n\\ndef vec2vec_rotmat(u, v):\\n    r\"\"\" rotation matrix from 2 unit vectors\\n\\n    u, v being unit 3d vectors return a 3x3 rotation matrix R than aligns u to\\n    v.\\n\\n    In general there are many rotations that will map u to v. If S is any\\n    rotation using v as an axis then R.S will also map u to v since (S.R)u =\\n    S(Ru) = Sv = v.  The rotation R returned by vec2vec_rotmat leaves fixed the\\n    perpendicular to the plane spanned by u and v.\\n\\n    The transpose of R will align v to u.\\n\\n    Parameters\\n    ----------\\n    u : array, shape(3,)\\n    v : array, shape(3,)\\n\\n    Returns\\n    -------\\n    R : array, shape(3,3)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\geometry.py.txt'}),\n",
       " Document(page_content='In general there are many rotations that will map u to v. If S is any\\n    rotation using v as an axis then R.S will also map u to v since (S.R)u =\\n    S(Ru) = Sv = v.  The rotation R returned by vec2vec_rotmat leaves fixed the\\n    perpendicular to the plane spanned by u and v.\\n\\n    The transpose of R will align v to u.\\n\\n    Parameters\\n    ----------\\n    u : array, shape(3,)\\n    v : array, shape(3,)\\n\\n    Returns\\n    -------\\n    R : array, shape(3,3)\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from dipy.core.geometry import vec2vec_rotmat\\n    >>> u=np.array([1,0,0])\\n    >>> v=np.array([0,1,0])\\n    >>> R=vec2vec_rotmat(u,v)\\n    >>> np.dot(R,u)\\n    array([ 0.,  1.,  0.])\\n    >>> np.dot(R.T,v)\\n    array([ 1.,  0.,  0.])\\n\\n    \"\"\"\\n    # Cross product is the first step to find R\\n    # Rely on numpy instead of manual checking for failing\\n    # cases\\n    w = np.cross(u, v)\\n    wn = np.linalg.norm(w)\\n\\n    # Check that cross product is OK and vectors\\n    # u, v are not collinear (norm(w)>0.0)\\n    if np.isnan(wn) or wn < np.finfo(float).eps:\\n        norm_u_v = np.linalg.norm(u - v)\\n        # This is the case of two antipodal vectors:\\n        # ** former checking assumed norm(u) == norm(v)\\n        if norm_u_v > np.linalg.norm(u):\\n            return -np.eye(3)\\n        return np.eye(3)\\n\\n    # if everything ok, normalize w\\n    w = w / wn\\n\\n    # vp is in plane of u,v,  perpendicular to u\\n    vp = (v - (np.dot(u, v) * u))\\n    vp = vp / np.linalg.norm(vp)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\geometry.py.txt'}),\n",
       " Document(page_content='# if everything ok, normalize w\\n    w = w / wn\\n\\n    # vp is in plane of u,v,  perpendicular to u\\n    vp = (v - (np.dot(u, v) * u))\\n    vp = vp / np.linalg.norm(vp)\\n\\n    # (u vp w) is an orthonormal basis\\n    P = np.array([u, vp, w])\\n    Pt = P.T\\n    cosa = np.clip(np.dot(u, v), -1, 1)\\n    sina = np.sqrt(1 - cosa ** 2)\\n    R = np.array([[cosa, -sina, 0], [sina, cosa, 0], [0, 0, 1]])\\n    Rp = np.dot(Pt, np.dot(R, P))\\n\\n    # make sure that you don\\'t return any Nans\\n    # check using the appropriate tool in numpy\\n    if np.any(np.isnan(Rp)):\\n        return np.eye(3)\\n\\n    return Rp\\n\\n\\ndef compose_transformations(*mats):\\n    \"\"\" Compose multiple 4x4 affine transformations in one 4x4 matrix\\n\\n    Parameters\\n    ----------\\n\\n    mat1 : array, (4, 4)\\n    mat2 : array, (4, 4)\\n    ...\\n    matN : array, (4, 4)\\n\\n    Returns\\n    -------\\n    matN x ... x mat2 x mat1 : array, (4, 4)\\n\\n    \"\"\"\\n    prev = mats[0]\\n    if len(mats) < 2:\\n        raise ValueError(\\'At least two or more matrices are needed\\')\\n\\n    for mat in mats[1:]:\\n\\n        prev = np.dot(mat, prev)\\n\\n    return prev\\n\\n\\ndef perpendicular_directions(v, num=30, half=False):\\n    r\"\"\" Computes n evenly spaced perpendicular directions relative to a given\\n    vector v\\n\\n    Parameters\\n    ----------\\n    v : array (3,)\\n        Array containing the three cartesian coordinates of vector v\\n    num : int, optional\\n        Number of perpendicular directions to generate\\n    half : bool, optional\\n        If half is True, perpendicular directions are sampled on half of the\\n        unit circumference perpendicular to v, otherwive perpendicular\\n        directions are sampled on the full circumference. Default of half is\\n        False', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\geometry.py.txt'}),\n",
       " Document(page_content='Parameters\\n    ----------\\n    v : array (3,)\\n        Array containing the three cartesian coordinates of vector v\\n    num : int, optional\\n        Number of perpendicular directions to generate\\n    half : bool, optional\\n        If half is True, perpendicular directions are sampled on half of the\\n        unit circumference perpendicular to v, otherwive perpendicular\\n        directions are sampled on the full circumference. Default of half is\\n        False\\n\\n    Returns\\n    -------\\n    psamples : array (n, 3)\\n        array of vectors perpendicular to v\\n\\n    Notes\\n    -----\\n    Perpendicular directions are estimated using the following two step\\n    procedure:\\n\\n        1) the perpendicular directions are first sampled in a unit\\n        circumference parallel to the plane normal to the x-axis.\\n\\n        2) Samples are then rotated and aligned to the plane normal to vector\\n        v. The rotational matrix for this rotation is constructed as reference\\n        frame basis which axis are the following:\\n            - The first axis is vector v\\n            - The second axis is defined as the normalized vector given by the\\n            cross product between vector v and the unit vector aligned to the\\n            x-axis\\n            - The third axis is defined as the cross product between the\\n            previous computed vector and vector v.\\n\\n    Following this two steps, coordinates of the final perpendicular directions\\n    are given as:\\n\\n    .. math::', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\geometry.py.txt'}),\n",
       " Document(page_content='Following this two steps, coordinates of the final perpendicular directions\\n    are given as:\\n\\n    .. math::\\n\\n        \\\\left [ -\\\\sin(a_{i}) \\\\sqrt{{v_{y}}^{2}+{v_{z}}^{2}}\\n        \\\\; , \\\\;\\n        \\\\frac{v_{x}v_{y}\\\\sin(a_{i})-v_{z}\\\\cos(a_{i})}\\n        {\\\\sqrt{{v_{y}}^{2}+{v_{z}}^{2}}}\\n        \\\\; , \\\\;\\n        \\\\frac{v_{x}v_{z}\\\\sin(a_{i})-v_{y}\\\\cos(a_{i})}\\n        {\\\\sqrt{{v_{y}}^{2}+{v_{z}}^{2}}} \\\\right  ]\\n\\n    This procedure has a singularity when vector v is aligned to the x-axis. To\\n    solve this singularity, perpendicular directions in procedure\\'s step 1 are\\n    defined in the plane normal to y-axis and the second axis of the rotated\\n    frame of reference is computed as the normalized vector given by the cross\\n    product between vector v and the unit vector aligned to the y-axis.\\n    Following this, the coordinates of the perpendicular directions are given\\n    as:\\n\\n        \\\\left [ -\\\\frac{\\\\left (v_{x}v_{y}\\\\sin(a_{i})+v_{z}\\\\cos(a_{i}) \\\\right )}\\n        {\\\\sqrt{{v_{x}}^{2}+{v_{z}}^{2}}}\\n        \\\\; , \\\\;\\n        \\\\sin(a_{i}) \\\\sqrt{{v_{x}}^{2}+{v_{z}}^{2}}\\n        \\\\; , \\\\;\\n        \\\\frac{v_{y}v_{z}\\\\sin(a_{i})+v_{x}\\\\cos(a_{i})}\\n        {\\\\sqrt{{v_{x}}^{2}+{v_{z}}^{2}}} \\\\right  ]\\n\\n    For more details on this calculation, see\\n    `here <https://gsoc2015dipydki.blogspot.com/2015/07/rnh-post-8-computing-perpendicular.html>`_.\\n\\n    \"\"\"  # noqa: E501\\n    v = np.array(v, dtype=float)\\n\\n    # Float error used for floats comparison\\n    er = np.finfo(v[0]).eps * 1e3\\n\\n    # Define circumference or semi-circumference\\n    if half is True:\\n        a = np.linspace(0., math.pi, num=num, endpoint=False)\\n    else:\\n        a = np.linspace(0., 2 * math.pi, num=num, endpoint=False)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\geometry.py.txt'}),\n",
       " Document(page_content='For more details on this calculation, see\\n    `here <https://gsoc2015dipydki.blogspot.com/2015/07/rnh-post-8-computing-perpendicular.html>`_.\\n\\n    \"\"\"  # noqa: E501\\n    v = np.array(v, dtype=float)\\n\\n    # Float error used for floats comparison\\n    er = np.finfo(v[0]).eps * 1e3\\n\\n    # Define circumference or semi-circumference\\n    if half is True:\\n        a = np.linspace(0., math.pi, num=num, endpoint=False)\\n    else:\\n        a = np.linspace(0., 2 * math.pi, num=num, endpoint=False)\\n\\n    cosa = np.cos(a)\\n    sina = np.sin(a)\\n\\n    # Check if vector is not aligned to the x axis\\n    if abs(v[0] - 1.) > er:\\n        sq = np.sqrt(v[1]**2 + v[2]**2)\\n        psamples = np.array([- sq*sina, (v[0]*v[1]*sina - v[2]*cosa) / sq,\\n                             (v[0]*v[2]*sina + v[1]*cosa) / sq])\\n    else:\\n        sq = np.sqrt(v[0]**2 + v[2]**2)\\n        psamples = np.array([- (v[2]*cosa + v[0]*v[1]*sina) / sq, sina*sq,\\n                             (v[0]*cosa - v[2]*v[1]*sina) / sq])\\n\\n    return psamples.T\\n\\n\\ndef dist_to_corner(affine):\\n    \"\"\"Calculate the maximal distance from the center to a corner of a voxel,\\n    given an affine\\n\\n    Parameters\\n    ----------\\n    affine : 4 by 4 array.\\n        The spatial transformation from the measurement to the scanner space.\\n\\n    Returns\\n    -------\\n    dist: float\\n        The maximal distance to the corner of a voxel, given voxel size encoded\\n        in the affine.\\n\\n    \"\"\"\\n    R = affine[0:3, 0:3]\\n    vox_dim = np.diag(np.linalg.cholesky(R.T.dot(R)))\\n    return np.sqrt(np.sum((vox_dim / 2) ** 2))\\n\\n\\ndef is_hemispherical(vecs):\\n    \"\"\"Test whether all points on a unit sphere lie in the same hemisphere.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\geometry.py.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    dist: float\\n        The maximal distance to the corner of a voxel, given voxel size encoded\\n        in the affine.\\n\\n    \"\"\"\\n    R = affine[0:3, 0:3]\\n    vox_dim = np.diag(np.linalg.cholesky(R.T.dot(R)))\\n    return np.sqrt(np.sum((vox_dim / 2) ** 2))\\n\\n\\ndef is_hemispherical(vecs):\\n    \"\"\"Test whether all points on a unit sphere lie in the same hemisphere.\\n\\n    Parameters\\n    ----------\\n    vecs : numpy.ndarray\\n        2D numpy array with shape (N, 3) where N is the number of points.\\n        All points must lie on the unit sphere.\\n\\n    Returns\\n    -------\\n    is_hemi : bool\\n        If True, one can find a hemisphere that contains all the points.\\n        If False, then the points do not lie in any hemisphere\\n\\n    pole : numpy.ndarray\\n        If `is_hemi == True`, then pole is the \"central\" pole of the\\n        input vectors. Otherwise, pole is the zero vector.\\n\\n    References\\n    ----------\\n    https://rstudio-pubs-static.s3.amazonaws.com/27121_a22e51b47c544980bad594d5e0bb2d04.html  # noqa\\n\\n    \"\"\"\\n    if vecs.shape[1] != 3:\\n        raise ValueError(\"Input vectors must be 3D vectors\")\\n    if not np.allclose(1, np.linalg.norm(vecs, axis=1)):\\n        raise ValueError(\"Input vectors must be unit vectors\")\\n\\n    # Generate all pairwise cross products\\n    v0, v1 = zip(*[p for p in itertools.permutations(vecs, 2)])\\n    cross_prods = np.cross(v0, v1)\\n\\n    # Normalize them\\n    cross_prods /= np.linalg.norm(cross_prods, axis=1)[:, np.newaxis]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\geometry.py.txt'}),\n",
       " Document(page_content='\"\"\"\\n    if vecs.shape[1] != 3:\\n        raise ValueError(\"Input vectors must be 3D vectors\")\\n    if not np.allclose(1, np.linalg.norm(vecs, axis=1)):\\n        raise ValueError(\"Input vectors must be unit vectors\")\\n\\n    # Generate all pairwise cross products\\n    v0, v1 = zip(*[p for p in itertools.permutations(vecs, 2)])\\n    cross_prods = np.cross(v0, v1)\\n\\n    # Normalize them\\n    cross_prods /= np.linalg.norm(cross_prods, axis=1)[:, np.newaxis]\\n\\n    # `cross_prods` now contains all candidate vertex points for \"the polygon\"\\n    # in the reference. \"The polygon\" is a subset. Find which points belong to\\n    # the polygon using a dot product test with each of the original vectors\\n    angles = np.arccos(np.dot(cross_prods, vecs.transpose()))\\n\\n    # And test whether it is orthogonal or less\\n    dot_prod_test = angles <= np.pi / 2.0\\n\\n    # If there is at least one point that is orthogonal or less to each\\n    # input vector, then the points lie on some hemisphere\\n    is_hemi = len(vecs) in np.sum(dot_prod_test.astype(int), axis=1)\\n\\n    if is_hemi:\\n        vertices = cross_prods[\\n            np.sum(dot_prod_test.astype(int), axis=1) == len(vecs)\\n        ]\\n\\n        pole = np.mean(vertices, axis=0)\\n        pole /= np.linalg.norm(pole)\\n    else:\\n        pole = np.array([0.0, 0.0, 0.0])\\n    return is_hemi, pole', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\geometry.py.txt'}),\n",
       " Document(page_content='import logging\\nfrom warnings import warn\\n\\nimport numpy as np\\nfrom scipy.linalg import inv, polar\\n\\nfrom dipy.io import gradients as io\\nfrom dipy.core.onetime import auto_attr\\nfrom dipy.core.geometry import vector_norm, vec2vec_rotmat\\nfrom dipy.core.sphere import disperse_charges, HemiSphere\\n\\nfrom dipy.utils.deprecator import deprecate_with_version\\n\\n\\nWATER_GYROMAGNETIC_RATIO = 267.513e6  # 1/(sT)\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\n@deprecate_with_version(\"dipy.core.gradients.unique_bvals is deprecated, \"\\n                        \"Please use \"\\n                        \"dipy.core.gradients.unique_bvals_magnitude instead\",\\n                        since=\\'1.2\\', until=\\'1.4\\')\\ndef unique_bvals(bvals, bmag=None, rbvals=False):\\n    \"\"\" This function gives the unique rounded b-values of the data\\n\\n    Parameters\\n    ----------\\n    bvals : ndarray\\n        Array containing the b-values\\n\\n    bmag : int\\n        The order of magnitude that the bvalues have to differ to be\\n        considered an unique b-value. B-values are also rounded up to\\n        this order of magnitude. Default: derive this value from the\\n        maximal b-value provided: $bmag=log_{10}(max(bvals)) - 1$.\\n\\n    rbvals : bool, optional\\n        If True function also returns all individual rounded b-values.\\n        Default: False\\n\\n    Returns\\n    -------\\n    ubvals : ndarray\\n        Array containing the rounded unique b-values\\n    \"\"\"\\n    b = round_bvals(bvals, bmag)\\n    if rbvals:\\n        return np.unique(b), b\\n\\n    return np.unique(b)\\n\\n\\ndef b0_threshold_empty_gradient_message(bvals, idx, b0_threshold):\\n    \"\"\"Message about the ``b0_threshold`` value resulting in no gradient\\n    selection.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='rbvals : bool, optional\\n        If True function also returns all individual rounded b-values.\\n        Default: False\\n\\n    Returns\\n    -------\\n    ubvals : ndarray\\n        Array containing the rounded unique b-values\\n    \"\"\"\\n    b = round_bvals(bvals, bmag)\\n    if rbvals:\\n        return np.unique(b), b\\n\\n    return np.unique(b)\\n\\n\\ndef b0_threshold_empty_gradient_message(bvals, idx, b0_threshold):\\n    \"\"\"Message about the ``b0_threshold`` value resulting in no gradient\\n    selection.\\n\\n    Parameters\\n    ----------\\n    bvals : (N,) ndarray\\n        The b-value, or magnitude, of each gradient direction.\\n    idx : ndarray\\n        Indices of the gradients to be selected.\\n    b0_threshold : float\\n        Gradients with b-value less than or equal to `b0_threshold` are\\n        considered to not have diffusion weighting.\\n\\n    Returns\\n    -------\\n    str\\n        Message.\\n    \"\"\"\\n\\n    return (\\n        \"Filtering gradient values with a b0 threshold value \"\\n        f\"of {b0_threshold} results in no gradients being \"\\n        f\"selected for the b-values ({bvals[idx]}) corresponding \"\\n        f\"to the requested indices ({idx}). Lower the b0 threshold \"\\n        \"value.\")\\n\\n\\ndef b0_threshold_update_slicing_message(slice_start):\\n    \"\"\"Message for b0 threshold value update for slicing.\\n\\n    Parameters\\n    ----------\\n    slice_start : int\\n        Starting index for slicing.\\n\\n    Returns\\n    -------\\n    str\\n        Message.\\n    \"\"\"\\n\\n    return f\"Updating b0_threshold to {slice_start} for slicing.\"\\n\\n\\ndef mask_non_weighted_bvals(bvals, b0_threshold):\\n    \"\"\"Create a diffusion gradient-weighting mask for the b-values according to\\n    the provided b0 threshold value.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='def b0_threshold_update_slicing_message(slice_start):\\n    \"\"\"Message for b0 threshold value update for slicing.\\n\\n    Parameters\\n    ----------\\n    slice_start : int\\n        Starting index for slicing.\\n\\n    Returns\\n    -------\\n    str\\n        Message.\\n    \"\"\"\\n\\n    return f\"Updating b0_threshold to {slice_start} for slicing.\"\\n\\n\\ndef mask_non_weighted_bvals(bvals, b0_threshold):\\n    \"\"\"Create a diffusion gradient-weighting mask for the b-values according to\\n    the provided b0 threshold value.\\n\\n    Parameters\\n    ----------\\n    bvals : (N,) ndarray\\n        The b-value, or magnitude, of each gradient direction.\\n    b0_threshold : float\\n        Gradients with b-value less than or equal to `b0_threshold` are\\n        considered to not have diffusion weighting.\\n\\n    Returns\\n    -------\\n    ndarray\\n        Gradient-weighting mask: True for all b-value indices whose value is\\n        smaller or equal to ``b0_threshold``; False otherwise.\\n     \"\"\"\\n\\n    return bvals <= b0_threshold\\n\\n\\nclass GradientTable:\\n    \"\"\"Diffusion gradient information\\n\\n    Parameters\\n    ----------\\n    gradients : array_like (N, 3)\\n        Diffusion gradients. The direction of each of these vectors corresponds\\n        to the b-vector, and the length corresponds to the b-value.\\n    b0_threshold : float\\n        Gradients with b-value less than or equal to `b0_threshold` are\\n        considered as b0s i.e. without diffusion weighting.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='return bvals <= b0_threshold\\n\\n\\nclass GradientTable:\\n    \"\"\"Diffusion gradient information\\n\\n    Parameters\\n    ----------\\n    gradients : array_like (N, 3)\\n        Diffusion gradients. The direction of each of these vectors corresponds\\n        to the b-vector, and the length corresponds to the b-value.\\n    b0_threshold : float\\n        Gradients with b-value less than or equal to `b0_threshold` are\\n        considered as b0s i.e. without diffusion weighting.\\n\\n    Attributes\\n    ----------\\n    gradients : (N,3) ndarray\\n        diffusion gradients\\n    bvals : (N,) ndarray\\n        The b-value, or magnitude, of each gradient direction.\\n    qvals: (N,) ndarray\\n        The q-value for each gradient direction. Needs big and small\\n        delta.\\n    bvecs : (N,3) ndarray\\n        The direction, represented as a unit vector, of each gradient.\\n    b0s_mask : (N,) ndarray\\n        Boolean array indicating which gradients have no diffusion\\n        weighting, ie b-value is close to 0.\\n    b0_threshold : float\\n        Gradients with b-value less than or equal to `b0_threshold` are\\n        considered to not have diffusion weighting.\\n    btens : (N,3,3) ndarray\\n        The b-tensor of each gradient direction.\\n\\n    See Also\\n    --------\\n    gradient_table\\n\\n    Notes\\n    -----\\n    The GradientTable object is immutable. Do NOT assign attributes.\\n    If you have your gradient table in a bval & bvec format, we recommend\\n    using the factory function gradient_table', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='\"\"\"\\n    def __init__(self, gradients, big_delta=None, small_delta=None,\\n                 b0_threshold=50, btens=None):\\n        \"\"\"Constructor for GradientTable class\"\"\"\\n        gradients = np.asarray(gradients)\\n        if gradients.ndim != 2 or gradients.shape[1] != 3:\\n            raise ValueError(\"gradients should be an (N, 3) array\")\\n        self.gradients = gradients\\n        # Avoid nan gradients. Set these to 0 instead:\\n        self.gradients = np.where(np.isnan(gradients), 0., gradients)\\n        self.big_delta = big_delta\\n        self.small_delta = small_delta\\n        self.b0_threshold = b0_threshold\\n        if btens is not None:\\n            linear_tensor = np.array([[1, 0, 0],\\n                                      [0, 0, 0],\\n                                      [0, 0, 0]])\\n            planar_tensor = np.array([[0, 0, 0],\\n                                      [0, 1, 0],\\n                                      [0, 0, 1]]) / 2\\n            spherical_tensor = np.array([[1, 0, 0],\\n                                         [0, 1, 0],\\n                                         [0, 0, 1]]) / 3\\n            cigar_tensor = np.array([[2, 0, 0],\\n                                     [0, .5, 0],\\n                                     [0, 0, .5]]) / 3\\n            if isinstance(btens, str):\\n                b_tensors = np.zeros((len(self.bvals), 3, 3))\\n                if btens == \\'LTE\\':\\n                    b_tensor = linear_tensor\\n                elif btens == \\'PTE\\':\\n                    b_tensor = planar_tensor\\n                elif btens == \\'STE\\':\\n                    b_tensor = spherical_tensor\\n                elif btens == \\'CTE\\':\\n                    b_tensor = cigar_tensor\\n                else:', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='if isinstance(btens, str):\\n                b_tensors = np.zeros((len(self.bvals), 3, 3))\\n                if btens == \\'LTE\\':\\n                    b_tensor = linear_tensor\\n                elif btens == \\'PTE\\':\\n                    b_tensor = planar_tensor\\n                elif btens == \\'STE\\':\\n                    b_tensor = spherical_tensor\\n                elif btens == \\'CTE\\':\\n                    b_tensor = cigar_tensor\\n                else:\\n                    raise ValueError(\"%s is an invalid value for btens. \"%btens\\n                                     + \"Please provide one of the following: \"\\n                                     + \"\\'LTE\\', \\'PTE\\', \\'STE\\', \\'CTE\\'.\")\\n                for i, (bvec, bval) in enumerate(zip(self.bvecs, self.bvals)):\\n                    if btens == \\'STE\\':\\n                        b_tensors[i] = b_tensor * bval\\n                    else:\\n                        R = vec2vec_rotmat(np.array([1, 0, 0]), bvec)\\n                        b_tensors[i] = (np.matmul(np.matmul(R, b_tensor), R.T)\\n                                        * bval)\\n                self.btens = b_tensors\\n            elif (isinstance(btens, np.ndarray)\\n                  and btens.shape in ((gradients.shape[0],),\\n                                      (gradients.shape[0], 1),\\n                                      (1, gradients.shape[0]))):\\n                b_tensors = np.zeros((len(self.bvals), 3, 3))\\n                if btens.shape == (1, gradients.shape[0]):\\n                    btens = btens.reshape((gradients.shape[0], 1))\\n                for i, (bvec, bval) in enumerate(zip(self.bvecs, self.bvals)):\\n                    R = vec2vec_rotmat(np.array([1, 0, 0]), bvec)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='(gradients.shape[0], 1),\\n                                      (1, gradients.shape[0]))):\\n                b_tensors = np.zeros((len(self.bvals), 3, 3))\\n                if btens.shape == (1, gradients.shape[0]):\\n                    btens = btens.reshape((gradients.shape[0], 1))\\n                for i, (bvec, bval) in enumerate(zip(self.bvecs, self.bvals)):\\n                    R = vec2vec_rotmat(np.array([1, 0, 0]), bvec)\\n                    if btens[i] == \\'LTE\\':\\n                        b_tensors[i] = (np.matmul(np.matmul(R, linear_tensor),\\n                                        R.T) * bval)\\n                    elif btens[i] == \\'PTE\\':\\n                        b_tensors[i] = (np.matmul(np.matmul(R, planar_tensor),\\n                                        R.T) * bval)\\n                    elif btens[i] == \\'STE\\':\\n                        b_tensors[i] = spherical_tensor * bval\\n                    elif btens[i] == \\'CTE\\':\\n                        b_tensors[i] = (np.matmul(np.matmul(R, cigar_tensor),\\n                                        R.T) * bval)\\n                    else:\\n                        raise ValueError(\\n                                \"%s is an invalid value in btens. \"%btens[i]\\n                                + \"Array element options: \\'LTE\\', \\'PTE\\', \\'STE\\', \"\\n                                + \"\\'CTE\\'.\")\\n                self.btens = b_tensors\\n            elif (isinstance(btens, np.ndarray) and btens.shape ==\\n                    (gradients.shape[0], 3, 3)):\\n                self.btens = btens\\n            else:\\n                raise ValueError(\"%s is an invalid value for btens. \"%btens', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='\"%s is an invalid value in btens. \"%btens[i]\\n                                + \"Array element options: \\'LTE\\', \\'PTE\\', \\'STE\\', \"\\n                                + \"\\'CTE\\'.\")\\n                self.btens = b_tensors\\n            elif (isinstance(btens, np.ndarray) and btens.shape ==\\n                    (gradients.shape[0], 3, 3)):\\n                self.btens = btens\\n            else:\\n                raise ValueError(\"%s is an invalid value for btens. \"%btens\\n                                 + \"Please provide a string, an array of \"\\n                                 + \"strings, or an array of exact b-tensors. \"\\n                                 + \"String options: \\'LTE\\', \\'PTE\\', \\'STE\\', \\'CTE\\'\")\\n        else:\\n            self.btens = None', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='@auto_attr\\n    def bvals(self):\\n        return vector_norm(self.gradients)\\n\\n    @auto_attr\\n    def tau(self):\\n        return self.big_delta - self.small_delta / 3.0\\n\\n    @auto_attr\\n    def qvals(self):\\n        tau = self.big_delta - self.small_delta / 3.0\\n        return np.sqrt(self.bvals / tau) / (2 * np.pi)\\n\\n    @auto_attr\\n    def gradient_strength(self):\\n        tau = self.big_delta - self.small_delta / 3.0\\n        qvals = np.sqrt(self.bvals / tau) / (2 * np.pi)\\n        gradient_strength = (qvals * (2 * np.pi) /\\n                             (self.small_delta * WATER_GYROMAGNETIC_RATIO))\\n        return gradient_strength\\n\\n    @auto_attr\\n    def b0s_mask(self):\\n        return mask_non_weighted_bvals(self.bvals, self.b0_threshold)\\n\\n    @auto_attr\\n    def bvecs(self):\\n        # To get unit vectors we divide by bvals, where bvals is 0 we divide by\\n        # 1 to avoid making nans\\n        denom = self.bvals + (self.bvals == 0)\\n        denom = denom.reshape((-1, 1))\\n        return self.gradients / denom\\n\\n    def __getitem__(self, idx):\\n        if isinstance(idx, int):\\n            idx = [idx]  # convert in a list if integer.\\n\\n        elif isinstance(idx, slice):\\n            # Get the lower bound of the slice\\n            slice_start = idx.start if idx.start is not None else 0\\n            # Check if it is different from b0_threshold\\n            if slice_start != self.b0_threshold:\\n                # Update b0_threshold and warn the user\\n                self.b0_threshold = slice_start\\n                warn(b0_threshold_update_slicing_message(slice_start),\\n                     UserWarning, stacklevel=2)\\n                idx = range(*idx.indices(len(self.bvals)))', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content=\"mask = np.logical_not(\\n            mask_non_weighted_bvals(self.bvals[idx], self.b0_threshold))\\n        if not any(mask):\\n            raise ValueError(\\n                b0_threshold_empty_gradient_message(\\n                    self.bvals, idx, self.b0_threshold)\\n            )\\n\\n        # Apply the mask to select the desired b-values and b-vectors\\n        bvals_selected = self.bvals[idx][mask]\\n        bvecs_selected = self.bvecs[idx, :][mask, :]\\n\\n        # Create a new MyGradientTable object with the selected b-values\\n        # and b-vectors\\n        return gradient_table_from_bvals_bvecs(bvals_selected,\\n                                               bvecs_selected,\\n                                               big_delta=self.big_delta,\\n                                               small_delta=self.small_delta,\\n                                               b0_threshold=self.b0_threshold,\\n                                               btens=self.btens)\\n        # removing atol parameter as it's not in the constructor\\n        # of GradientTable.\\n\\n    @property\\n    def info(self, use_logging=False):\\n        show = logging.info if use_logging else print\\n        show(self.__str__())\\n\\n    def __str__(self):\\n        msg = 'B-values shape {}\\\\n'.format(self.bvals.shape)\\n        msg += '         min {:f}\\\\n'.format(self.bvals.min())\\n        msg += '         max {:f}\\\\n'.format(self.bvals.max())\\n        msg += 'B-vectors shape {}\\\\n'.format(self.bvecs.shape)\\n        msg += '          min {:f}\\\\n'.format(self.bvecs.min())\\n        msg += '          max {:f}\\\\n'.format(self.bvecs.max())\\n        return msg\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='def __str__(self):\\n        msg = \\'B-values shape {}\\\\n\\'.format(self.bvals.shape)\\n        msg += \\'         min {:f}\\\\n\\'.format(self.bvals.min())\\n        msg += \\'         max {:f}\\\\n\\'.format(self.bvals.max())\\n        msg += \\'B-vectors shape {}\\\\n\\'.format(self.bvecs.shape)\\n        msg += \\'          min {:f}\\\\n\\'.format(self.bvecs.min())\\n        msg += \\'          max {:f}\\\\n\\'.format(self.bvecs.max())\\n        return msg\\n\\n\\ndef gradient_table_from_bvals_bvecs(bvals, bvecs, b0_threshold=50, atol=1e-2,\\n                                    btens=None, **kwargs):\\n    \"\"\"Creates a GradientTable from a bvals array and a bvecs array', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='Parameters\\n    ----------\\n    bvals : array_like (N,)\\n        The b-value, or magnitude, of each gradient direction.\\n    bvecs : array_like (N, 3)\\n        The direction, represented as a unit vector, of each gradient.\\n    b0_threshold : float\\n        Gradients with b-value less than or equal to `b0_threshold` are\\n        considered to not have diffusion weighting. If its value is equal to or\\n        larger than all values in b-vals, then it is assumed that no\\n        thresholding is requested.\\n    atol : float\\n        Each vector in `bvecs` must be a unit vectors up to a tolerance of\\n        `atol`.\\n    btens : can be any of three options\\n        1. a string specifying the shape of the encoding tensor for all volumes\\n           in data. Options: \\'LTE\\', \\'PTE\\', \\'STE\\', \\'CTE\\' corresponding to\\n           linear, planar, spherical, and \"cigar-shaped\" tensor encoding.\\n           Tensors are rotated so that linear and cigar tensors are aligned\\n           with the corresponding gradient direction and the planar tensor\\'s\\n           normal is aligned with the corresponding gradient direction.\\n           Magnitude is scaled to match the b-value.\\n        2. an array of strings of shape (N,), (N, 1), or (1, N) specifying\\n           encoding tensor shape for each volume separately. N corresponds to\\n           the number volumes in data. Options for elements in array: \\'LTE\\',\\n           \\'PTE\\', \\'STE\\', \\'CTE\\' corresponding to linear, planar, spherical, and\\n           \"cigar-shaped\" tensor encoding. Tensors are rotated so that linear\\n           and cigar tensors are aligned with the corresponding gradient\\n           direction and the planar tensor\\'s normal is aligned with the', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='encoding tensor shape for each volume separately. N corresponds to\\n           the number volumes in data. Options for elements in array: \\'LTE\\',\\n           \\'PTE\\', \\'STE\\', \\'CTE\\' corresponding to linear, planar, spherical, and\\n           \"cigar-shaped\" tensor encoding. Tensors are rotated so that linear\\n           and cigar tensors are aligned with the corresponding gradient\\n           direction and the planar tensor\\'s normal is aligned with the\\n           corresponding gradient direction. Magnitude is scaled to match the\\n           b-value.\\n        3. an array of shape (N,3,3) specifying the b-tensor of each volume\\n           exactly. N corresponds to the number volumes in data. No rotation or\\n           scaling is performed.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='Other Parameters\\n    ----------------\\n    **kwargs : dict\\n        Other keyword inputs are passed to GradientTable.\\n\\n    Returns\\n    -------\\n    gradients : GradientTable\\n        A GradientTable with all the gradient information.\\n\\n    See Also\\n    --------\\n    GradientTable, gradient_table\\n\\n    \"\"\"\\n    bvals = np.asarray(bvals, float)\\n    bvecs = np.asarray(bvecs, float)\\n    dwi_mask = bvals > b0_threshold\\n\\n    # check that bvals is (N,) array and bvecs is (N, 3) unit vectors\\n    if bvals.ndim != 1 or bvecs.ndim != 2 or bvecs.shape[0] != bvals.shape[0]:\\n        raise ValueError(\"bvals and bvecs should be (N,) and (N, 3) arrays \"\\n                         \"respectively, where N is the number of diffusion \"\\n                         \"gradients\")\\n    # checking for negative bvals\\n    if b0_threshold < 0:\\n        raise ValueError(\"Negative bvals in the data are not feasible\")\\n\\n    # Upper bound for the b0_threshold\\n    if b0_threshold >= 200:\\n        warn(\"b0_threshold has a value > 199\")\\n\\n    # If all b-values are smaller or equal to the b0 threshold, it is assumed\\n    # that no thresholding is requested\\n    if any(mask_non_weighted_bvals(bvals, b0_threshold)):\\n        # checking for the correctness of bvals\\n        if b0_threshold < bvals.min():\\n            warn(\"b0_threshold (value: {0}) is too low, increase your \"\\n                 \"b0_threshold. It should be higher than the lowest b0 value \"\\n                 \"({1}).\".format(b0_threshold, bvals.min()))', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='# If all b-values are smaller or equal to the b0 threshold, it is assumed\\n    # that no thresholding is requested\\n    if any(mask_non_weighted_bvals(bvals, b0_threshold)):\\n        # checking for the correctness of bvals\\n        if b0_threshold < bvals.min():\\n            warn(\"b0_threshold (value: {0}) is too low, increase your \"\\n                 \"b0_threshold. It should be higher than the lowest b0 value \"\\n                 \"({1}).\".format(b0_threshold, bvals.min()))\\n\\n    bvecs = np.where(np.isnan(bvecs), 0, bvecs)\\n    bvecs_close_to_1 = abs(vector_norm(bvecs) - 1) <= atol\\n    if bvecs.shape[1] != 3:\\n        raise ValueError(\"bvecs should be (N, 3)\")\\n    if not np.all(bvecs_close_to_1[dwi_mask]):\\n        raise ValueError(\"The vectors in bvecs should be unit (The tolerance \"\\n                         \"can be modified as an input parameter)\")\\n\\n    bvecs = np.where(bvecs_close_to_1[:, None], bvecs, 0)\\n    bvals = bvals * bvecs_close_to_1\\n    gradients = bvals[:, None] * bvecs\\n\\n    grad_table = GradientTable(gradients, b0_threshold=b0_threshold,\\n                               btens=btens, **kwargs)\\n    grad_table.bvals = bvals\\n    grad_table.bvecs = bvecs\\n    grad_table.b0s_mask = ~dwi_mask\\n\\n    return grad_table\\n\\n\\ndef gradient_table_from_qvals_bvecs(qvals, bvecs, big_delta, small_delta,\\n                                    b0_threshold=50, atol=1e-2):\\n    \"\"\"A general function for creating diffusion MR gradients.\\n\\n    It reads, loads and prepares scanner parameters like the b-values and\\n    b-vectors so that they can be useful during the reconstruction process.\\n\\n    Parameters\\n    ----------\\n\\n    qvals : an array of shape (N,),\\n        q-value given in 1/mm', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='return grad_table\\n\\n\\ndef gradient_table_from_qvals_bvecs(qvals, bvecs, big_delta, small_delta,\\n                                    b0_threshold=50, atol=1e-2):\\n    \"\"\"A general function for creating diffusion MR gradients.\\n\\n    It reads, loads and prepares scanner parameters like the b-values and\\n    b-vectors so that they can be useful during the reconstruction process.\\n\\n    Parameters\\n    ----------\\n\\n    qvals : an array of shape (N,),\\n        q-value given in 1/mm\\n\\n    bvecs : can be any of two options\\n\\n        1. an array of shape (N, 3) or (3, N) with the b-vectors.\\n        2. a path for the file which contains an array like the previous.\\n\\n    big_delta : float or array of shape (N,)\\n        acquisition pulse separation time in seconds\\n\\n    small_delta : float\\n        acquisition pulse duration time in seconds\\n\\n    b0_threshold : float\\n        All b-values with values less than or equal to `bo_threshold` are\\n        considered as b0s i.e. without diffusion weighting.\\n\\n    atol : float\\n        All b-vectors need to be unit vectors up to a tolerance.\\n\\n    Returns\\n    -------\\n    gradients : GradientTable\\n        A GradientTable with all the gradient information.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='small_delta : float\\n        acquisition pulse duration time in seconds\\n\\n    b0_threshold : float\\n        All b-values with values less than or equal to `bo_threshold` are\\n        considered as b0s i.e. without diffusion weighting.\\n\\n    atol : float\\n        All b-vectors need to be unit vectors up to a tolerance.\\n\\n    Returns\\n    -------\\n    gradients : GradientTable\\n        A GradientTable with all the gradient information.\\n\\n    Examples\\n    --------\\n    >>> from dipy.core.gradients import gradient_table_from_qvals_bvecs\\n    >>> qvals = 30. * np.ones(7)\\n    >>> big_delta = .03  # pulse separation of 30ms\\n    >>> small_delta = 0.01  # pulse duration of 10ms\\n    >>> qvals[0] = 0\\n    >>> sq2 = np.sqrt(2) / 2\\n    >>> bvecs = np.array([[0, 0, 0],\\n    ...                   [1, 0, 0],\\n    ...                   [0, 1, 0],\\n    ...                   [0, 0, 1],\\n    ...                   [sq2, sq2, 0],\\n    ...                   [sq2, 0, sq2],\\n    ...                   [0, sq2, sq2]])\\n    >>> gt = gradient_table_from_qvals_bvecs(qvals, bvecs,\\n    ...                                      big_delta, small_delta)\\n\\n    Notes\\n    -----\\n    1. Often b0s (b-values which correspond to images without diffusion\\n       weighting) have 0 values however in some cases the scanner cannot\\n       provide b0s of an exact 0 value and it gives a bit higher values\\n       e.g. 6 or 12. This is the purpose of the b0_threshold in the __init__.\\n    2. We assume that the minimum number of b-values is 7.\\n    3. B-vectors should be unit vectors.\\n\\n    \"\"\"\\n    qvals = np.asarray(qvals)\\n    bvecs = np.asarray(bvecs)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='Notes\\n    -----\\n    1. Often b0s (b-values which correspond to images without diffusion\\n       weighting) have 0 values however in some cases the scanner cannot\\n       provide b0s of an exact 0 value and it gives a bit higher values\\n       e.g. 6 or 12. This is the purpose of the b0_threshold in the __init__.\\n    2. We assume that the minimum number of b-values is 7.\\n    3. B-vectors should be unit vectors.\\n\\n    \"\"\"\\n    qvals = np.asarray(qvals)\\n    bvecs = np.asarray(bvecs)\\n\\n    if (bvecs.shape[1] > bvecs.shape[0]) and bvecs.shape[0] > 1:\\n        bvecs = bvecs.T\\n    bvals = (qvals * 2 * np.pi) ** 2 * (big_delta - small_delta / 3.)\\n    return gradient_table_from_bvals_bvecs(bvals, bvecs, big_delta=big_delta,\\n                                           small_delta=small_delta,\\n                                           b0_threshold=b0_threshold,\\n                                           atol=atol)\\n\\n\\ndef gradient_table_from_gradient_strength_bvecs(gradient_strength, bvecs,\\n                                                big_delta, small_delta,\\n                                                b0_threshold=50, atol=1e-2):\\n    \"\"\"A general function for creating diffusion MR gradients.\\n\\n    It reads, loads and prepares scanner parameters like the b-values and\\n    b-vectors so that they can be useful during the reconstruction process.\\n\\n    Parameters\\n    ----------\\n\\n    gradient_strength : an array of shape (N,),\\n        gradient strength given in T/mm\\n\\n    bvecs : can be any of two options\\n\\n        1. an array of shape (N, 3) or (3, N) with the b-vectors.\\n        2. a path for the file which contains an array like the previous.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='It reads, loads and prepares scanner parameters like the b-values and\\n    b-vectors so that they can be useful during the reconstruction process.\\n\\n    Parameters\\n    ----------\\n\\n    gradient_strength : an array of shape (N,),\\n        gradient strength given in T/mm\\n\\n    bvecs : can be any of two options\\n\\n        1. an array of shape (N, 3) or (3, N) with the b-vectors.\\n        2. a path for the file which contains an array like the previous.\\n\\n    big_delta : float or array of shape (N,)\\n        acquisition pulse separation time in seconds\\n\\n    small_delta : float\\n        acquisition pulse duration time in seconds\\n\\n    b0_threshold : float\\n        All b-values with values less than or equal to `bo_threshold` are\\n        considered as b0s i.e. without diffusion weighting.\\n\\n    atol : float\\n        All b-vectors need to be unit vectors up to a tolerance.\\n\\n    Returns\\n    -------\\n    gradients : GradientTable\\n        A GradientTable with all the gradient information.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='small_delta : float\\n        acquisition pulse duration time in seconds\\n\\n    b0_threshold : float\\n        All b-values with values less than or equal to `bo_threshold` are\\n        considered as b0s i.e. without diffusion weighting.\\n\\n    atol : float\\n        All b-vectors need to be unit vectors up to a tolerance.\\n\\n    Returns\\n    -------\\n    gradients : GradientTable\\n        A GradientTable with all the gradient information.\\n\\n    Examples\\n    --------\\n    >>> from dipy.core.gradients import (\\n    ...    gradient_table_from_gradient_strength_bvecs)\\n    >>> gradient_strength = .03e-3 * np.ones(7)  # clinical strength at 30 mT/m\\n    >>> big_delta = .03  # pulse separation of 30ms\\n    >>> small_delta = 0.01  # pulse duration of 10ms\\n    >>> gradient_strength[0] = 0\\n    >>> sq2 = np.sqrt(2) / 2\\n    >>> bvecs = np.array([[0, 0, 0],\\n    ...                   [1, 0, 0],\\n    ...                   [0, 1, 0],\\n    ...                   [0, 0, 1],\\n    ...                   [sq2, sq2, 0],\\n    ...                   [sq2, 0, sq2],\\n    ...                   [0, sq2, sq2]])\\n    >>> gt = gradient_table_from_gradient_strength_bvecs(\\n    ...     gradient_strength, bvecs, big_delta, small_delta)\\n\\n    Notes\\n    -----\\n    1. Often b0s (b-values which correspond to images without diffusion\\n       weighting) have 0 values however in some cases the scanner cannot\\n       provide b0s of an exact 0 value and it gives a bit higher values\\n       e.g. 6 or 12. This is the purpose of the b0_threshold in the __init__.\\n    2. We assume that the minimum number of b-values is 7.\\n    3. B-vectors should be unit vectors.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='Notes\\n    -----\\n    1. Often b0s (b-values which correspond to images without diffusion\\n       weighting) have 0 values however in some cases the scanner cannot\\n       provide b0s of an exact 0 value and it gives a bit higher values\\n       e.g. 6 or 12. This is the purpose of the b0_threshold in the __init__.\\n    2. We assume that the minimum number of b-values is 7.\\n    3. B-vectors should be unit vectors.\\n\\n    \"\"\"\\n    gradient_strength = np.asarray(gradient_strength)\\n    bvecs = np.asarray(bvecs)\\n    if (bvecs.shape[1] > bvecs.shape[0]) and bvecs.shape[0] > 1:\\n        bvecs = bvecs.T\\n    qvals = gradient_strength * small_delta * WATER_GYROMAGNETIC_RATIO /\\\\\\n        (2 * np.pi)\\n    bvals = (qvals * 2 * np.pi) ** 2 * (big_delta - small_delta / 3.)\\n    return gradient_table_from_bvals_bvecs(bvals, bvecs, big_delta=big_delta,\\n                                           small_delta=small_delta,\\n                                           b0_threshold=b0_threshold,\\n                                           atol=atol)\\n\\n\\ndef gradient_table(bvals, bvecs=None, big_delta=None, small_delta=None,\\n                   b0_threshold=50, atol=1e-2, btens=None):\\n    \"\"\"A general function for creating diffusion MR gradients.\\n\\n    It reads, loads and prepares scanner parameters like the b-values and\\n    b-vectors so that they can be useful during the reconstruction process.\\n\\n    Parameters\\n    ----------\\n\\n    bvals : can be any of the four options', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='def gradient_table(bvals, bvecs=None, big_delta=None, small_delta=None,\\n                   b0_threshold=50, atol=1e-2, btens=None):\\n    \"\"\"A general function for creating diffusion MR gradients.\\n\\n    It reads, loads and prepares scanner parameters like the b-values and\\n    b-vectors so that they can be useful during the reconstruction process.\\n\\n    Parameters\\n    ----------\\n\\n    bvals : can be any of the four options\\n\\n        1. an array of shape (N,) or (1, N) or (N, 1) with the b-values.\\n        2. a path for the file which contains an array like the above (1).\\n        3. an array of shape (N, 4) or (4, N). Then this parameter is\\n           considered to be a b-table which contains both bvals and bvecs. In\\n           this case the next parameter is skipped.\\n        4. a path for the file which contains an array like the one at (3).\\n\\n    bvecs : can be any of two options\\n\\n        1. an array of shape (N, 3) or (3, N) with the b-vectors.\\n        2. a path for the file which contains an array like the previous.\\n\\n    big_delta : float\\n        acquisition pulse separation time in seconds (default None)\\n\\n    small_delta : float\\n        acquisition pulse duration time in seconds (default None)\\n\\n    b0_threshold : float\\n        All b-values with values less than or equal to `bo_threshold` are\\n        considered as b0s i.e. without diffusion weighting.\\n\\n    atol : float\\n        All b-vectors need to be unit vectors up to a tolerance.\\n\\n    btens : can be any of three options', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='small_delta : float\\n        acquisition pulse duration time in seconds (default None)\\n\\n    b0_threshold : float\\n        All b-values with values less than or equal to `bo_threshold` are\\n        considered as b0s i.e. without diffusion weighting.\\n\\n    atol : float\\n        All b-vectors need to be unit vectors up to a tolerance.\\n\\n    btens : can be any of three options\\n\\n        1. a string specifying the shape of the encoding tensor for all volumes\\n           in data. Options: \\'LTE\\', \\'PTE\\', \\'STE\\', \\'CTE\\' corresponding to\\n           linear, planar, spherical, and \"cigar-shaped\" tensor encoding.\\n           Tensors are rotated so that linear and cigar tensors are aligned\\n           with the corresponding gradient direction and the planar tensor\\'s\\n           normal is aligned with the corresponding gradient direction.\\n           Magnitude is scaled to match the b-value.\\n        2. an array of strings of shape (N,), (N, 1), or (1, N) specifying\\n           encoding tensor shape for each volume separately. N corresponds to\\n           the number volumes in data. Options for elements in array: \\'LTE\\',\\n           \\'PTE\\', \\'STE\\', \\'CTE\\' corresponding to linear, planar, spherical, and\\n           \"cigar-shaped\" tensor encoding. Tensors are rotated so that linear\\n           and cigar tensors are aligned with the corresponding gradient\\n           direction and the planar tensor\\'s normal is aligned with the\\n           corresponding gradient direction. Magnitude is scaled to match the\\n           b-value.\\n        3. an array of shape (N,3,3) specifying the b-tensor of each volume\\n           exactly. N corresponds to the number volumes in data. No rotation or\\n           scaling is performed.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    gradients : GradientTable\\n        A GradientTable with all the gradient information.\\n\\n    Examples\\n    --------\\n    >>> from dipy.core.gradients import gradient_table\\n    >>> bvals = 1500 * np.ones(7)\\n    >>> bvals[0] = 0\\n    >>> sq2 = np.sqrt(2) / 2\\n    >>> bvecs = np.array([[0, 0, 0],\\n    ...                   [1, 0, 0],\\n    ...                   [0, 1, 0],\\n    ...                   [0, 0, 1],\\n    ...                   [sq2, sq2, 0],\\n    ...                   [sq2, 0, sq2],\\n    ...                   [0, sq2, sq2]])\\n    >>> gt = gradient_table(bvals, bvecs)\\n    >>> gt.bvecs.shape == bvecs.shape\\n    True\\n    >>> gt = gradient_table(bvals, bvecs.T)\\n    >>> gt.bvecs.shape == bvecs.T.shape\\n    False\\n\\n    Notes\\n    -----\\n    1. Often b0s (b-values which correspond to images without diffusion\\n       weighting) have 0 values however in some cases the scanner cannot\\n       provide b0s of an exact 0 value and it gives a bit higher values\\n       e.g. 6 or 12. This is the purpose of the b0_threshold in the __init__.\\n    2. We assume that the minimum number of b-values is 7.\\n    3. B-vectors should be unit vectors.\\n\\n    \"\"\"\\n\\n    # If you provided strings with full paths, we go and load those from\\n    # the files:\\n    if isinstance(bvals, str):\\n        bvals, _ = io.read_bvals_bvecs(bvals, None)\\n    if isinstance(bvecs, str):\\n        _, bvecs = io.read_bvals_bvecs(None, bvecs)\\n\\n    bvals = np.asarray(bvals)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='\"\"\"\\n\\n    # If you provided strings with full paths, we go and load those from\\n    # the files:\\n    if isinstance(bvals, str):\\n        bvals, _ = io.read_bvals_bvecs(bvals, None)\\n    if isinstance(bvecs, str):\\n        _, bvecs = io.read_bvals_bvecs(None, bvecs)\\n\\n    bvals = np.asarray(bvals)\\n\\n    # If bvecs is None we expect bvals to be an (N, 4) or (4, N) array.\\n    if bvecs is None:\\n        if bvals.shape[-1] == 4:\\n            bvecs = bvals[:, 1:]\\n            bvals = np.squeeze(bvals[:, 0])\\n        elif bvals.shape[0] == 4:\\n            bvecs = bvals[1:, :].T\\n            bvals = np.squeeze(bvals[0, :])\\n        else:\\n            raise ValueError(\"input should be bvals and bvecs OR an (N, 4)\"\\n                             \" array containing both bvals and bvecs\")\\n    else:\\n        bvecs = np.asarray(bvecs)\\n        if bvecs.shape[1] != 3 and bvecs.shape[0] > 1:\\n            bvecs = bvecs.T\\n    return gradient_table_from_bvals_bvecs(bvals, bvecs, big_delta=big_delta,\\n                                           small_delta=small_delta,\\n                                           b0_threshold=b0_threshold,\\n                                           atol=atol, btens=btens)\\n\\n\\ndef reorient_bvecs(gtab, affines, atol=1e-2):\\n    \"\"\"Reorient the directions in a GradientTable.\\n\\n    When correcting for motion, rotation of the diffusion-weighted volumes\\n    might cause systematic bias in rotationally invariant measures, such as FA\\n    and MD, and also cause characteristic biases in tractography, unless the\\n    gradient directions are appropriately reoriented to compensate for this\\n    effect [Leemans2009]_.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='def reorient_bvecs(gtab, affines, atol=1e-2):\\n    \"\"\"Reorient the directions in a GradientTable.\\n\\n    When correcting for motion, rotation of the diffusion-weighted volumes\\n    might cause systematic bias in rotationally invariant measures, such as FA\\n    and MD, and also cause characteristic biases in tractography, unless the\\n    gradient directions are appropriately reoriented to compensate for this\\n    effect [Leemans2009]_.\\n\\n    Parameters\\n    ----------\\n    gtab : GradientTable\\n        The nominal gradient table with which the data were acquired.\\n    affines : list or ndarray of shape (4, 4, n) or (3, 3, n)\\n        Each entry in this list or array contain either an affine\\n        transformation (4,4) or a rotation matrix (3, 3).\\n        In both cases, the transformations encode the rotation that was applied\\n        to the image corresponding to one of the non-zero gradient directions\\n        (ordered according to their order in `gtab.bvecs[~gtab.b0s_mask]`)\\n    atol: see gradient_table()\\n\\n    Returns\\n    -------\\n    gtab : a GradientTable class instance with the reoriented directions\\n\\n    References\\n    ----------\\n    .. [Leemans2009] The B-Matrix Must Be Rotated When Correcting for\\n       Subject Motion in DTI Data. Leemans, A. and Jones, D.K. (2009).\\n       MRM, 61: 1336-1349\\n    \"\"\"\\n    if isinstance(affines, list):\\n        affines = np.stack(affines, axis=-1)\\n\\n    if affines.shape[0] != affines.shape[1]:\\n        msg = \\'\\'\\'reorient_bvecs has changed to require affines as\\n        (4, 4, n) or (3, 3, n). Shape of (n, 4, 4) or (n, 3, 3)\\n        is deprecated and will fail in the future.\\'\\'\\'\\n        warn(msg, UserWarning)\\n        affines = np.moveaxis(affines, 0, -1)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='if affines.shape[0] != affines.shape[1]:\\n        msg = \\'\\'\\'reorient_bvecs has changed to require affines as\\n        (4, 4, n) or (3, 3, n). Shape of (n, 4, 4) or (n, 3, 3)\\n        is deprecated and will fail in the future.\\'\\'\\'\\n        warn(msg, UserWarning)\\n        affines = np.moveaxis(affines, 0, -1)\\n\\n    new_bvecs = gtab.bvecs[~gtab.b0s_mask]\\n\\n    if new_bvecs.shape[0] != affines.shape[-1]:\\n        e_s = \"Number of affine transformations must match number of \"\\n        e_s += \"non-zero gradients\"\\n        raise ValueError(e_s)\\n\\n    # moving axis to make life easier\\n    affines = np.moveaxis(affines, -1, 0)\\n\\n    for i, aff in enumerate(affines):\\n        if aff.shape == (4, 4):\\n            # This must be an affine!\\n            # Remove the translation component:\\n            aff = aff[:3, :3]\\n        # Decompose into rotation and scaling components:\\n        R, S = polar(aff)\\n        Rinv = inv(R)\\n        # Apply the inverse of the rotation to the corresponding gradient\\n        # direction:\\n        new_bvecs[i] = np.dot(Rinv, new_bvecs[i])\\n\\n    return_bvecs = np.zeros(gtab.bvecs.shape)\\n    return_bvecs[~gtab.b0s_mask] = new_bvecs\\n    return gradient_table(gtab.bvals, return_bvecs, big_delta=gtab.big_delta,\\n                          small_delta=gtab.small_delta,\\n                          b0_threshold=gtab.b0_threshold, atol=atol)\\n\\n\\ndef generate_bvecs(N, iters=5000, rng=None):\\n    \"\"\"Generates N bvectors.\\n\\n    Uses dipy.core.sphere.disperse_charges to model electrostatic repulsion on\\n    a unit sphere.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='return_bvecs = np.zeros(gtab.bvecs.shape)\\n    return_bvecs[~gtab.b0s_mask] = new_bvecs\\n    return gradient_table(gtab.bvals, return_bvecs, big_delta=gtab.big_delta,\\n                          small_delta=gtab.small_delta,\\n                          b0_threshold=gtab.b0_threshold, atol=atol)\\n\\n\\ndef generate_bvecs(N, iters=5000, rng=None):\\n    \"\"\"Generates N bvectors.\\n\\n    Uses dipy.core.sphere.disperse_charges to model electrostatic repulsion on\\n    a unit sphere.\\n\\n    Parameters\\n    ----------\\n    N : int\\n        The number of bvectors to generate. This should be equal to the number\\n        of bvals used.\\n    iters : int\\n        Number of iterations to run.\\n    rng : numpy.random.Generator, optional\\n        Numpy\\'s random number generator. If None, the generator is created.\\n        Default is None.\\n\\n    Returns\\n    -------\\n    bvecs : (N,3) ndarray\\n        The generated directions, represented as a unit vector, of each\\n        gradient.\\n    \"\"\"\\n    if rng is None:\\n        rng = np.random.default_rng()\\n    theta = np.pi * rng.random(N)\\n    phi = 2 * np.pi * rng.random(N)\\n    hsph_initial = HemiSphere(theta=theta, phi=phi)\\n    hsph_updated, potential = disperse_charges(hsph_initial, iters)\\n    bvecs = hsph_updated.vertices\\n    return bvecs\\n\\n\\ndef round_bvals(bvals, bmag=None):\\n    \"\"\"\"This function rounds the b-values\\n\\n    Parameters\\n    ----------\\n    bvals : ndarray\\n        Array containing the b-values\\n\\n    bmag : int\\n        The order of magnitude to round the b-values. If not given b-values\\n        will be rounded relative to the order of magnitude\\n        $bmag = (bmagmax - 1)$, where bmaxmag is the magnitude order of the\\n        larger b-value.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='def round_bvals(bvals, bmag=None):\\n    \"\"\"\"This function rounds the b-values\\n\\n    Parameters\\n    ----------\\n    bvals : ndarray\\n        Array containing the b-values\\n\\n    bmag : int\\n        The order of magnitude to round the b-values. If not given b-values\\n        will be rounded relative to the order of magnitude\\n        $bmag = (bmagmax - 1)$, where bmaxmag is the magnitude order of the\\n        larger b-value.\\n\\n    Returns\\n    -------\\n    rbvals : ndarray\\n        Array containing the rounded b-values\\n    \"\"\"\\n    if bmag is None:\\n        bmag = int(np.log10(np.max(bvals))) - 1\\n\\n    b = bvals / (10 ** bmag)  # normalize b units\\n    return b.round() * (10 ** bmag)\\n\\n\\ndef unique_bvals_tolerance(bvals, tol=20):\\n    \"\"\" Gives the unique b-values of the data, within a tolerance gap\\n\\n    The b-values must be regrouped in clusters easily separated by a\\n    distance greater than the tolerance gap. If all the b-values of a\\n    cluster fit within the tolerance gap, the highest b-value is kept.\\n\\n    Parameters\\n    ----------\\n    bvals : ndarray\\n        Array containing the b-values\\n\\n    tol : int\\n        The tolerated gap between the b-values to extract\\n        and the actual b-values.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='The b-values must be regrouped in clusters easily separated by a\\n    distance greater than the tolerance gap. If all the b-values of a\\n    cluster fit within the tolerance gap, the highest b-value is kept.\\n\\n    Parameters\\n    ----------\\n    bvals : ndarray\\n        Array containing the b-values\\n\\n    tol : int\\n        The tolerated gap between the b-values to extract\\n        and the actual b-values.\\n\\n    Returns\\n    -------\\n    ubvals : ndarray\\n        Array containing the unique b-values using the median value\\n        for each cluster\\n    \"\"\"\\n    b = np.unique(bvals)\\n    ubvals = []\\n    lower_part = np.where(b <= b[0] + tol)[0]\\n    upper_part = np.where(np.logical_and(b <= b[lower_part[-1]] + tol,\\n                                         b > b[lower_part[-1]]))[0]\\n    ubvals.append(b[lower_part[-1]])\\n    if len(upper_part) != 0:\\n        b_index = upper_part[-1] + 1\\n    else:\\n        b_index = lower_part[-1] + 1\\n    while b_index != len(b):\\n        lower_part = np.where(np.logical_and(b <= b[b_index] + tol,\\n                                             b > b[b_index-1]))[0]\\n        upper_part = np.where(np.logical_and(b <= b[lower_part[-1]] + tol,\\n                                             b > b[lower_part[-1]]))[0]\\n        ubvals.append(b[lower_part[-1]])\\n        if len(upper_part) != 0:\\n            b_index = upper_part[-1] + 1\\n        else:\\n            b_index = lower_part[-1] + 1', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='# Checking for overlap with get_bval_indices\\n    for i, ubval in enumerate(ubvals[:-1]):\\n        indices_1 = get_bval_indices(bvals, ubval, tol)\\n        indices_2 = get_bval_indices(bvals, ubvals[i+1], tol)\\n        if len(np.intersect1d(indices_1, indices_2)) != 0:\\n            msg = \\'\\'\\'There is overlap in clustering of b-values.\\n            The tolerance factor might be too high.\\'\\'\\'\\n            warn(msg, UserWarning)\\n\\n    return np.asarray(ubvals)\\n\\n\\ndef get_bval_indices(bvals, bval, tol=20):\\n    \"\"\"\\n    Get indices where the b-value is `bval`\\n\\n    Parameters\\n    ----------\\n    bvals: ndarray\\n        Array containing the b-values\\n\\n    bval: float or int\\n        b-value to extract indices\\n\\n    tol: int\\n        The tolerated gap between the b-values to extract\\n        and the actual b-values.\\n\\n    Returns\\n    -------\\n    Array of indices where the b-value is `bval`\\n    \"\"\"\\n    return np.where(np.logical_and(bvals <= bval + tol,\\n                                   bvals >= bval - tol))[0]\\n\\n\\ndef unique_bvals_magnitude(bvals, bmag=None, rbvals=False):\\n    \"\"\" This function gives the unique rounded b-values of the data\\n\\n    Parameters\\n    ----------\\n    bvals : ndarray\\n        Array containing the b-values\\n\\n    bmag : int\\n        The order of magnitude that the bvalues have to differ to be\\n        considered an unique b-value. B-values are also rounded up to\\n        this order of magnitude. Default: derive this value from the\\n        maximal b-value provided: $bmag=log_{10}(max(bvals)) - 1$.\\n\\n    rbvals : bool, optional\\n        If True function also returns all individual rounded b-values.\\n        Default: False', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='bmag : int\\n        The order of magnitude that the bvalues have to differ to be\\n        considered an unique b-value. B-values are also rounded up to\\n        this order of magnitude. Default: derive this value from the\\n        maximal b-value provided: $bmag=log_{10}(max(bvals)) - 1$.\\n\\n    rbvals : bool, optional\\n        If True function also returns all individual rounded b-values.\\n        Default: False\\n\\n    Returns\\n    -------\\n    ubvals : ndarray\\n        Array containing the rounded unique b-values\\n    \"\"\"\\n    b = round_bvals(bvals, bmag)\\n    if rbvals:\\n        return np.unique(b), b\\n\\n    return np.unique(b)\\n\\n\\ndef check_multi_b(gtab, n_bvals, non_zero=True, bmag=None):\\n    \"\"\"\\n    Check if you have enough different b-values in your gradient table\\n\\n    Parameters\\n    ----------\\n    gtab : GradientTable class instance.\\n\\n    n_bvals : int\\n        The number of different b-values you are checking for.\\n    non_zero : bool\\n        Whether to check only non-zero bvalues. In this case, we will require\\n        at least `n_bvals` *non-zero* b-values (where non-zero is defined\\n        depending on the `gtab` object\\'s `b0_threshold` attribute)\\n    bmag : int\\n        The order of magnitude of the b-values used. The function will\\n        normalize the b-values relative $10^{bmag}$. Default: derive this\\n        value from the maximal b-value provided:\\n        $bmag=log_{10}(max(bvals)) - 1$.\\n\\n    Returns\\n    -------\\n    bool : Whether there are at least `n_bvals` different b-values in the\\n    gradient table used.\\n    \"\"\"\\n    bvals = gtab.bvals.copy()\\n    if non_zero:\\n        bvals = bvals[~gtab.b0s_mask]', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    bool : Whether there are at least `n_bvals` different b-values in the\\n    gradient table used.\\n    \"\"\"\\n    bvals = gtab.bvals.copy()\\n    if non_zero:\\n        bvals = bvals[~gtab.b0s_mask]\\n\\n    uniqueb = unique_bvals_magnitude(bvals, bmag=bmag)\\n    if uniqueb.shape[0] < n_bvals:\\n        return False\\n    else:\\n        return True\\n\\n\\ndef _btens_to_params_2d(btens_2d, ztol):\\n    \"\"\"Compute trace, anisotropy and asymmetry parameters from a single b-tensor\\n\\n    Auxiliary function where calculation of `bval`, bdelta` and `b_eta` from a\\n    (3,3) b-tensor takes place. The main function `btens_to_params` then wraps\\n    around this to enable support of input (N, 3, 3) arrays, where N = number of\\n    b-tensors\\n\\n    Parameters\\n    ----------\\n    btens_2d : (3, 3) numpy.ndarray\\n        input b-tensor\\n    ztol : float\\n        Any parameters smaller than this value are considered to be 0\\n\\n    Returns\\n    -------\\n    bval: float\\n        b-value (trace)\\n    bdelta: float\\n        normalized tensor anisotropy\\n    bdelta: float\\n        tensor asymmetry\\n\\n    Notes\\n    -----\\n    Implementation following [1].\\n\\n    References\\n    ----------\\n    .. [1] D. Topgaard, NMR methods for studying microscopic diffusion\\n    anisotropy, in: R. Valiullin (Ed.), Diffusion NMR of Confined Systems: Fluid\\n    Transport in Porous Solids and Heterogeneous Materials, Royal Society of\\n    Chemistry, Cambridge, UK, 2016.\\n\\n    \"\"\"\\n    btens_2d[abs(btens_2d) <= ztol] = 0\\n\\n    evals = np.real(np.linalg.eig(btens_2d)[0])\\n    bval = np.sum(evals)\\n    bval_is_zero = np.abs(bval) <= ztol\\n\\n    if bval_is_zero:\\n        bval = 0\\n        bdelta = 0\\n        b_eta = 0\\n    else:\\n        lambda_iso = (1/3)*bval', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='\"\"\"\\n    btens_2d[abs(btens_2d) <= ztol] = 0\\n\\n    evals = np.real(np.linalg.eig(btens_2d)[0])\\n    bval = np.sum(evals)\\n    bval_is_zero = np.abs(bval) <= ztol\\n\\n    if bval_is_zero:\\n        bval = 0\\n        bdelta = 0\\n        b_eta = 0\\n    else:\\n        lambda_iso = (1/3)*bval\\n\\n        diff_lambdas = np.abs(evals - lambda_iso)\\n        evals_zzxxyy = evals[np.argsort(diff_lambdas)[::-1]]\\n\\n        lambda_zz = evals_zzxxyy[0]\\n        lambda_xx = evals_zzxxyy[1]\\n        lambda_yy = evals_zzxxyy[2]\\n\\n        bdelta = (1/(3*lambda_iso))*(lambda_zz-((lambda_yy+lambda_xx)/2))\\n\\n        if np.abs(bdelta) <= ztol:\\n            bdelta = 0\\n\\n        yyxx_diff = lambda_yy-lambda_xx\\n        if abs(yyxx_diff) <= np.spacing(1):\\n            yyxx_diff = 0\\n\\n        b_eta = yyxx_diff/(2*lambda_iso*bdelta+np.spacing(1))\\n\\n        if np.abs(b_eta) <= b_eta:\\n            b_eta = 0\\n\\n    return float(bval), float(bdelta), float(b_eta)\\n\\n\\ndef btens_to_params(btens, ztol=1e-10):\\n    r\"\"\"Compute trace, anisotropy and asymmetry parameters from b-tensors.\\n\\n    Parameters\\n    ----------\\n    btens : (3, 3) OR (N, 3, 3) numpy.ndarray\\n        input b-tensor, or b-tensors, where N = number of b-tensors\\n    ztol : float\\n        Any parameters smaller than this value are considered to be 0\\n\\n    Returns\\n    -------\\n    bval: numpy.ndarray\\n        b-value(s) (trace(s))\\n    bdelta: numpy.ndarray\\n        normalized tensor anisotropy(s)\\n    b_eta: numpy.ndarray\\n        tensor asymmetry(s)\\n\\n    Notes\\n    -----\\n    This function can be used to get b-tensor parameters directly from the\\n    GradientTable `btens` attribute.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    bval: numpy.ndarray\\n        b-value(s) (trace(s))\\n    bdelta: numpy.ndarray\\n        normalized tensor anisotropy(s)\\n    b_eta: numpy.ndarray\\n        tensor asymmetry(s)\\n\\n    Notes\\n    -----\\n    This function can be used to get b-tensor parameters directly from the\\n    GradientTable `btens` attribute.\\n\\n    Examples\\n    --------\\n    >>> lte = np.array([[1, 0, 0], [0, 0, 0], [0, 0, 0]])\\n    >>> bval, bdelta, b_eta = btens_to_params(lte)\\n    >>> print(\"bval={}; bdelta={}; b_eta={}\".format(bdelta, bval, b_eta))\\n    bval=[ 1.]; bdelta=[ 1.]; b_eta=[ 0.]\\n\\n    \"\"\"\\n    # Bad input checks\\n    value_error_msg = \"`btens` must be a 2D or 3D numpy array, respectively\" \\\\\\n                      \" with (3, 3) or (N, 3, 3) shape, where N corresponds\" \\\\\\n                      \" to the number of b-tensors\"\\n    if not isinstance(btens, np.ndarray):\\n        raise ValueError(value_error_msg)\\n\\n    nd = np.ndim(btens)\\n    if nd == 2:\\n        btens_shape = btens.shape\\n    elif nd == 3:\\n        btens_shape = btens.shape[1:]\\n    else:\\n        raise ValueError(value_error_msg)\\n\\n    if not btens_shape == (3, 3):\\n        raise ValueError(value_error_msg)\\n\\n    # Reshape so that loop below works when only one input b-tensor is provided\\n    if nd == 2:\\n        btens = btens.reshape((1, 3, 3))\\n\\n    # Pre-allocate\\n    n_btens = btens.shape[0]\\n    bval = np.empty(n_btens)\\n    bdelta = np.empty(n_btens)\\n    b_eta = np.empty(n_btens)\\n\\n    # Loop over b-tensor(s)\\n    for i in range(btens.shape[0]):\\n        i_btens = btens[i, :, :]\\n        i_bval, i_bdelta, i_b_eta = _btens_to_params_2d(i_btens, ztol)\\n        bval[i] = i_bval\\n        bdelta[i] = i_bdelta\\n        b_eta[i] = i_b_eta', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='# Pre-allocate\\n    n_btens = btens.shape[0]\\n    bval = np.empty(n_btens)\\n    bdelta = np.empty(n_btens)\\n    b_eta = np.empty(n_btens)\\n\\n    # Loop over b-tensor(s)\\n    for i in range(btens.shape[0]):\\n        i_btens = btens[i, :, :]\\n        i_bval, i_bdelta, i_b_eta = _btens_to_params_2d(i_btens, ztol)\\n        bval[i] = i_bval\\n        bdelta[i] = i_bdelta\\n        b_eta[i] = i_b_eta\\n\\n    return bval, bdelta, b_eta\\n\\n\\ndef params_to_btens(bval, bdelta, b_eta):\\n    \"\"\"Compute b-tensor from trace, anisotropy and asymmetry parameters.\\n\\n    Parameters\\n    ----------\\n    bval: int or float\\n        b-value (>= 0)\\n    bdelta: int or float\\n        normalized tensor anisotropy (>= -0.5 and <= 1)\\n    b_eta: int or float\\n        tensor asymmetry (>= 0 and <= 1)\\n\\n    Returns\\n    -------\\n    (3, 3) numpy.ndarray\\n        output b-tensor\\n\\n    Notes\\n    -----\\n    Implements eq. 7.11. p. 231 in [1].\\n\\n    References\\n    ----------\\n    .. [1] D. Topgaard, NMR methods for studying microscopic diffusion\\n    anisotropy, in: R. Valiullin (Ed.), Diffusion NMR of Confined Systems: Fluid\\n    Transport in Porous Solids and Heterogeneous Materials, Royal Society of\\n    Chemistry, Cambridge, UK, 2016.\\n\\n    \"\"\"\\n\\n    # Check input times are OK\\n    expected_input_types = (float, int)\\n    input_types_all_ok = (isinstance(bval, expected_input_types) and\\n                          isinstance(bdelta, expected_input_types) and\\n                          isinstance(b_eta, expected_input_types))\\n\\n    if not input_types_all_ok:\\n        s = [x.__name__ for x in expected_input_types]\\n        it_msg = \"All input types should any of: {}\".format(s)\\n        raise ValueError(it_msg)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='\"\"\"\\n\\n    # Check input times are OK\\n    expected_input_types = (float, int)\\n    input_types_all_ok = (isinstance(bval, expected_input_types) and\\n                          isinstance(bdelta, expected_input_types) and\\n                          isinstance(b_eta, expected_input_types))\\n\\n    if not input_types_all_ok:\\n        s = [x.__name__ for x in expected_input_types]\\n        it_msg = \"All input types should any of: {}\".format(s)\\n        raise ValueError(it_msg)\\n\\n    # Check input values within expected ranges\\n    if bval < 0:\\n        raise ValueError(\"`bval` must be >= 0\")\\n\\n    if not -0.5 <= bdelta <= 1:\\n        raise ValueError(\"`delta` must be >= -0.5 and <= 1\")\\n\\n    if not 0 <= b_eta <= 1:\\n        raise ValueError(\"`b_eta` must be >= 0 and <= 1\")\\n\\n    m1 = np.array([[-1, 0, 0], [0, -1, 0], [0, 0, 2]])\\n    m2 = np.array([[-1, 0, 0], [0, 1, 0], [0, 0, 0]])\\n    btens = bval/3*(np.eye(3)+bdelta*(m1+b_eta*m2))\\n\\n    return btens\\n\\n\\ndef ornt_mapping(ornt1, ornt2):\\n    \"\"\"Calculate the mapping needing to get from orn1 to orn2.\"\"\"\\n\\n    mapping = np.empty((len(ornt1), 2), \\'int\\')\\n    mapping[:, 0] = -1\\n    A = ornt1[:, 0].argsort()\\n    B = ornt2[:, 0].argsort()\\n    mapping[B, 0] = A\\n    assert (mapping[:, 0] != -1).all()\\n    sign = ornt2[:, 1] * ornt1[mapping[:, 0], 1]\\n    mapping[:, 1] = sign\\n    return mapping\\n\\n\\ndef reorient_vectors(bvecs, current_ornt, new_ornt, axis=0):\\n    \"\"\"Change the orientation of gradients or other vectors.\\n\\n    Moves vectors, storted along axis, from current_ornt to new_ornt. For\\n    example the vector [x, y, z] in \"RAS\" will be [-x, -y, z] in \"LPS\".\\n\\n    R: Right\\n    A: Anterior\\n    S: Superior\\n    L: Left\\n    P: Posterior\\n    I: Inferior', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='def reorient_vectors(bvecs, current_ornt, new_ornt, axis=0):\\n    \"\"\"Change the orientation of gradients or other vectors.\\n\\n    Moves vectors, storted along axis, from current_ornt to new_ornt. For\\n    example the vector [x, y, z] in \"RAS\" will be [-x, -y, z] in \"LPS\".\\n\\n    R: Right\\n    A: Anterior\\n    S: Superior\\n    L: Left\\n    P: Posterior\\n    I: Inferior\\n\\n    \"\"\"\\n    if isinstance(current_ornt, str):\\n        current_ornt = orientation_from_string(current_ornt)\\n    if isinstance(new_ornt, str):\\n        new_ornt = orientation_from_string(new_ornt)\\n\\n    n = bvecs.shape[axis]\\n    if current_ornt.shape != (n, 2) or new_ornt.shape != (n, 2):\\n        raise ValueError(\"orientations do not match\")\\n\\n    bvecs = np.asarray(bvecs)\\n    mapping = ornt_mapping(current_ornt, new_ornt)\\n    output = bvecs.take(mapping[:, 0], axis)\\n    out_view = np.rollaxis(output, axis, output.ndim)\\n    out_view *= mapping[:, 1]\\n    return output\\n\\n\\ndef reorient_on_axis(bvecs, current_ornt, new_ornt, axis=0):\\n    if isinstance(current_ornt, str):\\n        current_ornt = orientation_from_string(current_ornt)\\n    if isinstance(new_ornt, str):\\n        new_ornt = orientation_from_string(new_ornt)\\n\\n    n = bvecs.shape[axis]\\n    if current_ornt.shape != (n, 2) or new_ornt.shape != (n, 2):\\n        raise ValueError(\"orientations do not match\")\\n\\n    mapping = ornt_mapping(current_ornt, new_ornt)\\n    order = [slice(None)] * bvecs.ndim\\n    order[axis] = mapping[:, 0]\\n    shape = [1] * bvecs.ndim\\n    shape[axis] = -1\\n    sign = mapping[:, 1]\\n    sign.shape = shape\\n    output = bvecs[order]\\n    output *= sign\\n    return output', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='n = bvecs.shape[axis]\\n    if current_ornt.shape != (n, 2) or new_ornt.shape != (n, 2):\\n        raise ValueError(\"orientations do not match\")\\n\\n    mapping = ornt_mapping(current_ornt, new_ornt)\\n    order = [slice(None)] * bvecs.ndim\\n    order[axis] = mapping[:, 0]\\n    shape = [1] * bvecs.ndim\\n    shape[axis] = -1\\n    sign = mapping[:, 1]\\n    sign.shape = shape\\n    output = bvecs[order]\\n    output *= sign\\n    return output\\n\\n\\ndef orientation_from_string(string_ornt):\\n    \"\"\"Return an array representation of an ornt string.\"\"\"\\n    orientation_dict = dict(r=(0, 1), l=(0, -1), a=(1, 1),\\n                            p=(1, -1), s=(2, 1), i=(2, -1))\\n    ornt = tuple(orientation_dict[ii] for ii in string_ornt.lower())\\n    ornt = np.array(ornt)\\n    if _check_ornt(ornt):\\n        msg = string_ornt + \" does not seem to be a valid orientation string\"\\n        raise ValueError(msg)\\n    return ornt\\n\\n\\ndef orientation_to_string(ornt):\\n    \"\"\"Return a string representation of a 3d ornt.\"\"\"\\n    if _check_ornt(ornt):\\n        msg = repr(ornt) + \" does not seem to be a valid orientation\"\\n        raise ValueError(msg)\\n    orientation_dict = {(0, 1): \\'r\\', (0, -1): \\'l\\', (1, 1): \\'a\\',\\n                        (1, -1): \\'p\\', (2, 1): \\'s\\', (2, -1): \\'i\\'}\\n    ornt_string = \\'\\'\\n    for ii in ornt:\\n        ornt_string += orientation_dict[(ii[0], ii[1])]\\n    return ornt_string\\n\\n\\ndef _check_ornt(ornt):\\n    uniq = np.unique(ornt[:, 0])\\n    if len(uniq) != len(ornt):\\n        print(len(uniq))\\n        return True\\n    uniq = np.unique(ornt[:, 1])\\n    if tuple(uniq) not in {(-1, 1), (-1,), (1,)}:\\n        print(tuple(uniq))\\n        return True', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\gradients.py.txt'}),\n",
       " Document(page_content='\"\"\" A simple graph class \"\"\"\\n\\nclass Graph:\\n    \"\"\" A simple graph class\\n\\n    \"\"\"\\n\\n    def __init__(self):\\n        \"\"\" A graph class with nodes and edges :-)\\n\\n        This class allows us to:\\n\\n        1. find the shortest path\\n        2. find all paths\\n        3. add/delete nodes and edges\\n        4. get parent & children nodes\\n\\n        Examples\\n        --------\\n        >>> from dipy.core.graph import Graph\\n        >>> g=Graph()\\n        >>> g.add_node(\\'a\\',5)\\n        >>> g.add_node(\\'b\\',6)\\n        >>> g.add_node(\\'c\\',10)\\n        >>> g.add_node(\\'d\\',11)\\n        >>> g.add_edge(\\'a\\',\\'b\\')\\n        >>> g.add_edge(\\'b\\',\\'c\\')\\n        >>> g.add_edge(\\'c\\',\\'d\\')\\n        >>> g.add_edge(\\'b\\',\\'d\\')\\n        >>> g.up_short(\\'d\\')\\n        [\\'d\\', \\'b\\', \\'a\\']\\n\\n        \"\"\"\\n\\n        self.node = {}\\n        self.pred = {}\\n        self.succ = {}\\n\\n    def add_node(self, n, attr=None):\\n        self.succ[n] = {}\\n        self.pred[n] = {}\\n        self.node[n] = attr\\n\\n    def add_edge(self, n, m, ws=True, wp=True):\\n        self.succ[n][m] = ws\\n        self.pred[m][n] = wp\\n\\n    def parents(self, n):\\n        return self.pred[n].keys()\\n\\n    def children(self, n):\\n        return self.succ[n].keys()\\n\\n    def up(self, n):\\n        return self.all_paths(self.pred, n)\\n\\n    def down(self, n):\\n        return self.all_paths(self.succ, n)\\n\\n    def up_short(self, n):\\n        return self.shortest_path(self.pred, n)\\n\\n    def down_short(self, n):\\n        return self.shortest_path(self.succ, n)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\graph.py.txt'}),\n",
       " Document(page_content=\"def parents(self, n):\\n        return self.pred[n].keys()\\n\\n    def children(self, n):\\n        return self.succ[n].keys()\\n\\n    def up(self, n):\\n        return self.all_paths(self.pred, n)\\n\\n    def down(self, n):\\n        return self.all_paths(self.succ, n)\\n\\n    def up_short(self, n):\\n        return self.shortest_path(self.pred, n)\\n\\n    def down_short(self, n):\\n        return self.shortest_path(self.succ, n)\\n\\n    def all_paths(self, graph, start, end=None, path=None):\\n        path = path or []\\n        path = path + [start]\\n        if start == end or graph[start] == {}:\\n            return [path]\\n        if start not in graph:\\n            return []\\n        paths = []\\n        for node in graph[start]:\\n            if node not in path:\\n                newpaths = self.all_paths(graph, node, end, path)\\n                for newpath in newpaths:\\n                    paths.append(newpath)\\n        return paths\\n\\n    def shortest_path(self, graph, start, end=None, path=None):\\n        path = path or []\\n        path = path + [start]\\n        if graph[start] == {} or start == end:\\n            return path\\n        if start not in graph:\\n            return []\\n        shortest = None\\n        for node in graph[start]:\\n            if node not in path:\\n                newpath = self.shortest_path(graph, node, end, path)\\n                if newpath:\\n                    if not shortest or len(newpath) < len(shortest):\\n                        shortest = newpath\\n        return shortest\\n\\n    def del_node_and_edges(self, n):\\n        try:\\n            del self.node[n]\\n        except KeyError:\\n            raise KeyError('node not in the graph')\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\graph.py.txt'}),\n",
       " Document(page_content=\"def del_node_and_edges(self, n):\\n        try:\\n            del self.node[n]\\n        except KeyError:\\n            raise KeyError('node not in the graph')\\n\\n        for s in self.succ[n]:\\n            del self.pred[s][n]\\n        del self.succ[n]\\n\\n        for p in self.pred[n]:\\n            del self.succ[p][n]\\n        del self.pred[n]\\n\\n    def del_node(self, n):\\n        try:\\n            del self.node[n]\\n        except KeyError:\\n            raise KeyError('node not in the graph')\\n\\n        for s in self.succ[n]:\\n            for p in self.pred[n]:\\n                self.succ[p][s] = self.succ[n][s]\\n                self.pred[s][p] = self.pred[s][n]\\n        for s in self.succ.keys():\\n            try:\\n                del self.succ[s][n]\\n            except KeyError:\\n                pass\\n        for p in self.pred.keys():\\n            try:\\n                del self.pred[p][n]\\n            except KeyError:\\n                pass\\n        del self.succ[n]\\n        del self.pred[n]\", metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\graph.py.txt'}),\n",
       " Document(page_content='import numpy as np\\n\\n\\ndef histeq(arr, num_bins=256):\\n    \"\"\" Performs an histogram equalization on ``arr``.\\n    This was taken from:\\n    http://www.janeriksolem.net/2009/06/histogram-equalization-with-python-and.html\\n\\n    Parameters\\n    ----------\\n    arr : ndarray\\n        Image on which to perform histogram equalization.\\n    num_bins : int\\n        Number of bins used to construct the histogram.\\n\\n    Returns\\n    -------\\n    result : ndarray\\n        Histogram equalized image.\\n    \"\"\"\\n    # get image histogram\\n    histo, bins = np.histogram(arr.flatten(), num_bins, density=True)\\n    cdf = histo.cumsum()\\n    cdf = 255 * cdf / cdf[-1]\\n\\n    # use linear interpolation of cdf to find new pixel values\\n    result = np.interp(arr.flatten(), bins[:-1], cdf)\\n\\n    return result.reshape(arr.shape)', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\histeq.py.txt'}),\n",
       " Document(page_content='# cython: boundscheck=False, wraparound=False, cdivision=True\\n\\ncimport cython\\ncimport numpy as cnp\\n\\nimport numpy as np\\nimport warnings\\n\\nfrom libc.math cimport floor\\n\\nfrom dipy.align.fused_types cimport floating, number\\n\\n\\ndef interp_rbf(data, sphere_origin, sphere_target,\\n               function=\\'multiquadric\\', epsilon=None, smooth=0.1,\\n               norm=\"angle\"):\\n    \"\"\"Interpolate data on the sphere, using radial basis functions.\\n\\n    Parameters\\n    ----------\\n    data : (N,) ndarray\\n        Function values on the unit sphere.\\n    sphere_origin : Sphere\\n        Positions of data values.\\n    sphere_target : Sphere\\n        M target positions for which to interpolate.\\n\\n    function : {\\'multiquadric\\', \\'inverse\\', \\'gaussian\\'}\\n        Radial basis function.\\n    epsilon : float\\n        Radial basis function spread parameter. Defaults to approximate average\\n        distance between nodes.\\n    a good start\\n    smooth : float\\n        values greater than zero increase the smoothness of the\\n        approximation with 0 as pure interpolation. Default: 0.1\\n    norm : str\\n        A string indicating the function that returns the\\n        \"distance\" between two points.\\n        \\'angle\\' - The angle between two vectors\\n        \\'euclidean_norm\\' - The Euclidean distance\\n\\n    Returns\\n    -------\\n    v : (M,) ndarray\\n        Interpolated values.\\n\\n    See Also\\n    --------\\n    scipy.interpolate.Rbf\\n\\n    \"\"\"\\n    from scipy.interpolate import Rbf\\n\\n    def angle(x1, x2):\\n        xx = np.arccos(np.clip((x1 * x2).sum(axis=0), -1, 1))\\n        return np.nan_to_num(xx)\\n\\n    def euclidean_norm(x1, x2):\\n        return np.sqrt(((x1 - x2)**2).sum(axis=0))', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\interpolation.pyx.txt'}),\n",
       " Document(page_content='Returns\\n    -------\\n    v : (M,) ndarray\\n        Interpolated values.\\n\\n    See Also\\n    --------\\n    scipy.interpolate.Rbf\\n\\n    \"\"\"\\n    from scipy.interpolate import Rbf\\n\\n    def angle(x1, x2):\\n        xx = np.arccos(np.clip((x1 * x2).sum(axis=0), -1, 1))\\n        return np.nan_to_num(xx)\\n\\n    def euclidean_norm(x1, x2):\\n        return np.sqrt(((x1 - x2)**2).sum(axis=0))\\n\\n    if norm == \"angle\":\\n        norm = angle\\n    elif norm == \"euclidean_norm\":\\n        w_s = \"The Euclidean norm used for interpolation is inaccurate \"\\n        w_s += \"and will be deprecated in future versions. Please consider \"\\n        w_s += \"using the \\'angle\\' norm instead\"\\n        warnings.warn(w_s, PendingDeprecationWarning)\\n        norm = euclidean_norm\\n\\n    # Workaround for bug in older versions of SciPy that don\\'t allow\\n    # specification of epsilon None:\\n    if epsilon is not None:\\n        kwargs = {\\'function\\': function,\\n                  \\'epsilon\\': epsilon,\\n                  \\'smooth\\': smooth,\\n                  \\'norm\\': norm}\\n    else:\\n        kwargs = {\\'function\\': function,\\n                  \\'smooth\\': smooth,\\n                  \\'norm\\': norm}\\n\\n    rbfi = Rbf(sphere_origin.x, sphere_origin.y, sphere_origin.z, data,\\n               **kwargs)\\n    return rbfi(sphere_target.x, sphere_target.y, sphere_target.z)\\n\\n\\n@cython.cdivision(True)\\ncdef cnp.npy_intp offset(cnp.npy_intp *indices,\\n                         cnp.npy_intp *strides,\\n                         int lenind,\\n                         int typesize) noexcept nogil:\\n    \"\"\" Access any element of any ndimensional numpy array using cython.', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\interpolation.pyx.txt'}),\n",
       " Document(page_content='rbfi = Rbf(sphere_origin.x, sphere_origin.y, sphere_origin.z, data,\\n               **kwargs)\\n    return rbfi(sphere_target.x, sphere_target.y, sphere_target.z)\\n\\n\\n@cython.cdivision(True)\\ncdef cnp.npy_intp offset(cnp.npy_intp *indices,\\n                         cnp.npy_intp *strides,\\n                         int lenind,\\n                         int typesize) noexcept nogil:\\n    \"\"\" Access any element of any ndimensional numpy array using cython.\\n\\n    Parameters\\n    ----------\\n    indices : cnp.npy_intp * (int64 *)\\n        Indices of the array for which we want to find the offset.\\n    strides : cnp.npy_intp * strides\\n    lenind : int, len(indices)\\n    typesize : int\\n        Number of bytes for data type e.g. if 8 for double, 4 for int32\\n\\n    Returns\\n    -------\\n    offset : integer\\n        Element position in array\\n    \"\"\"\\n    cdef cnp.npy_intp i\\n    cdef cnp.npy_intp summ = 0\\n    for i from 0 <= i < lenind:\\n        summ += strides[i] * indices[i]\\n    summ /= <cnp.npy_intp>typesize\\n    return summ\\n\\n\\ncdef void splitoffset(float *offset, cnp.npy_intp *index, cnp.npy_intp shape) noexcept nogil:\\n    \"\"\"Splits a global offset into an integer index and a relative offset\"\"\"\\n    offset[0] -= .5\\n    if offset[0] <= 0:\\n        index[0] = 0\\n        offset[0] = 0.\\n    elif offset[0] >= (shape - 1):\\n        index[0] = shape - 2\\n        offset[0] = 1.\\n    else:\\n        index[0] = <cnp.npy_intp> offset[0]\\n        offset[0] = offset[0] - index[0]\\n\\n\\n@cython.profile(False)\\ncdef inline float wght(int i, float r) noexcept nogil:\\n    if i:\\n        return r\\n    else:\\n        return 1.-r', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\interpolation.pyx.txt'}),\n",
       " Document(page_content='@cython.profile(False)\\ncdef inline float wght(int i, float r) noexcept nogil:\\n    if i:\\n        return r\\n    else:\\n        return 1.-r\\n\\n\\n@cython.boundscheck(False)\\n@cython.wraparound(False)\\n@cython.cdivision(True)\\ndef trilinear_interp(cnp.float32_t[:, :, :, :] data, cython.floating[:] index,\\n                     cython.floating[::1] voxel_size):\\n    \"\"\"Interpolates vector from 4D `data` at 3D point given by `index`\\n\\n    Interpolates a vector of length T from a 4D volume of shape (I, J, K, T),\\n    given point (x, y, z) where (x, y, z) are the coordinates of the point in\\n    real units (not yet adjusted for voxel size).\\n    \"\"\"\\n    cdef:\\n        float x = index[0] / voxel_size[0]\\n        float y = index[1] / voxel_size[1]\\n        float z = index[2] / voxel_size[2]\\n        float weight\\n        cnp.npy_intp x_ind, y_ind, z_ind, ii, jj, kk, LL\\n        cnp.npy_intp last_d = data.shape[3]\\n        bint bounds_check\\n        cnp.ndarray[cnp.float32_t, ndim=1, mode=\\'c\\'] result\\n\\n    bounds_check = (x < 0 or y < 0 or z < 0 or\\n                    x > data.shape[0] or\\n                    y > data.shape[1] or\\n                    z > data.shape[2])\\n    if bounds_check:\\n        raise IndexError\\n\\n    splitoffset(&x, &x_ind, data.shape[0])\\n    splitoffset(&y, &y_ind, data.shape[1])\\n    splitoffset(&z, &z_ind, data.shape[2])\\n\\n    result = np.zeros(last_d, dtype=\\'float32\\')\\n    for ii from 0 <= ii <= 1:\\n        for jj from 0 <= jj <= 1:\\n            for kk from 0 <= kk <= 1:\\n                weight = wght(ii, x)*wght(jj, y)*wght(kk, z)\\n                for LL from 0 <= LL < last_d:\\n                    result[LL] += data[x_ind+ii,y_ind+jj,z_ind+kk,LL]*weight\\n    return result', metadata={'source': 'C:\\\\Users\\\\Aayush\\\\OneDrive\\\\Desktop\\\\projects\\\\dipy_code_assistant\\\\converted_codebase\\\\dipy\\\\core\\\\interpolation.pyx.txt'}),\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171290\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for file in repo_files:\n",
    "    if max_len < len(file.page_content):\n",
    "        max_len = len(file.page_content)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
